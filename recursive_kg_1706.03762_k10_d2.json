{
  "paper_metadata": {
    "id": "1706.03762",
    "title": "Attention Is All You Need",
    "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
    "published_date": "2017-06-12",
    "pdf_url": "https://arxiv.org/pdf/1706.03762v7",
    "authors": [
      "Ashish Vaswani",
      "Noam Shazeer",
      "Niki Parmar",
      "Jakob Uszkoreit",
      "Llion Jones",
      "Aidan N. Gomez",
      "Lukasz Kaiser",
      "Illia Polosukhin"
    ]
  },
  "related_papers_count": {
    "references": 10,
    "citations": 10
  },
  "related_papers": [
    {
      "title": "Deep Residual Learning for Image Recognition",
      "arxiv_id": "1512.03385",
      "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.\n  The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "published_date": "2015-12-10",
      "pdf_url": "https://arxiv.org/pdf/1512.03385v1",
      "citation_count": 219322,
      "year": 2015
    },
    {
      "title": "Adam: A Method for Stochastic Optimization",
      "arxiv_id": "",
      "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
      "authors": [
        "Diederik P. Kingma",
        "Jimmy Ba"
      ],
      "published_date": "2014-12-22",
      "pdf_url": "https://arxiv.org/pdf/1412.6980v9",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Long Short-Term Memory",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Sepp Hochreiter",
        "J. Schmidhuber"
      ],
      "published_date": "1997",
      "pdf_url": "",
      "citation_count": 100213,
      "year": 1997
    },
    {
      "title": "Dropout: a simple way to prevent neural networks from overfitting",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Nitish Srivastava",
        "Geoffrey E. Hinton",
        "A. Krizhevsky",
        "I. Sutskever",
        "R. Salakhutdinov"
      ],
      "published_date": "2014",
      "pdf_url": "",
      "citation_count": 42312,
      "year": 2014
    },
    {
      "title": "Rethinking the Inception Architecture for Computer Vision",
      "arxiv_id": "1512.00567",
      "abstract": "Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the validation set.",
      "authors": [
        "Christian Szegedy",
        "Vincent Vanhoucke",
        "Sergey Ioffe",
        "Jonathon Shlens",
        "Zbigniew Wojna"
      ],
      "published_date": "2015-12-02",
      "pdf_url": "https://arxiv.org/pdf/1512.00567v3",
      "citation_count": 30057,
      "year": 2015
    },
    {
      "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
      "arxiv_id": "1409.0473",
      "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
      "authors": [
        "Dzmitry Bahdanau",
        "Kyunghyun Cho",
        "Yoshua Bengio"
      ],
      "published_date": "2014-09-01",
      "pdf_url": "https://arxiv.org/pdf/1409.0473v7",
      "citation_count": 28849,
      "year": 2014
    },
    {
      "title": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation",
      "arxiv_id": "1406.1078",
      "abstract": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
      "authors": [
        "Kyunghyun Cho",
        "Bart van Merrienboer",
        "Caglar Gulcehre",
        "Dzmitry Bahdanau",
        "Fethi Bougares",
        "Holger Schwenk",
        "Yoshua Bengio"
      ],
      "published_date": "2014-06-03",
      "pdf_url": "https://arxiv.org/pdf/1406.1078v3",
      "citation_count": 25480,
      "year": 2014
    },
    {
      "title": "Sequence to Sequence Learning with Neural Networks",
      "arxiv_id": "1409.3215",
      "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",
      "authors": [
        "Ilya Sutskever",
        "Oriol Vinyals",
        "Quoc V. Le"
      ],
      "published_date": "2014-09-10",
      "pdf_url": "https://arxiv.org/pdf/1409.3215v3",
      "citation_count": 21667,
      "year": 2014
    },
    {
      "title": "Xception: Deep Learning with Depthwise Separable Convolutions",
      "arxiv_id": "",
      "abstract": "We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.",
      "authors": [
        "François Chollet"
      ],
      "published_date": "2016-10-07",
      "pdf_url": "https://arxiv.org/pdf/1610.02357v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling",
      "arxiv_id": "1412.3555",
      "abstract": "In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.",
      "authors": [
        "Junyoung Chung",
        "Caglar Gulcehre",
        "KyungHyun Cho",
        "Yoshua Bengio"
      ],
      "published_date": "2014-12-11",
      "pdf_url": "https://arxiv.org/pdf/1412.3555v1",
      "citation_count": 14176,
      "year": 2014
    },
    {
      "title": "A comprehensive review of recommender systems: Transitioning from theory to practice",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Shaina Raza",
        "Mizanur Rahman",
        "Safiullah Kamawal",
        "Armin Toroghi",
        "Ananya Raval",
        "F. Navah",
        "Amirmohammad Kazemeini"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 7,
      "year": 2026
    },
    {
      "title": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
      "arxiv_id": "",
      "abstract": "Autonomous driving relies on robust models trained on high-quality, large-scale multi-view driving videos. While world models offer a cost-effective solution for generating realistic driving videos, they struggle to maintain instance-level temporal consistency and spatial geometric fidelity. To address these challenges, we propose InstaDrive, a novel framework that enhances driving video realism through two key advancements: (1) Instance Flow Guider, which extracts and propagates instance features across frames to enforce temporal consistency, preserving instance identity over time. (2) Spatial Geometric Aligner, which improves spatial reasoning, ensures precise instance positioning, and explicitly models occlusion hierarchies. By incorporating these instance-aware mechanisms, InstaDrive achieves state-of-the-art video generation quality and enhances downstream autonomous driving tasks on the nuScenes dataset. Additionally, we utilize CARLA's autopilot to procedurally and stochastically simulate rare but safety-critical driving scenarios across diverse maps and regions, enabling rigorous safety evaluation for autonomous systems. Our project page is https://shanpoyang654.github.io/InstaDrive/page.html.",
      "authors": [
        "Zhuoran Yang",
        "Xi Guo",
        "Chenjing Ding",
        "Chiyu Wang",
        "Wei Wu",
        "Yanyong Zhang"
      ],
      "published_date": "2026-02-03",
      "pdf_url": "https://arxiv.org/pdf/2602.03242v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Zisheng Wang",
        "Junjie Chen",
        "Chisen Wang",
        "Cong Peng",
        "Jianping Xuan",
        "Tielin Shi",
        "Ming J. Zuo"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 4,
      "year": 2026
    },
    {
      "title": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Jinghuan Zhang",
        "Wang Chen",
        "Jian Zhang"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 3,
      "year": 2026
    },
    {
      "title": "DiffusionEngine: Diffusion model is scalable data engine for object detection",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Manlin Zhang",
        "Jie Wu",
        "Yuxi Ren",
        "Jiahong Yang",
        "Ming Li",
        "Andy J. Ma"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 2,
      "year": 2026
    },
    {
      "title": "Enhanced Transformer-BiLSTM Deep Learning Framework for Day-Ahead Energy Price Forecasting",
      "arxiv_id": null,
      "abstract": "Accurate forecasting of energy prices is critical to effectively mitigate operational risks and make strategic bidding decisions in day-ahead (DA) electricity markets. However, it is highly challenging due to volatile characteristics, seasonality, rapid spikes, and other nonlinear factors of price signals. In the given context, deep learning (DL) has gained attention in recent years due to its high potential in nonlinear approximation, but each model has its strengths and limitations. Therefore, this paper proposes a hybrid DL approach for time-series DA energy price forecasting based on the Transformer and Bidirectional Long Short-Term Memory (BiLSTM) model that facilitates strategically combining various components to extract patterns and further improve the sequence processing task. The proposed model uses a transformer architecture to capture patterns, temporal dynamics, and BiLSTM networks to forecast energy price fluctuations. The proposed approach is validated with simulations based on price data from the New York Independent System Operator, and the results show that the proposed approach consistently outperforms state-of-the-art DL models, achieving the lowest MAE (2.7818 $/MWh), RMSE (6.4937 $/MWh), sMAPE (6.6060%), MAPE (6.3741%) and highest R$^{2}$ (0.9393). The effectiveness of the proposed approach was justified through various case studies from different perspectives.",
      "authors": [
        "Abdullah Al Ahad Khan",
        "Md Habib Ullah",
        "Ruchira Tabassum",
        "Md Faisal Kabir"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 2,
      "year": 2026
    },
    {
      "title": "Conditional Generative Adversarial Network-based framework for multi-feature uncertainty modeling in energy systems",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Mojtaba Moradi‐Sepahvand"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 2,
      "year": 2026
    },
    {
      "title": "CityVLM: Towards sustainable urban development via multi-view coordinated vision–language model",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Junjue Wang",
        "Weihao Xuan",
        "Heli Qi",
        "Zihang Chen",
        "Hongruixuan Chen",
        "Zhuo Zheng",
        "Junshi Xia",
        "Yanfei Zhong",
        "Naoto Yokoya"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 2,
      "year": 2026
    },
    {
      "title": "FullPerception: Network-Level Collaborative Perception for Eliminating Vehicular Blind Spots",
      "arxiv_id": null,
      "abstract": "Collaborative perception can significantly enhance the perceptual capabilities of autonomous vehicles by sharing sensing information through vehicular communications. However, large-scale sharing of sensing information often results in unsustainable network loads, making it challenging to maximize perception performance with limited communication resources in complex environments. To address this challenge, we propose FullPerception, an innovative cooperative perception framework that jointly orchestrates sensing information sharing and communication resource allocation at the network level. FullPerception advocates for the sharing of semantic information (neural network features) within critical areas, i.e., blind spots. With limited communication resources, FullPerception strategically eliminates these blind spots to maximize the accumulated perception performance. We formulate this strategy as a weighted optimization problem and prove its NP-hardness. We propose a simple yet effective algorithm, Proactive Conflict-free Scheduling (PCS), which guarantees a good performance ratio by considering broader contexts. PCS is meticulously combined with recursive structure, accounting for both the overall and future contexts to determine link scheduling and resource allocation. We demonstrate that FullPerception improves perception accuracy by 20% relative to single-vehicle systems and by 10% compared to existing scheduling methods through large-scale comprehensive joint simulation experiments.",
      "authors": [
        "Lin Liang",
        "Guiyang Luo",
        "Yijing Lin",
        "Lei Deng",
        "Nan Cheng",
        "Quan Yuan",
        "Jinglin Li",
        "Dusit Niyato"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 2,
      "year": 2026
    },
    {
      "title": "A survey of features used for representing black-box single-objective continuous optimization",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Gjorgjina Cenikj",
        "Ana Nikolikj",
        "G. Petelin",
        "Niki van Stein",
        "Carola Doerr",
        "T. Eftimov"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 2,
      "year": 2026
    },
    {
      "title": "ImageNet classification with deep convolutional neural networks",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "A. Krizhevsky",
        "I. Sutskever",
        "Geoffrey E. Hinton"
      ],
      "published_date": "2012",
      "pdf_url": "",
      "citation_count": 126487,
      "year": 2012
    },
    {
      "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "arxiv_id": "",
      "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.",
      "authors": [
        "Karen Simonyan",
        "Andrew Zisserman"
      ],
      "published_date": "2014-09-04",
      "pdf_url": "https://arxiv.org/pdf/1409.1556v6",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Et al",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "P. Cochat",
        "L. Vaucoret",
        "J. Sarles"
      ],
      "published_date": "2008",
      "pdf_url": "",
      "citation_count": 74031,
      "year": 2008
    },
    {
      "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
      "arxiv_id": "1506.01497",
      "abstract": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.",
      "authors": [
        "Shaoqing Ren",
        "Kaiming He",
        "Ross Girshick",
        "Jian Sun"
      ],
      "published_date": "2015-06-04",
      "pdf_url": "https://arxiv.org/pdf/1506.01497v3",
      "citation_count": 70038,
      "year": 2015
    },
    {
      "title": "Microsoft COCO: Common Objects in Context",
      "arxiv_id": "",
      "abstract": "We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.",
      "authors": [
        "Tsung-Yi Lin",
        "Michael Maire",
        "Serge Belongie",
        "Lubomir Bourdev",
        "Ross Girshick",
        "James Hays",
        "Pietro Perona",
        "Deva Ramanan",
        "C. Lawrence Zitnick",
        "Piotr Dollár"
      ],
      "published_date": "2014-05-01",
      "pdf_url": "https://arxiv.org/pdf/1405.0312v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Going deeper with convolutions",
      "arxiv_id": "1409.4842",
      "abstract": "We propose a deep convolutional neural network architecture codenamed \"Inception\", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.",
      "authors": [
        "Christian Szegedy",
        "Wei Liu",
        "Yangqing Jia",
        "Pierre Sermanet",
        "Scott Reed",
        "Dragomir Anguelov",
        "Dumitru Erhan",
        "Vincent Vanhoucke",
        "Andrew Rabinovich"
      ],
      "published_date": "2014-09-17",
      "pdf_url": "https://arxiv.org/pdf/1409.4842v1",
      "citation_count": 46399,
      "year": 2014
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "arxiv_id": "",
      "abstract": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.",
      "authors": [
        "Sergey Ioffe",
        "Christian Szegedy"
      ],
      "published_date": "2015-02-11",
      "pdf_url": "https://arxiv.org/pdf/1502.03167v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "ImageNet Large Scale Visual Recognition Challenge",
      "arxiv_id": "",
      "abstract": "The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions.\n  This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.",
      "authors": [
        "Olga Russakovsky",
        "Jia Deng",
        "Hao Su",
        "Jonathan Krause",
        "Sanjeev Satheesh",
        "Sean Ma",
        "Zhiheng Huang",
        "Andrej Karpathy",
        "Aditya Khosla",
        "Michael Bernstein",
        "Alexander C. Berg",
        "Li Fei-Fei"
      ],
      "published_date": "2014-09-01",
      "pdf_url": "https://arxiv.org/pdf/1409.0575v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Fully convolutional networks for semantic segmentation",
      "arxiv_id": "1411.4038",
      "abstract": "Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build \"fully convolutional\" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.",
      "authors": [
        "Jonathan Long",
        "Evan Shelhamer",
        "Trevor Darrell"
      ],
      "published_date": "2014-11-14",
      "pdf_url": "https://arxiv.org/pdf/1411.4038v2",
      "citation_count": 40954,
      "year": 2014
    },
    {
      "title": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Yuqi Cheng",
        "Yunkang Cao",
        "Haiming Yao",
        "Wei Luo",
        "Cheng Jiang",
        "Hui Zhang",
        "Weiming Shen"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 6,
      "year": 2026
    },
    {
      "title": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning",
      "arxiv_id": null,
      "abstract": "Federated learning (FL) has emerged as a promising solution to enable distributed learning without sharing sensitive data. However, FL is vulnerable to data poisoning attacks, where malicious clients inject malicious data during training to compromise the global model. Existing FL defenses suffer from the assumptions of independent and identically distributed (IID) model updates, asymptotic optimal error rate bounds, and strong convexity in the optimization problem. Hence, we propose a novel framework called Federated Learning Optimal Transport (FLOT) that leverages the Wasserstein barycentric technique to obtain a global model from a set of locally trained non-IID models on client devices. In addition, we introduce a loss function-based rejection (LFR) mechanism to suppress malicious updates and a dynamic weighting scheme to optimize the Wasserstein barycentric aggregation function. We provide the theoretical proof of the Byzantine resilience and convergence of FLOT to highlight its efficacy. We evaluate FLOT on four benchmark datasets: GTSRB, KBTS, CIFAR10, and EMNIST. The experimental results underscore the practical significance of FLOT as an effective defense mechanism against data poisoning attacks in FL while maintaining high accuracy and scalability. Also, we observe that FLOT serves as a robust client selection technique under no attack, which demonstrates its effectiveness.",
      "authors": [
        "Naveen Kumar Srinivasa",
        "Ajeet Rao Chalamala",
        "Kumar Singh",
        "Ieee Krishna Mohan Senior Member",
        "K. Naveen",
        "Srinivasa Rao",
        "Ajeet Kumar Singh"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 5,
      "year": 2026
    },
    {
      "title": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID",
      "arxiv_id": null,
      "abstract": "Person re-identification (Re-ID) can recognize users based on their clothing, body shape, and other information without the need for clear facial images, and is widely applied in the field of intelligent security. Traditional Re-ID systems mainly rely on high-definition RGB cameras, but the deployment of large-scale high-definition RGB cameras indoors has caused serious privacy and ethical concerns. Recently, wireless-based Re-ID systems (Wi-Fi, RFID, millimeter-wave radar, etc.) have shown promising prospects, but the limited sensing resolution hinders their practical deployment. In this paper, we propose WarmGait, a Re-ID system based on thermal array sensors, which can achieve high-precision Re-ID at low cost and minimize the invasion of user privacy. However, using thermal arrays for Re-ID still faces two major challenges. The first is the low and unclear texture resolution of images caused by low-cost infrared devices. The second is that existing gait recognition methods require maintaining the sequential constraint of gait images, which reduces the flexibility of gait recognition or Re-ID. To address these two challenges, we first designed an edge module inspired by Taylor Finite Difference (TFD) to aggregate image edge information to help improve the resolution of infrared devices. Then, we considered gait as a collection of gait profiles and extracted features from the frame level and collection level for recognition, breaking through the limitations of the number and order of input images. After extensive experimental evaluation, our model can achieve an average recognition accuracy of 87.3% in various scenarios, demonstrating the potential of WarmGait in Re-ID.",
      "authors": [
        "Hongbo Jiang",
        "Lei Ye",
        "Jingyang Hu",
        "Xiaotian Chen",
        "Siyu Chen",
        "Wei Zhang",
        "Kehua Yang"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 2,
      "year": 2026
    },
    {
      "title": "Noise Perception Self-Supervised Learning for Unsupervised Person Re-Identification",
      "arxiv_id": null,
      "abstract": "Most existing unsupervised person re-identification (ReID) methods utilize a transfer learning paradigm that requires independent source annotations. Although recent Unsupervised Domain Adaptation techniques can achieve promising results, they still suffer from the issues of source domain variance and privacy due to the access of raw source data. In this paper, we propose a novel Noise Perception Self-Supervised Learning (NPSSL) paradigm based on the idea that visual tracking would provide useful spatio-temporal localized constraints for improving ReID model learning. Apart from using visual similarity, we fully exploit spatial-temporal motion consistency to assist the person tracklet formulation, complement with visual cues to enhance re-association in crowded scenes. To further alleviate the noise raised by multi-person tracking, we introduce a Noise Perception Self-Paced Learning method to learn from the most confident examples progressively. Specifically, a cluster-level filter and a sample-level filter are devised to jointly exploit the pseudo label during the training process. Extensive experiments on Duke dataset demonstrate the superiority of the NPSSL model over a wide range of unsupervised learning methods and the competitiveness of this paradigm with unsupervised transfer learning methods.",
      "authors": [
        "Jingya Wang",
        "Jianfeng Wen",
        "Weiping Ding",
        "Chunlin Yu",
        "Xiatian Zhu",
        "Zhiyong Wang"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 2,
      "year": 2026
    },
    {
      "title": "Towards Reliable Tracking: An Uncertainty-Aware Siamese Tracker",
      "arxiv_id": null,
      "abstract": "The recent years have witnessed the great advancement in accuracy made by Siamese trackers, while the performance is still limited by the insufficiency in robustness. Most of existing Siamese trackers regard their model output as deterministic results without explicitly estimating or optimizing the confidence level or reliability. As such, unreliable tracking results may be still utilized in model learning, and thus degrade the tracking model. In this paper, we introduce the notion of reliability into the tracking model, and propose a new tracking framework called Uncertainty-Aware Siamese Network to address this problem. Specifically, within the framework, we introduce the uncertainty awareness into the tracking model by designing a new network structure, which explicitly characterizes the confidence level about the tracker outputs in terms of regression accuracy and classification discriminability. In addition, with the uncertainty estimation module, we propose a new collaborative optimization model which aims to optimize tracking models under the constraint of reliability to make the tracking results as reliable as possible. Furthermore, based on the uncertainty estimation results, an online model updating scheme is developed to ensure both the adaptability and reliability of the model in dealing with appearance variations. Experiments performed on eight standard visual tracking benchmarks, including VOT2018, VOT2019, OTB100, NFS, UAV123, LaSOT, TrackingNet and Got-10 k, show that our uncertainty-aware Siamese tracker achieves state-of-the-art performance.",
      "authors": [
        "Jie Ma",
        "Xiangyuan Lan",
        "Qihua Liang",
        "Guorong Li",
        "Zhiyi Mo",
        "Bineng Zhong"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 2,
      "year": 2026
    },
    {
      "title": "Bridging the gap, not forcing the tie: dual-space alignment and fusion framework for toxic memes detection",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Yulin Lei",
        "Jin Yang",
        "Huijia Liang",
        "Tianrui Li"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 1,
      "year": 2026
    },
    {
      "title": "Perturb and restore: Efficient category revocation in federated unlearning",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Ning Pang",
        "Zou Li",
        "Pengcheng Wan",
        "Hongchao Wu",
        "Yuchen Bing",
        "Xiang Zhao"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 1,
      "year": 2026
    },
    {
      "title": "DCM-Net: A novel dual-branch CNN-Mamba cross-layer feature fusion network for medical image segmentation",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Yanling Liu",
        "Hongmin Deng",
        "Jinghao Fu"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 1,
      "year": 2026
    },
    {
      "title": "AdveDiffNet: adversarial diffusion network for unbalanced melanoma diagnosis",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Yu Fu",
        "Chao Liu",
        "Shaoqiang Wang",
        "Hui Xia"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 1,
      "year": 2026
    },
    {
      "title": "Auto-Encoding Variational Bayes",
      "arxiv_id": "",
      "abstract": "How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.",
      "authors": [
        "Diederik P Kingma",
        "Max Welling"
      ],
      "published_date": "2013-12-20",
      "pdf_url": "https://arxiv.org/pdf/1312.6114v11",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Supporting Online Material for Reducing the Dimensionality of Data with Neural Networks",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Geoffrey E. Hinton",
        "R. Salakhutdinov"
      ],
      "published_date": "2006",
      "pdf_url": "",
      "citation_count": 11470,
      "year": 2006
    },
    {
      "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "John C. Duchi",
        "Elad Hazan",
        "Y. Singer"
      ],
      "published_date": "2011",
      "pdf_url": "",
      "citation_count": 11099,
      "year": 2011
    },
    {
      "title": "Speech recognition with deep recurrent neural networks",
      "arxiv_id": "1303.5778",
      "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \\emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.",
      "authors": [
        "Alex Graves",
        "Abdel-rahman Mohamed",
        "Geoffrey Hinton"
      ],
      "published_date": "2013-03-22",
      "pdf_url": "https://arxiv.org/pdf/1303.5778v1",
      "citation_count": 8846,
      "year": 2013
    },
    {
      "title": "Improving neural networks by preventing co-adaptation of feature detectors",
      "arxiv_id": "",
      "abstract": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.",
      "authors": [
        "Geoffrey E. Hinton",
        "Nitish Srivastava",
        "Alex Krizhevsky",
        "Ilya Sutskever",
        "Ruslan R. Salakhutdinov"
      ],
      "published_date": "2012-07-03",
      "pdf_url": "https://arxiv.org/pdf/1207.0580v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "ADADELTA: An Adaptive Learning Rate Method",
      "arxiv_id": "",
      "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.",
      "authors": [
        "Matthew D. Zeiler"
      ],
      "published_date": "2012-12-22",
      "pdf_url": "https://arxiv.org/pdf/1212.5701v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Learning Word Vectors for Sentiment Analysis",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Andrew L. Maas",
        "Raymond E. Daly",
        "Peter T. Pham",
        "Dan Huang",
        "A. Ng",
        "Christopher Potts"
      ],
      "published_date": "2011",
      "pdf_url": "",
      "citation_count": 5799,
      "year": 2011
    },
    {
      "title": "On the importance of initialization and momentum in deep learning",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "I. Sutskever",
        "James Martens",
        "George E. Dahl",
        "Geoffrey E. Hinton"
      ],
      "published_date": "2013",
      "pdf_url": "",
      "citation_count": 5132,
      "year": 2013
    },
    {
      "title": "Generating Sequences With Recurrent Neural Networks",
      "arxiv_id": "",
      "abstract": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.",
      "authors": [
        "Alex Graves"
      ],
      "published_date": "2013-08-04",
      "pdf_url": "https://arxiv.org/pdf/1308.0850v5",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Weakly Supervised Image Dehazing via Physics-Based Decomposition",
      "arxiv_id": null,
      "abstract": "Recent weakly supervised image dehazing (WSID) works have succeeded to improve models’ generalization ability to real scene dehazing by using generative adversarial network (GAN) for unpaired image training. However, it is still difficult for current WSID methods to train one effective dehazing model for various scenes since 1) they always result in residual haze due to insufficient generalization to the feature distribution of real scenes, and 2) they are prone to cause distortions like color shifts, artifacts or halos etc, owing to embedding manual prior or threshold hypothesis for image reconstruction. To solve above problems, in this paper, we propose a novel WSID model via physics-based decomposition (PBD), which estimates atmospheric light, scattering coefficient and scene depth of real haze input to effectively capture the illumination information and haze distribution to recover a preliminary dehazed image by minimizing reconstruction loss. With this constraint, we subtly design a discrete wavelet discriminator (DWD) to effectively improve the generalization to real scene from both spatial and frequency aspect under the supervision of unpaired real clear image. Our PBD is a purely data-driven model freeing from any manual setting or partially correct prior, thus simultaneously ensuring the realness and visibility of dehazed images. Experiments on seven benchmarks verified the strong generalization ability of our PBD, which achieves SOTA dehazing performance with realistic details. Code will be published at https://github.com/NianWang-HJJGCDX/PBD",
      "authors": [
        "Nian Wang",
        "Zhigao Cui",
        "Yanzhao Su",
        "Yunwei Lan",
        "Yuanliang Xue",
        "Cong Zhang",
        "Aihua Li"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 10,
      "year": 2026
    },
    {
      "title": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer",
      "arxiv_id": null,
      "abstract": "As Adam optimizer’s learning rate decay hyperparameter has recently been deprecated, this journal article focuses not only on providing an alternate optimizer but also on comparing the performance of the said optimizer, AdamW, with the Adam optimizer using a face mask detection model. This study experiments with different weight decay values and finds that a weight decay of 0.00009 with the AdamW optimizer consistently achieves a 98% accuracy rate. Aside from that, this study also discusses the differences between Adam with L2-regularization and AdamW on how the weight decay is decoupled from the Adam optimizer’s gradient-based update that impacts the performance of AdamW. Overall, the study provides insights to those new to AdamW and looking for a starting point in optimizing deep learning models.",
      "authors": [
        "Leong Kah Meng",
        "Ho Hooi Yi",
        "Ng Bo Wei",
        "Lim Jia Xin",
        "Zailan Arabee Abdul Salam"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 8,
      "year": 2026
    },
    {
      "title": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
      "arxiv_id": "",
      "abstract": "While Mixture-of-Experts (MoE) scales capacity via conditional computation, Transformers lack a native primitive for knowledge lookup, forcing them to inefficiently simulate retrieval through computation. To address this, we introduce conditional memory as a complementary sparsity axis, instantiated via Engram, a module that modernizes classic $N$-gram embedding for O(1) lookup. By formulating the Sparsity Allocation problem, we uncover a U-shaped scaling law that optimizes the trade-off between neural computation (MoE) and static memory (Engram). Guided by this law, we scale Engram to 27B parameters, achieving superior performance over a strictly iso-parameter and iso-FLOPs MoE baseline. Most notably, while the memory module is expected to aid knowledge retrieval (e.g., MMLU +3.4; CMMLU +4.0), we observe even larger gains in general reasoning (e.g., BBH +5.0; ARC-Challenge +3.7) and code/math domains~(HumanEval +3.0; MATH +2.4). Mechanistic analyses reveal that Engram relieves the backbone's early layers from static reconstruction, effectively deepening the network for complex reasoning. Furthermore, by delegating local dependencies to lookups, it frees up attention capacity for global context, substantially boosting long-context retrieval (e.g., Multi-Query NIAH: 84.2 to 97.0). Finally, Engram establishes infrastructure-aware efficiency: its deterministic addressing enables runtime prefetching from host memory, incurring negligible overhead. We envision conditional memory as an indispensable modeling primitive for next-generation sparse models.",
      "authors": [
        "Xin Cheng",
        "Wangding Zeng",
        "Damai Dai",
        "Qinyu Chen",
        "Bingxuan Wang",
        "Zhenda Xie",
        "Kezhao Huang",
        "Xingkai Yu",
        "Zhewen Hao",
        "Yukun Li",
        "Han Zhang",
        "Huishuai Zhang",
        "Dongyan Zhao",
        "Wenfeng Liang"
      ],
      "published_date": "2026-01-12",
      "pdf_url": "https://arxiv.org/pdf/2601.07372v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction",
      "arxiv_id": null,
      "abstract": "Abstract. The past few years have witnessed the rise of neural networks (NNs) applications for hydrological time series modeling. By virtue of their capabilities, NN models can achieve unprecedented levels of performance when learning how to solve increasingly complex rainfall-runoff processes via data, making them pivotal for the development of computational hydrologic tasks such as flood predictions. The NN models should, to be considered practical, provide a probabilistic understanding of the model mechanisms and predictions and hints on what could perturb the model. In this paper, we developed two NN models, i.e., Neural Hierarchical Interpolation for Time Series Forecasting (N-HiTS) and Network-Based Expansion Analysis for Interpretable Time Series Forecasting (N-BEATS) with a probabilistic multi-quantile objective and benchmarked them with long short-term memory (LSTM) for flood prediction across two headwater streams in Georgia and North Carolina, USA. To generate a probabilistic prediction, a Multi-Quantile Loss was used to assess the 95th percentile prediction uncertainty (95 PPU) of multiple flooding events. Extensive experiments demonstrated the advantages of hierarchical interpolation and interpretable architecture, where both N-HiTS and N-BEATS provided an average accuracy improvement of ∼ 5 % over the LSTM benchmarking model. On a variety of flooding events, both N-HiTS and N-BEATS demonstrated significant performance improvements over the LSTM benchmark and showcased their probabilistic predictions by specifying a likelihood objective.",
      "authors": [
        "Mostafa Saberian",
        "Vidya Samadi",
        "Ioana Popescu"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 5,
      "year": 2026
    },
    {
      "title": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Husheng Fang",
        "Shunlin Liang",
        "Wenyuan Li",
        "Yongzhe Chen",
        "Han Ma",
        "Jianglei Xu",
        "Yichuan Ma",
        "Tao He",
        "Feng Tian",
        "Fengjiao Zhang",
        "Hui Liang"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 3,
      "year": 2026
    },
    {
      "title": "Unveiling cross-modal consistency: Taming inter- and intra-modal noise for robust multi-modal knowledge graph completion",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Bin Chen",
        "Hanting Shen",
        "Zhangtao Cheng",
        "Xueting Liu",
        "Ting Zhong",
        "Fan Zhou"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 2,
      "year": 2026
    },
    {
      "title": "Low-Resolution Massive MIMO Channel Estimation With LSTM Attention-Based CBDNet",
      "arxiv_id": null,
      "abstract": "Channel estimation of a massive multi-input multi-output (MIMO) system that utilizes a one-bit analog-to-digital converter (ADC) is a foremost challenge. Traditional deep learning (DL) approaches have been recently employed to circumvent this problem; however, they are limited to noise levels. Unlike the existing works, we use a DL-based denoise architecture for channel estimation from one-bit received signals, improving the estimation performance as the signal-to-noise ratio (SNR) increases. The model leverages a dual-branch architecture to estimate and remove noise from input data. We propose a DL model: a long short-term memory (LSTM) attention-based convolutional blind denoising network (LA-CBDNet) comprising the noise estimation subnetwork and the non-blind denoising subnetwork. The noise estimation subnetwork comprises convolutional layers estimating the noise map, followed by an LSTM to converge the noise estimation. The non-blind subnetwork comprises accompanying attention and LSTM layers estimating the noise matrix. The numerical results demonstrate that our model performs better than benchmark approaches for varying SNRs and base station (BS) antennas. In addition, it outperforms the comparative methods for different pilot lengths and number of users.",
      "authors": [
        "I. Helmy",
        "Wooyeol Choi"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 2,
      "year": 2026
    },
    {
      "title": "Quantum solver for single-impurity Anderson models with particle-hole symmetry",
      "arxiv_id": "",
      "abstract": "Quantum embedding methods, such as dynamical mean-field theory (DMFT), provide a powerful framework for investigating strongly correlated materials. A central computational bottleneck in DMFT is in solving the Anderson impurity model (AIM), whose exact solution is classically intractable for large bath sizes. In this work, we develop and benchmark a quantum-classical hybrid solver tailored for DMFT applications, using the variational quantum eigensolver (VQE) to prepare the ground state of the AIM with shallow quantum circuits. The solver uses a unified ansatz framework to prepare the particle and hole excitations of the ground-state from parameter-shifted circuits, enabling the reconstruction of the impurity Green's function through a continued-fraction expansion. We evaluate the performance of this approach across a few bath sizes and interaction strengths under noisy, shot-limited conditions. We compare three optimization routines (COBYLA, Adam, and L-BFGS-B) in terms of convergence and fidelity, assess the benefits of estimating a quantum-computed moment (QCM) correction to the variational energies, and benchmark the approach by comparing the reconstructed density of states (DOS) against that obtained using a classical pipeline. Our results demonstrate the feasibility of Green's function reconstruction on near-term devices and establish practical benchmarks for quantum impurity solvers embedded within self-consistent DMFT loops.",
      "authors": [
        "Mariia Karabin",
        "Tanvir Sohail",
        "Dmytro Bykov",
        "Eduardo Antonio Coello Pérez",
        "Swarnava Ghosh",
        "Murali Gopalakrishnan Meena",
        "Seongmin Kim",
        "Amir Shehata",
        "In-Saeng Suh",
        "Hanna Terletska",
        "Markus Eisenbach"
      ],
      "published_date": "2026-01-15",
      "pdf_url": "https://arxiv.org/pdf/2601.10594v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Sentinel-1 SAR-based globally distributed co-seismic landslide detection by deep neural networks",
      "arxiv_id": null,
      "abstract": "Abstract. Rapid response to multiple landslide events demands accurate, all-weather, day-and-night detection capabilities. Optical remote sensing has advanced landslide detection but remains limited under adverse weather and lighting conditions. Synthetic Aperture Radar, resilient to these constraints, remains underexplored for automated landslide detection due to challenges such as complex pre-processing and geometric distortions. This study integrates Deep Neural Networks with Sentinel-1 backscatter data for co-seismic landslide detection, utilizing a data-centric approach. We train and test the models using 11 earthquake-induced widespread landslide events, covering ≈ 73 000 landslides across diverse geologic and climatic settings. Inference on unseen events in Haiti (2021) and Sumatra (2022) demonstrates robust transferability, achieving F1-scores up to 82 %. Using explainable artificial intelligence, we highlight the discriminative capability of change detection bands over backscatter alone. Our findings emphasize the potential of SAR-based DNN models for worldwide, generalized, and rapid landslide detection, addressing critical gaps in current methods that solely use optical data. This research lays a foundation for broader applications in automated SAR-based earth surface change detection, particularly in complex, hilly and mountainous terrains.",
      "authors": [
        "Lorenzo Nava",
        "A. Mondini",
        "Kushanav Bhuyan",
        "Chengyong Fang",
        "Oriol Monserrat",
        "A. Novellino",
        "Filippo Catani"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 2,
      "year": 2026
    },
    {
      "title": "Increasing resolution and accuracy in sub-seasonal forecasting through 3D U-Net: the western US",
      "arxiv_id": null,
      "abstract": "Abstract. Sub-seasonal weather forecasting is a major challenge, particularly when high spatial resolution is needed to capture complex patterns and extreme events. Traditional Numerical Weather Prediction (NWP) models struggle with accurate forecasting at finer scales, especially for precipitation. In this study, we investigate the use of 3D U-Net architecture for post-processing sub-seasonal forecasts to enhance both predictability and spatial resolution, focusing on the western U.S. Using the ECMWF ensemble forecasting system (input) and high-resolution PRISM data (target), we tested different combinations of ensemble members and meteorological variables. Our results demonstrate that the 3D U-Net model significantly improves temperature predictability and consistently outperforms NWP models across multiple metrics. However, challenges remain in accurately forecasting extreme precipitation events, as the model tends to underestimate precipitation in coastal and mountainous regions. While ensemble members contribute to forecast accuracy, their impact is modest compared to the improvements achieved through downscaling. The model using the ensemble mean and only the target variables was most efficient. This model improved the pattern correlation coefficient for temperature and precipitation by 0.12 and 0.19, respectively, over a 32 d lead time. This study lays the groundwork for further development of neural network-based post-processing methods, showing their potential to enhance weather forecasts at sub-seasonal timescales.",
      "authors": [
        "Jihun Ryu",
        "Hisu Kim",
        "Simon Wang",
        "Jin-Ho Yoon"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 2,
      "year": 2026
    },
    {
      "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications",
      "arxiv_id": "",
      "abstract": "We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.",
      "authors": [
        "Andrew G. Howard",
        "Menglong Zhu",
        "Bo Chen",
        "Dmitry Kalenichenko",
        "Weijun Wang",
        "Tobias Weyand",
        "Marco Andreetto",
        "Hartwig Adam"
      ],
      "published_date": "2017-04-17",
      "pdf_url": "https://arxiv.org/pdf/1704.04861v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Distilling the Knowledge in a Neural Network",
      "arxiv_id": "",
      "abstract": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.",
      "authors": [
        "Geoffrey Hinton",
        "Oriol Vinyals",
        "Jeff Dean"
      ],
      "published_date": "2015-03-09",
      "pdf_url": "https://arxiv.org/pdf/1503.02531v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Visualizing and Understanding Convolutional Networks",
      "arxiv_id": "",
      "abstract": "Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \\etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.",
      "authors": [
        "Matthew D Zeiler",
        "Rob Fergus"
      ],
      "published_date": "2013-11-12",
      "pdf_url": "https://arxiv.org/pdf/1311.2901v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "mHC: Manifold-Constrained Hyper-Connections",
      "arxiv_id": "",
      "abstract": "Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.",
      "authors": [
        "Zhenda Xie",
        "Yixuan Wei",
        "Huanqi Cao",
        "Chenggang Zhao",
        "Chengqi Deng",
        "Jiashi Li",
        "Damai Dai",
        "Huazuo Gao",
        "Jiang Chang",
        "Kuai Yu",
        "Liang Zhao",
        "Shangyan Zhou",
        "Zhean Xu",
        "Zhengyan Zhang",
        "Wangding Zeng",
        "Shengding Hu",
        "Yuqing Wang",
        "Jingyang Yuan",
        "Lean Wang",
        "Wenfeng Liang"
      ],
      "published_date": "2025-12-31",
      "pdf_url": "https://arxiv.org/pdf/2512.24880v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot",
      "arxiv_id": null,
      "abstract": "To solve the labor shortage, robots have dramatically changed the world by combining powerful deep learning (DL) technology. Certainly, DL technology has become the key point of the widespread robot application. Efficient DL models depend on high-quality datasets and optimized architectures. However, some open datasets contain anomalous data that degrade model performance. Moreover, complex structures and high computational costs limit the adoption of DL models. This study proposes a dataset purification-based lightweight DL model construction strategy to solve these challenges. Initially, a dataset purification method is developed to filter out the anomaly data in a newly created dataset, which utilizes a lightweight cross-scale DL model OGNet to detect the anomaly data to achieve dataset purification. Subsequently, a highly efficient lightweight OGNet-based object detection (OD) model family, YOLO-OG, is presented to train the purified dataset. To evaluate the proposal, the strategy is implemented on the Empty-dish Recycling Robot. Experiments show that OGNet achieves excellent accuracy with only 0.68 M parameters and 0.35 GFLOPs. On purification Dish-10 dataset, the mean Average Precision (mAP) of YOLO-OG increases a maximum of 4.28% than original Dish-10 dataset. Meanwhile, YOLO-OG outperforms other advanced OD models, achieving the best accuracy of 99.20% mAP and the smallest 1.60 M parameters. YOLO-OG also reaches 99.86% mAP on the Dish-20 open dataset. On three other open datasets, YOLO-OG also shows excellent performance and surpasses most of the other OD models, which confirms the strong generalization ability of YOLO-OG.",
      "authors": [
        "Yifei Ge",
        "Zhuo Li",
        "Xuebin Yue",
        "Hengyi Li",
        "Lin Meng"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 10,
      "year": 2025
    },
    {
      "title": "Physics-guided deep neural networks for bathymetric mapping using Sentinel-2 multi-spectral imagery",
      "arxiv_id": null,
      "abstract": "Satellite-derived bathymetry (SDB) based on multi-spectral imagery data has been a critical tool for large-scale water depth in shallow water regions. Traditional SDB models primarily rely on known laws relating the exponential attenuation of light with the path length it traveled. In the past few years, deep computer vision models have emerged as valuable new technologies for bathymetry measurement. However, due to the black-box nature of these deep models, they may produce bathymetry results that are inconsistent with physical laws and exhibit limited generalizability across diverse areas. In this paper, we propose a novel hybrid architecture, HybridBathNet, that integrates UNet (extracting spatial and spectral feature) with a physical bathymetry network (ensuring physical relationships). By embedding physical constraints directly into the model architecture, HybridBathNet achieves improved bathymetric inversion accuracy while maintaining consistency with established optical attenuation laws. Experimental results demonstrate that the proposed model delivers high-quality bathymetric estimations across diverse island regions. Comparative evaluations against state-of-the-art methods further validate the superior accuracy and generalization capability of HybridBathNet. The code of HybridBathNet is available at https://github.com/qiushibupt/HybridBathNet.",
      "authors": [
        "Shuo Qian",
        "Yingying Chen",
        "Wei Wang",
        "Gaowei Zhang",
        "Lei Li",
        "Zengzhou Hao",
        "Yi Wang"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 9,
      "year": 2025
    },
    {
      "title": "Spike Memory Transformer: An Energy-Efficient Model in Distributed Learning Framework for Autonomous Depression Detection",
      "arxiv_id": null,
      "abstract": "Objective depression assessments based on physiological data offer new opportunities for clinical diagnostic support, but their usage is constrained by the limitations of data collection devices. Ubiquitous digital devices, now deeply integrated into everyday life, have the potential to effectively capture and represent digital phenotypes of depression. However, existing research in this field faces fundamental problems, such as device thresholds and insufficient utilization of distributed computational power, which have hindered significant research and application efforts in this area. In this article, we proposed the Computation-Oriented Hierarchical Depression Detection Internet of Things (IoT) Framework, which allows IoT devices to collaborate in a layered and distributed manner for psychological data collection and depression detection. In addition, we developed a depression detection model, i.e., spike memory transformer (SMT), which significantly reduces inference energy consumption, facilitating the deployment of depression detection capabilities across various IoT terminal devices. Experimental results demonstrate that our model achieved up to 70% accuracy on the D-Vlog dataset while reducing inference power consumption by an average of 38% compared to classical deep learning methods, thus validating the feasibility of proposed method.",
      "authors": [
        "Minqiang Yang",
        "Yueze Liu",
        "Yongfeng Tao",
        "Bin Hu"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 8,
      "year": 2025
    },
    {
      "title": "Multi-Perspective Semantic Segmentation of Ground Penetrating Radar Images for Pavement Subsurface Objects",
      "arxiv_id": null,
      "abstract": "Effective infrastructure health monitoring is crucial within transportation cyber-physical systems, where accurate road health detection is vital for ensuring road safety and the stability of intelligent transportation systems. To address the challenges of identifying pavement subsurface objects using 3D ground penetrating radar (GPR) data, we propose a multi-perspective cascading recognition method that integrates B-scan and C-scan images. This method is built on a lightweight dual-stream semantic segmentation model called AttnGPRNet, developed in this work to improve feature extraction through attention mechanisms and enhance subsurface object recognition. Initially, the model segments B-scan images to identify potential target regions, followed by more precise segmentation of 3L-C-scan images based on preliminary results. Additionally, we constructed a multi-view dataset using 3D GPR scans from over 100 kilometers of urban roads and evaluated the effectiveness of the proposed method through experiments. Experimental results show that our model outperforms existing advanced methods, achieving mIoU of 78.80% and 83.96% on B-scan and 3L-C-scan, and F1 scores of 87.65% and 91.07%, respectively. Moreover, the method has been deployed in Xiaoning Road GPR image intelligent recognition system and verified through on-site drilling, demonstrating its practical potential for road health monitoring.",
      "authors": [
        "Sibo Huang",
        "Guijie Zhu",
        "Jiaming Tang",
        "Weixiong Li",
        "Zhun Fan"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 8,
      "year": 2025
    },
    {
      "title": "A comprehensive review of facial beauty prediction using deep learning techniques",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "D. E. Boukhari",
        "F. Dornaika",
        "A. Chemsa",
        "Abdelmalik Taleb-Ahmed"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 7,
      "year": 2025
    },
    {
      "title": "Seeing Before Reasoning: A Unified Framework for Generalizable and Explainable Fake Image Detection",
      "arxiv_id": "",
      "abstract": "Detecting AI-generated images with multimodal large language models (MLLMs) has gained increasing attention, due to their rich world knowledge, common-sense reasoning, and potential for explainability. However, naively applying those MLLMs for detection often leads to suboptimal performance. We argue that the root of this failure lies in a fundamental mismatch: MLLMs are asked to reason about fakes before they can truly see them. First, they do not really see: existing MLLMs' vision encoders are primarily optimized for semantic-oriented recognition rather than the perception of low-level signals, leaving them insensitive to subtle forgery traces. Without access to reliable perceptual evidence, the model grounds its judgment on incomplete and limited visual observations. Second, existing finetuning data for detection typically uses narrow, instruction-style formats, which diverge sharply from the diverse, heterogeneous distributions seen in pretraining. In the absence of meaningful visual cues, the model therefore exploits these linguistic shortcuts, resulting in catastrophic forgetting of pretrained knowledge (even the basic dialogue capabilities). In response, we advocate for a new paradigm: seeing before reasoning. We propose that MLLMs should first be trained to perceive artifacts-strengthening their artifact-aware visual perception-so that subsequent reasoning is grounded in actual observations. We therefore propose Forensic-Chat, a generalizable, explainable, and still-conversational (for multi-round dialogue) assistant for fake image detection. We also propose ExplainFake-Bench, a benchmark tailored for the evaluation of the MLLM's explainability for image forensics from five key aspects. Extensive experiments show its superiority of generalization and genuinely reliable explainability.",
      "authors": [
        "Kaiqing Lin",
        "Zhiyuan Yan",
        "Ruoxin Chen",
        "Junyan Ye",
        "Ke-Yue Zhang",
        "Yue Zhou",
        "Peng Jin",
        "Bin Li",
        "Taiping Yao",
        "Shouhong Ding"
      ],
      "published_date": "2025-09-29",
      "pdf_url": "https://arxiv.org/pdf/2509.25502v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "DenseNet Model with Attention Mechanisms for Robust Date Fruit Image Classification",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Esraa Hassan",
        "Sarah Abu Ghazalah",
        "Nora El-Rashidy",
        "Tarek Abd El-Hafeez",
        "Mahmoud Y. Shams"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 7,
      "year": 2025
    },
    {
      "title": "Evaluation of deep learning models using explainable AI with qualitative and quantitative analysis for rice leaf disease detection",
      "arxiv_id": null,
      "abstract": "Deep learning models have shown remarkable success in disease detection and classification tasks, but lack transparency in their decision-making process, creating reliability and trust issues. Although traditional evaluation methods focus entirely on performance metrics such as classification accuracy, precision and recall, they fail to assess whether the models are considering relevant features for decision-making. The main objective of this work is to develop and validate a comprehensive three-stage methodology that combines conventional performance evaluation with qualitative and quantitative evaluation of explainable artificial intelligence (XAI) visualizations to assess both the accuracy and reliability of deep learning models. Eight pre-trained deep learning models - ResNet50, InceptionResNetV2, DenseNet 201, InceptionV3, EfficientNetB0, Xception, VGG16 and AlexNet,were evaluated using a three-stage methodology. First, the models are assessed using traditional classification metrics. Second, Local Interpretable Model-agnostic Explanations (LIME) is employed to visualize and quantitatively evaluate feature selection using metrics such as Intersection over Union (IoU) and the Dice Similarity Coefficient (DSC). Third, a novel overfitting ratio metric is introduced to quantify the reliance of the models on insignificant features. In the experimental analysis, ResNet50 emerged as the most accurate model, achieving 99.13% classification accuracy as well as the most reliable model demonstrating superior feature selection capabilities (IoU: 0.432, overfitting ratio: 0.284). Despite the high classification accuracies, models such as InceptionV3 and EfficientNetB0 showed poor feature selection capabilities with low IoU scores (0.295 and 0.326) and high overfitting ratios (0.544 and 0.458), indicating potential reliability issues in real-world applications. This study introduces a novel quantitative methodology for evaluating deep learning models that goes beyond traditional accuracy metrics, enabling more reliable and trustworthy AI systems for agricultural applications. This methodology is generic and researchers can explore the possibilities of extending it to other domains that require transparent and interpretable AI systems.",
      "authors": [
        "Hari Kishan Kondaveeti",
        "Chinna Gopi Simhadri"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 7,
      "year": 2025
    },
    {
      "title": "Wind turbine blade surface defect detection model based on improved you only look once version 10 small and integrated compression",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Hang Liu",
        "Sheng Liu",
        "Zhijian Liu",
        "Ben Niu",
        "Jing Xie",
        "Chi Luo",
        "Zhiyu Shi"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 6,
      "year": 2025
    },
    {
      "title": "Attention is All you Need",
      "arxiv_id": "1706.03762",
      "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan N. Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "published_date": "2017-06-12",
      "pdf_url": "https://arxiv.org/pdf/1706.03762v7",
      "citation_count": 164494,
      "year": 2017
    },
    {
      "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
      "arxiv_id": "",
      "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",
      "authors": [
        "Colin Raffel",
        "Noam Shazeer",
        "Adam Roberts",
        "Katherine Lee",
        "Sharan Narang",
        "Michael Matena",
        "Yanqi Zhou",
        "Wei Li",
        "Peter J. Liu"
      ],
      "published_date": "2019-10-23",
      "pdf_url": "https://arxiv.org/pdf/1910.10683v4",
      "citation_count": null,
      "year": null
    },
    {
      "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "M. Heusel",
        "Hubert Ramsauer",
        "Thomas Unterthiner",
        "Bernhard Nessler",
        "Sepp Hochreiter"
      ],
      "published_date": "2017",
      "pdf_url": "",
      "citation_count": 16741,
      "year": 2017
    },
    {
      "title": "nuScenes: A Multimodal Dataset for Autonomous Driving",
      "arxiv_id": "1903.11027",
      "abstract": "Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image based benchmark datasets have driven development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. In this work we present nuTonomy scenes (nuScenes), the first dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. nuScenes comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. We define novel 3D detection and tracking metrics. We also provide careful dataset analysis as well as baselines for lidar and image based detection and tracking. Data, development kit and more information are available online.",
      "authors": [
        "Holger Caesar",
        "Varun Bankiti",
        "Alex H. Lang",
        "Sourabh Vora",
        "Venice Erin Liong",
        "Qiang Xu",
        "Anush Krishnan",
        "Yu Pan",
        "Giancarlo Baldan",
        "Oscar Beijbom"
      ],
      "published_date": "2019-03-26",
      "pdf_url": "https://arxiv.org/pdf/1903.11027v5",
      "citation_count": 7334,
      "year": 2019
    },
    {
      "title": "CARLA: An Open Urban Driving Simulator",
      "arxiv_id": "",
      "abstract": "We introduce CARLA, an open-source simulator for autonomous driving research. CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems. In addition to open-source code and protocols, CARLA provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. The simulation platform supports flexible specification of sensor suites and environmental conditions. We use CARLA to study the performance of three approaches to autonomous driving: a classic modular pipeline, an end-to-end model trained via imitation learning, and an end-to-end model trained via reinforcement learning. The approaches are evaluated in controlled scenarios of increasing difficulty, and their performance is examined via metrics provided by CARLA, illustrating the platform's utility for autonomous driving research. The supplementary video can be viewed at https://youtu.be/Hp8Dz-Zek2E",
      "authors": [
        "Alexey Dosovitskiy",
        "German Ros",
        "Felipe Codevilla",
        "Antonio Lopez",
        "Vladlen Koltun"
      ],
      "published_date": "2017-11-10",
      "pdf_url": "https://arxiv.org/pdf/1711.03938v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Adding Conditional Control to Text-to-Image Diffusion Models",
      "arxiv_id": "",
      "abstract": "We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with \"zero convolutions\" (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, eg, edges, depth, segmentation, human pose, etc, with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small (<50k) and large (>1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models.",
      "authors": [
        "Lvmin Zhang",
        "Anyi Rao",
        "Maneesh Agrawala"
      ],
      "published_date": "2023-02-10",
      "pdf_url": "https://arxiv.org/pdf/2302.05543v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Scalable Diffusion Models with Transformers",
      "arxiv_id": "",
      "abstract": "We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops -- through increased transformer depth/width or increased number of input tokens -- consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512x512 and 256x256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.",
      "authors": [
        "William Peebles",
        "Saining Xie"
      ],
      "published_date": "2022-12-19",
      "pdf_url": "https://arxiv.org/pdf/2212.09748v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis",
      "arxiv_id": "",
      "abstract": "We present SDXL, a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone: The increase of model parameters is mainly due to more attention blocks and a larger cross-attention context as SDXL uses a second text encoder. We design multiple novel conditioning schemes and train SDXL on multiple aspect ratios. We also introduce a refinement model which is used to improve the visual fidelity of samples generated by SDXL using a post-hoc image-to-image technique. We demonstrate that SDXL shows drastically improved performance compared the previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators. In the spirit of promoting open research and fostering transparency in large model training and evaluation, we provide access to code and model weights at https://github.com/Stability-AI/generative-models",
      "authors": [
        "Dustin Podell",
        "Zion English",
        "Kyle Lacey",
        "Andreas Blattmann",
        "Tim Dockhorn",
        "Jonas Müller",
        "Joe Penna",
        "Robin Rombach"
      ],
      "published_date": "2023-07-04",
      "pdf_url": "https://arxiv.org/pdf/2307.01952v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Domain randomization for transferring deep neural networks from simulation to the real world",
      "arxiv_id": "1703.06907",
      "abstract": "Bridging the 'reality gap' that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to $1.5$cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.",
      "authors": [
        "Josh Tobin",
        "Rachel Fong",
        "Alex Ray",
        "Jonas Schneider",
        "Wojciech Zaremba",
        "Pieter Abbeel"
      ],
      "published_date": "2017-03-20",
      "pdf_url": "https://arxiv.org/pdf/1703.06907v1",
      "citation_count": 3419,
      "year": 2017
    },
    {
      "title": "The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "G. Ros",
        "Laura Sellart",
        "Joanna Materzynska",
        "David Vázquez",
        "Antonio M. López"
      ],
      "published_date": "2016",
      "pdf_url": "",
      "citation_count": 2379,
      "year": 2016
    },
    {
      "title": "Sora as a World Model? A Complete Survey on Text-to-Video Generation",
      "arxiv_id": "",
      "abstract": "The evolution of video generation from text, from animating MNIST to simulating the world with Sora, has progressed at a breakneck speed. Here, we systematically discuss how far text-to-video generation technology supports essential requirements in world modeling. We curate 250+ studies on text-based video synthesis and world modeling. We then observe that recent models increasingly support spatial, action, and strategic intelligences in world modeling through adherence to completeness, consistency, invention, as well as human interaction and control. We conclude that text-to-video generation is adept at world modeling, although homework in several aspects, such as the diversity-consistency trade-offs, remains to be addressed.",
      "authors": [
        "Fachrina Dewi Puspitasari",
        "Chaoning Zhang",
        "Joseph Cho",
        "Adnan Haider",
        "Noor Ul Eman",
        "Omer Amin",
        "Alexis Mankowski",
        "Muhammad Umair",
        "Jingyao Zheng",
        "Sheng Zheng",
        "Lik-Hang Lee",
        "Caiyan Qin",
        "Tae-Ho Kim",
        "Choong Seon Hong",
        "Yang Yang",
        "Heng Tao Shen"
      ],
      "published_date": "2024-03-08",
      "pdf_url": "https://arxiv.org/pdf/2403.05131v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "OmniNWM: Omniscient Driving Navigation World Models",
      "arxiv_id": "",
      "abstract": "Autonomous driving world models are expected to work effectively across three core dimensions: state, action, and reward. Existing models, however, are typically restricted to limited state modalities, short video sequences, imprecise action control, and a lack of reward awareness. In this paper, we introduce OmniNWM, an omniscient panoramic navigation world model that addresses all three dimensions within a unified framework. For state, OmniNWM jointly generates panoramic videos of RGB, semantics, metric depth, and 3D occupancy. A flexible forcing strategy enables high-quality long-horizon auto-regressive generation. For action, we introduce a normalized panoramic Plucker ray-map representation that encodes input trajectories into pixel-level signals, enabling highly precise and generalizable control over panoramic video generation. Regarding reward, we move beyond learning reward functions with external image-based models: instead, we leverage the generated 3D occupancy to directly define rule-based dense rewards for driving compliance and safety. Extensive experiments demonstrate that OmniNWM achieves state-of-the-art performance in video generation, control accuracy, and long-horizon stability, while providing a reliable closed-loop evaluation framework through occupancy-grounded rewards. Project page is available at https://arlo0o.github.io/OmniNWM/.",
      "authors": [
        "Bohan Li",
        "Zhuang Ma",
        "Dalong Du",
        "Baorui Peng",
        "Zhujin Liang",
        "Zhenqiang Liu",
        "Chao Ma",
        "Yueming Jin",
        "Hao Zhao",
        "Wenjun Zeng",
        "Xin Jin"
      ],
      "published_date": "2025-10-21",
      "pdf_url": "https://arxiv.org/pdf/2510.18313v4",
      "citation_count": null,
      "year": null
    },
    {
      "title": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask",
      "arxiv_id": "",
      "abstract": "Autonomous driving relies on robust models trained on large-scale, high-quality multi-view driving videos. Although world models provide a cost-effective solution for generating realistic driving data, they often suffer from identity drift, where the same object changes its appearance or category across frames due to the absence of instance-level temporal constraints. We introduce ConsisDrive, an identity-preserving driving world model designed to enforce temporal consistency at the instance level. Our framework incorporates two key components: (1) Instance-Masked Attention, which applies instance identity masks and trajectory masks within attention blocks to ensure that visual tokens interact only with their corresponding instance features across spatial and temporal dimensions, thereby preserving object identity consistency; and (2) Instance-Masked Loss, which adaptively emphasizes foreground regions with probabilistic instance masking, reducing background noise while maintaining overall scene fidelity. By integrating these mechanisms, ConsisDrive achieves state-of-the-art driving video generation quality and demonstrates significant improvements in downstream autonomous driving tasks on the nuScenes dataset. Our project page is https://shanpoyang654.github.io/ConsisDrive/page.html.",
      "authors": [
        "Zhuoran Yang",
        "Yanyong Zhang"
      ],
      "published_date": "2026-02-03",
      "pdf_url": "https://arxiv.org/pdf/2602.03213v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving",
      "arxiv_id": "",
      "abstract": "World models have demonstrated significant promise for data synthesis in autonomous driving. However, existing methods predominantly concentrate on single-modality generation, typically focusing on either multi-camera video or LiDAR sequence synthesis. In this paper, we propose UniDriveDreamer, a single-stage unified multimodal world model for autonomous driving, which directly generates multimodal future observations without relying on intermediate representations or cascaded modules. Our framework introduces a LiDAR-specific variational autoencoder (VAE) designed to encode input LiDAR sequences, alongside a video VAE for multi-camera images. To ensure cross-modal compatibility and training stability, we propose Unified Latent Anchoring (ULA), which explicitly aligns the latent distributions of the two modalities. The aligned features are fused and processed by a diffusion transformer that jointly models their geometric correspondence and temporal evolution. Additionally, structured scene layout information is projected per modality as a conditioning signal to guide the synthesis. Extensive experiments demonstrate that UniDriveDreamer outperforms previous state-of-the-art methods in both video and LiDAR generation, while also yielding measurable improvements in downstream",
      "authors": [
        "Guosheng Zhao",
        "Yaozeng Wang",
        "Xiaofeng Wang",
        "Zheng Zhu",
        "Tingdong Yu",
        "Guan Huang",
        "Yongchen Zai",
        "Ji Jiao",
        "Changliang Xue",
        "Xiaole Wang",
        "Zhen Yang",
        "Futang Zhu",
        "Xingang Wang"
      ],
      "published_date": "2026-02-02",
      "pdf_url": "https://arxiv.org/pdf/2602.02002v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
      "arxiv_id": "",
      "abstract": "Recent video diffusion models generate photorealistic, temporally coherent videos, yet they fall short as reliable world models for autonomous driving, where structured motion and physically consistent interactions are essential. Adapting these generalist video models to driving domains has shown promise but typically requires massive domain-specific data and costly fine-tuning. We propose an efficient adaptation framework that converts generalist video diffusion models into controllable driving world models with minimal supervision. The key idea is to decouple motion learning from appearance synthesis. First, the model is adapted to predict structured motion in a simplified form: videos of skeletonized agents and scene elements, focusing learning on physical and social plausibility. Then, the same backbone is reused to synthesize realistic RGB videos conditioned on these motion sequences, effectively \"dressing\" the motion with texture and lighting. This two-stage process mirrors a reasoning-rendering paradigm: first infer dynamics, then render appearance. Our experiments show this decoupled approach is exceptionally efficient: adapting SVD, we match prior SOTA models with less than 6% of their compute. Scaling to LTX, our MAD-LTX model outperforms all open-source competitors, and supports a comprehensive suite of text, ego, and object controls. Project page: https://vita-epfl.github.io/MAD-World-Model/",
      "authors": [
        "Ahmad Rahimi",
        "Valentin Gerard",
        "Eloi Zablocki",
        "Matthieu Cord",
        "Alexandre Alahi"
      ],
      "published_date": "2026-01-14",
      "pdf_url": "https://arxiv.org/pdf/2601.09452v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "View-Consistent Diffusion Representations for 3D-Consistent Video Generation",
      "arxiv_id": "",
      "abstract": "Video generation models have made significant progress in generating realistic content, enabling applications in simulation, gaming, and film making. However, current generated videos still contain visual artifacts arising from 3D inconsistencies, e.g., objects and structures deforming under changes in camera pose, which can undermine user experience and simulation fidelity. Motivated by recent findings on representation alignment for diffusion models, we hypothesize that improving the multi-view consistency of video diffusion representations will yield more 3D-consistent video generation. Through detailed analysis on multiple recent camera-controlled video diffusion models we reveal strong correlations between 3D-consistent representations and videos. We also propose ViCoDR, a new approach for improving the 3D consistency of video models by learning multi-view consistent diffusion representations. We evaluate ViCoDR on camera controlled image-to-video, text-to-video, and multi-view generation models, demonstrating significant improvements in the 3D consistency of the generated videos. Project page: https://danier97.github.io/ViCoDR.",
      "authors": [
        "Duolikun Danier",
        "Ge Gao",
        "Steven McDonagh",
        "Changjian Li",
        "Hakan Bilen",
        "Oisin Mac Aodha"
      ],
      "published_date": "2025-11-24",
      "pdf_url": "https://arxiv.org/pdf/2511.18991v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Fully Convolutional Networks for Semantic Segmentation",
      "arxiv_id": "",
      "abstract": "Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build \"fully convolutional\" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.",
      "authors": [
        "Jonathan Long",
        "Evan Shelhamer",
        "Trevor Darrell"
      ],
      "published_date": "2014-11-14",
      "pdf_url": "https://arxiv.org/pdf/1411.4038v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Speech Recognition with Deep Recurrent Neural Networks",
      "arxiv_id": "",
      "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \\emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.",
      "authors": [
        "Alex Graves",
        "Abdel-rahman Mohamed",
        "Geoffrey Hinton"
      ],
      "published_date": "2013-03-22",
      "pdf_url": "https://arxiv.org/pdf/1303.5778v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "nuScenes: A multimodal dataset for autonomous driving",
      "arxiv_id": "",
      "abstract": "Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image based benchmark datasets have driven development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. In this work we present nuTonomy scenes (nuScenes), the first dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. nuScenes comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. We define novel 3D detection and tracking metrics. We also provide careful dataset analysis as well as baselines for lidar and image based detection and tracking. Data, development kit and more information are available online.",
      "authors": [
        "Holger Caesar",
        "Varun Bankiti",
        "Alex H. Lang",
        "Sourabh Vora",
        "Venice Erin Liong",
        "Qiang Xu",
        "Anush Krishnan",
        "Yu Pan",
        "Giancarlo Baldan",
        "Oscar Beijbom"
      ],
      "published_date": "2019-03-26",
      "pdf_url": "https://arxiv.org/pdf/1903.11027v5",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World",
      "arxiv_id": "",
      "abstract": "Bridging the 'reality gap' that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to $1.5$cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.",
      "authors": [
        "Josh Tobin",
        "Rachel Fong",
        "Alex Ray",
        "Jonas Schneider",
        "Wojciech Zaremba",
        "Pieter Abbeel"
      ],
      "published_date": "2017-03-20",
      "pdf_url": "https://arxiv.org/pdf/1703.06907v1",
      "citation_count": null,
      "year": null
    }
  ],
  "top_k": 10,
  "depth": 2,
  "knowledge_graph": {
    "entities": [
      {
        "name": "Attention Is All You Need",
        "type": "AIPaper",
        "arxiv_id": "1706.03762"
      },
      {
        "name": "Deep Residual Learning for Image Recognition",
        "type": "AIPaper",
        "arxiv_id": "1512.03385"
      },
      {
        "name": "Adam: A Method for Stochastic Optimization",
        "type": "AIPaper",
        "arxiv_id": "1412.6980"
      },
      {
        "name": "Long Short-Term Memory",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Dropout: a simple way to prevent neural networks from overfitting",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Rethinking the Inception Architecture for Computer Vision",
        "type": "AIPaper",
        "arxiv_id": "1512.00567"
      },
      {
        "name": "Neural Machine Translation by Jointly Learning to Align and Translate",
        "type": "AIPaper",
        "arxiv_id": "1409.0473"
      },
      {
        "name": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation",
        "type": "AIPaper",
        "arxiv_id": "1406.1078"
      },
      {
        "name": "Sequence to Sequence Learning with Neural Networks",
        "type": "AIPaper",
        "arxiv_id": "1409.3215"
      },
      {
        "name": "Xception: Deep Learning with Depthwise Separable Convolutions",
        "type": "AIPaper",
        "arxiv_id": "1610.02357"
      },
      {
        "name": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling",
        "type": "AIPaper",
        "arxiv_id": "1412.3555"
      },
      {
        "name": "A comprehensive review of recommender systems: Transitioning from theory to practice",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
        "type": "AIPaper",
        "arxiv_id": "2602.03242"
      },
      {
        "name": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "DiffusionEngine: Diffusion model is scalable data engine for object detection",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Enhanced Transformer-BiLSTM Deep Learning Framework for Day-Ahead Energy Price Forecasting",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Conditional Generative Adversarial Network-based framework for multi-feature uncertainty modeling in energy systems",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "CityVLM: Towards sustainable urban development via multi-view coordinated vision–language model",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "FullPerception: Network-Level Collaborative Perception for Eliminating Vehicular Blind Spots",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "A survey of features used for representing black-box single-objective continuous optimization",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "ImageNet classification with deep convolutional neural networks",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
        "type": "AIPaper",
        "arxiv_id": "1409.1556"
      },
      {
        "name": "Et al",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
        "type": "AIPaper",
        "arxiv_id": "1506.01497"
      },
      {
        "name": "Microsoft COCO: Common Objects in Context",
        "type": "AIPaper",
        "arxiv_id": "1405.0312"
      },
      {
        "name": "Going deeper with convolutions",
        "type": "AIPaper",
        "arxiv_id": "1409.4842"
      },
      {
        "name": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
        "type": "AIPaper",
        "arxiv_id": "1502.03167"
      },
      {
        "name": "ImageNet Large Scale Visual Recognition Challenge",
        "type": "AIPaper",
        "arxiv_id": "1409.0575"
      },
      {
        "name": "Fully convolutional networks for semantic segmentation",
        "type": "AIPaper",
        "arxiv_id": "1411.4038"
      },
      {
        "name": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Noise Perception Self-Supervised Learning for Unsupervised Person Re-Identification",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Towards Reliable Tracking: An Uncertainty-Aware Siamese Tracker",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Bridging the gap, not forcing the tie: dual-space alignment and fusion framework for toxic memes detection",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Perturb and restore: Efficient category revocation in federated unlearning",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "DCM-Net: A novel dual-branch CNN-Mamba cross-layer feature fusion network for medical image segmentation",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "AdveDiffNet: adversarial diffusion network for unbalanced melanoma diagnosis",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Auto-Encoding Variational Bayes",
        "type": "AIPaper",
        "arxiv_id": "1312.6114"
      },
      {
        "name": "Supporting Online Material for Reducing the Dimensionality of Data with Neural Networks",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Speech recognition with deep recurrent neural networks",
        "type": "AIPaper",
        "arxiv_id": "1303.5778"
      },
      {
        "name": "Improving neural networks by preventing co-adaptation of feature detectors",
        "type": "AIPaper",
        "arxiv_id": "1207.0580"
      },
      {
        "name": "ADADELTA: An Adaptive Learning Rate Method",
        "type": "AIPaper",
        "arxiv_id": "1212.5701"
      },
      {
        "name": "Learning Word Vectors for Sentiment Analysis",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "On the importance of initialization and momentum in deep learning",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Generating Sequences With Recurrent Neural Networks",
        "type": "AIPaper",
        "arxiv_id": "1308.0850"
      },
      {
        "name": "Weakly Supervised Image Dehazing via Physics-Based Decomposition",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
        "type": "AIPaper",
        "arxiv_id": "2601.07372"
      },
      {
        "name": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Unveiling cross-modal consistency: Taming inter- and intra-modal noise for robust multi-modal knowledge graph completion",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Low-Resolution Massive MIMO Channel Estimation With LSTM Attention-Based CBDNet",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Quantum solver for single-impurity Anderson models with particle-hole symmetry",
        "type": "AIPaper",
        "arxiv_id": "2601.10594"
      },
      {
        "name": "Sentinel-1 SAR-based globally distributed co-seismic landslide detection by deep neural networks",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Increasing resolution and accuracy in sub-seasonal forecasting through 3D U-Net: the western US",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications",
        "type": "AIPaper",
        "arxiv_id": "1704.04861"
      },
      {
        "name": "Distilling the Knowledge in a Neural Network",
        "type": "AIPaper",
        "arxiv_id": "1503.02531"
      },
      {
        "name": "Visualizing and Understanding Convolutional Networks",
        "type": "AIPaper",
        "arxiv_id": "1311.2901"
      },
      {
        "name": "mHC: Manifold-Constrained Hyper-Connections",
        "type": "AIPaper",
        "arxiv_id": "2512.24880"
      },
      {
        "name": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Physics-guided deep neural networks for bathymetric mapping using Sentinel-2 multi-spectral imagery",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Spike Memory Transformer: An Energy-Efficient Model in Distributed Learning Framework for Autonomous Depression Detection",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Multi-Perspective Semantic Segmentation of Ground Penetrating Radar Images for Pavement Subsurface Objects",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "A comprehensive review of facial beauty prediction using deep learning techniques",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Seeing Before Reasoning: A Unified Framework for Generalizable and Explainable Fake Image Detection",
        "type": "AIPaper",
        "arxiv_id": "2509.25502"
      },
      {
        "name": "DenseNet Model with Attention Mechanisms for Robust Date Fruit Image Classification",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Evaluation of deep learning models using explainable AI with qualitative and quantitative analysis for rice leaf disease detection",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Wind turbine blade surface defect detection model based on improved you only look once version 10 small and integrated compression",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Attention is All you Need",
        "type": "AIPaper",
        "arxiv_id": "1706.03762"
      },
      {
        "name": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "type": "AIPaper",
        "arxiv_id": "1910.10683"
      },
      {
        "name": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "nuScenes: A Multimodal Dataset for Autonomous Driving",
        "type": "AIPaper",
        "arxiv_id": "1903.11027"
      },
      {
        "name": "CARLA: An Open Urban Driving Simulator",
        "type": "AIPaper",
        "arxiv_id": "1711.03938"
      },
      {
        "name": "Adding Conditional Control to Text-to-Image Diffusion Models",
        "type": "AIPaper",
        "arxiv_id": "2302.05543"
      },
      {
        "name": "Scalable Diffusion Models with Transformers",
        "type": "AIPaper",
        "arxiv_id": "2212.09748"
      },
      {
        "name": "SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis",
        "type": "AIPaper",
        "arxiv_id": "2307.01952"
      },
      {
        "name": "Domain randomization for transferring deep neural networks from simulation to the real world",
        "type": "AIPaper",
        "arxiv_id": "1703.06907"
      },
      {
        "name": "The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Sora as a World Model? A Complete Survey on Text-to-Video Generation",
        "type": "AIPaper",
        "arxiv_id": "2403.05131"
      },
      {
        "name": "OmniNWM: Omniscient Driving Navigation World Models",
        "type": "AIPaper",
        "arxiv_id": "2510.18313"
      },
      {
        "name": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask",
        "type": "AIPaper",
        "arxiv_id": "2602.03213"
      },
      {
        "name": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving",
        "type": "AIPaper",
        "arxiv_id": "2602.02002"
      },
      {
        "name": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
        "type": "AIPaper",
        "arxiv_id": "2601.09452"
      },
      {
        "name": "View-Consistent Diffusion Representations for 3D-Consistent Video Generation",
        "type": "AIPaper",
        "arxiv_id": "2511.18991"
      },
      {
        "name": "Fully Convolutional Networks for Semantic Segmentation",
        "type": "AIPaper",
        "arxiv_id": "1411.4038"
      },
      {
        "name": "Speech Recognition with Deep Recurrent Neural Networks",
        "type": "AIPaper",
        "arxiv_id": "1303.5778"
      },
      {
        "name": "nuScenes: A multimodal dataset for autonomous driving",
        "type": "AIPaper",
        "arxiv_id": "1903.11027"
      },
      {
        "name": "Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World",
        "type": "AIPaper",
        "arxiv_id": "1703.06907"
      },
      {
        "name": "Ashish Vaswani",
        "type": "Researcher"
      },
      {
        "name": "Noam Shazeer",
        "type": "Researcher"
      },
      {
        "name": "Niki Parmar",
        "type": "Researcher"
      },
      {
        "name": "Jakob Uszkoreit",
        "type": "Researcher"
      },
      {
        "name": "Llion Jones",
        "type": "Researcher"
      },
      {
        "name": "Aidan N. Gomez",
        "type": "Researcher"
      },
      {
        "name": "Lukasz Kaiser",
        "type": "Researcher"
      },
      {
        "name": "Illia Polosukhin",
        "type": "Researcher"
      },
      {
        "name": "Kaiming He",
        "type": "Researcher"
      },
      {
        "name": "Xiangyu Zhang",
        "type": "Researcher"
      },
      {
        "name": "Shaoqing Ren",
        "type": "Researcher"
      },
      {
        "name": "Jian Sun",
        "type": "Researcher"
      },
      {
        "name": "Diederik P. Kingma",
        "type": "Researcher"
      },
      {
        "name": "Jimmy Ba",
        "type": "Researcher"
      },
      {
        "name": "Sepp Hochreiter",
        "type": "Researcher"
      },
      {
        "name": "J. Schmidhuber",
        "type": "Researcher"
      },
      {
        "name": "Nitish Srivastava",
        "type": "Researcher"
      },
      {
        "name": "Geoffrey E. Hinton",
        "type": "Researcher"
      },
      {
        "name": "A. Krizhevsky",
        "type": "Researcher"
      },
      {
        "name": "I. Sutskever",
        "type": "Researcher"
      },
      {
        "name": "R. Salakhutdinov",
        "type": "Researcher"
      },
      {
        "name": "Christian Szegedy",
        "type": "Researcher"
      },
      {
        "name": "Vincent Vanhoucke",
        "type": "Researcher"
      },
      {
        "name": "Sergey Ioffe",
        "type": "Researcher"
      },
      {
        "name": "Jonathon Shlens",
        "type": "Researcher"
      },
      {
        "name": "Zbigniew Wojna",
        "type": "Researcher"
      },
      {
        "name": "Dzmitry Bahdanau",
        "type": "Researcher"
      },
      {
        "name": "Kyunghyun Cho",
        "type": "Researcher"
      },
      {
        "name": "Yoshua Bengio",
        "type": "Researcher"
      },
      {
        "name": "Bart van Merrienboer",
        "type": "Researcher"
      },
      {
        "name": "Caglar Gulcehre",
        "type": "Researcher"
      },
      {
        "name": "Fethi Bougares",
        "type": "Researcher"
      },
      {
        "name": "Holger Schwenk",
        "type": "Researcher"
      },
      {
        "name": "Ilya Sutskever",
        "type": "Researcher"
      },
      {
        "name": "Oriol Vinyals",
        "type": "Researcher"
      },
      {
        "name": "Quoc V. Le",
        "type": "Researcher"
      },
      {
        "name": "François Chollet",
        "type": "Researcher"
      },
      {
        "name": "Junyoung Chung",
        "type": "Researcher"
      },
      {
        "name": "KyungHyun Cho",
        "type": "Researcher"
      },
      {
        "name": "Shaina Raza",
        "type": "Researcher"
      },
      {
        "name": "Mizanur Rahman",
        "type": "Researcher"
      },
      {
        "name": "Safiullah Kamawal",
        "type": "Researcher"
      },
      {
        "name": "Armin Toroghi",
        "type": "Researcher"
      },
      {
        "name": "Ananya Raval",
        "type": "Researcher"
      },
      {
        "name": "F. Navah",
        "type": "Researcher"
      },
      {
        "name": "Amirmohammad Kazemeini",
        "type": "Researcher"
      },
      {
        "name": "Zhuoran Yang",
        "type": "Researcher"
      },
      {
        "name": "Xi Guo",
        "type": "Researcher"
      },
      {
        "name": "Chenjing Ding",
        "type": "Researcher"
      },
      {
        "name": "Chiyu Wang",
        "type": "Researcher"
      },
      {
        "name": "Wei Wu",
        "type": "Researcher"
      },
      {
        "name": "Yanyong Zhang",
        "type": "Researcher"
      },
      {
        "name": "Zisheng Wang",
        "type": "Researcher"
      },
      {
        "name": "Junjie Chen",
        "type": "Researcher"
      },
      {
        "name": "Chisen Wang",
        "type": "Researcher"
      },
      {
        "name": "Cong Peng",
        "type": "Researcher"
      },
      {
        "name": "Jianping Xuan",
        "type": "Researcher"
      },
      {
        "name": "Tielin Shi",
        "type": "Researcher"
      },
      {
        "name": "Ming J. Zuo",
        "type": "Researcher"
      },
      {
        "name": "Jinghuan Zhang",
        "type": "Researcher"
      },
      {
        "name": "Wang Chen",
        "type": "Researcher"
      },
      {
        "name": "Jian Zhang",
        "type": "Researcher"
      },
      {
        "name": "Manlin Zhang",
        "type": "Researcher"
      },
      {
        "name": "Jie Wu",
        "type": "Researcher"
      },
      {
        "name": "Yuxi Ren",
        "type": "Researcher"
      },
      {
        "name": "Jiahong Yang",
        "type": "Researcher"
      },
      {
        "name": "Ming Li",
        "type": "Researcher"
      },
      {
        "name": "Andy J. Ma",
        "type": "Researcher"
      },
      {
        "name": "Abdullah Al Ahad Khan",
        "type": "Researcher"
      },
      {
        "name": "Md Habib Ullah",
        "type": "Researcher"
      },
      {
        "name": "Ruchira Tabassum",
        "type": "Researcher"
      },
      {
        "name": "Md Faisal Kabir",
        "type": "Researcher"
      },
      {
        "name": "Mojtaba Moradi‐Sepahvand",
        "type": "Researcher"
      },
      {
        "name": "Junjue Wang",
        "type": "Researcher"
      },
      {
        "name": "Weihao Xuan",
        "type": "Researcher"
      },
      {
        "name": "Heli Qi",
        "type": "Researcher"
      },
      {
        "name": "Zihang Chen",
        "type": "Researcher"
      },
      {
        "name": "Hongruixuan Chen",
        "type": "Researcher"
      },
      {
        "name": "Zhuo Zheng",
        "type": "Researcher"
      },
      {
        "name": "Junshi Xia",
        "type": "Researcher"
      },
      {
        "name": "Yanfei Zhong",
        "type": "Researcher"
      },
      {
        "name": "Naoto Yokoya",
        "type": "Researcher"
      },
      {
        "name": "Lin Liang",
        "type": "Researcher"
      },
      {
        "name": "Guiyang Luo",
        "type": "Researcher"
      },
      {
        "name": "Yijing Lin",
        "type": "Researcher"
      },
      {
        "name": "Lei Deng",
        "type": "Researcher"
      },
      {
        "name": "Nan Cheng",
        "type": "Researcher"
      },
      {
        "name": "Quan Yuan",
        "type": "Researcher"
      },
      {
        "name": "Jinglin Li",
        "type": "Researcher"
      },
      {
        "name": "Dusit Niyato",
        "type": "Researcher"
      },
      {
        "name": "Gjorgjina Cenikj",
        "type": "Researcher"
      },
      {
        "name": "Ana Nikolikj",
        "type": "Researcher"
      },
      {
        "name": "G. Petelin",
        "type": "Researcher"
      },
      {
        "name": "Niki van Stein",
        "type": "Researcher"
      },
      {
        "name": "Carola Doerr",
        "type": "Researcher"
      },
      {
        "name": "T. Eftimov",
        "type": "Researcher"
      },
      {
        "name": "Karen Simonyan",
        "type": "Researcher"
      },
      {
        "name": "Andrew Zisserman",
        "type": "Researcher"
      },
      {
        "name": "P. Cochat",
        "type": "Researcher"
      },
      {
        "name": "L. Vaucoret",
        "type": "Researcher"
      },
      {
        "name": "J. Sarles",
        "type": "Researcher"
      },
      {
        "name": "Ross Girshick",
        "type": "Researcher"
      },
      {
        "name": "Tsung-Yi Lin",
        "type": "Researcher"
      },
      {
        "name": "Michael Maire",
        "type": "Researcher"
      },
      {
        "name": "Serge Belongie",
        "type": "Researcher"
      },
      {
        "name": "Lubomir Bourdev",
        "type": "Researcher"
      },
      {
        "name": "James Hays",
        "type": "Researcher"
      },
      {
        "name": "Pietro Perona",
        "type": "Researcher"
      },
      {
        "name": "Deva Ramanan",
        "type": "Researcher"
      },
      {
        "name": "C. Lawrence Zitnick",
        "type": "Researcher"
      },
      {
        "name": "Piotr Dollár",
        "type": "Researcher"
      },
      {
        "name": "Wei Liu",
        "type": "Researcher"
      },
      {
        "name": "Yangqing Jia",
        "type": "Researcher"
      },
      {
        "name": "Pierre Sermanet",
        "type": "Researcher"
      },
      {
        "name": "Scott Reed",
        "type": "Researcher"
      },
      {
        "name": "Dragomir Anguelov",
        "type": "Researcher"
      },
      {
        "name": "Dumitru Erhan",
        "type": "Researcher"
      },
      {
        "name": "Andrew Rabinovich",
        "type": "Researcher"
      },
      {
        "name": "Olga Russakovsky",
        "type": "Researcher"
      },
      {
        "name": "Jia Deng",
        "type": "Researcher"
      },
      {
        "name": "Hao Su",
        "type": "Researcher"
      },
      {
        "name": "Jonathan Krause",
        "type": "Researcher"
      },
      {
        "name": "Sanjeev Satheesh",
        "type": "Researcher"
      },
      {
        "name": "Sean Ma",
        "type": "Researcher"
      },
      {
        "name": "Zhiheng Huang",
        "type": "Researcher"
      },
      {
        "name": "Andrej Karpathy",
        "type": "Researcher"
      },
      {
        "name": "Aditya Khosla",
        "type": "Researcher"
      },
      {
        "name": "Michael Bernstein",
        "type": "Researcher"
      },
      {
        "name": "Alexander C. Berg",
        "type": "Researcher"
      },
      {
        "name": "Li Fei-Fei",
        "type": "Researcher"
      },
      {
        "name": "Jonathan Long",
        "type": "Researcher"
      },
      {
        "name": "Evan Shelhamer",
        "type": "Researcher"
      },
      {
        "name": "Trevor Darrell",
        "type": "Researcher"
      },
      {
        "name": "Yuqi Cheng",
        "type": "Researcher"
      },
      {
        "name": "Yunkang Cao",
        "type": "Researcher"
      },
      {
        "name": "Haiming Yao",
        "type": "Researcher"
      },
      {
        "name": "Wei Luo",
        "type": "Researcher"
      },
      {
        "name": "Cheng Jiang",
        "type": "Researcher"
      },
      {
        "name": "Hui Zhang",
        "type": "Researcher"
      },
      {
        "name": "Weiming Shen",
        "type": "Researcher"
      },
      {
        "name": "Naveen Kumar Srinivasa",
        "type": "Researcher"
      },
      {
        "name": "Ajeet Rao Chalamala",
        "type": "Researcher"
      },
      {
        "name": "Kumar Singh",
        "type": "Researcher"
      },
      {
        "name": "Ieee Krishna Mohan Senior Member",
        "type": "Researcher"
      },
      {
        "name": "K. Naveen",
        "type": "Researcher"
      },
      {
        "name": "Srinivasa Rao",
        "type": "Researcher"
      },
      {
        "name": "Ajeet Kumar Singh",
        "type": "Researcher"
      },
      {
        "name": "Hongbo Jiang",
        "type": "Researcher"
      },
      {
        "name": "Lei Ye",
        "type": "Researcher"
      },
      {
        "name": "Jingyang Hu",
        "type": "Researcher"
      },
      {
        "name": "Xiaotian Chen",
        "type": "Researcher"
      },
      {
        "name": "Siyu Chen",
        "type": "Researcher"
      },
      {
        "name": "Wei Zhang",
        "type": "Researcher"
      },
      {
        "name": "Kehua Yang",
        "type": "Researcher"
      },
      {
        "name": "Jingya Wang",
        "type": "Researcher"
      },
      {
        "name": "Jianfeng Wen",
        "type": "Researcher"
      },
      {
        "name": "Weiping Ding",
        "type": "Researcher"
      },
      {
        "name": "Chunlin Yu",
        "type": "Researcher"
      },
      {
        "name": "Xiatian Zhu",
        "type": "Researcher"
      },
      {
        "name": "Zhiyong Wang",
        "type": "Researcher"
      },
      {
        "name": "Jie Ma",
        "type": "Researcher"
      },
      {
        "name": "Xiangyuan Lan",
        "type": "Researcher"
      },
      {
        "name": "Qihua Liang",
        "type": "Researcher"
      },
      {
        "name": "Guorong Li",
        "type": "Researcher"
      },
      {
        "name": "Zhiyi Mo",
        "type": "Researcher"
      },
      {
        "name": "Bineng Zhong",
        "type": "Researcher"
      },
      {
        "name": "Yulin Lei",
        "type": "Researcher"
      },
      {
        "name": "Jin Yang",
        "type": "Researcher"
      },
      {
        "name": "Huijia Liang",
        "type": "Researcher"
      },
      {
        "name": "Tianrui Li",
        "type": "Researcher"
      },
      {
        "name": "Ning Pang",
        "type": "Researcher"
      },
      {
        "name": "Zou Li",
        "type": "Researcher"
      },
      {
        "name": "Pengcheng Wan",
        "type": "Researcher"
      },
      {
        "name": "Hongchao Wu",
        "type": "Researcher"
      },
      {
        "name": "Yuchen Bing",
        "type": "Researcher"
      },
      {
        "name": "Xiang Zhao",
        "type": "Researcher"
      },
      {
        "name": "Yanling Liu",
        "type": "Researcher"
      },
      {
        "name": "Hongmin Deng",
        "type": "Researcher"
      },
      {
        "name": "Jinghao Fu",
        "type": "Researcher"
      },
      {
        "name": "Yu Fu",
        "type": "Researcher"
      },
      {
        "name": "Chao Liu",
        "type": "Researcher"
      },
      {
        "name": "Shaoqiang Wang",
        "type": "Researcher"
      },
      {
        "name": "Hui Xia",
        "type": "Researcher"
      },
      {
        "name": "Diederik P Kingma",
        "type": "Researcher"
      },
      {
        "name": "Max Welling",
        "type": "Researcher"
      },
      {
        "name": "John C. Duchi",
        "type": "Researcher"
      },
      {
        "name": "Elad Hazan",
        "type": "Researcher"
      },
      {
        "name": "Y. Singer",
        "type": "Researcher"
      },
      {
        "name": "Alex Graves",
        "type": "Researcher"
      },
      {
        "name": "Abdel-rahman Mohamed",
        "type": "Researcher"
      },
      {
        "name": "Geoffrey Hinton",
        "type": "Researcher"
      },
      {
        "name": "Alex Krizhevsky",
        "type": "Researcher"
      },
      {
        "name": "Ruslan R. Salakhutdinov",
        "type": "Researcher"
      },
      {
        "name": "Matthew D. Zeiler",
        "type": "Researcher"
      },
      {
        "name": "Andrew L. Maas",
        "type": "Researcher"
      },
      {
        "name": "Raymond E. Daly",
        "type": "Researcher"
      },
      {
        "name": "Peter T. Pham",
        "type": "Researcher"
      },
      {
        "name": "Dan Huang",
        "type": "Researcher"
      },
      {
        "name": "A. Ng",
        "type": "Researcher"
      },
      {
        "name": "Christopher Potts",
        "type": "Researcher"
      },
      {
        "name": "James Martens",
        "type": "Researcher"
      },
      {
        "name": "George E. Dahl",
        "type": "Researcher"
      },
      {
        "name": "Nian Wang",
        "type": "Researcher"
      },
      {
        "name": "Zhigao Cui",
        "type": "Researcher"
      },
      {
        "name": "Yanzhao Su",
        "type": "Researcher"
      },
      {
        "name": "Yunwei Lan",
        "type": "Researcher"
      },
      {
        "name": "Yuanliang Xue",
        "type": "Researcher"
      },
      {
        "name": "Cong Zhang",
        "type": "Researcher"
      },
      {
        "name": "Aihua Li",
        "type": "Researcher"
      },
      {
        "name": "Leong Kah Meng",
        "type": "Researcher"
      },
      {
        "name": "Ho Hooi Yi",
        "type": "Researcher"
      },
      {
        "name": "Ng Bo Wei",
        "type": "Researcher"
      },
      {
        "name": "Lim Jia Xin",
        "type": "Researcher"
      },
      {
        "name": "Zailan Arabee Abdul Salam",
        "type": "Researcher"
      },
      {
        "name": "Xin Cheng",
        "type": "Researcher"
      },
      {
        "name": "Wangding Zeng",
        "type": "Researcher"
      },
      {
        "name": "Damai Dai",
        "type": "Researcher"
      },
      {
        "name": "Qinyu Chen",
        "type": "Researcher"
      },
      {
        "name": "Bingxuan Wang",
        "type": "Researcher"
      },
      {
        "name": "Zhenda Xie",
        "type": "Researcher"
      },
      {
        "name": "Kezhao Huang",
        "type": "Researcher"
      },
      {
        "name": "Xingkai Yu",
        "type": "Researcher"
      },
      {
        "name": "Zhewen Hao",
        "type": "Researcher"
      },
      {
        "name": "Yukun Li",
        "type": "Researcher"
      },
      {
        "name": "Han Zhang",
        "type": "Researcher"
      },
      {
        "name": "Huishuai Zhang",
        "type": "Researcher"
      },
      {
        "name": "Dongyan Zhao",
        "type": "Researcher"
      },
      {
        "name": "Wenfeng Liang",
        "type": "Researcher"
      },
      {
        "name": "Mostafa Saberian",
        "type": "Researcher"
      },
      {
        "name": "Vidya Samadi",
        "type": "Researcher"
      },
      {
        "name": "Ioana Popescu",
        "type": "Researcher"
      },
      {
        "name": "Husheng Fang",
        "type": "Researcher"
      },
      {
        "name": "Shunlin Liang",
        "type": "Researcher"
      },
      {
        "name": "Wenyuan Li",
        "type": "Researcher"
      },
      {
        "name": "Yongzhe Chen",
        "type": "Researcher"
      },
      {
        "name": "Han Ma",
        "type": "Researcher"
      },
      {
        "name": "Jianglei Xu",
        "type": "Researcher"
      },
      {
        "name": "Yichuan Ma",
        "type": "Researcher"
      },
      {
        "name": "Tao He",
        "type": "Researcher"
      },
      {
        "name": "Feng Tian",
        "type": "Researcher"
      },
      {
        "name": "Fengjiao Zhang",
        "type": "Researcher"
      },
      {
        "name": "Hui Liang",
        "type": "Researcher"
      },
      {
        "name": "Bin Chen",
        "type": "Researcher"
      },
      {
        "name": "Hanting Shen",
        "type": "Researcher"
      },
      {
        "name": "Zhangtao Cheng",
        "type": "Researcher"
      },
      {
        "name": "Xueting Liu",
        "type": "Researcher"
      },
      {
        "name": "Ting Zhong",
        "type": "Researcher"
      },
      {
        "name": "Fan Zhou",
        "type": "Researcher"
      },
      {
        "name": "I. Helmy",
        "type": "Researcher"
      },
      {
        "name": "Wooyeol Choi",
        "type": "Researcher"
      },
      {
        "name": "Mariia Karabin",
        "type": "Researcher"
      },
      {
        "name": "Tanvir Sohail",
        "type": "Researcher"
      },
      {
        "name": "Dmytro Bykov",
        "type": "Researcher"
      },
      {
        "name": "Eduardo Antonio Coello Pérez",
        "type": "Researcher"
      },
      {
        "name": "Swarnava Ghosh",
        "type": "Researcher"
      },
      {
        "name": "Murali Gopalakrishnan Meena",
        "type": "Researcher"
      },
      {
        "name": "Seongmin Kim",
        "type": "Researcher"
      },
      {
        "name": "Amir Shehata",
        "type": "Researcher"
      },
      {
        "name": "In-Saeng Suh",
        "type": "Researcher"
      },
      {
        "name": "Hanna Terletska",
        "type": "Researcher"
      },
      {
        "name": "Markus Eisenbach",
        "type": "Researcher"
      },
      {
        "name": "Lorenzo Nava",
        "type": "Researcher"
      },
      {
        "name": "A. Mondini",
        "type": "Researcher"
      },
      {
        "name": "Kushanav Bhuyan",
        "type": "Researcher"
      },
      {
        "name": "Chengyong Fang",
        "type": "Researcher"
      },
      {
        "name": "Oriol Monserrat",
        "type": "Researcher"
      },
      {
        "name": "A. Novellino",
        "type": "Researcher"
      },
      {
        "name": "Filippo Catani",
        "type": "Researcher"
      },
      {
        "name": "Jihun Ryu",
        "type": "Researcher"
      },
      {
        "name": "Hisu Kim",
        "type": "Researcher"
      },
      {
        "name": "Simon Wang",
        "type": "Researcher"
      },
      {
        "name": "Jin-Ho Yoon",
        "type": "Researcher"
      },
      {
        "name": "Andrew G. Howard",
        "type": "Researcher"
      },
      {
        "name": "Menglong Zhu",
        "type": "Researcher"
      },
      {
        "name": "Bo Chen",
        "type": "Researcher"
      },
      {
        "name": "Dmitry Kalenichenko",
        "type": "Researcher"
      },
      {
        "name": "Weijun Wang",
        "type": "Researcher"
      },
      {
        "name": "Tobias Weyand",
        "type": "Researcher"
      },
      {
        "name": "Marco Andreetto",
        "type": "Researcher"
      },
      {
        "name": "Hartwig Adam",
        "type": "Researcher"
      },
      {
        "name": "Jeff Dean",
        "type": "Researcher"
      },
      {
        "name": "Matthew D Zeiler",
        "type": "Researcher"
      },
      {
        "name": "Rob Fergus",
        "type": "Researcher"
      },
      {
        "name": "Yixuan Wei",
        "type": "Researcher"
      },
      {
        "name": "Huanqi Cao",
        "type": "Researcher"
      },
      {
        "name": "Chenggang Zhao",
        "type": "Researcher"
      },
      {
        "name": "Chengqi Deng",
        "type": "Researcher"
      },
      {
        "name": "Jiashi Li",
        "type": "Researcher"
      },
      {
        "name": "Huazuo Gao",
        "type": "Researcher"
      },
      {
        "name": "Jiang Chang",
        "type": "Researcher"
      },
      {
        "name": "Kuai Yu",
        "type": "Researcher"
      },
      {
        "name": "Liang Zhao",
        "type": "Researcher"
      },
      {
        "name": "Shangyan Zhou",
        "type": "Researcher"
      },
      {
        "name": "Zhean Xu",
        "type": "Researcher"
      },
      {
        "name": "Zhengyan Zhang",
        "type": "Researcher"
      },
      {
        "name": "Shengding Hu",
        "type": "Researcher"
      },
      {
        "name": "Yuqing Wang",
        "type": "Researcher"
      },
      {
        "name": "Jingyang Yuan",
        "type": "Researcher"
      },
      {
        "name": "Lean Wang",
        "type": "Researcher"
      },
      {
        "name": "Yifei Ge",
        "type": "Researcher"
      },
      {
        "name": "Zhuo Li",
        "type": "Researcher"
      },
      {
        "name": "Xuebin Yue",
        "type": "Researcher"
      },
      {
        "name": "Hengyi Li",
        "type": "Researcher"
      },
      {
        "name": "Lin Meng",
        "type": "Researcher"
      },
      {
        "name": "Shuo Qian",
        "type": "Researcher"
      },
      {
        "name": "Yingying Chen",
        "type": "Researcher"
      },
      {
        "name": "Wei Wang",
        "type": "Researcher"
      },
      {
        "name": "Gaowei Zhang",
        "type": "Researcher"
      },
      {
        "name": "Lei Li",
        "type": "Researcher"
      },
      {
        "name": "Zengzhou Hao",
        "type": "Researcher"
      },
      {
        "name": "Yi Wang",
        "type": "Researcher"
      },
      {
        "name": "Minqiang Yang",
        "type": "Researcher"
      },
      {
        "name": "Yueze Liu",
        "type": "Researcher"
      },
      {
        "name": "Yongfeng Tao",
        "type": "Researcher"
      },
      {
        "name": "Bin Hu",
        "type": "Researcher"
      },
      {
        "name": "Sibo Huang",
        "type": "Researcher"
      },
      {
        "name": "Guijie Zhu",
        "type": "Researcher"
      },
      {
        "name": "Jiaming Tang",
        "type": "Researcher"
      },
      {
        "name": "Weixiong Li",
        "type": "Researcher"
      },
      {
        "name": "Zhun Fan",
        "type": "Researcher"
      },
      {
        "name": "D. E. Boukhari",
        "type": "Researcher"
      },
      {
        "name": "F. Dornaika",
        "type": "Researcher"
      },
      {
        "name": "A. Chemsa",
        "type": "Researcher"
      },
      {
        "name": "Abdelmalik Taleb-Ahmed",
        "type": "Researcher"
      },
      {
        "name": "Kaiqing Lin",
        "type": "Researcher"
      },
      {
        "name": "Zhiyuan Yan",
        "type": "Researcher"
      },
      {
        "name": "Ruoxin Chen",
        "type": "Researcher"
      },
      {
        "name": "Junyan Ye",
        "type": "Researcher"
      },
      {
        "name": "Ke-Yue Zhang",
        "type": "Researcher"
      },
      {
        "name": "Yue Zhou",
        "type": "Researcher"
      },
      {
        "name": "Peng Jin",
        "type": "Researcher"
      },
      {
        "name": "Bin Li",
        "type": "Researcher"
      },
      {
        "name": "Taiping Yao",
        "type": "Researcher"
      },
      {
        "name": "Shouhong Ding",
        "type": "Researcher"
      },
      {
        "name": "Esraa Hassan",
        "type": "Researcher"
      },
      {
        "name": "Sarah Abu Ghazalah",
        "type": "Researcher"
      },
      {
        "name": "Nora El-Rashidy",
        "type": "Researcher"
      },
      {
        "name": "Tarek Abd El-Hafeez",
        "type": "Researcher"
      },
      {
        "name": "Mahmoud Y. Shams",
        "type": "Researcher"
      },
      {
        "name": "Hari Kishan Kondaveeti",
        "type": "Researcher"
      },
      {
        "name": "Chinna Gopi Simhadri",
        "type": "Researcher"
      },
      {
        "name": "Hang Liu",
        "type": "Researcher"
      },
      {
        "name": "Sheng Liu",
        "type": "Researcher"
      },
      {
        "name": "Zhijian Liu",
        "type": "Researcher"
      },
      {
        "name": "Ben Niu",
        "type": "Researcher"
      },
      {
        "name": "Jing Xie",
        "type": "Researcher"
      },
      {
        "name": "Chi Luo",
        "type": "Researcher"
      },
      {
        "name": "Zhiyu Shi",
        "type": "Researcher"
      },
      {
        "name": "Colin Raffel",
        "type": "Researcher"
      },
      {
        "name": "Adam Roberts",
        "type": "Researcher"
      },
      {
        "name": "Katherine Lee",
        "type": "Researcher"
      },
      {
        "name": "Sharan Narang",
        "type": "Researcher"
      },
      {
        "name": "Michael Matena",
        "type": "Researcher"
      },
      {
        "name": "Yanqi Zhou",
        "type": "Researcher"
      },
      {
        "name": "Wei Li",
        "type": "Researcher"
      },
      {
        "name": "Peter J. Liu",
        "type": "Researcher"
      },
      {
        "name": "M. Heusel",
        "type": "Researcher"
      },
      {
        "name": "Hubert Ramsauer",
        "type": "Researcher"
      },
      {
        "name": "Thomas Unterthiner",
        "type": "Researcher"
      },
      {
        "name": "Bernhard Nessler",
        "type": "Researcher"
      },
      {
        "name": "Holger Caesar",
        "type": "Researcher"
      },
      {
        "name": "Varun Bankiti",
        "type": "Researcher"
      },
      {
        "name": "Alex H. Lang",
        "type": "Researcher"
      },
      {
        "name": "Sourabh Vora",
        "type": "Researcher"
      },
      {
        "name": "Venice Erin Liong",
        "type": "Researcher"
      },
      {
        "name": "Qiang Xu",
        "type": "Researcher"
      },
      {
        "name": "Anush Krishnan",
        "type": "Researcher"
      },
      {
        "name": "Yu Pan",
        "type": "Researcher"
      },
      {
        "name": "Giancarlo Baldan",
        "type": "Researcher"
      },
      {
        "name": "Oscar Beijbom",
        "type": "Researcher"
      },
      {
        "name": "Alexey Dosovitskiy",
        "type": "Researcher"
      },
      {
        "name": "German Ros",
        "type": "Researcher"
      },
      {
        "name": "Felipe Codevilla",
        "type": "Researcher"
      },
      {
        "name": "Antonio Lopez",
        "type": "Researcher"
      },
      {
        "name": "Vladlen Koltun",
        "type": "Researcher"
      },
      {
        "name": "Lvmin Zhang",
        "type": "Researcher"
      },
      {
        "name": "Anyi Rao",
        "type": "Researcher"
      },
      {
        "name": "Maneesh Agrawala",
        "type": "Researcher"
      },
      {
        "name": "William Peebles",
        "type": "Researcher"
      },
      {
        "name": "Saining Xie",
        "type": "Researcher"
      },
      {
        "name": "Dustin Podell",
        "type": "Researcher"
      },
      {
        "name": "Zion English",
        "type": "Researcher"
      },
      {
        "name": "Kyle Lacey",
        "type": "Researcher"
      },
      {
        "name": "Andreas Blattmann",
        "type": "Researcher"
      },
      {
        "name": "Tim Dockhorn",
        "type": "Researcher"
      },
      {
        "name": "Jonas Müller",
        "type": "Researcher"
      },
      {
        "name": "Joe Penna",
        "type": "Researcher"
      },
      {
        "name": "Robin Rombach",
        "type": "Researcher"
      },
      {
        "name": "Josh Tobin",
        "type": "Researcher"
      },
      {
        "name": "Rachel Fong",
        "type": "Researcher"
      },
      {
        "name": "Alex Ray",
        "type": "Researcher"
      },
      {
        "name": "Jonas Schneider",
        "type": "Researcher"
      },
      {
        "name": "Wojciech Zaremba",
        "type": "Researcher"
      },
      {
        "name": "Pieter Abbeel",
        "type": "Researcher"
      },
      {
        "name": "G. Ros",
        "type": "Researcher"
      },
      {
        "name": "Laura Sellart",
        "type": "Researcher"
      },
      {
        "name": "Joanna Materzynska",
        "type": "Researcher"
      },
      {
        "name": "David Vázquez",
        "type": "Researcher"
      },
      {
        "name": "Antonio M. López",
        "type": "Researcher"
      },
      {
        "name": "Fachrina Dewi Puspitasari",
        "type": "Researcher"
      },
      {
        "name": "Chaoning Zhang",
        "type": "Researcher"
      },
      {
        "name": "Joseph Cho",
        "type": "Researcher"
      },
      {
        "name": "Adnan Haider",
        "type": "Researcher"
      },
      {
        "name": "Noor Ul Eman",
        "type": "Researcher"
      },
      {
        "name": "Omer Amin",
        "type": "Researcher"
      },
      {
        "name": "Alexis Mankowski",
        "type": "Researcher"
      },
      {
        "name": "Muhammad Umair",
        "type": "Researcher"
      },
      {
        "name": "Jingyao Zheng",
        "type": "Researcher"
      },
      {
        "name": "Sheng Zheng",
        "type": "Researcher"
      },
      {
        "name": "Lik-Hang Lee",
        "type": "Researcher"
      },
      {
        "name": "Caiyan Qin",
        "type": "Researcher"
      },
      {
        "name": "Tae-Ho Kim",
        "type": "Researcher"
      },
      {
        "name": "Choong Seon Hong",
        "type": "Researcher"
      },
      {
        "name": "Yang Yang",
        "type": "Researcher"
      },
      {
        "name": "Heng Tao Shen",
        "type": "Researcher"
      },
      {
        "name": "Bohan Li",
        "type": "Researcher"
      },
      {
        "name": "Zhuang Ma",
        "type": "Researcher"
      },
      {
        "name": "Dalong Du",
        "type": "Researcher"
      },
      {
        "name": "Baorui Peng",
        "type": "Researcher"
      },
      {
        "name": "Zhujin Liang",
        "type": "Researcher"
      },
      {
        "name": "Zhenqiang Liu",
        "type": "Researcher"
      },
      {
        "name": "Chao Ma",
        "type": "Researcher"
      },
      {
        "name": "Yueming Jin",
        "type": "Researcher"
      },
      {
        "name": "Hao Zhao",
        "type": "Researcher"
      },
      {
        "name": "Wenjun Zeng",
        "type": "Researcher"
      },
      {
        "name": "Xin Jin",
        "type": "Researcher"
      },
      {
        "name": "Guosheng Zhao",
        "type": "Researcher"
      },
      {
        "name": "Yaozeng Wang",
        "type": "Researcher"
      },
      {
        "name": "Xiaofeng Wang",
        "type": "Researcher"
      },
      {
        "name": "Zheng Zhu",
        "type": "Researcher"
      },
      {
        "name": "Tingdong Yu",
        "type": "Researcher"
      },
      {
        "name": "Guan Huang",
        "type": "Researcher"
      },
      {
        "name": "Yongchen Zai",
        "type": "Researcher"
      },
      {
        "name": "Ji Jiao",
        "type": "Researcher"
      },
      {
        "name": "Changliang Xue",
        "type": "Researcher"
      },
      {
        "name": "Xiaole Wang",
        "type": "Researcher"
      },
      {
        "name": "Zhen Yang",
        "type": "Researcher"
      },
      {
        "name": "Futang Zhu",
        "type": "Researcher"
      },
      {
        "name": "Xingang Wang",
        "type": "Researcher"
      },
      {
        "name": "Ahmad Rahimi",
        "type": "Researcher"
      },
      {
        "name": "Valentin Gerard",
        "type": "Researcher"
      },
      {
        "name": "Eloi Zablocki",
        "type": "Researcher"
      },
      {
        "name": "Matthieu Cord",
        "type": "Researcher"
      },
      {
        "name": "Alexandre Alahi",
        "type": "Researcher"
      },
      {
        "name": "Duolikun Danier",
        "type": "Researcher"
      },
      {
        "name": "Ge Gao",
        "type": "Researcher"
      },
      {
        "name": "Steven McDonagh",
        "type": "Researcher"
      },
      {
        "name": "Changjian Li",
        "type": "Researcher"
      },
      {
        "name": "Hakan Bilen",
        "type": "Researcher"
      },
      {
        "name": "Oisin Mac Aodha",
        "type": "Researcher"
      },
      {
        "name": "Transformer",
        "type": "AIModel"
      },
      {
        "name": "WMT 2014 English-to-German translation task",
        "type": "Dataset"
      },
      {
        "name": "WMT 2014 English-to-French translation task",
        "type": "Dataset"
      },
      {
        "name": "BLEU",
        "type": "Metric"
      },
      {
        "name": "residual learning framework",
        "type": "AIModel"
      },
      {
        "name": "VGG nets",
        "type": "AIModel"
      },
      {
        "name": "ImageNet",
        "type": "Dataset"
      },
      {
        "name": "CIFAR-10",
        "type": "Dataset"
      },
      {
        "name": "COCO object detection dataset",
        "type": "Dataset"
      },
      {
        "name": "error",
        "type": "Metric"
      },
      {
        "name": "relative improvement",
        "type": "Metric"
      },
      {
        "name": "Adam",
        "type": "AIModel"
      },
      {
        "name": "AdaMax",
        "type": "AIModel"
      },
      {
        "name": "Dropout",
        "type": "AIModel"
      },
      {
        "name": "neural networks",
        "type": "AIModel"
      },
      {
        "name": "Inception Architecture",
        "type": "AIModel"
      },
      {
        "name": "ILSVRC 2012 classification challenge validation set",
        "type": "Dataset"
      },
      {
        "name": "top-1 error",
        "type": "Metric"
      },
      {
        "name": "top-5 error",
        "type": "Metric"
      },
      {
        "name": "encoder-decoder",
        "type": "AIModel"
      },
      {
        "name": "phrase-based system",
        "type": "AIModel"
      },
      {
        "name": "English-to-French translation",
        "type": "Dataset"
      },
      {
        "name": "translation performance",
        "type": "Metric"
      },
      {
        "name": "RNN Encoder-Decoder",
        "type": "AIModel"
      },
      {
        "name": "statistical machine translation system",
        "type": "AIModel"
      },
      {
        "name": "log-linear model",
        "type": "AIModel"
      },
      {
        "name": "LSTM",
        "type": "AIModel"
      },
      {
        "name": "phrase-based SMT system",
        "type": "AIModel"
      },
      {
        "name": "WMT'14 dataset",
        "type": "Dataset"
      },
      {
        "name": "Xception",
        "type": "AIModel"
      },
      {
        "name": "Inception V3",
        "type": "AIModel"
      },
      {
        "name": "larger image classification dataset comprising 350 million images and 17,000 classes",
        "type": "Dataset"
      },
      {
        "name": "long short-term memory (LSTM)",
        "type": "AIModel"
      },
      {
        "name": "gated recurrent unit (GRU)",
        "type": "AIModel"
      },
      {
        "name": "tanh units",
        "type": "AIModel"
      },
      {
        "name": "polyphonic music modeling",
        "type": "Dataset"
      },
      {
        "name": "speech signal modeling",
        "type": "Dataset"
      },
      {
        "name": "InstaDrive",
        "type": "AIModel"
      },
      {
        "name": "Instance Flow Guider",
        "type": "AIModel"
      },
      {
        "name": "Spatial Geometric Aligner",
        "type": "AIModel"
      },
      {
        "name": "nuScenes",
        "type": "Dataset"
      },
      {
        "name": "CARLA",
        "type": "Dataset"
      },
      {
        "name": "video generation quality",
        "type": "Metric"
      },
      {
        "name": "safety evaluation",
        "type": "Metric"
      },
      {
        "name": "CNC-VLM",
        "type": "AIModel"
      },
      {
        "name": "CNC fault detection dataset",
        "type": "Dataset"
      },
      {
        "name": "Accuracy",
        "type": "Metric"
      },
      {
        "name": "F1-score",
        "type": "Metric"
      },
      {
        "name": "mamba segmentation",
        "type": "AIModel"
      },
      {
        "name": "four-point laser metric calibration",
        "type": "Metric"
      },
      {
        "name": "DiffusionEngine",
        "type": "AIModel"
      },
      {
        "name": "Transformer and Bidirectional Long Short-Term Memory (BiLSTM) model",
        "type": "AIModel"
      },
      {
        "name": "New York Independent System Operator",
        "type": "Dataset"
      },
      {
        "name": "MAE",
        "type": "Metric"
      },
      {
        "name": "RMSE",
        "type": "Metric"
      },
      {
        "name": "sMAPE",
        "type": "Metric"
      },
      {
        "name": "MAPE",
        "type": "Metric"
      },
      {
        "name": "R²",
        "type": "Metric"
      },
      {
        "name": "Conditional Generative Adversarial Network",
        "type": "AIModel"
      },
      {
        "name": "CityVLM",
        "type": "AIModel"
      },
      {
        "name": "FullPerception",
        "type": "AIModel"
      },
      {
        "name": "Proactive Conflict-free Scheduling (PCS)",
        "type": "AIModel"
      },
      {
        "name": "single-vehicle systems",
        "type": "AIModel"
      },
      {
        "name": "existing scheduling methods",
        "type": "AIModel"
      },
      {
        "name": "large-scale comprehensive joint simulation experiments",
        "type": "Dataset"
      },
      {
        "name": "perception accuracy",
        "type": "Metric"
      },
      {
        "name": "deep convolutional neural networks",
        "type": "AIModel"
      },
      {
        "name": "ConvNet models",
        "type": "AIModel"
      },
      {
        "name": "ImageNet Challenge 2014",
        "type": "Dataset"
      },
      {
        "name": "other datasets",
        "type": "Dataset"
      },
      {
        "name": "accuracy",
        "type": "Metric"
      },
      {
        "name": "Region Proposal Network (RPN)",
        "type": "AIModel"
      },
      {
        "name": "SPPnet",
        "type": "AIModel"
      },
      {
        "name": "Fast R-CNN",
        "type": "AIModel"
      },
      {
        "name": "VGG-16",
        "type": "AIModel"
      },
      {
        "name": "PASCAL VOC 2007",
        "type": "Dataset"
      },
      {
        "name": "PASCAL VOC 2012",
        "type": "Dataset"
      },
      {
        "name": "MS COCO",
        "type": "Dataset"
      },
      {
        "name": "ILSVRC",
        "type": "Dataset"
      },
      {
        "name": "COCO 2015",
        "type": "Dataset"
      },
      {
        "name": "object detection accuracy",
        "type": "Metric"
      },
      {
        "name": "PASCAL",
        "type": "Dataset"
      },
      {
        "name": "SUN",
        "type": "Dataset"
      },
      {
        "name": "Deformable Parts Model",
        "type": "AIModel"
      },
      {
        "name": "bounding box detection",
        "type": "Metric"
      },
      {
        "name": "segmentation detection",
        "type": "Metric"
      },
      {
        "name": "Inception",
        "type": "AIModel"
      },
      {
        "name": "GoogLeNet",
        "type": "AIModel"
      },
      {
        "name": "ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014)",
        "type": "Dataset"
      },
      {
        "name": "Batch Normalization",
        "type": "AIModel"
      },
      {
        "name": "top-5 validation error",
        "type": "Metric"
      },
      {
        "name": "test error",
        "type": "Metric"
      },
      {
        "name": "object category classification",
        "type": "Metric"
      },
      {
        "name": "object detection",
        "type": "Metric"
      },
      {
        "name": "human accuracy",
        "type": "Metric"
      },
      {
        "name": "fully convolutional networks",
        "type": "AIModel"
      },
      {
        "name": "AlexNet",
        "type": "AIModel"
      },
      {
        "name": "VGG net",
        "type": "AIModel"
      },
      {
        "name": "PASCAL VOC",
        "type": "Dataset"
      },
      {
        "name": "NYUDv2",
        "type": "Dataset"
      },
      {
        "name": "SIFT Flow",
        "type": "Dataset"
      },
      {
        "name": "mean IU",
        "type": "Metric"
      },
      {
        "name": "FLOT",
        "type": "AIModel"
      },
      {
        "name": "GTSRB",
        "type": "Dataset"
      },
      {
        "name": "KBTS",
        "type": "Dataset"
      },
      {
        "name": "CIFAR10",
        "type": "Dataset"
      },
      {
        "name": "EMNIST",
        "type": "Dataset"
      },
      {
        "name": "scalability",
        "type": "Metric"
      },
      {
        "name": "WarmGait",
        "type": "AIModel"
      },
      {
        "name": "Taylor Finite Difference (TFD)",
        "type": "AIModel"
      },
      {
        "name": "thermal array sensors",
        "type": "Dataset"
      },
      {
        "name": "average recognition accuracy",
        "type": "Metric"
      },
      {
        "name": "NPSSL",
        "type": "AIModel"
      },
      {
        "name": "Duke dataset",
        "type": "Dataset"
      },
      {
        "name": "Unsupervised Domain Adaptation",
        "type": "AIModel"
      },
      {
        "name": "Noise Perception Self-Paced Learning",
        "type": "AIModel"
      },
      {
        "name": "Uncertainty-Aware Siamese Network",
        "type": "AIModel"
      },
      {
        "name": "VOT2018",
        "type": "Dataset"
      },
      {
        "name": "VOT2019",
        "type": "Dataset"
      },
      {
        "name": "OTB100",
        "type": "Dataset"
      },
      {
        "name": "NFS",
        "type": "Dataset"
      },
      {
        "name": "UAV123",
        "type": "Dataset"
      },
      {
        "name": "LaSOT",
        "type": "Dataset"
      },
      {
        "name": "TrackingNet",
        "type": "Dataset"
      },
      {
        "name": "Got-10 k",
        "type": "Dataset"
      },
      {
        "name": "dual-space alignment and fusion framework",
        "type": "AIModel"
      },
      {
        "name": "DCM-Net",
        "type": "AIModel"
      },
      {
        "name": "AdveDiffNet",
        "type": "AIModel"
      },
      {
        "name": "stochastic variational inference and learning algorithm",
        "type": "AIModel"
      },
      {
        "name": "reparameterization of the variational lower bound",
        "type": "AIModel"
      },
      {
        "name": "approximate inference model",
        "type": "AIModel"
      },
      {
        "name": "i.i.d. datasets",
        "type": "Dataset"
      },
      {
        "name": "variational lower bound",
        "type": "Metric"
      },
      {
        "name": "Recurrent neural networks (RNNs)",
        "type": "AIModel"
      },
      {
        "name": "Connectionist Temporal Classification",
        "type": "AIModel"
      },
      {
        "name": "Long Short-term Memory RNN",
        "type": "AIModel"
      },
      {
        "name": "deep recurrent neural networks",
        "type": "AIModel"
      },
      {
        "name": "deep Long Short-term Memory RNNs",
        "type": "AIModel"
      },
      {
        "name": "deep feedforward networks",
        "type": "AIModel"
      },
      {
        "name": "TIMIT phoneme recognition benchmark",
        "type": "Dataset"
      },
      {
        "name": "test set error",
        "type": "Metric"
      },
      {
        "name": "feedforward neural network",
        "type": "AIModel"
      },
      {
        "name": "dropout",
        "type": "AIModel"
      },
      {
        "name": "small training set",
        "type": "Dataset"
      },
      {
        "name": "held-out test data",
        "type": "Dataset"
      },
      {
        "name": "speech recognition",
        "type": "Metric"
      },
      {
        "name": "object recognition",
        "type": "Metric"
      },
      {
        "name": "ADADELTA",
        "type": "AIModel"
      },
      {
        "name": "MNIST digit classification task",
        "type": "Dataset"
      },
      {
        "name": "large scale voice dataset",
        "type": "Dataset"
      },
      {
        "name": "Word Vectors",
        "type": "AIModel"
      },
      {
        "name": "Sentiment Analysis",
        "type": "Dataset"
      },
      {
        "name": "Long Short-term Memory recurrent neural networks",
        "type": "AIModel"
      },
      {
        "name": "text",
        "type": "Dataset"
      },
      {
        "name": "online handwriting",
        "type": "Dataset"
      },
      {
        "name": "PBD",
        "type": "AIModel"
      },
      {
        "name": "GAN",
        "type": "AIModel"
      },
      {
        "name": "DWD",
        "type": "AIModel"
      },
      {
        "name": "seven benchmarks",
        "type": "Dataset"
      },
      {
        "name": "reconstruction loss",
        "type": "Metric"
      },
      {
        "name": "AdamW",
        "type": "AIModel"
      },
      {
        "name": "face mask detection model",
        "type": "AIModel"
      },
      {
        "name": "Engram",
        "type": "AIModel"
      },
      {
        "name": "Mixture-of-Experts (MoE)",
        "type": "AIModel"
      },
      {
        "name": "MMLU",
        "type": "Dataset"
      },
      {
        "name": "CMMLU",
        "type": "Dataset"
      },
      {
        "name": "BBH",
        "type": "Dataset"
      },
      {
        "name": "ARC-Challenge",
        "type": "Dataset"
      },
      {
        "name": "HumanEval",
        "type": "Dataset"
      },
      {
        "name": "MATH",
        "type": "Dataset"
      },
      {
        "name": "Multi-Query NIAH",
        "type": "Dataset"
      },
      {
        "name": "Neural Hierarchical Interpolation for Time Series Forecasting (N-HiTS)",
        "type": "AIModel"
      },
      {
        "name": "Network-Based Expansion Analysis for Interpretable Time Series Forecasting (N-BEATS)",
        "type": "AIModel"
      },
      {
        "name": "two headwater streams in Georgia and North Carolina, USA",
        "type": "Dataset"
      },
      {
        "name": "Multi-Quantile Loss",
        "type": "Metric"
      },
      {
        "name": "95th percentile prediction uncertainty (95 PPU)",
        "type": "Metric"
      },
      {
        "name": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model",
        "type": "AIPaper"
      },
      {
        "name": "NASA-IBM geospatial foundation model",
        "type": "AIModel"
      },
      {
        "name": "harmonized Landsat and Sentinel-2 data",
        "type": "Dataset"
      },
      {
        "name": "LA-CBDNet",
        "type": "AIModel"
      },
      {
        "name": "benchmark approaches",
        "type": "AIModel"
      },
      {
        "name": "comparative methods",
        "type": "AIModel"
      },
      {
        "name": "one-bit received signals",
        "type": "Dataset"
      },
      {
        "name": "varying SNRs",
        "type": "Metric"
      },
      {
        "name": "different pilot lengths",
        "type": "Metric"
      },
      {
        "name": "number of users",
        "type": "Metric"
      },
      {
        "name": "variational quantum eigensolver (VQE)",
        "type": "AIModel"
      },
      {
        "name": "Anderson impurity model (AIM)",
        "type": "Dataset"
      },
      {
        "name": "density of states (DOS)",
        "type": "Metric"
      },
      {
        "name": "quantum-computed moment (QCM)",
        "type": "Metric"
      },
      {
        "name": "impurity Green's function",
        "type": "Metric"
      },
      {
        "name": "Deep Neural Networks",
        "type": "AIModel"
      },
      {
        "name": "Sentinel-1 backscatter data",
        "type": "Dataset"
      },
      {
        "name": "11 earthquake-induced widespread landslide events",
        "type": "Dataset"
      },
      {
        "name": "Haiti (2021) and Sumatra (2022) events",
        "type": "Dataset"
      },
      {
        "name": "3D U-Net",
        "type": "AIModel"
      },
      {
        "name": "NWP models",
        "type": "AIModel"
      },
      {
        "name": "ECMWF ensemble forecasting system",
        "type": "Dataset"
      },
      {
        "name": "PRISM data",
        "type": "Dataset"
      },
      {
        "name": "pattern correlation coefficient",
        "type": "Metric"
      },
      {
        "name": "MobileNets",
        "type": "AIModel"
      },
      {
        "name": "ensemble of models",
        "type": "AIModel"
      },
      {
        "name": "single model",
        "type": "AIModel"
      },
      {
        "name": "specialist models",
        "type": "AIModel"
      },
      {
        "name": "MNIST",
        "type": "Dataset"
      },
      {
        "name": "acoustic model",
        "type": "AIModel"
      },
      {
        "name": "Krizhevsky et al.",
        "type": "AIModel"
      },
      {
        "name": "Caltech-101",
        "type": "Dataset"
      },
      {
        "name": "Caltech-256",
        "type": "Dataset"
      },
      {
        "name": "ImageNet classification benchmark",
        "type": "Metric"
      },
      {
        "name": "state-of-the-art results",
        "type": "Metric"
      },
      {
        "name": "Hyper-Connections (HC)",
        "type": "AIModel"
      },
      {
        "name": "Manifold-Constrained Hyper-Connections (mHC)",
        "type": "AIModel"
      },
      {
        "name": "OGNet",
        "type": "AIModel"
      },
      {
        "name": "YOLO-OG",
        "type": "AIModel"
      },
      {
        "name": "Dish-10",
        "type": "Dataset"
      },
      {
        "name": "Dish-20",
        "type": "Dataset"
      },
      {
        "name": "mean Average Precision (mAP)",
        "type": "Metric"
      },
      {
        "name": "HybridBathNet",
        "type": "AIModel"
      },
      {
        "name": "UNet",
        "type": "AIModel"
      },
      {
        "name": "physical bathymetry network",
        "type": "AIModel"
      },
      {
        "name": "state-of-the-art methods",
        "type": "AIModel"
      },
      {
        "name": "Sentinel-2 multi-spectral imagery",
        "type": "Dataset"
      },
      {
        "name": "diverse island regions",
        "type": "Dataset"
      },
      {
        "name": "bathymetric inversion accuracy",
        "type": "Metric"
      },
      {
        "name": "generalization capability",
        "type": "Metric"
      },
      {
        "name": "spike memory transformer (SMT)",
        "type": "AIModel"
      },
      {
        "name": "Computation-Oriented Hierarchical Depression Detection Internet of Things (IoT) Framework",
        "type": "AIModel"
      },
      {
        "name": "classical deep learning methods",
        "type": "AIModel"
      },
      {
        "name": "D-Vlog dataset",
        "type": "Dataset"
      },
      {
        "name": "inference power consumption",
        "type": "Metric"
      },
      {
        "name": "AttnGPRNet",
        "type": "AIModel"
      },
      {
        "name": "multi-view dataset using 3D GPR scans from over 100 kilometers of urban roads",
        "type": "Dataset"
      },
      {
        "name": "mIoU",
        "type": "Metric"
      },
      {
        "name": "F1 score",
        "type": "Metric"
      },
      {
        "name": "Forensic-Chat",
        "type": "AIModel"
      },
      {
        "name": "ExplainFake-Bench",
        "type": "Dataset"
      },
      {
        "name": "multimodal large language models (MLLMs)",
        "type": "AIModel"
      },
      {
        "name": "generalization",
        "type": "Metric"
      },
      {
        "name": "explainability",
        "type": "Metric"
      },
      {
        "name": "DenseNet with Attention Mechanisms",
        "type": "AIModel"
      },
      {
        "name": "Date Fruit Image Dataset",
        "type": "Dataset"
      },
      {
        "name": "Classification Accuracy",
        "type": "Metric"
      },
      {
        "name": "ResNet50",
        "type": "AIModel"
      },
      {
        "name": "InceptionResNetV2",
        "type": "AIModel"
      },
      {
        "name": "DenseNet 201",
        "type": "AIModel"
      },
      {
        "name": "InceptionV3",
        "type": "AIModel"
      },
      {
        "name": "EfficientNetB0",
        "type": "AIModel"
      },
      {
        "name": "VGG16",
        "type": "AIModel"
      },
      {
        "name": "Local Interpretable Model-agnostic Explanations (LIME)",
        "type": "AIModel"
      },
      {
        "name": "rice leaf disease detection dataset",
        "type": "Dataset"
      },
      {
        "name": "classification accuracy",
        "type": "Metric"
      },
      {
        "name": "precision",
        "type": "Metric"
      },
      {
        "name": "recall",
        "type": "Metric"
      },
      {
        "name": "Intersection over Union (IoU)",
        "type": "Metric"
      },
      {
        "name": "Dice Similarity Coefficient (DSC)",
        "type": "Metric"
      },
      {
        "name": "overfitting ratio",
        "type": "Metric"
      },
      {
        "name": "improved you only look once version 10 small and integrated compression",
        "type": "AIModel"
      },
      {
        "name": "T5",
        "type": "AIModel"
      },
      {
        "name": "Colossal Clean Crawled Corpus",
        "type": "Dataset"
      },
      {
        "name": "summarization",
        "type": "Metric"
      },
      {
        "name": "question answering",
        "type": "Metric"
      },
      {
        "name": "text classification",
        "type": "Metric"
      },
      {
        "name": "Two Time-Scale Update Rule",
        "type": "AIModel"
      },
      {
        "name": "GANs",
        "type": "AIModel"
      },
      {
        "name": "KITTI",
        "type": "Dataset"
      },
      {
        "name": "3D detection and tracking metrics",
        "type": "Metric"
      },
      {
        "name": "classic modular pipeline",
        "type": "AIModel"
      },
      {
        "name": "end-to-end model trained via imitation learning",
        "type": "AIModel"
      },
      {
        "name": "end-to-end model trained via reinforcement learning",
        "type": "AIModel"
      },
      {
        "name": "metrics provided by CARLA",
        "type": "Metric"
      },
      {
        "name": "ControlNet",
        "type": "AIModel"
      },
      {
        "name": "Stable Diffusion",
        "type": "AIModel"
      },
      {
        "name": "small (<50k) and large (>1m) datasets",
        "type": "Dataset"
      },
      {
        "name": "Diffusion Transformers (DiTs)",
        "type": "AIModel"
      },
      {
        "name": "DiT-XL/2",
        "type": "AIModel"
      },
      {
        "name": "U-Net",
        "type": "AIModel"
      },
      {
        "name": "ImageNet 512x512",
        "type": "Dataset"
      },
      {
        "name": "ImageNet 256x256",
        "type": "Dataset"
      },
      {
        "name": "FID",
        "type": "Metric"
      },
      {
        "name": "Gflops",
        "type": "Metric"
      },
      {
        "name": "SDXL",
        "type": "AIModel"
      },
      {
        "name": "refinement model",
        "type": "AIModel"
      },
      {
        "name": "deep neural network",
        "type": "AIModel"
      },
      {
        "name": "simulated images",
        "type": "Dataset"
      },
      {
        "name": "real images",
        "type": "Dataset"
      },
      {
        "name": "object localization",
        "type": "Metric"
      },
      {
        "name": "object detector",
        "type": "AIModel"
      },
      {
        "name": "simulator with non-realistic random textures",
        "type": "Dataset"
      },
      {
        "name": "grasping in a cluttered environment",
        "type": "Metric"
      },
      {
        "name": "SYNTHIA",
        "type": "Dataset"
      },
      {
        "name": "Sora",
        "type": "AIModel"
      },
      {
        "name": "OmniNWM",
        "type": "AIModel"
      },
      {
        "name": "Existing models",
        "type": "AIModel"
      },
      {
        "name": "panoramic videos",
        "type": "Dataset"
      },
      {
        "name": "video generation",
        "type": "Metric"
      },
      {
        "name": "control accuracy",
        "type": "Metric"
      },
      {
        "name": "long-horizon stability",
        "type": "Metric"
      },
      {
        "name": "ConsisDrive",
        "type": "AIModel"
      },
      {
        "name": "Instance-Masked Attention",
        "type": "AIModel"
      },
      {
        "name": "Instance-Masked Loss",
        "type": "AIModel"
      },
      {
        "name": "driving video generation quality",
        "type": "Metric"
      },
      {
        "name": "UniDriveDreamer",
        "type": "AIModel"
      },
      {
        "name": "LiDAR-specific variational autoencoder (VAE)",
        "type": "AIModel"
      },
      {
        "name": "video VAE",
        "type": "AIModel"
      },
      {
        "name": "diffusion transformer",
        "type": "AIModel"
      },
      {
        "name": "Unified Latent Anchoring (ULA)",
        "type": "AIModel"
      },
      {
        "name": "previous state-of-the-art methods",
        "type": "AIModel"
      },
      {
        "name": "multi-camera video",
        "type": "Dataset"
      },
      {
        "name": "LiDAR sequence",
        "type": "Dataset"
      },
      {
        "name": "LiDAR generation",
        "type": "Metric"
      },
      {
        "name": "MAD-LTX",
        "type": "AIModel"
      },
      {
        "name": "SVD",
        "type": "AIModel"
      },
      {
        "name": "LTX",
        "type": "AIModel"
      },
      {
        "name": "video diffusion models",
        "type": "AIModel"
      },
      {
        "name": "driving world models",
        "type": "AIModel"
      },
      {
        "name": "autonomous driving",
        "type": "Dataset"
      },
      {
        "name": "driving domains",
        "type": "Dataset"
      },
      {
        "name": "structured motion",
        "type": "Metric"
      },
      {
        "name": "physical and social plausibility",
        "type": "Metric"
      },
      {
        "name": "compute",
        "type": "Metric"
      },
      {
        "name": "ViCoDR",
        "type": "AIModel"
      },
      {
        "name": "camera-controlled video diffusion models",
        "type": "AIModel"
      },
      {
        "name": "camera controlled image-to-video, text-to-video, and multi-view generation models",
        "type": "Dataset"
      },
      {
        "name": "Fully Convolutional Networks",
        "type": "AIModel"
      },
      {
        "name": "Recurrent neural networks",
        "type": "AIModel"
      },
      {
        "name": "KITTI dataset",
        "type": "Dataset"
      },
      {
        "name": "novel 3D detection and tracking metrics",
        "type": "Metric"
      },
      {
        "name": "lidar based detection and tracking",
        "type": "AIModel"
      },
      {
        "name": "image based detection and tracking",
        "type": "AIModel"
      },
      {
        "name": "domain randomization",
        "type": "AIModel"
      },
      {
        "name": "grasping",
        "type": "Metric"
      }
    ],
    "triples": [
      {
        "head": "Ashish Vaswani",
        "relation": "author_of",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "Noam Shazeer",
        "relation": "author_of",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "Niki Parmar",
        "relation": "author_of",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "Jakob Uszkoreit",
        "relation": "author_of",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "Llion Jones",
        "relation": "author_of",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "Aidan N. Gomez",
        "relation": "author_of",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "Lukasz Kaiser",
        "relation": "author_of",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "Illia Polosukhin",
        "relation": "author_of",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "Kaiming He",
        "relation": "author_of",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Xiangyu Zhang",
        "relation": "author_of",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Shaoqing Ren",
        "relation": "author_of",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Jian Sun",
        "relation": "author_of",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Diederik P. Kingma",
        "relation": "author_of",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "Jimmy Ba",
        "relation": "author_of",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "Sepp Hochreiter",
        "relation": "author_of",
        "tail": "Long Short-Term Memory"
      },
      {
        "head": "J. Schmidhuber",
        "relation": "author_of",
        "tail": "Long Short-Term Memory"
      },
      {
        "head": "Nitish Srivastava",
        "relation": "author_of",
        "tail": "Dropout: a simple way to prevent neural networks from overfitting"
      },
      {
        "head": "Geoffrey E. Hinton",
        "relation": "author_of",
        "tail": "Dropout: a simple way to prevent neural networks from overfitting"
      },
      {
        "head": "A. Krizhevsky",
        "relation": "author_of",
        "tail": "Dropout: a simple way to prevent neural networks from overfitting"
      },
      {
        "head": "I. Sutskever",
        "relation": "author_of",
        "tail": "Dropout: a simple way to prevent neural networks from overfitting"
      },
      {
        "head": "R. Salakhutdinov",
        "relation": "author_of",
        "tail": "Dropout: a simple way to prevent neural networks from overfitting"
      },
      {
        "head": "Christian Szegedy",
        "relation": "author_of",
        "tail": "Rethinking the Inception Architecture for Computer Vision"
      },
      {
        "head": "Vincent Vanhoucke",
        "relation": "author_of",
        "tail": "Rethinking the Inception Architecture for Computer Vision"
      },
      {
        "head": "Sergey Ioffe",
        "relation": "author_of",
        "tail": "Rethinking the Inception Architecture for Computer Vision"
      },
      {
        "head": "Jonathon Shlens",
        "relation": "author_of",
        "tail": "Rethinking the Inception Architecture for Computer Vision"
      },
      {
        "head": "Zbigniew Wojna",
        "relation": "author_of",
        "tail": "Rethinking the Inception Architecture for Computer Vision"
      },
      {
        "head": "Dzmitry Bahdanau",
        "relation": "author_of",
        "tail": "Neural Machine Translation by Jointly Learning to Align and Translate"
      },
      {
        "head": "Kyunghyun Cho",
        "relation": "author_of",
        "tail": "Neural Machine Translation by Jointly Learning to Align and Translate"
      },
      {
        "head": "Yoshua Bengio",
        "relation": "author_of",
        "tail": "Neural Machine Translation by Jointly Learning to Align and Translate"
      },
      {
        "head": "Kyunghyun Cho",
        "relation": "author_of",
        "tail": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"
      },
      {
        "head": "Bart van Merrienboer",
        "relation": "author_of",
        "tail": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"
      },
      {
        "head": "Caglar Gulcehre",
        "relation": "author_of",
        "tail": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"
      },
      {
        "head": "Dzmitry Bahdanau",
        "relation": "author_of",
        "tail": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"
      },
      {
        "head": "Fethi Bougares",
        "relation": "author_of",
        "tail": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"
      },
      {
        "head": "Holger Schwenk",
        "relation": "author_of",
        "tail": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"
      },
      {
        "head": "Yoshua Bengio",
        "relation": "author_of",
        "tail": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"
      },
      {
        "head": "Ilya Sutskever",
        "relation": "author_of",
        "tail": "Sequence to Sequence Learning with Neural Networks"
      },
      {
        "head": "Oriol Vinyals",
        "relation": "author_of",
        "tail": "Sequence to Sequence Learning with Neural Networks"
      },
      {
        "head": "Quoc V. Le",
        "relation": "author_of",
        "tail": "Sequence to Sequence Learning with Neural Networks"
      },
      {
        "head": "François Chollet",
        "relation": "author_of",
        "tail": "Xception: Deep Learning with Depthwise Separable Convolutions"
      },
      {
        "head": "Junyoung Chung",
        "relation": "author_of",
        "tail": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"
      },
      {
        "head": "Caglar Gulcehre",
        "relation": "author_of",
        "tail": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"
      },
      {
        "head": "KyungHyun Cho",
        "relation": "author_of",
        "tail": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"
      },
      {
        "head": "Yoshua Bengio",
        "relation": "author_of",
        "tail": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"
      },
      {
        "head": "Shaina Raza",
        "relation": "author_of",
        "tail": "A comprehensive review of recommender systems: Transitioning from theory to practice"
      },
      {
        "head": "Mizanur Rahman",
        "relation": "author_of",
        "tail": "A comprehensive review of recommender systems: Transitioning from theory to practice"
      },
      {
        "head": "Safiullah Kamawal",
        "relation": "author_of",
        "tail": "A comprehensive review of recommender systems: Transitioning from theory to practice"
      },
      {
        "head": "Armin Toroghi",
        "relation": "author_of",
        "tail": "A comprehensive review of recommender systems: Transitioning from theory to practice"
      },
      {
        "head": "Ananya Raval",
        "relation": "author_of",
        "tail": "A comprehensive review of recommender systems: Transitioning from theory to practice"
      },
      {
        "head": "F. Navah",
        "relation": "author_of",
        "tail": "A comprehensive review of recommender systems: Transitioning from theory to practice"
      },
      {
        "head": "Amirmohammad Kazemeini",
        "relation": "author_of",
        "tail": "A comprehensive review of recommender systems: Transitioning from theory to practice"
      },
      {
        "head": "Zhuoran Yang",
        "relation": "author_of",
        "tail": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation"
      },
      {
        "head": "Xi Guo",
        "relation": "author_of",
        "tail": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation"
      },
      {
        "head": "Chenjing Ding",
        "relation": "author_of",
        "tail": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation"
      },
      {
        "head": "Chiyu Wang",
        "relation": "author_of",
        "tail": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation"
      },
      {
        "head": "Wei Wu",
        "relation": "author_of",
        "tail": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation"
      },
      {
        "head": "Yanyong Zhang",
        "relation": "author_of",
        "tail": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation"
      },
      {
        "head": "Zisheng Wang",
        "relation": "author_of",
        "tail": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection"
      },
      {
        "head": "Junjie Chen",
        "relation": "author_of",
        "tail": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection"
      },
      {
        "head": "Chisen Wang",
        "relation": "author_of",
        "tail": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection"
      },
      {
        "head": "Cong Peng",
        "relation": "author_of",
        "tail": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection"
      },
      {
        "head": "Jianping Xuan",
        "relation": "author_of",
        "tail": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection"
      },
      {
        "head": "Tielin Shi",
        "relation": "author_of",
        "tail": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection"
      },
      {
        "head": "Ming J. Zuo",
        "relation": "author_of",
        "tail": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection"
      },
      {
        "head": "Jinghuan Zhang",
        "relation": "author_of",
        "tail": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation"
      },
      {
        "head": "Wang Chen",
        "relation": "author_of",
        "tail": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation"
      },
      {
        "head": "Jian Zhang",
        "relation": "author_of",
        "tail": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation"
      },
      {
        "head": "Manlin Zhang",
        "relation": "author_of",
        "tail": "DiffusionEngine: Diffusion model is scalable data engine for object detection"
      },
      {
        "head": "Jie Wu",
        "relation": "author_of",
        "tail": "DiffusionEngine: Diffusion model is scalable data engine for object detection"
      },
      {
        "head": "Yuxi Ren",
        "relation": "author_of",
        "tail": "DiffusionEngine: Diffusion model is scalable data engine for object detection"
      },
      {
        "head": "Jiahong Yang",
        "relation": "author_of",
        "tail": "DiffusionEngine: Diffusion model is scalable data engine for object detection"
      },
      {
        "head": "Ming Li",
        "relation": "author_of",
        "tail": "DiffusionEngine: Diffusion model is scalable data engine for object detection"
      },
      {
        "head": "Andy J. Ma",
        "relation": "author_of",
        "tail": "DiffusionEngine: Diffusion model is scalable data engine for object detection"
      },
      {
        "head": "Abdullah Al Ahad Khan",
        "relation": "author_of",
        "tail": "Enhanced Transformer-BiLSTM Deep Learning Framework for Day-Ahead Energy Price Forecasting"
      },
      {
        "head": "Md Habib Ullah",
        "relation": "author_of",
        "tail": "Enhanced Transformer-BiLSTM Deep Learning Framework for Day-Ahead Energy Price Forecasting"
      },
      {
        "head": "Ruchira Tabassum",
        "relation": "author_of",
        "tail": "Enhanced Transformer-BiLSTM Deep Learning Framework for Day-Ahead Energy Price Forecasting"
      },
      {
        "head": "Md Faisal Kabir",
        "relation": "author_of",
        "tail": "Enhanced Transformer-BiLSTM Deep Learning Framework for Day-Ahead Energy Price Forecasting"
      },
      {
        "head": "Mojtaba Moradi‐Sepahvand",
        "relation": "author_of",
        "tail": "Conditional Generative Adversarial Network-based framework for multi-feature uncertainty modeling in energy systems"
      },
      {
        "head": "Junjue Wang",
        "relation": "author_of",
        "tail": "CityVLM: Towards sustainable urban development via multi-view coordinated vision–language model"
      },
      {
        "head": "Weihao Xuan",
        "relation": "author_of",
        "tail": "CityVLM: Towards sustainable urban development via multi-view coordinated vision–language model"
      },
      {
        "head": "Heli Qi",
        "relation": "author_of",
        "tail": "CityVLM: Towards sustainable urban development via multi-view coordinated vision–language model"
      },
      {
        "head": "Zihang Chen",
        "relation": "author_of",
        "tail": "CityVLM: Towards sustainable urban development via multi-view coordinated vision–language model"
      },
      {
        "head": "Hongruixuan Chen",
        "relation": "author_of",
        "tail": "CityVLM: Towards sustainable urban development via multi-view coordinated vision–language model"
      },
      {
        "head": "Zhuo Zheng",
        "relation": "author_of",
        "tail": "CityVLM: Towards sustainable urban development via multi-view coordinated vision–language model"
      },
      {
        "head": "Junshi Xia",
        "relation": "author_of",
        "tail": "CityVLM: Towards sustainable urban development via multi-view coordinated vision–language model"
      },
      {
        "head": "Yanfei Zhong",
        "relation": "author_of",
        "tail": "CityVLM: Towards sustainable urban development via multi-view coordinated vision–language model"
      },
      {
        "head": "Naoto Yokoya",
        "relation": "author_of",
        "tail": "CityVLM: Towards sustainable urban development via multi-view coordinated vision–language model"
      },
      {
        "head": "Lin Liang",
        "relation": "author_of",
        "tail": "FullPerception: Network-Level Collaborative Perception for Eliminating Vehicular Blind Spots"
      },
      {
        "head": "Guiyang Luo",
        "relation": "author_of",
        "tail": "FullPerception: Network-Level Collaborative Perception for Eliminating Vehicular Blind Spots"
      },
      {
        "head": "Yijing Lin",
        "relation": "author_of",
        "tail": "FullPerception: Network-Level Collaborative Perception for Eliminating Vehicular Blind Spots"
      },
      {
        "head": "Lei Deng",
        "relation": "author_of",
        "tail": "FullPerception: Network-Level Collaborative Perception for Eliminating Vehicular Blind Spots"
      },
      {
        "head": "Nan Cheng",
        "relation": "author_of",
        "tail": "FullPerception: Network-Level Collaborative Perception for Eliminating Vehicular Blind Spots"
      },
      {
        "head": "Quan Yuan",
        "relation": "author_of",
        "tail": "FullPerception: Network-Level Collaborative Perception for Eliminating Vehicular Blind Spots"
      },
      {
        "head": "Jinglin Li",
        "relation": "author_of",
        "tail": "FullPerception: Network-Level Collaborative Perception for Eliminating Vehicular Blind Spots"
      },
      {
        "head": "Dusit Niyato",
        "relation": "author_of",
        "tail": "FullPerception: Network-Level Collaborative Perception for Eliminating Vehicular Blind Spots"
      },
      {
        "head": "Gjorgjina Cenikj",
        "relation": "author_of",
        "tail": "A survey of features used for representing black-box single-objective continuous optimization"
      },
      {
        "head": "Ana Nikolikj",
        "relation": "author_of",
        "tail": "A survey of features used for representing black-box single-objective continuous optimization"
      },
      {
        "head": "G. Petelin",
        "relation": "author_of",
        "tail": "A survey of features used for representing black-box single-objective continuous optimization"
      },
      {
        "head": "Niki van Stein",
        "relation": "author_of",
        "tail": "A survey of features used for representing black-box single-objective continuous optimization"
      },
      {
        "head": "Carola Doerr",
        "relation": "author_of",
        "tail": "A survey of features used for representing black-box single-objective continuous optimization"
      },
      {
        "head": "T. Eftimov",
        "relation": "author_of",
        "tail": "A survey of features used for representing black-box single-objective continuous optimization"
      },
      {
        "head": "A. Krizhevsky",
        "relation": "author_of",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "I. Sutskever",
        "relation": "author_of",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "Geoffrey E. Hinton",
        "relation": "author_of",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "Karen Simonyan",
        "relation": "author_of",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "Andrew Zisserman",
        "relation": "author_of",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "P. Cochat",
        "relation": "author_of",
        "tail": "Et al"
      },
      {
        "head": "L. Vaucoret",
        "relation": "author_of",
        "tail": "Et al"
      },
      {
        "head": "J. Sarles",
        "relation": "author_of",
        "tail": "Et al"
      },
      {
        "head": "Shaoqing Ren",
        "relation": "author_of",
        "tail": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
      },
      {
        "head": "Kaiming He",
        "relation": "author_of",
        "tail": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
      },
      {
        "head": "Ross Girshick",
        "relation": "author_of",
        "tail": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
      },
      {
        "head": "Jian Sun",
        "relation": "author_of",
        "tail": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
      },
      {
        "head": "Tsung-Yi Lin",
        "relation": "author_of",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Michael Maire",
        "relation": "author_of",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Serge Belongie",
        "relation": "author_of",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Lubomir Bourdev",
        "relation": "author_of",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Ross Girshick",
        "relation": "author_of",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "James Hays",
        "relation": "author_of",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Pietro Perona",
        "relation": "author_of",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Deva Ramanan",
        "relation": "author_of",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "C. Lawrence Zitnick",
        "relation": "author_of",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Piotr Dollár",
        "relation": "author_of",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Christian Szegedy",
        "relation": "author_of",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "Wei Liu",
        "relation": "author_of",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "Yangqing Jia",
        "relation": "author_of",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "Pierre Sermanet",
        "relation": "author_of",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "Scott Reed",
        "relation": "author_of",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "Dragomir Anguelov",
        "relation": "author_of",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "Dumitru Erhan",
        "relation": "author_of",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "Vincent Vanhoucke",
        "relation": "author_of",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "Andrew Rabinovich",
        "relation": "author_of",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "Sergey Ioffe",
        "relation": "author_of",
        "tail": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
      },
      {
        "head": "Christian Szegedy",
        "relation": "author_of",
        "tail": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
      },
      {
        "head": "Olga Russakovsky",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Jia Deng",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Hao Su",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Jonathan Krause",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Sanjeev Satheesh",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Sean Ma",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Zhiheng Huang",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Andrej Karpathy",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Aditya Khosla",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Michael Bernstein",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Alexander C. Berg",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Li Fei-Fei",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Jonathan Long",
        "relation": "author_of",
        "tail": "Fully convolutional networks for semantic segmentation"
      },
      {
        "head": "Evan Shelhamer",
        "relation": "author_of",
        "tail": "Fully convolutional networks for semantic segmentation"
      },
      {
        "head": "Trevor Darrell",
        "relation": "author_of",
        "tail": "Fully convolutional networks for semantic segmentation"
      },
      {
        "head": "Yuqi Cheng",
        "relation": "author_of",
        "tail": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects"
      },
      {
        "head": "Yunkang Cao",
        "relation": "author_of",
        "tail": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects"
      },
      {
        "head": "Haiming Yao",
        "relation": "author_of",
        "tail": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects"
      },
      {
        "head": "Wei Luo",
        "relation": "author_of",
        "tail": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects"
      },
      {
        "head": "Cheng Jiang",
        "relation": "author_of",
        "tail": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects"
      },
      {
        "head": "Hui Zhang",
        "relation": "author_of",
        "tail": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects"
      },
      {
        "head": "Weiming Shen",
        "relation": "author_of",
        "tail": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects"
      },
      {
        "head": "Naveen Kumar Srinivasa",
        "relation": "author_of",
        "tail": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning"
      },
      {
        "head": "Ajeet Rao Chalamala",
        "relation": "author_of",
        "tail": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning"
      },
      {
        "head": "Kumar Singh",
        "relation": "author_of",
        "tail": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning"
      },
      {
        "head": "Ieee Krishna Mohan Senior Member",
        "relation": "author_of",
        "tail": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning"
      },
      {
        "head": "K. Naveen",
        "relation": "author_of",
        "tail": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning"
      },
      {
        "head": "Srinivasa Rao",
        "relation": "author_of",
        "tail": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning"
      },
      {
        "head": "Ajeet Kumar Singh",
        "relation": "author_of",
        "tail": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning"
      },
      {
        "head": "Hongbo Jiang",
        "relation": "author_of",
        "tail": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID"
      },
      {
        "head": "Lei Ye",
        "relation": "author_of",
        "tail": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID"
      },
      {
        "head": "Jingyang Hu",
        "relation": "author_of",
        "tail": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID"
      },
      {
        "head": "Xiaotian Chen",
        "relation": "author_of",
        "tail": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID"
      },
      {
        "head": "Siyu Chen",
        "relation": "author_of",
        "tail": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID"
      },
      {
        "head": "Wei Zhang",
        "relation": "author_of",
        "tail": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID"
      },
      {
        "head": "Kehua Yang",
        "relation": "author_of",
        "tail": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID"
      },
      {
        "head": "Jingya Wang",
        "relation": "author_of",
        "tail": "Noise Perception Self-Supervised Learning for Unsupervised Person Re-Identification"
      },
      {
        "head": "Jianfeng Wen",
        "relation": "author_of",
        "tail": "Noise Perception Self-Supervised Learning for Unsupervised Person Re-Identification"
      },
      {
        "head": "Weiping Ding",
        "relation": "author_of",
        "tail": "Noise Perception Self-Supervised Learning for Unsupervised Person Re-Identification"
      },
      {
        "head": "Chunlin Yu",
        "relation": "author_of",
        "tail": "Noise Perception Self-Supervised Learning for Unsupervised Person Re-Identification"
      },
      {
        "head": "Xiatian Zhu",
        "relation": "author_of",
        "tail": "Noise Perception Self-Supervised Learning for Unsupervised Person Re-Identification"
      },
      {
        "head": "Zhiyong Wang",
        "relation": "author_of",
        "tail": "Noise Perception Self-Supervised Learning for Unsupervised Person Re-Identification"
      },
      {
        "head": "Jie Ma",
        "relation": "author_of",
        "tail": "Towards Reliable Tracking: An Uncertainty-Aware Siamese Tracker"
      },
      {
        "head": "Xiangyuan Lan",
        "relation": "author_of",
        "tail": "Towards Reliable Tracking: An Uncertainty-Aware Siamese Tracker"
      },
      {
        "head": "Qihua Liang",
        "relation": "author_of",
        "tail": "Towards Reliable Tracking: An Uncertainty-Aware Siamese Tracker"
      },
      {
        "head": "Guorong Li",
        "relation": "author_of",
        "tail": "Towards Reliable Tracking: An Uncertainty-Aware Siamese Tracker"
      },
      {
        "head": "Zhiyi Mo",
        "relation": "author_of",
        "tail": "Towards Reliable Tracking: An Uncertainty-Aware Siamese Tracker"
      },
      {
        "head": "Bineng Zhong",
        "relation": "author_of",
        "tail": "Towards Reliable Tracking: An Uncertainty-Aware Siamese Tracker"
      },
      {
        "head": "Yulin Lei",
        "relation": "author_of",
        "tail": "Bridging the gap, not forcing the tie: dual-space alignment and fusion framework for toxic memes detection"
      },
      {
        "head": "Jin Yang",
        "relation": "author_of",
        "tail": "Bridging the gap, not forcing the tie: dual-space alignment and fusion framework for toxic memes detection"
      },
      {
        "head": "Huijia Liang",
        "relation": "author_of",
        "tail": "Bridging the gap, not forcing the tie: dual-space alignment and fusion framework for toxic memes detection"
      },
      {
        "head": "Tianrui Li",
        "relation": "author_of",
        "tail": "Bridging the gap, not forcing the tie: dual-space alignment and fusion framework for toxic memes detection"
      },
      {
        "head": "Ning Pang",
        "relation": "author_of",
        "tail": "Perturb and restore: Efficient category revocation in federated unlearning"
      },
      {
        "head": "Zou Li",
        "relation": "author_of",
        "tail": "Perturb and restore: Efficient category revocation in federated unlearning"
      },
      {
        "head": "Pengcheng Wan",
        "relation": "author_of",
        "tail": "Perturb and restore: Efficient category revocation in federated unlearning"
      },
      {
        "head": "Hongchao Wu",
        "relation": "author_of",
        "tail": "Perturb and restore: Efficient category revocation in federated unlearning"
      },
      {
        "head": "Yuchen Bing",
        "relation": "author_of",
        "tail": "Perturb and restore: Efficient category revocation in federated unlearning"
      },
      {
        "head": "Xiang Zhao",
        "relation": "author_of",
        "tail": "Perturb and restore: Efficient category revocation in federated unlearning"
      },
      {
        "head": "Yanling Liu",
        "relation": "author_of",
        "tail": "DCM-Net: A novel dual-branch CNN-Mamba cross-layer feature fusion network for medical image segmentation"
      },
      {
        "head": "Hongmin Deng",
        "relation": "author_of",
        "tail": "DCM-Net: A novel dual-branch CNN-Mamba cross-layer feature fusion network for medical image segmentation"
      },
      {
        "head": "Jinghao Fu",
        "relation": "author_of",
        "tail": "DCM-Net: A novel dual-branch CNN-Mamba cross-layer feature fusion network for medical image segmentation"
      },
      {
        "head": "Yu Fu",
        "relation": "author_of",
        "tail": "AdveDiffNet: adversarial diffusion network for unbalanced melanoma diagnosis"
      },
      {
        "head": "Chao Liu",
        "relation": "author_of",
        "tail": "AdveDiffNet: adversarial diffusion network for unbalanced melanoma diagnosis"
      },
      {
        "head": "Shaoqiang Wang",
        "relation": "author_of",
        "tail": "AdveDiffNet: adversarial diffusion network for unbalanced melanoma diagnosis"
      },
      {
        "head": "Hui Xia",
        "relation": "author_of",
        "tail": "AdveDiffNet: adversarial diffusion network for unbalanced melanoma diagnosis"
      },
      {
        "head": "Diederik P Kingma",
        "relation": "author_of",
        "tail": "Auto-Encoding Variational Bayes"
      },
      {
        "head": "Max Welling",
        "relation": "author_of",
        "tail": "Auto-Encoding Variational Bayes"
      },
      {
        "head": "Geoffrey E. Hinton",
        "relation": "author_of",
        "tail": "Supporting Online Material for Reducing the Dimensionality of Data with Neural Networks"
      },
      {
        "head": "R. Salakhutdinov",
        "relation": "author_of",
        "tail": "Supporting Online Material for Reducing the Dimensionality of Data with Neural Networks"
      },
      {
        "head": "John C. Duchi",
        "relation": "author_of",
        "tail": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"
      },
      {
        "head": "Elad Hazan",
        "relation": "author_of",
        "tail": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"
      },
      {
        "head": "Y. Singer",
        "relation": "author_of",
        "tail": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"
      },
      {
        "head": "Alex Graves",
        "relation": "author_of",
        "tail": "Speech recognition with deep recurrent neural networks"
      },
      {
        "head": "Abdel-rahman Mohamed",
        "relation": "author_of",
        "tail": "Speech recognition with deep recurrent neural networks"
      },
      {
        "head": "Geoffrey Hinton",
        "relation": "author_of",
        "tail": "Speech recognition with deep recurrent neural networks"
      },
      {
        "head": "Geoffrey E. Hinton",
        "relation": "author_of",
        "tail": "Improving neural networks by preventing co-adaptation of feature detectors"
      },
      {
        "head": "Nitish Srivastava",
        "relation": "author_of",
        "tail": "Improving neural networks by preventing co-adaptation of feature detectors"
      },
      {
        "head": "Alex Krizhevsky",
        "relation": "author_of",
        "tail": "Improving neural networks by preventing co-adaptation of feature detectors"
      },
      {
        "head": "Ilya Sutskever",
        "relation": "author_of",
        "tail": "Improving neural networks by preventing co-adaptation of feature detectors"
      },
      {
        "head": "Ruslan R. Salakhutdinov",
        "relation": "author_of",
        "tail": "Improving neural networks by preventing co-adaptation of feature detectors"
      },
      {
        "head": "Matthew D. Zeiler",
        "relation": "author_of",
        "tail": "ADADELTA: An Adaptive Learning Rate Method"
      },
      {
        "head": "Andrew L. Maas",
        "relation": "author_of",
        "tail": "Learning Word Vectors for Sentiment Analysis"
      },
      {
        "head": "Raymond E. Daly",
        "relation": "author_of",
        "tail": "Learning Word Vectors for Sentiment Analysis"
      },
      {
        "head": "Peter T. Pham",
        "relation": "author_of",
        "tail": "Learning Word Vectors for Sentiment Analysis"
      },
      {
        "head": "Dan Huang",
        "relation": "author_of",
        "tail": "Learning Word Vectors for Sentiment Analysis"
      },
      {
        "head": "A. Ng",
        "relation": "author_of",
        "tail": "Learning Word Vectors for Sentiment Analysis"
      },
      {
        "head": "Christopher Potts",
        "relation": "author_of",
        "tail": "Learning Word Vectors for Sentiment Analysis"
      },
      {
        "head": "I. Sutskever",
        "relation": "author_of",
        "tail": "On the importance of initialization and momentum in deep learning"
      },
      {
        "head": "James Martens",
        "relation": "author_of",
        "tail": "On the importance of initialization and momentum in deep learning"
      },
      {
        "head": "George E. Dahl",
        "relation": "author_of",
        "tail": "On the importance of initialization and momentum in deep learning"
      },
      {
        "head": "Geoffrey E. Hinton",
        "relation": "author_of",
        "tail": "On the importance of initialization and momentum in deep learning"
      },
      {
        "head": "Alex Graves",
        "relation": "author_of",
        "tail": "Generating Sequences With Recurrent Neural Networks"
      },
      {
        "head": "Nian Wang",
        "relation": "author_of",
        "tail": "Weakly Supervised Image Dehazing via Physics-Based Decomposition"
      },
      {
        "head": "Zhigao Cui",
        "relation": "author_of",
        "tail": "Weakly Supervised Image Dehazing via Physics-Based Decomposition"
      },
      {
        "head": "Yanzhao Su",
        "relation": "author_of",
        "tail": "Weakly Supervised Image Dehazing via Physics-Based Decomposition"
      },
      {
        "head": "Yunwei Lan",
        "relation": "author_of",
        "tail": "Weakly Supervised Image Dehazing via Physics-Based Decomposition"
      },
      {
        "head": "Yuanliang Xue",
        "relation": "author_of",
        "tail": "Weakly Supervised Image Dehazing via Physics-Based Decomposition"
      },
      {
        "head": "Cong Zhang",
        "relation": "author_of",
        "tail": "Weakly Supervised Image Dehazing via Physics-Based Decomposition"
      },
      {
        "head": "Aihua Li",
        "relation": "author_of",
        "tail": "Weakly Supervised Image Dehazing via Physics-Based Decomposition"
      },
      {
        "head": "Leong Kah Meng",
        "relation": "author_of",
        "tail": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer"
      },
      {
        "head": "Ho Hooi Yi",
        "relation": "author_of",
        "tail": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer"
      },
      {
        "head": "Ng Bo Wei",
        "relation": "author_of",
        "tail": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer"
      },
      {
        "head": "Lim Jia Xin",
        "relation": "author_of",
        "tail": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer"
      },
      {
        "head": "Zailan Arabee Abdul Salam",
        "relation": "author_of",
        "tail": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer"
      },
      {
        "head": "Xin Cheng",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Wangding Zeng",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Damai Dai",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Qinyu Chen",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Bingxuan Wang",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Zhenda Xie",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Kezhao Huang",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Xingkai Yu",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Zhewen Hao",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Yukun Li",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Han Zhang",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Huishuai Zhang",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Dongyan Zhao",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Wenfeng Liang",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Mostafa Saberian",
        "relation": "author_of",
        "tail": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction"
      },
      {
        "head": "Vidya Samadi",
        "relation": "author_of",
        "tail": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction"
      },
      {
        "head": "Ioana Popescu",
        "relation": "author_of",
        "tail": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction"
      },
      {
        "head": "Husheng Fang",
        "relation": "author_of",
        "tail": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model"
      },
      {
        "head": "Shunlin Liang",
        "relation": "author_of",
        "tail": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model"
      },
      {
        "head": "Wenyuan Li",
        "relation": "author_of",
        "tail": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model"
      },
      {
        "head": "Yongzhe Chen",
        "relation": "author_of",
        "tail": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model"
      },
      {
        "head": "Han Ma",
        "relation": "author_of",
        "tail": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model"
      },
      {
        "head": "Jianglei Xu",
        "relation": "author_of",
        "tail": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model"
      },
      {
        "head": "Yichuan Ma",
        "relation": "author_of",
        "tail": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model"
      },
      {
        "head": "Tao He",
        "relation": "author_of",
        "tail": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model"
      },
      {
        "head": "Feng Tian",
        "relation": "author_of",
        "tail": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model"
      },
      {
        "head": "Fengjiao Zhang",
        "relation": "author_of",
        "tail": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model"
      },
      {
        "head": "Hui Liang",
        "relation": "author_of",
        "tail": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model"
      },
      {
        "head": "Bin Chen",
        "relation": "author_of",
        "tail": "Unveiling cross-modal consistency: Taming inter- and intra-modal noise for robust multi-modal knowledge graph completion"
      },
      {
        "head": "Hanting Shen",
        "relation": "author_of",
        "tail": "Unveiling cross-modal consistency: Taming inter- and intra-modal noise for robust multi-modal knowledge graph completion"
      },
      {
        "head": "Zhangtao Cheng",
        "relation": "author_of",
        "tail": "Unveiling cross-modal consistency: Taming inter- and intra-modal noise for robust multi-modal knowledge graph completion"
      },
      {
        "head": "Xueting Liu",
        "relation": "author_of",
        "tail": "Unveiling cross-modal consistency: Taming inter- and intra-modal noise for robust multi-modal knowledge graph completion"
      },
      {
        "head": "Ting Zhong",
        "relation": "author_of",
        "tail": "Unveiling cross-modal consistency: Taming inter- and intra-modal noise for robust multi-modal knowledge graph completion"
      },
      {
        "head": "Fan Zhou",
        "relation": "author_of",
        "tail": "Unveiling cross-modal consistency: Taming inter- and intra-modal noise for robust multi-modal knowledge graph completion"
      },
      {
        "head": "I. Helmy",
        "relation": "author_of",
        "tail": "Low-Resolution Massive MIMO Channel Estimation With LSTM Attention-Based CBDNet"
      },
      {
        "head": "Wooyeol Choi",
        "relation": "author_of",
        "tail": "Low-Resolution Massive MIMO Channel Estimation With LSTM Attention-Based CBDNet"
      },
      {
        "head": "Mariia Karabin",
        "relation": "author_of",
        "tail": "Quantum solver for single-impurity Anderson models with particle-hole symmetry"
      },
      {
        "head": "Tanvir Sohail",
        "relation": "author_of",
        "tail": "Quantum solver for single-impurity Anderson models with particle-hole symmetry"
      },
      {
        "head": "Dmytro Bykov",
        "relation": "author_of",
        "tail": "Quantum solver for single-impurity Anderson models with particle-hole symmetry"
      },
      {
        "head": "Eduardo Antonio Coello Pérez",
        "relation": "author_of",
        "tail": "Quantum solver for single-impurity Anderson models with particle-hole symmetry"
      },
      {
        "head": "Swarnava Ghosh",
        "relation": "author_of",
        "tail": "Quantum solver for single-impurity Anderson models with particle-hole symmetry"
      },
      {
        "head": "Murali Gopalakrishnan Meena",
        "relation": "author_of",
        "tail": "Quantum solver for single-impurity Anderson models with particle-hole symmetry"
      },
      {
        "head": "Seongmin Kim",
        "relation": "author_of",
        "tail": "Quantum solver for single-impurity Anderson models with particle-hole symmetry"
      },
      {
        "head": "Amir Shehata",
        "relation": "author_of",
        "tail": "Quantum solver for single-impurity Anderson models with particle-hole symmetry"
      },
      {
        "head": "In-Saeng Suh",
        "relation": "author_of",
        "tail": "Quantum solver for single-impurity Anderson models with particle-hole symmetry"
      },
      {
        "head": "Hanna Terletska",
        "relation": "author_of",
        "tail": "Quantum solver for single-impurity Anderson models with particle-hole symmetry"
      },
      {
        "head": "Markus Eisenbach",
        "relation": "author_of",
        "tail": "Quantum solver for single-impurity Anderson models with particle-hole symmetry"
      },
      {
        "head": "Lorenzo Nava",
        "relation": "author_of",
        "tail": "Sentinel-1 SAR-based globally distributed co-seismic landslide detection by deep neural networks"
      },
      {
        "head": "A. Mondini",
        "relation": "author_of",
        "tail": "Sentinel-1 SAR-based globally distributed co-seismic landslide detection by deep neural networks"
      },
      {
        "head": "Kushanav Bhuyan",
        "relation": "author_of",
        "tail": "Sentinel-1 SAR-based globally distributed co-seismic landslide detection by deep neural networks"
      },
      {
        "head": "Chengyong Fang",
        "relation": "author_of",
        "tail": "Sentinel-1 SAR-based globally distributed co-seismic landslide detection by deep neural networks"
      },
      {
        "head": "Oriol Monserrat",
        "relation": "author_of",
        "tail": "Sentinel-1 SAR-based globally distributed co-seismic landslide detection by deep neural networks"
      },
      {
        "head": "A. Novellino",
        "relation": "author_of",
        "tail": "Sentinel-1 SAR-based globally distributed co-seismic landslide detection by deep neural networks"
      },
      {
        "head": "Filippo Catani",
        "relation": "author_of",
        "tail": "Sentinel-1 SAR-based globally distributed co-seismic landslide detection by deep neural networks"
      },
      {
        "head": "Jihun Ryu",
        "relation": "author_of",
        "tail": "Increasing resolution and accuracy in sub-seasonal forecasting through 3D U-Net: the western US"
      },
      {
        "head": "Hisu Kim",
        "relation": "author_of",
        "tail": "Increasing resolution and accuracy in sub-seasonal forecasting through 3D U-Net: the western US"
      },
      {
        "head": "Simon Wang",
        "relation": "author_of",
        "tail": "Increasing resolution and accuracy in sub-seasonal forecasting through 3D U-Net: the western US"
      },
      {
        "head": "Jin-Ho Yoon",
        "relation": "author_of",
        "tail": "Increasing resolution and accuracy in sub-seasonal forecasting through 3D U-Net: the western US"
      },
      {
        "head": "Andrew G. Howard",
        "relation": "author_of",
        "tail": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
      },
      {
        "head": "Menglong Zhu",
        "relation": "author_of",
        "tail": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
      },
      {
        "head": "Bo Chen",
        "relation": "author_of",
        "tail": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
      },
      {
        "head": "Dmitry Kalenichenko",
        "relation": "author_of",
        "tail": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
      },
      {
        "head": "Weijun Wang",
        "relation": "author_of",
        "tail": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
      },
      {
        "head": "Tobias Weyand",
        "relation": "author_of",
        "tail": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
      },
      {
        "head": "Marco Andreetto",
        "relation": "author_of",
        "tail": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
      },
      {
        "head": "Hartwig Adam",
        "relation": "author_of",
        "tail": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
      },
      {
        "head": "Geoffrey Hinton",
        "relation": "author_of",
        "tail": "Distilling the Knowledge in a Neural Network"
      },
      {
        "head": "Oriol Vinyals",
        "relation": "author_of",
        "tail": "Distilling the Knowledge in a Neural Network"
      },
      {
        "head": "Jeff Dean",
        "relation": "author_of",
        "tail": "Distilling the Knowledge in a Neural Network"
      },
      {
        "head": "Matthew D Zeiler",
        "relation": "author_of",
        "tail": "Visualizing and Understanding Convolutional Networks"
      },
      {
        "head": "Rob Fergus",
        "relation": "author_of",
        "tail": "Visualizing and Understanding Convolutional Networks"
      },
      {
        "head": "Zhenda Xie",
        "relation": "author_of",
        "tail": "mHC: Manifold-Constrained Hyper-Connections"
      },
      {
        "head": "Yixuan Wei",
        "relation": "author_of",
        "tail": "mHC: Manifold-Constrained Hyper-Connections"
      },
      {
        "head": "Huanqi Cao",
        "relation": "author_of",
        "tail": "mHC: Manifold-Constrained Hyper-Connections"
      },
      {
        "head": "Chenggang Zhao",
        "relation": "author_of",
        "tail": "mHC: Manifold-Constrained Hyper-Connections"
      },
      {
        "head": "Chengqi Deng",
        "relation": "author_of",
        "tail": "mHC: Manifold-Constrained Hyper-Connections"
      },
      {
        "head": "Jiashi Li",
        "relation": "author_of",
        "tail": "mHC: Manifold-Constrained Hyper-Connections"
      },
      {
        "head": "Damai Dai",
        "relation": "author_of",
        "tail": "mHC: Manifold-Constrained Hyper-Connections"
      },
      {
        "head": "Huazuo Gao",
        "relation": "author_of",
        "tail": "mHC: Manifold-Constrained Hyper-Connections"
      },
      {
        "head": "Jiang Chang",
        "relation": "author_of",
        "tail": "mHC: Manifold-Constrained Hyper-Connections"
      },
      {
        "head": "Kuai Yu",
        "relation": "author_of",
        "tail": "mHC: Manifold-Constrained Hyper-Connections"
      },
      {
        "head": "Liang Zhao",
        "relation": "author_of",
        "tail": "mHC: Manifold-Constrained Hyper-Connections"
      },
      {
        "head": "Shangyan Zhou",
        "relation": "author_of",
        "tail": "mHC: Manifold-Constrained Hyper-Connections"
      },
      {
        "head": "Zhean Xu",
        "relation": "author_of",
        "tail": "mHC: Manifold-Constrained Hyper-Connections"
      },
      {
        "head": "Zhengyan Zhang",
        "relation": "author_of",
        "tail": "mHC: Manifold-Constrained Hyper-Connections"
      },
      {
        "head": "Wangding Zeng",
        "relation": "author_of",
        "tail": "mHC: Manifold-Constrained Hyper-Connections"
      },
      {
        "head": "Shengding Hu",
        "relation": "author_of",
        "tail": "mHC: Manifold-Constrained Hyper-Connections"
      },
      {
        "head": "Yuqing Wang",
        "relation": "author_of",
        "tail": "mHC: Manifold-Constrained Hyper-Connections"
      },
      {
        "head": "Jingyang Yuan",
        "relation": "author_of",
        "tail": "mHC: Manifold-Constrained Hyper-Connections"
      },
      {
        "head": "Lean Wang",
        "relation": "author_of",
        "tail": "mHC: Manifold-Constrained Hyper-Connections"
      },
      {
        "head": "Wenfeng Liang",
        "relation": "author_of",
        "tail": "mHC: Manifold-Constrained Hyper-Connections"
      },
      {
        "head": "Yifei Ge",
        "relation": "author_of",
        "tail": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot"
      },
      {
        "head": "Zhuo Li",
        "relation": "author_of",
        "tail": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot"
      },
      {
        "head": "Xuebin Yue",
        "relation": "author_of",
        "tail": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot"
      },
      {
        "head": "Hengyi Li",
        "relation": "author_of",
        "tail": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot"
      },
      {
        "head": "Lin Meng",
        "relation": "author_of",
        "tail": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot"
      },
      {
        "head": "Shuo Qian",
        "relation": "author_of",
        "tail": "Physics-guided deep neural networks for bathymetric mapping using Sentinel-2 multi-spectral imagery"
      },
      {
        "head": "Yingying Chen",
        "relation": "author_of",
        "tail": "Physics-guided deep neural networks for bathymetric mapping using Sentinel-2 multi-spectral imagery"
      },
      {
        "head": "Wei Wang",
        "relation": "author_of",
        "tail": "Physics-guided deep neural networks for bathymetric mapping using Sentinel-2 multi-spectral imagery"
      },
      {
        "head": "Gaowei Zhang",
        "relation": "author_of",
        "tail": "Physics-guided deep neural networks for bathymetric mapping using Sentinel-2 multi-spectral imagery"
      },
      {
        "head": "Lei Li",
        "relation": "author_of",
        "tail": "Physics-guided deep neural networks for bathymetric mapping using Sentinel-2 multi-spectral imagery"
      },
      {
        "head": "Zengzhou Hao",
        "relation": "author_of",
        "tail": "Physics-guided deep neural networks for bathymetric mapping using Sentinel-2 multi-spectral imagery"
      },
      {
        "head": "Yi Wang",
        "relation": "author_of",
        "tail": "Physics-guided deep neural networks for bathymetric mapping using Sentinel-2 multi-spectral imagery"
      },
      {
        "head": "Minqiang Yang",
        "relation": "author_of",
        "tail": "Spike Memory Transformer: An Energy-Efficient Model in Distributed Learning Framework for Autonomous Depression Detection"
      },
      {
        "head": "Yueze Liu",
        "relation": "author_of",
        "tail": "Spike Memory Transformer: An Energy-Efficient Model in Distributed Learning Framework for Autonomous Depression Detection"
      },
      {
        "head": "Yongfeng Tao",
        "relation": "author_of",
        "tail": "Spike Memory Transformer: An Energy-Efficient Model in Distributed Learning Framework for Autonomous Depression Detection"
      },
      {
        "head": "Bin Hu",
        "relation": "author_of",
        "tail": "Spike Memory Transformer: An Energy-Efficient Model in Distributed Learning Framework for Autonomous Depression Detection"
      },
      {
        "head": "Sibo Huang",
        "relation": "author_of",
        "tail": "Multi-Perspective Semantic Segmentation of Ground Penetrating Radar Images for Pavement Subsurface Objects"
      },
      {
        "head": "Guijie Zhu",
        "relation": "author_of",
        "tail": "Multi-Perspective Semantic Segmentation of Ground Penetrating Radar Images for Pavement Subsurface Objects"
      },
      {
        "head": "Jiaming Tang",
        "relation": "author_of",
        "tail": "Multi-Perspective Semantic Segmentation of Ground Penetrating Radar Images for Pavement Subsurface Objects"
      },
      {
        "head": "Weixiong Li",
        "relation": "author_of",
        "tail": "Multi-Perspective Semantic Segmentation of Ground Penetrating Radar Images for Pavement Subsurface Objects"
      },
      {
        "head": "Zhun Fan",
        "relation": "author_of",
        "tail": "Multi-Perspective Semantic Segmentation of Ground Penetrating Radar Images for Pavement Subsurface Objects"
      },
      {
        "head": "D. E. Boukhari",
        "relation": "author_of",
        "tail": "A comprehensive review of facial beauty prediction using deep learning techniques"
      },
      {
        "head": "F. Dornaika",
        "relation": "author_of",
        "tail": "A comprehensive review of facial beauty prediction using deep learning techniques"
      },
      {
        "head": "A. Chemsa",
        "relation": "author_of",
        "tail": "A comprehensive review of facial beauty prediction using deep learning techniques"
      },
      {
        "head": "Abdelmalik Taleb-Ahmed",
        "relation": "author_of",
        "tail": "A comprehensive review of facial beauty prediction using deep learning techniques"
      },
      {
        "head": "Kaiqing Lin",
        "relation": "author_of",
        "tail": "Seeing Before Reasoning: A Unified Framework for Generalizable and Explainable Fake Image Detection"
      },
      {
        "head": "Zhiyuan Yan",
        "relation": "author_of",
        "tail": "Seeing Before Reasoning: A Unified Framework for Generalizable and Explainable Fake Image Detection"
      },
      {
        "head": "Ruoxin Chen",
        "relation": "author_of",
        "tail": "Seeing Before Reasoning: A Unified Framework for Generalizable and Explainable Fake Image Detection"
      },
      {
        "head": "Junyan Ye",
        "relation": "author_of",
        "tail": "Seeing Before Reasoning: A Unified Framework for Generalizable and Explainable Fake Image Detection"
      },
      {
        "head": "Ke-Yue Zhang",
        "relation": "author_of",
        "tail": "Seeing Before Reasoning: A Unified Framework for Generalizable and Explainable Fake Image Detection"
      },
      {
        "head": "Yue Zhou",
        "relation": "author_of",
        "tail": "Seeing Before Reasoning: A Unified Framework for Generalizable and Explainable Fake Image Detection"
      },
      {
        "head": "Peng Jin",
        "relation": "author_of",
        "tail": "Seeing Before Reasoning: A Unified Framework for Generalizable and Explainable Fake Image Detection"
      },
      {
        "head": "Bin Li",
        "relation": "author_of",
        "tail": "Seeing Before Reasoning: A Unified Framework for Generalizable and Explainable Fake Image Detection"
      },
      {
        "head": "Taiping Yao",
        "relation": "author_of",
        "tail": "Seeing Before Reasoning: A Unified Framework for Generalizable and Explainable Fake Image Detection"
      },
      {
        "head": "Shouhong Ding",
        "relation": "author_of",
        "tail": "Seeing Before Reasoning: A Unified Framework for Generalizable and Explainable Fake Image Detection"
      },
      {
        "head": "Esraa Hassan",
        "relation": "author_of",
        "tail": "DenseNet Model with Attention Mechanisms for Robust Date Fruit Image Classification"
      },
      {
        "head": "Sarah Abu Ghazalah",
        "relation": "author_of",
        "tail": "DenseNet Model with Attention Mechanisms for Robust Date Fruit Image Classification"
      },
      {
        "head": "Nora El-Rashidy",
        "relation": "author_of",
        "tail": "DenseNet Model with Attention Mechanisms for Robust Date Fruit Image Classification"
      },
      {
        "head": "Tarek Abd El-Hafeez",
        "relation": "author_of",
        "tail": "DenseNet Model with Attention Mechanisms for Robust Date Fruit Image Classification"
      },
      {
        "head": "Mahmoud Y. Shams",
        "relation": "author_of",
        "tail": "DenseNet Model with Attention Mechanisms for Robust Date Fruit Image Classification"
      },
      {
        "head": "Hari Kishan Kondaveeti",
        "relation": "author_of",
        "tail": "Evaluation of deep learning models using explainable AI with qualitative and quantitative analysis for rice leaf disease detection"
      },
      {
        "head": "Chinna Gopi Simhadri",
        "relation": "author_of",
        "tail": "Evaluation of deep learning models using explainable AI with qualitative and quantitative analysis for rice leaf disease detection"
      },
      {
        "head": "Hang Liu",
        "relation": "author_of",
        "tail": "Wind turbine blade surface defect detection model based on improved you only look once version 10 small and integrated compression"
      },
      {
        "head": "Sheng Liu",
        "relation": "author_of",
        "tail": "Wind turbine blade surface defect detection model based on improved you only look once version 10 small and integrated compression"
      },
      {
        "head": "Zhijian Liu",
        "relation": "author_of",
        "tail": "Wind turbine blade surface defect detection model based on improved you only look once version 10 small and integrated compression"
      },
      {
        "head": "Ben Niu",
        "relation": "author_of",
        "tail": "Wind turbine blade surface defect detection model based on improved you only look once version 10 small and integrated compression"
      },
      {
        "head": "Jing Xie",
        "relation": "author_of",
        "tail": "Wind turbine blade surface defect detection model based on improved you only look once version 10 small and integrated compression"
      },
      {
        "head": "Chi Luo",
        "relation": "author_of",
        "tail": "Wind turbine blade surface defect detection model based on improved you only look once version 10 small and integrated compression"
      },
      {
        "head": "Zhiyu Shi",
        "relation": "author_of",
        "tail": "Wind turbine blade surface defect detection model based on improved you only look once version 10 small and integrated compression"
      },
      {
        "head": "Ashish Vaswani",
        "relation": "author_of",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Noam Shazeer",
        "relation": "author_of",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Niki Parmar",
        "relation": "author_of",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Jakob Uszkoreit",
        "relation": "author_of",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Llion Jones",
        "relation": "author_of",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Aidan N. Gomez",
        "relation": "author_of",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Lukasz Kaiser",
        "relation": "author_of",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Illia Polosukhin",
        "relation": "author_of",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Colin Raffel",
        "relation": "author_of",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "Noam Shazeer",
        "relation": "author_of",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "Adam Roberts",
        "relation": "author_of",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "Katherine Lee",
        "relation": "author_of",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "Sharan Narang",
        "relation": "author_of",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "Michael Matena",
        "relation": "author_of",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "Yanqi Zhou",
        "relation": "author_of",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "Wei Li",
        "relation": "author_of",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "Peter J. Liu",
        "relation": "author_of",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "M. Heusel",
        "relation": "author_of",
        "tail": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"
      },
      {
        "head": "Hubert Ramsauer",
        "relation": "author_of",
        "tail": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"
      },
      {
        "head": "Thomas Unterthiner",
        "relation": "author_of",
        "tail": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"
      },
      {
        "head": "Bernhard Nessler",
        "relation": "author_of",
        "tail": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"
      },
      {
        "head": "Sepp Hochreiter",
        "relation": "author_of",
        "tail": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"
      },
      {
        "head": "Holger Caesar",
        "relation": "author_of",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "Varun Bankiti",
        "relation": "author_of",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "Alex H. Lang",
        "relation": "author_of",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "Sourabh Vora",
        "relation": "author_of",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "Venice Erin Liong",
        "relation": "author_of",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "Qiang Xu",
        "relation": "author_of",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "Anush Krishnan",
        "relation": "author_of",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "Yu Pan",
        "relation": "author_of",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "Giancarlo Baldan",
        "relation": "author_of",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "Oscar Beijbom",
        "relation": "author_of",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "Alexey Dosovitskiy",
        "relation": "author_of",
        "tail": "CARLA: An Open Urban Driving Simulator"
      },
      {
        "head": "German Ros",
        "relation": "author_of",
        "tail": "CARLA: An Open Urban Driving Simulator"
      },
      {
        "head": "Felipe Codevilla",
        "relation": "author_of",
        "tail": "CARLA: An Open Urban Driving Simulator"
      },
      {
        "head": "Antonio Lopez",
        "relation": "author_of",
        "tail": "CARLA: An Open Urban Driving Simulator"
      },
      {
        "head": "Vladlen Koltun",
        "relation": "author_of",
        "tail": "CARLA: An Open Urban Driving Simulator"
      },
      {
        "head": "Lvmin Zhang",
        "relation": "author_of",
        "tail": "Adding Conditional Control to Text-to-Image Diffusion Models"
      },
      {
        "head": "Anyi Rao",
        "relation": "author_of",
        "tail": "Adding Conditional Control to Text-to-Image Diffusion Models"
      },
      {
        "head": "Maneesh Agrawala",
        "relation": "author_of",
        "tail": "Adding Conditional Control to Text-to-Image Diffusion Models"
      },
      {
        "head": "William Peebles",
        "relation": "author_of",
        "tail": "Scalable Diffusion Models with Transformers"
      },
      {
        "head": "Saining Xie",
        "relation": "author_of",
        "tail": "Scalable Diffusion Models with Transformers"
      },
      {
        "head": "Dustin Podell",
        "relation": "author_of",
        "tail": "SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis"
      },
      {
        "head": "Zion English",
        "relation": "author_of",
        "tail": "SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis"
      },
      {
        "head": "Kyle Lacey",
        "relation": "author_of",
        "tail": "SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis"
      },
      {
        "head": "Andreas Blattmann",
        "relation": "author_of",
        "tail": "SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis"
      },
      {
        "head": "Tim Dockhorn",
        "relation": "author_of",
        "tail": "SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis"
      },
      {
        "head": "Jonas Müller",
        "relation": "author_of",
        "tail": "SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis"
      },
      {
        "head": "Joe Penna",
        "relation": "author_of",
        "tail": "SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis"
      },
      {
        "head": "Robin Rombach",
        "relation": "author_of",
        "tail": "SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis"
      },
      {
        "head": "Josh Tobin",
        "relation": "author_of",
        "tail": "Domain randomization for transferring deep neural networks from simulation to the real world"
      },
      {
        "head": "Rachel Fong",
        "relation": "author_of",
        "tail": "Domain randomization for transferring deep neural networks from simulation to the real world"
      },
      {
        "head": "Alex Ray",
        "relation": "author_of",
        "tail": "Domain randomization for transferring deep neural networks from simulation to the real world"
      },
      {
        "head": "Jonas Schneider",
        "relation": "author_of",
        "tail": "Domain randomization for transferring deep neural networks from simulation to the real world"
      },
      {
        "head": "Wojciech Zaremba",
        "relation": "author_of",
        "tail": "Domain randomization for transferring deep neural networks from simulation to the real world"
      },
      {
        "head": "Pieter Abbeel",
        "relation": "author_of",
        "tail": "Domain randomization for transferring deep neural networks from simulation to the real world"
      },
      {
        "head": "G. Ros",
        "relation": "author_of",
        "tail": "The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes"
      },
      {
        "head": "Laura Sellart",
        "relation": "author_of",
        "tail": "The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes"
      },
      {
        "head": "Joanna Materzynska",
        "relation": "author_of",
        "tail": "The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes"
      },
      {
        "head": "David Vázquez",
        "relation": "author_of",
        "tail": "The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes"
      },
      {
        "head": "Antonio M. López",
        "relation": "author_of",
        "tail": "The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes"
      },
      {
        "head": "Fachrina Dewi Puspitasari",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Chaoning Zhang",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Joseph Cho",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Adnan Haider",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Noor Ul Eman",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Omer Amin",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Alexis Mankowski",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Muhammad Umair",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Jingyao Zheng",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Sheng Zheng",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Lik-Hang Lee",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Caiyan Qin",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Tae-Ho Kim",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Choong Seon Hong",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Yang Yang",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Heng Tao Shen",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Bohan Li",
        "relation": "author_of",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "Zhuang Ma",
        "relation": "author_of",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "Dalong Du",
        "relation": "author_of",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "Baorui Peng",
        "relation": "author_of",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "Zhujin Liang",
        "relation": "author_of",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "Zhenqiang Liu",
        "relation": "author_of",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "Chao Ma",
        "relation": "author_of",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "Yueming Jin",
        "relation": "author_of",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "Hao Zhao",
        "relation": "author_of",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "Wenjun Zeng",
        "relation": "author_of",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "Xin Jin",
        "relation": "author_of",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "Zhuoran Yang",
        "relation": "author_of",
        "tail": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask"
      },
      {
        "head": "Yanyong Zhang",
        "relation": "author_of",
        "tail": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask"
      },
      {
        "head": "Guosheng Zhao",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Yaozeng Wang",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Xiaofeng Wang",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Zheng Zhu",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Tingdong Yu",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Guan Huang",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Yongchen Zai",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Ji Jiao",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Changliang Xue",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Xiaole Wang",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Zhen Yang",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Futang Zhu",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Xingang Wang",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Ahmad Rahimi",
        "relation": "author_of",
        "tail": "MAD: Motion Appearance Decoupling for efficient Driving World Models"
      },
      {
        "head": "Valentin Gerard",
        "relation": "author_of",
        "tail": "MAD: Motion Appearance Decoupling for efficient Driving World Models"
      },
      {
        "head": "Eloi Zablocki",
        "relation": "author_of",
        "tail": "MAD: Motion Appearance Decoupling for efficient Driving World Models"
      },
      {
        "head": "Matthieu Cord",
        "relation": "author_of",
        "tail": "MAD: Motion Appearance Decoupling for efficient Driving World Models"
      },
      {
        "head": "Alexandre Alahi",
        "relation": "author_of",
        "tail": "MAD: Motion Appearance Decoupling for efficient Driving World Models"
      },
      {
        "head": "Duolikun Danier",
        "relation": "author_of",
        "tail": "View-Consistent Diffusion Representations for 3D-Consistent Video Generation"
      },
      {
        "head": "Ge Gao",
        "relation": "author_of",
        "tail": "View-Consistent Diffusion Representations for 3D-Consistent Video Generation"
      },
      {
        "head": "Steven McDonagh",
        "relation": "author_of",
        "tail": "View-Consistent Diffusion Representations for 3D-Consistent Video Generation"
      },
      {
        "head": "Changjian Li",
        "relation": "author_of",
        "tail": "View-Consistent Diffusion Representations for 3D-Consistent Video Generation"
      },
      {
        "head": "Hakan Bilen",
        "relation": "author_of",
        "tail": "View-Consistent Diffusion Representations for 3D-Consistent Video Generation"
      },
      {
        "head": "Oisin Mac Aodha",
        "relation": "author_of",
        "tail": "View-Consistent Diffusion Representations for 3D-Consistent Video Generation"
      },
      {
        "head": "Jonathan Long",
        "relation": "author_of",
        "tail": "Fully Convolutional Networks for Semantic Segmentation"
      },
      {
        "head": "Evan Shelhamer",
        "relation": "author_of",
        "tail": "Fully Convolutional Networks for Semantic Segmentation"
      },
      {
        "head": "Trevor Darrell",
        "relation": "author_of",
        "tail": "Fully Convolutional Networks for Semantic Segmentation"
      },
      {
        "head": "Alex Graves",
        "relation": "author_of",
        "tail": "Speech Recognition with Deep Recurrent Neural Networks"
      },
      {
        "head": "Abdel-rahman Mohamed",
        "relation": "author_of",
        "tail": "Speech Recognition with Deep Recurrent Neural Networks"
      },
      {
        "head": "Geoffrey Hinton",
        "relation": "author_of",
        "tail": "Speech Recognition with Deep Recurrent Neural Networks"
      },
      {
        "head": "Holger Caesar",
        "relation": "author_of",
        "tail": "nuScenes: A multimodal dataset for autonomous driving"
      },
      {
        "head": "Varun Bankiti",
        "relation": "author_of",
        "tail": "nuScenes: A multimodal dataset for autonomous driving"
      },
      {
        "head": "Alex H. Lang",
        "relation": "author_of",
        "tail": "nuScenes: A multimodal dataset for autonomous driving"
      },
      {
        "head": "Sourabh Vora",
        "relation": "author_of",
        "tail": "nuScenes: A multimodal dataset for autonomous driving"
      },
      {
        "head": "Venice Erin Liong",
        "relation": "author_of",
        "tail": "nuScenes: A multimodal dataset for autonomous driving"
      },
      {
        "head": "Qiang Xu",
        "relation": "author_of",
        "tail": "nuScenes: A multimodal dataset for autonomous driving"
      },
      {
        "head": "Anush Krishnan",
        "relation": "author_of",
        "tail": "nuScenes: A multimodal dataset for autonomous driving"
      },
      {
        "head": "Yu Pan",
        "relation": "author_of",
        "tail": "nuScenes: A multimodal dataset for autonomous driving"
      },
      {
        "head": "Giancarlo Baldan",
        "relation": "author_of",
        "tail": "nuScenes: A multimodal dataset for autonomous driving"
      },
      {
        "head": "Oscar Beijbom",
        "relation": "author_of",
        "tail": "nuScenes: A multimodal dataset for autonomous driving"
      },
      {
        "head": "Josh Tobin",
        "relation": "author_of",
        "tail": "Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World"
      },
      {
        "head": "Rachel Fong",
        "relation": "author_of",
        "tail": "Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World"
      },
      {
        "head": "Alex Ray",
        "relation": "author_of",
        "tail": "Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World"
      },
      {
        "head": "Jonas Schneider",
        "relation": "author_of",
        "tail": "Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World"
      },
      {
        "head": "Wojciech Zaremba",
        "relation": "author_of",
        "tail": "Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World"
      },
      {
        "head": "Pieter Abbeel",
        "relation": "author_of",
        "tail": "Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World"
      },
      {
        "head": "Attention Is All You Need",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Attention Is All You Need",
        "relation": "cites",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "Attention Is All You Need",
        "relation": "cites",
        "tail": "Long Short-Term Memory"
      },
      {
        "head": "Attention Is All You Need",
        "relation": "cites",
        "tail": "Dropout: a simple way to prevent neural networks from overfitting"
      },
      {
        "head": "Attention Is All You Need",
        "relation": "cites",
        "tail": "Rethinking the Inception Architecture for Computer Vision"
      },
      {
        "head": "Attention Is All You Need",
        "relation": "cites",
        "tail": "Neural Machine Translation by Jointly Learning to Align and Translate"
      },
      {
        "head": "Attention Is All You Need",
        "relation": "cites",
        "tail": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"
      },
      {
        "head": "Attention Is All You Need",
        "relation": "cites",
        "tail": "Sequence to Sequence Learning with Neural Networks"
      },
      {
        "head": "Attention Is All You Need",
        "relation": "cites",
        "tail": "Xception: Deep Learning with Depthwise Separable Convolutions"
      },
      {
        "head": "Attention Is All You Need",
        "relation": "cites",
        "tail": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"
      },
      {
        "head": "A comprehensive review of recommender systems: Transitioning from theory to practice",
        "relation": "cites",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
        "relation": "cites",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection",
        "relation": "cites",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation",
        "relation": "cites",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "DiffusionEngine: Diffusion model is scalable data engine for object detection",
        "relation": "cites",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "Enhanced Transformer-BiLSTM Deep Learning Framework for Day-Ahead Energy Price Forecasting",
        "relation": "cites",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "Conditional Generative Adversarial Network-based framework for multi-feature uncertainty modeling in energy systems",
        "relation": "cites",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "CityVLM: Towards sustainable urban development via multi-view coordinated vision–language model",
        "relation": "cites",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "FullPerception: Network-Level Collaborative Perception for Eliminating Vehicular Blind Spots",
        "relation": "cites",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "A survey of features used for representing black-box single-objective continuous optimization",
        "relation": "cites",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "cites",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "cites",
        "tail": "Long Short-Term Memory"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "cites",
        "tail": "Et al"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "cites",
        "tail": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "cites",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "cites",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "cites",
        "tail": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "cites",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "cites",
        "tail": "Fully convolutional networks for semantic segmentation"
      },
      {
        "head": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "DiffusionEngine: Diffusion model is scalable data engine for object detection",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Noise Perception Self-Supervised Learning for Unsupervised Person Re-Identification",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Towards Reliable Tracking: An Uncertainty-Aware Siamese Tracker",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Bridging the gap, not forcing the tie: dual-space alignment and fusion framework for toxic memes detection",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Perturb and restore: Efficient category revocation in federated unlearning",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "DCM-Net: A novel dual-branch CNN-Mamba cross-layer feature fusion network for medical image segmentation",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "AdveDiffNet: adversarial diffusion network for unbalanced melanoma diagnosis",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Adam: A Method for Stochastic Optimization",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "Adam: A Method for Stochastic Optimization",
        "relation": "cites",
        "tail": "Auto-Encoding Variational Bayes"
      },
      {
        "head": "Adam: A Method for Stochastic Optimization",
        "relation": "cites",
        "tail": "Supporting Online Material for Reducing the Dimensionality of Data with Neural Networks"
      },
      {
        "head": "Adam: A Method for Stochastic Optimization",
        "relation": "cites",
        "tail": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"
      },
      {
        "head": "Adam: A Method for Stochastic Optimization",
        "relation": "cites",
        "tail": "Speech recognition with deep recurrent neural networks"
      },
      {
        "head": "Adam: A Method for Stochastic Optimization",
        "relation": "cites",
        "tail": "Improving neural networks by preventing co-adaptation of feature detectors"
      },
      {
        "head": "Adam: A Method for Stochastic Optimization",
        "relation": "cites",
        "tail": "ADADELTA: An Adaptive Learning Rate Method"
      },
      {
        "head": "Adam: A Method for Stochastic Optimization",
        "relation": "cites",
        "tail": "Learning Word Vectors for Sentiment Analysis"
      },
      {
        "head": "Adam: A Method for Stochastic Optimization",
        "relation": "cites",
        "tail": "On the importance of initialization and momentum in deep learning"
      },
      {
        "head": "Adam: A Method for Stochastic Optimization",
        "relation": "cites",
        "tail": "Generating Sequences With Recurrent Neural Networks"
      },
      {
        "head": "Weakly Supervised Image Dehazing via Physics-Based Decomposition",
        "relation": "cites",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer",
        "relation": "cites",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
        "relation": "cites",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction",
        "relation": "cites",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model",
        "relation": "cites",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "Unveiling cross-modal consistency: Taming inter- and intra-modal noise for robust multi-modal knowledge graph completion",
        "relation": "cites",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "Low-Resolution Massive MIMO Channel Estimation With LSTM Attention-Based CBDNet",
        "relation": "cites",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "Quantum solver for single-impurity Anderson models with particle-hole symmetry",
        "relation": "cites",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "Sentinel-1 SAR-based globally distributed co-seismic landslide detection by deep neural networks",
        "relation": "cites",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "Increasing resolution and accuracy in sub-seasonal forecasting through 3D U-Net: the western US",
        "relation": "cites",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "Xception: Deep Learning with Depthwise Separable Convolutions",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Xception: Deep Learning with Depthwise Separable Convolutions",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "Xception: Deep Learning with Depthwise Separable Convolutions",
        "relation": "cites",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "Xception: Deep Learning with Depthwise Separable Convolutions",
        "relation": "cites",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "Xception: Deep Learning with Depthwise Separable Convolutions",
        "relation": "cites",
        "tail": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
      },
      {
        "head": "Xception: Deep Learning with Depthwise Separable Convolutions",
        "relation": "cites",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Xception: Deep Learning with Depthwise Separable Convolutions",
        "relation": "cites",
        "tail": "Rethinking the Inception Architecture for Computer Vision"
      },
      {
        "head": "Xception: Deep Learning with Depthwise Separable Convolutions",
        "relation": "cites",
        "tail": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
      },
      {
        "head": "Xception: Deep Learning with Depthwise Separable Convolutions",
        "relation": "cites",
        "tail": "Distilling the Knowledge in a Neural Network"
      },
      {
        "head": "Xception: Deep Learning with Depthwise Separable Convolutions",
        "relation": "cites",
        "tail": "Visualizing and Understanding Convolutional Networks"
      },
      {
        "head": "mHC: Manifold-Constrained Hyper-Connections",
        "relation": "cites",
        "tail": "Xception: Deep Learning with Depthwise Separable Convolutions"
      },
      {
        "head": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot",
        "relation": "cites",
        "tail": "Xception: Deep Learning with Depthwise Separable Convolutions"
      },
      {
        "head": "Physics-guided deep neural networks for bathymetric mapping using Sentinel-2 multi-spectral imagery",
        "relation": "cites",
        "tail": "Xception: Deep Learning with Depthwise Separable Convolutions"
      },
      {
        "head": "Spike Memory Transformer: An Energy-Efficient Model in Distributed Learning Framework for Autonomous Depression Detection",
        "relation": "cites",
        "tail": "Xception: Deep Learning with Depthwise Separable Convolutions"
      },
      {
        "head": "Multi-Perspective Semantic Segmentation of Ground Penetrating Radar Images for Pavement Subsurface Objects",
        "relation": "cites",
        "tail": "Xception: Deep Learning with Depthwise Separable Convolutions"
      },
      {
        "head": "A comprehensive review of facial beauty prediction using deep learning techniques",
        "relation": "cites",
        "tail": "Xception: Deep Learning with Depthwise Separable Convolutions"
      },
      {
        "head": "Seeing Before Reasoning: A Unified Framework for Generalizable and Explainable Fake Image Detection",
        "relation": "cites",
        "tail": "Xception: Deep Learning with Depthwise Separable Convolutions"
      },
      {
        "head": "DenseNet Model with Attention Mechanisms for Robust Date Fruit Image Classification",
        "relation": "cites",
        "tail": "Xception: Deep Learning with Depthwise Separable Convolutions"
      },
      {
        "head": "Evaluation of deep learning models using explainable AI with qualitative and quantitative analysis for rice leaf disease detection",
        "relation": "cites",
        "tail": "Xception: Deep Learning with Depthwise Separable Convolutions"
      },
      {
        "head": "Wind turbine blade surface defect detection model based on improved you only look once version 10 small and integrated compression",
        "relation": "cites",
        "tail": "Xception: Deep Learning with Depthwise Separable Convolutions"
      },
      {
        "head": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
        "relation": "cites",
        "tail": "Attention is All you Need"
      },
      {
        "head": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
        "relation": "cites",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
        "relation": "cites",
        "tail": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"
      },
      {
        "head": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
        "relation": "cites",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
        "relation": "cites",
        "tail": "CARLA: An Open Urban Driving Simulator"
      },
      {
        "head": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
        "relation": "cites",
        "tail": "Adding Conditional Control to Text-to-Image Diffusion Models"
      },
      {
        "head": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
        "relation": "cites",
        "tail": "Scalable Diffusion Models with Transformers"
      },
      {
        "head": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
        "relation": "cites",
        "tail": "SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis"
      },
      {
        "head": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
        "relation": "cites",
        "tail": "Domain randomization for transferring deep neural networks from simulation to the real world"
      },
      {
        "head": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
        "relation": "cites",
        "tail": "The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes"
      },
      {
        "head": "Sora as a World Model? A Complete Survey on Text-to-Video Generation",
        "relation": "cites",
        "tail": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation"
      },
      {
        "head": "OmniNWM: Omniscient Driving Navigation World Models",
        "relation": "cites",
        "tail": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation"
      },
      {
        "head": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask",
        "relation": "cites",
        "tail": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation"
      },
      {
        "head": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving",
        "relation": "cites",
        "tail": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation"
      },
      {
        "head": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
        "relation": "cites",
        "tail": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation"
      },
      {
        "head": "View-Consistent Diffusion Representations for 3D-Consistent Video Generation",
        "relation": "cites",
        "tail": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation"
      },
      {
        "head": "Attention Is All You Need",
        "relation": "proposed_model",
        "tail": "Transformer"
      },
      {
        "head": "Transformer",
        "relation": "evaluated_on",
        "tail": "WMT 2014 English-to-German translation task"
      },
      {
        "head": "Transformer",
        "relation": "evaluated_on",
        "tail": "WMT 2014 English-to-French translation task"
      },
      {
        "head": "Transformer",
        "relation": "uses_metric",
        "tail": "BLEU"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "proposed_model",
        "tail": "residual learning framework"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "baseline_model",
        "tail": "VGG nets"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "evaluated_on",
        "tail": "CIFAR-10"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "evaluated_on",
        "tail": "COCO object detection dataset"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "uses_metric",
        "tail": "error"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "uses_metric",
        "tail": "relative improvement"
      },
      {
        "head": "Adam: A Method for Stochastic Optimization",
        "relation": "proposed_model",
        "tail": "Adam"
      },
      {
        "head": "Adam: A Method for Stochastic Optimization",
        "relation": "proposed_model",
        "tail": "AdaMax"
      },
      {
        "head": "Long Short-Term Memory",
        "relation": "proposed_model",
        "tail": "Long Short-Term Memory"
      },
      {
        "head": "Dropout: a simple way to prevent neural networks from overfitting",
        "relation": "proposed_model",
        "tail": "Dropout"
      },
      {
        "head": "Dropout",
        "relation": "baseline_model",
        "tail": "neural networks"
      },
      {
        "head": "Rethinking the Inception Architecture for Computer Vision",
        "relation": "proposed_model",
        "tail": "Inception Architecture"
      },
      {
        "head": "Rethinking the Inception Architecture for Computer Vision",
        "relation": "evaluated_on",
        "tail": "ILSVRC 2012 classification challenge validation set"
      },
      {
        "head": "Rethinking the Inception Architecture for Computer Vision",
        "relation": "uses_metric",
        "tail": "top-1 error"
      },
      {
        "head": "Rethinking the Inception Architecture for Computer Vision",
        "relation": "uses_metric",
        "tail": "top-5 error"
      },
      {
        "head": "Neural Machine Translation by Jointly Learning to Align and Translate",
        "relation": "proposed_model",
        "tail": "encoder-decoder"
      },
      {
        "head": "Neural Machine Translation by Jointly Learning to Align and Translate",
        "relation": "baseline_model",
        "tail": "phrase-based system"
      },
      {
        "head": "Neural Machine Translation by Jointly Learning to Align and Translate",
        "relation": "evaluated_on",
        "tail": "English-to-French translation"
      },
      {
        "head": "Neural Machine Translation by Jointly Learning to Align and Translate",
        "relation": "uses_metric",
        "tail": "translation performance"
      },
      {
        "head": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation",
        "relation": "proposed_model",
        "tail": "RNN Encoder-Decoder"
      },
      {
        "head": "RNN Encoder-Decoder",
        "relation": "baseline_model",
        "tail": "statistical machine translation system"
      },
      {
        "head": "RNN Encoder-Decoder",
        "relation": "evaluated_on",
        "tail": "statistical machine translation system"
      },
      {
        "head": "statistical machine translation system",
        "relation": "uses_metric",
        "tail": "log-linear model"
      },
      {
        "head": "Sequence to Sequence Learning with Neural Networks",
        "relation": "proposed_model",
        "tail": "LSTM"
      },
      {
        "head": "Sequence to Sequence Learning with Neural Networks",
        "relation": "baseline_model",
        "tail": "phrase-based SMT system"
      },
      {
        "head": "Sequence to Sequence Learning with Neural Networks",
        "relation": "evaluated_on",
        "tail": "WMT'14 dataset"
      },
      {
        "head": "LSTM",
        "relation": "uses_metric",
        "tail": "BLEU"
      },
      {
        "head": "phrase-based SMT system",
        "relation": "uses_metric",
        "tail": "BLEU"
      },
      {
        "head": "Xception: Deep Learning with Depthwise Separable Convolutions",
        "relation": "proposed_model",
        "tail": "Xception"
      },
      {
        "head": "Xception: Deep Learning with Depthwise Separable Convolutions",
        "relation": "baseline_model",
        "tail": "Inception V3"
      },
      {
        "head": "Xception",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "Xception",
        "relation": "evaluated_on",
        "tail": "larger image classification dataset comprising 350 million images and 17,000 classes"
      },
      {
        "head": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling",
        "relation": "proposed_model",
        "tail": "gated recurrent unit (GRU)"
      },
      {
        "head": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling",
        "relation": "baseline_model",
        "tail": "long short-term memory (LSTM)"
      },
      {
        "head": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling",
        "relation": "baseline_model",
        "tail": "tanh units"
      },
      {
        "head": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling",
        "relation": "evaluated_on",
        "tail": "polyphonic music modeling"
      },
      {
        "head": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling",
        "relation": "evaluated_on",
        "tail": "speech signal modeling"
      },
      {
        "head": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
        "relation": "proposed_model",
        "tail": "InstaDrive"
      },
      {
        "head": "InstaDrive",
        "relation": "baseline_model",
        "tail": "world models"
      },
      {
        "head": "InstaDrive",
        "relation": "evaluated_on",
        "tail": "nuScenes"
      },
      {
        "head": "InstaDrive",
        "relation": "evaluated_on",
        "tail": "CARLA"
      },
      {
        "head": "InstaDrive",
        "relation": "uses_metric",
        "tail": "video generation quality"
      },
      {
        "head": "InstaDrive",
        "relation": "uses_metric",
        "tail": "safety evaluation"
      },
      {
        "head": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection",
        "relation": "proposed_model",
        "tail": "CNC-VLM"
      },
      {
        "head": "CNC-VLM",
        "relation": "evaluated_on",
        "tail": "CNC fault detection dataset"
      },
      {
        "head": "CNC-VLM",
        "relation": "uses_metric",
        "tail": "Accuracy"
      },
      {
        "head": "CNC-VLM",
        "relation": "uses_metric",
        "tail": "F1-score"
      },
      {
        "head": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation",
        "relation": "proposed_model",
        "tail": "mamba segmentation"
      },
      {
        "head": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation",
        "relation": "uses_metric",
        "tail": "four-point laser metric calibration"
      },
      {
        "head": "DiffusionEngine: Diffusion model is scalable data engine for object detection",
        "relation": "proposed_model",
        "tail": "DiffusionEngine"
      },
      {
        "head": "Enhanced Transformer-BiLSTM Deep Learning Framework for Day-Ahead Energy Price Forecasting",
        "relation": "proposed_model",
        "tail": "Transformer and Bidirectional Long Short-Term Memory (BiLSTM) model"
      },
      {
        "head": "Enhanced Transformer-BiLSTM Deep Learning Framework for Day-Ahead Energy Price Forecasting",
        "relation": "evaluated_on",
        "tail": "New York Independent System Operator"
      },
      {
        "head": "Enhanced Transformer-BiLSTM Deep Learning Framework for Day-Ahead Energy Price Forecasting",
        "relation": "uses_metric",
        "tail": "MAE"
      },
      {
        "head": "Enhanced Transformer-BiLSTM Deep Learning Framework for Day-Ahead Energy Price Forecasting",
        "relation": "uses_metric",
        "tail": "RMSE"
      },
      {
        "head": "Enhanced Transformer-BiLSTM Deep Learning Framework for Day-Ahead Energy Price Forecasting",
        "relation": "uses_metric",
        "tail": "sMAPE"
      },
      {
        "head": "Enhanced Transformer-BiLSTM Deep Learning Framework for Day-Ahead Energy Price Forecasting",
        "relation": "uses_metric",
        "tail": "MAPE"
      },
      {
        "head": "Enhanced Transformer-BiLSTM Deep Learning Framework for Day-Ahead Energy Price Forecasting",
        "relation": "uses_metric",
        "tail": "R²"
      },
      {
        "head": "Conditional Generative Adversarial Network-based framework for multi-feature uncertainty modeling in energy systems",
        "relation": "proposed_model",
        "tail": "Conditional Generative Adversarial Network"
      },
      {
        "head": "CityVLM: Towards sustainable urban development via multi-view coordinated vision–language model",
        "relation": "proposed_model",
        "tail": "CityVLM"
      },
      {
        "head": "FullPerception: Network-Level Collaborative Perception for Eliminating Vehicular Blind Spots",
        "relation": "proposed_model",
        "tail": "FullPerception"
      },
      {
        "head": "FullPerception: Network-Level Collaborative Perception for Eliminating Vehicular Blind Spots",
        "relation": "proposed_model",
        "tail": "Proactive Conflict-free Scheduling (PCS)"
      },
      {
        "head": "FullPerception",
        "relation": "baseline_model",
        "tail": "single-vehicle systems"
      },
      {
        "head": "FullPerception",
        "relation": "baseline_model",
        "tail": "existing scheduling methods"
      },
      {
        "head": "FullPerception",
        "relation": "evaluated_on",
        "tail": "large-scale comprehensive joint simulation experiments"
      },
      {
        "head": "FullPerception",
        "relation": "uses_metric",
        "tail": "perception accuracy"
      },
      {
        "head": "ImageNet classification with deep convolutional neural networks",
        "relation": "proposed_model",
        "tail": "deep convolutional neural networks"
      },
      {
        "head": "ImageNet classification with deep convolutional neural networks",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
        "relation": "proposed_model",
        "tail": "ConvNet models"
      },
      {
        "head": "ConvNet models",
        "relation": "evaluated_on",
        "tail": "ImageNet Challenge 2014"
      },
      {
        "head": "ConvNet models",
        "relation": "evaluated_on",
        "tail": "other datasets"
      },
      {
        "head": "ConvNet models",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
        "relation": "proposed_model",
        "tail": "Region Proposal Network (RPN)"
      },
      {
        "head": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
        "relation": "baseline_model",
        "tail": "SPPnet"
      },
      {
        "head": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
        "relation": "baseline_model",
        "tail": "Fast R-CNN"
      },
      {
        "head": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
        "relation": "evaluated_on",
        "tail": "PASCAL VOC 2007"
      },
      {
        "head": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
        "relation": "evaluated_on",
        "tail": "PASCAL VOC 2012"
      },
      {
        "head": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
        "relation": "evaluated_on",
        "tail": "MS COCO"
      },
      {
        "head": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
        "relation": "evaluated_on",
        "tail": "ILSVRC"
      },
      {
        "head": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
        "relation": "evaluated_on",
        "tail": "COCO 2015"
      },
      {
        "head": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
        "relation": "uses_metric",
        "tail": "object detection accuracy"
      },
      {
        "head": "Microsoft COCO: Common Objects in Context",
        "relation": "baseline_model",
        "tail": "Deformable Parts Model"
      },
      {
        "head": "Microsoft COCO: Common Objects in Context",
        "relation": "evaluated_on",
        "tail": "PASCAL"
      },
      {
        "head": "Microsoft COCO: Common Objects in Context",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "Microsoft COCO: Common Objects in Context",
        "relation": "evaluated_on",
        "tail": "SUN"
      },
      {
        "head": "Deformable Parts Model",
        "relation": "uses_metric",
        "tail": "bounding box detection"
      },
      {
        "head": "Deformable Parts Model",
        "relation": "uses_metric",
        "tail": "segmentation detection"
      },
      {
        "head": "Going deeper with convolutions",
        "relation": "proposed_model",
        "tail": "Inception"
      },
      {
        "head": "Going deeper with convolutions",
        "relation": "proposed_model",
        "tail": "GoogLeNet"
      },
      {
        "head": "Inception",
        "relation": "evaluated_on",
        "tail": "ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014)"
      },
      {
        "head": "GoogLeNet",
        "relation": "evaluated_on",
        "tail": "ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014)"
      },
      {
        "head": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
        "relation": "proposed_model",
        "tail": "Batch Normalization"
      },
      {
        "head": "Batch Normalization",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "Batch Normalization",
        "relation": "uses_metric",
        "tail": "top-5 validation error"
      },
      {
        "head": "Batch Normalization",
        "relation": "uses_metric",
        "tail": "test error"
      },
      {
        "head": "ImageNet Large Scale Visual Recognition Challenge",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "ImageNet Large Scale Visual Recognition Challenge",
        "relation": "uses_metric",
        "tail": "object category classification"
      },
      {
        "head": "ImageNet Large Scale Visual Recognition Challenge",
        "relation": "uses_metric",
        "tail": "object detection"
      },
      {
        "head": "ImageNet Large Scale Visual Recognition Challenge",
        "relation": "uses_metric",
        "tail": "human accuracy"
      },
      {
        "head": "Fully convolutional networks for semantic segmentation",
        "relation": "proposed_model",
        "tail": "fully convolutional networks"
      },
      {
        "head": "fully convolutional networks",
        "relation": "baseline_model",
        "tail": "AlexNet"
      },
      {
        "head": "fully convolutional networks",
        "relation": "baseline_model",
        "tail": "VGG net"
      },
      {
        "head": "fully convolutional networks",
        "relation": "baseline_model",
        "tail": "GoogLeNet"
      },
      {
        "head": "fully convolutional networks",
        "relation": "evaluated_on",
        "tail": "PASCAL VOC"
      },
      {
        "head": "fully convolutional networks",
        "relation": "evaluated_on",
        "tail": "NYUDv2"
      },
      {
        "head": "fully convolutional networks",
        "relation": "evaluated_on",
        "tail": "SIFT Flow"
      },
      {
        "head": "fully convolutional networks",
        "relation": "uses_metric",
        "tail": "mean IU"
      },
      {
        "head": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning",
        "relation": "proposed_model",
        "tail": "FLOT"
      },
      {
        "head": "FLOT",
        "relation": "evaluated_on",
        "tail": "GTSRB"
      },
      {
        "head": "FLOT",
        "relation": "evaluated_on",
        "tail": "KBTS"
      },
      {
        "head": "FLOT",
        "relation": "evaluated_on",
        "tail": "CIFAR10"
      },
      {
        "head": "FLOT",
        "relation": "evaluated_on",
        "tail": "EMNIST"
      },
      {
        "head": "FLOT",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "FLOT",
        "relation": "uses_metric",
        "tail": "scalability"
      },
      {
        "head": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID",
        "relation": "proposed_model",
        "tail": "WarmGait"
      },
      {
        "head": "WarmGait",
        "relation": "evaluated_on",
        "tail": "thermal array sensors"
      },
      {
        "head": "WarmGait",
        "relation": "uses_metric",
        "tail": "average recognition accuracy"
      },
      {
        "head": "Noise Perception Self-Supervised Learning for Unsupervised Person Re-Identification",
        "relation": "proposed_model",
        "tail": "NPSSL"
      },
      {
        "head": "NPSSL",
        "relation": "evaluated_on",
        "tail": "Duke dataset"
      },
      {
        "head": "Noise Perception Self-Supervised Learning for Unsupervised Person Re-Identification",
        "relation": "baseline_model",
        "tail": "Unsupervised Domain Adaptation"
      },
      {
        "head": "NPSSL",
        "relation": "uses_metric",
        "tail": "Noise Perception Self-Paced Learning"
      },
      {
        "head": "Towards Reliable Tracking: An Uncertainty-Aware Siamese Tracker",
        "relation": "proposed_model",
        "tail": "Uncertainty-Aware Siamese Network"
      },
      {
        "head": "Towards Reliable Tracking: An Uncertainty-Aware Siamese Tracker",
        "relation": "evaluated_on",
        "tail": "VOT2018"
      },
      {
        "head": "Towards Reliable Tracking: An Uncertainty-Aware Siamese Tracker",
        "relation": "evaluated_on",
        "tail": "VOT2019"
      },
      {
        "head": "Towards Reliable Tracking: An Uncertainty-Aware Siamese Tracker",
        "relation": "evaluated_on",
        "tail": "OTB100"
      },
      {
        "head": "Towards Reliable Tracking: An Uncertainty-Aware Siamese Tracker",
        "relation": "evaluated_on",
        "tail": "NFS"
      },
      {
        "head": "Towards Reliable Tracking: An Uncertainty-Aware Siamese Tracker",
        "relation": "evaluated_on",
        "tail": "UAV123"
      },
      {
        "head": "Towards Reliable Tracking: An Uncertainty-Aware Siamese Tracker",
        "relation": "evaluated_on",
        "tail": "LaSOT"
      },
      {
        "head": "Towards Reliable Tracking: An Uncertainty-Aware Siamese Tracker",
        "relation": "evaluated_on",
        "tail": "TrackingNet"
      },
      {
        "head": "Towards Reliable Tracking: An Uncertainty-Aware Siamese Tracker",
        "relation": "evaluated_on",
        "tail": "Got-10 k"
      },
      {
        "head": "Bridging the gap, not forcing the tie: dual-space alignment and fusion framework for toxic memes detection",
        "relation": "proposed_model",
        "tail": "dual-space alignment and fusion framework"
      },
      {
        "head": "DCM-Net: A novel dual-branch CNN-Mamba cross-layer feature fusion network for medical image segmentation",
        "relation": "proposed_model",
        "tail": "DCM-Net"
      },
      {
        "head": "Auto-Encoding Variational Bayes",
        "relation": "proposed_model",
        "tail": "stochastic variational inference and learning algorithm"
      },
      {
        "head": "Auto-Encoding Variational Bayes",
        "relation": "proposed_model",
        "tail": "reparameterization of the variational lower bound"
      },
      {
        "head": "Auto-Encoding Variational Bayes",
        "relation": "proposed_model",
        "tail": "approximate inference model"
      },
      {
        "head": "stochastic variational inference and learning algorithm",
        "relation": "evaluated_on",
        "tail": "i.i.d. datasets"
      },
      {
        "head": "reparameterization of the variational lower bound",
        "relation": "uses_metric",
        "tail": "variational lower bound"
      },
      {
        "head": "approximate inference model",
        "relation": "evaluated_on",
        "tail": "i.i.d. datasets"
      },
      {
        "head": "Speech recognition with deep recurrent neural networks",
        "relation": "proposed_model",
        "tail": "deep recurrent neural networks"
      },
      {
        "head": "Speech recognition with deep recurrent neural networks",
        "relation": "proposed_model",
        "tail": "deep Long Short-term Memory RNNs"
      },
      {
        "head": "Speech recognition with deep recurrent neural networks",
        "relation": "baseline_model",
        "tail": "deep feedforward networks"
      },
      {
        "head": "deep Long Short-term Memory RNNs",
        "relation": "evaluated_on",
        "tail": "TIMIT phoneme recognition benchmark"
      },
      {
        "head": "deep Long Short-term Memory RNNs",
        "relation": "uses_metric",
        "tail": "test set error"
      },
      {
        "head": "Improving neural networks by preventing co-adaptation of feature detectors",
        "relation": "proposed_model",
        "tail": "dropout"
      },
      {
        "head": "dropout",
        "relation": "baseline_model",
        "tail": "feedforward neural network"
      },
      {
        "head": "dropout",
        "relation": "evaluated_on",
        "tail": "small training set"
      },
      {
        "head": "dropout",
        "relation": "evaluated_on",
        "tail": "held-out test data"
      },
      {
        "head": "dropout",
        "relation": "uses_metric",
        "tail": "speech recognition"
      },
      {
        "head": "dropout",
        "relation": "uses_metric",
        "tail": "object recognition"
      },
      {
        "head": "ADADELTA: An Adaptive Learning Rate Method",
        "relation": "proposed_model",
        "tail": "ADADELTA"
      },
      {
        "head": "ADADELTA: An Adaptive Learning Rate Method",
        "relation": "evaluated_on",
        "tail": "MNIST digit classification task"
      },
      {
        "head": "ADADELTA: An Adaptive Learning Rate Method",
        "relation": "evaluated_on",
        "tail": "large scale voice dataset"
      },
      {
        "head": "Learning Word Vectors for Sentiment Analysis",
        "relation": "proposed_model",
        "tail": "Word Vectors"
      },
      {
        "head": "Learning Word Vectors for Sentiment Analysis",
        "relation": "evaluated_on",
        "tail": "Sentiment Analysis"
      },
      {
        "head": "Generating Sequences With Recurrent Neural Networks",
        "relation": "proposed_model",
        "tail": "Long Short-term Memory recurrent neural networks"
      },
      {
        "head": "Generating Sequences With Recurrent Neural Networks",
        "relation": "evaluated_on",
        "tail": "text"
      },
      {
        "head": "Generating Sequences With Recurrent Neural Networks",
        "relation": "evaluated_on",
        "tail": "online handwriting"
      },
      {
        "head": "Weakly Supervised Image Dehazing via Physics-Based Decomposition",
        "relation": "proposed_model",
        "tail": "PBD"
      },
      {
        "head": "Weakly Supervised Image Dehazing via Physics-Based Decomposition",
        "relation": "baseline_model",
        "tail": "GAN"
      },
      {
        "head": "PBD",
        "relation": "evaluated_on",
        "tail": "seven benchmarks"
      },
      {
        "head": "PBD",
        "relation": "uses_metric",
        "tail": "reconstruction loss"
      },
      {
        "head": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer",
        "relation": "proposed_model",
        "tail": "AdamW"
      },
      {
        "head": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer",
        "relation": "baseline_model",
        "tail": "Adam"
      },
      {
        "head": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer",
        "relation": "evaluated_on",
        "tail": "face mask detection model"
      },
      {
        "head": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
        "relation": "proposed_model",
        "tail": "Engram"
      },
      {
        "head": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
        "relation": "baseline_model",
        "tail": "Mixture-of-Experts (MoE)"
      },
      {
        "head": "Engram",
        "relation": "evaluated_on",
        "tail": "MMLU"
      },
      {
        "head": "Engram",
        "relation": "evaluated_on",
        "tail": "CMMLU"
      },
      {
        "head": "Engram",
        "relation": "evaluated_on",
        "tail": "BBH"
      },
      {
        "head": "Engram",
        "relation": "evaluated_on",
        "tail": "ARC-Challenge"
      },
      {
        "head": "Engram",
        "relation": "evaluated_on",
        "tail": "HumanEval"
      },
      {
        "head": "Engram",
        "relation": "evaluated_on",
        "tail": "MATH"
      },
      {
        "head": "Engram",
        "relation": "evaluated_on",
        "tail": "Multi-Query NIAH"
      },
      {
        "head": "Engram",
        "relation": "uses_metric",
        "tail": "MMLU"
      },
      {
        "head": "Engram",
        "relation": "uses_metric",
        "tail": "CMMLU"
      },
      {
        "head": "Engram",
        "relation": "uses_metric",
        "tail": "BBH"
      },
      {
        "head": "Engram",
        "relation": "uses_metric",
        "tail": "ARC-Challenge"
      },
      {
        "head": "Engram",
        "relation": "uses_metric",
        "tail": "HumanEval"
      },
      {
        "head": "Engram",
        "relation": "uses_metric",
        "tail": "MATH"
      },
      {
        "head": "Engram",
        "relation": "uses_metric",
        "tail": "Multi-Query NIAH"
      },
      {
        "head": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction",
        "relation": "proposed_model",
        "tail": "Neural Hierarchical Interpolation for Time Series Forecasting (N-HiTS)"
      },
      {
        "head": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction",
        "relation": "proposed_model",
        "tail": "Network-Based Expansion Analysis for Interpretable Time Series Forecasting (N-BEATS)"
      },
      {
        "head": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction",
        "relation": "baseline_model",
        "tail": "long short-term memory (LSTM)"
      },
      {
        "head": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction",
        "relation": "evaluated_on",
        "tail": "two headwater streams in Georgia and North Carolina, USA"
      },
      {
        "head": "Neural Hierarchical Interpolation for Time Series Forecasting (N-HiTS)",
        "relation": "uses_metric",
        "tail": "Multi-Quantile Loss"
      },
      {
        "head": "Network-Based Expansion Analysis for Interpretable Time Series Forecasting (N-BEATS)",
        "relation": "uses_metric",
        "tail": "Multi-Quantile Loss"
      },
      {
        "head": "Neural Hierarchical Interpolation for Time Series Forecasting (N-HiTS)",
        "relation": "uses_metric",
        "tail": "95th percentile prediction uncertainty (95 PPU)"
      },
      {
        "head": "Network-Based Expansion Analysis for Interpretable Time Series Forecasting (N-BEATS)",
        "relation": "uses_metric",
        "tail": "95th percentile prediction uncertainty (95 PPU)"
      },
      {
        "head": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model",
        "relation": "proposed_model",
        "tail": "NASA-IBM geospatial foundation model"
      },
      {
        "head": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model",
        "relation": "evaluated_on",
        "tail": "harmonized Landsat and Sentinel-2 data"
      },
      {
        "head": "Low-Resolution Massive MIMO Channel Estimation With LSTM Attention-Based CBDNet",
        "relation": "proposed_model",
        "tail": "LA-CBDNet"
      },
      {
        "head": "Low-Resolution Massive MIMO Channel Estimation With LSTM Attention-Based CBDNet",
        "relation": "baseline_model",
        "tail": "benchmark approaches"
      },
      {
        "head": "Low-Resolution Massive MIMO Channel Estimation With LSTM Attention-Based CBDNet",
        "relation": "baseline_model",
        "tail": "comparative methods"
      },
      {
        "head": "LA-CBDNet",
        "relation": "evaluated_on",
        "tail": "one-bit received signals"
      },
      {
        "head": "LA-CBDNet",
        "relation": "uses_metric",
        "tail": "varying SNRs"
      },
      {
        "head": "LA-CBDNet",
        "relation": "uses_metric",
        "tail": "different pilot lengths"
      },
      {
        "head": "LA-CBDNet",
        "relation": "uses_metric",
        "tail": "number of users"
      },
      {
        "head": "Quantum solver for single-impurity Anderson models with particle-hole symmetry",
        "relation": "proposed_model",
        "tail": "variational quantum eigensolver (VQE)"
      },
      {
        "head": "Quantum solver for single-impurity Anderson models with particle-hole symmetry",
        "relation": "evaluated_on",
        "tail": "Anderson impurity model (AIM)"
      },
      {
        "head": "Quantum solver for single-impurity Anderson models with particle-hole symmetry",
        "relation": "uses_metric",
        "tail": "density of states (DOS)"
      },
      {
        "head": "Quantum solver for single-impurity Anderson models with particle-hole symmetry",
        "relation": "uses_metric",
        "tail": "quantum-computed moment (QCM)"
      },
      {
        "head": "Quantum solver for single-impurity Anderson models with particle-hole symmetry",
        "relation": "uses_metric",
        "tail": "impurity Green's function"
      },
      {
        "head": "Sentinel-1 SAR-based globally distributed co-seismic landslide detection by deep neural networks",
        "relation": "proposed_model",
        "tail": "Deep Neural Networks"
      },
      {
        "head": "Deep Neural Networks",
        "relation": "evaluated_on",
        "tail": "11 earthquake-induced widespread landslide events"
      },
      {
        "head": "Deep Neural Networks",
        "relation": "evaluated_on",
        "tail": "Haiti (2021) and Sumatra (2022) events"
      },
      {
        "head": "Deep Neural Networks",
        "relation": "uses_metric",
        "tail": "F1-score"
      },
      {
        "head": "Deep Neural Networks",
        "relation": "baseline_model",
        "tail": "optical remote sensing"
      },
      {
        "head": "Increasing resolution and accuracy in sub-seasonal forecasting through 3D U-Net: the western US",
        "relation": "proposed_model",
        "tail": "3D U-Net"
      },
      {
        "head": "Increasing resolution and accuracy in sub-seasonal forecasting through 3D U-Net: the western US",
        "relation": "baseline_model",
        "tail": "NWP models"
      },
      {
        "head": "3D U-Net",
        "relation": "evaluated_on",
        "tail": "ECMWF ensemble forecasting system"
      },
      {
        "head": "3D U-Net",
        "relation": "evaluated_on",
        "tail": "PRISM data"
      },
      {
        "head": "3D U-Net",
        "relation": "uses_metric",
        "tail": "pattern correlation coefficient"
      },
      {
        "head": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications",
        "relation": "proposed_model",
        "tail": "MobileNets"
      },
      {
        "head": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "Distilling the Knowledge in a Neural Network",
        "relation": "proposed_model",
        "tail": "single model"
      },
      {
        "head": "Distilling the Knowledge in a Neural Network",
        "relation": "proposed_model",
        "tail": "specialist models"
      },
      {
        "head": "Distilling the Knowledge in a Neural Network",
        "relation": "evaluated_on",
        "tail": "MNIST"
      },
      {
        "head": "single model",
        "relation": "baseline_model",
        "tail": "ensemble of models"
      },
      {
        "head": "acoustic model",
        "relation": "evaluated_on",
        "tail": "commercial system"
      },
      {
        "head": "Visualizing and Understanding Convolutional Networks",
        "relation": "proposed_model",
        "tail": "Krizhevsky et al."
      },
      {
        "head": "Visualizing and Understanding Convolutional Networks",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "Visualizing and Understanding Convolutional Networks",
        "relation": "evaluated_on",
        "tail": "Caltech-101"
      },
      {
        "head": "Visualizing and Understanding Convolutional Networks",
        "relation": "evaluated_on",
        "tail": "Caltech-256"
      },
      {
        "head": "Visualizing and Understanding Convolutional Networks",
        "relation": "uses_metric",
        "tail": "ImageNet classification benchmark"
      },
      {
        "head": "Visualizing and Understanding Convolutional Networks",
        "relation": "uses_metric",
        "tail": "state-of-the-art results"
      },
      {
        "head": "mHC: Manifold-Constrained Hyper-Connections",
        "relation": "proposed_model",
        "tail": "Manifold-Constrained Hyper-Connections (mHC)"
      },
      {
        "head": "mHC: Manifold-Constrained Hyper-Connections",
        "relation": "baseline_model",
        "tail": "Hyper-Connections (HC)"
      },
      {
        "head": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot",
        "relation": "proposed_model",
        "tail": "OGNet"
      },
      {
        "head": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot",
        "relation": "proposed_model",
        "tail": "YOLO-OG"
      },
      {
        "head": "OGNet",
        "relation": "evaluated_on",
        "tail": "Dish-10"
      },
      {
        "head": "YOLO-OG",
        "relation": "evaluated_on",
        "tail": "Dish-10"
      },
      {
        "head": "YOLO-OG",
        "relation": "evaluated_on",
        "tail": "Dish-20"
      },
      {
        "head": "YOLO-OG",
        "relation": "uses_metric",
        "tail": "mean Average Precision (mAP)"
      },
      {
        "head": "Physics-guided deep neural networks for bathymetric mapping using Sentinel-2 multi-spectral imagery",
        "relation": "proposed_model",
        "tail": "HybridBathNet"
      },
      {
        "head": "HybridBathNet",
        "relation": "baseline_model",
        "tail": "state-of-the-art methods"
      },
      {
        "head": "HybridBathNet",
        "relation": "evaluated_on",
        "tail": "diverse island regions"
      },
      {
        "head": "HybridBathNet",
        "relation": "uses_metric",
        "tail": "bathymetric inversion accuracy"
      },
      {
        "head": "HybridBathNet",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "HybridBathNet",
        "relation": "uses_metric",
        "tail": "generalization capability"
      },
      {
        "head": "Spike Memory Transformer: An Energy-Efficient Model in Distributed Learning Framework for Autonomous Depression Detection",
        "relation": "proposed_model",
        "tail": "spike memory transformer (SMT)"
      },
      {
        "head": "Spike Memory Transformer: An Energy-Efficient Model in Distributed Learning Framework for Autonomous Depression Detection",
        "relation": "proposed_model",
        "tail": "Computation-Oriented Hierarchical Depression Detection Internet of Things (IoT) Framework"
      },
      {
        "head": "spike memory transformer (SMT)",
        "relation": "evaluated_on",
        "tail": "D-Vlog dataset"
      },
      {
        "head": "spike memory transformer (SMT)",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "spike memory transformer (SMT)",
        "relation": "uses_metric",
        "tail": "inference power consumption"
      },
      {
        "head": "spike memory transformer (SMT)",
        "relation": "baseline_model",
        "tail": "classical deep learning methods"
      },
      {
        "head": "Multi-Perspective Semantic Segmentation of Ground Penetrating Radar Images for Pavement Subsurface Objects",
        "relation": "proposed_model",
        "tail": "AttnGPRNet"
      },
      {
        "head": "Multi-Perspective Semantic Segmentation of Ground Penetrating Radar Images for Pavement Subsurface Objects",
        "relation": "evaluated_on",
        "tail": "multi-view dataset using 3D GPR scans from over 100 kilometers of urban roads"
      },
      {
        "head": "AttnGPRNet",
        "relation": "uses_metric",
        "tail": "mIoU"
      },
      {
        "head": "AttnGPRNet",
        "relation": "uses_metric",
        "tail": "F1 score"
      },
      {
        "head": "Seeing Before Reasoning: A Unified Framework for Generalizable and Explainable Fake Image Detection",
        "relation": "proposed_model",
        "tail": "Forensic-Chat"
      },
      {
        "head": "Seeing Before Reasoning: A Unified Framework for Generalizable and Explainable Fake Image Detection",
        "relation": "proposed_model",
        "tail": "ExplainFake-Bench"
      },
      {
        "head": "Forensic-Chat",
        "relation": "baseline_model",
        "tail": "multimodal large language models (MLLMs)"
      },
      {
        "head": "Forensic-Chat",
        "relation": "evaluated_on",
        "tail": "ExplainFake-Bench"
      },
      {
        "head": "Forensic-Chat",
        "relation": "uses_metric",
        "tail": "generalization"
      },
      {
        "head": "Forensic-Chat",
        "relation": "uses_metric",
        "tail": "explainability"
      },
      {
        "head": "DenseNet Model with Attention Mechanisms for Robust Date Fruit Image Classification",
        "relation": "proposed_model",
        "tail": "DenseNet with Attention Mechanisms"
      },
      {
        "head": "DenseNet Model with Attention Mechanisms for Robust Date Fruit Image Classification",
        "relation": "evaluated_on",
        "tail": "Date Fruit Image Dataset"
      },
      {
        "head": "DenseNet Model with Attention Mechanisms for Robust Date Fruit Image Classification",
        "relation": "uses_metric",
        "tail": "Classification Accuracy"
      },
      {
        "head": "Evaluation of deep learning models using explainable AI with qualitative and quantitative analysis for rice leaf disease detection",
        "relation": "proposed_model",
        "tail": "ResNet50"
      },
      {
        "head": "Evaluation of deep learning models using explainable AI with qualitative and quantitative analysis for rice leaf disease detection",
        "relation": "proposed_model",
        "tail": "InceptionResNetV2"
      },
      {
        "head": "Evaluation of deep learning models using explainable AI with qualitative and quantitative analysis for rice leaf disease detection",
        "relation": "proposed_model",
        "tail": "DenseNet 201"
      },
      {
        "head": "Evaluation of deep learning models using explainable AI with qualitative and quantitative analysis for rice leaf disease detection",
        "relation": "proposed_model",
        "tail": "InceptionV3"
      },
      {
        "head": "Evaluation of deep learning models using explainable AI with qualitative and quantitative analysis for rice leaf disease detection",
        "relation": "proposed_model",
        "tail": "EfficientNetB0"
      },
      {
        "head": "Evaluation of deep learning models using explainable AI with qualitative and quantitative analysis for rice leaf disease detection",
        "relation": "proposed_model",
        "tail": "Xception"
      },
      {
        "head": "Evaluation of deep learning models using explainable AI with qualitative and quantitative analysis for rice leaf disease detection",
        "relation": "proposed_model",
        "tail": "VGG16"
      },
      {
        "head": "Evaluation of deep learning models using explainable AI with qualitative and quantitative analysis for rice leaf disease detection",
        "relation": "proposed_model",
        "tail": "AlexNet"
      },
      {
        "head": "Evaluation of deep learning models using explainable AI with qualitative and quantitative analysis for rice leaf disease detection",
        "relation": "baseline_model",
        "tail": "Local Interpretable Model-agnostic Explanations (LIME)"
      },
      {
        "head": "Evaluation of deep learning models using explainable AI with qualitative and quantitative analysis for rice leaf disease detection",
        "relation": "evaluated_on",
        "tail": "rice leaf disease detection dataset"
      },
      {
        "head": "Evaluation of deep learning models using explainable AI with qualitative and quantitative analysis for rice leaf disease detection",
        "relation": "uses_metric",
        "tail": "classification accuracy"
      },
      {
        "head": "Evaluation of deep learning models using explainable AI with qualitative and quantitative analysis for rice leaf disease detection",
        "relation": "uses_metric",
        "tail": "precision"
      },
      {
        "head": "Evaluation of deep learning models using explainable AI with qualitative and quantitative analysis for rice leaf disease detection",
        "relation": "uses_metric",
        "tail": "recall"
      },
      {
        "head": "Evaluation of deep learning models using explainable AI with qualitative and quantitative analysis for rice leaf disease detection",
        "relation": "uses_metric",
        "tail": "Intersection over Union (IoU)"
      },
      {
        "head": "Evaluation of deep learning models using explainable AI with qualitative and quantitative analysis for rice leaf disease detection",
        "relation": "uses_metric",
        "tail": "Dice Similarity Coefficient (DSC)"
      },
      {
        "head": "Evaluation of deep learning models using explainable AI with qualitative and quantitative analysis for rice leaf disease detection",
        "relation": "uses_metric",
        "tail": "overfitting ratio"
      },
      {
        "head": "Wind turbine blade surface defect detection model based on improved you only look once version 10 small and integrated compression",
        "relation": "proposed_model",
        "tail": "improved you only look once version 10 small and integrated compression"
      },
      {
        "head": "Attention is All you Need",
        "relation": "proposed_model",
        "tail": "Transformer"
      },
      {
        "head": "Transformer",
        "relation": "evaluated_on",
        "tail": "WMT 2014 English-to-German translation task"
      },
      {
        "head": "Transformer",
        "relation": "evaluated_on",
        "tail": "WMT 2014 English-to-French translation task"
      },
      {
        "head": "Transformer",
        "relation": "uses_metric",
        "tail": "BLEU"
      },
      {
        "head": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "relation": "proposed_model",
        "tail": "T5"
      },
      {
        "head": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "relation": "evaluated_on",
        "tail": "Colossal Clean Crawled Corpus"
      },
      {
        "head": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "relation": "uses_metric",
        "tail": "summarization"
      },
      {
        "head": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "relation": "uses_metric",
        "tail": "question answering"
      },
      {
        "head": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "relation": "uses_metric",
        "tail": "text classification"
      },
      {
        "head": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
        "relation": "proposed_model",
        "tail": "Two Time-Scale Update Rule"
      },
      {
        "head": "Two Time-Scale Update Rule",
        "relation": "baseline_model",
        "tail": "GANs"
      },
      {
        "head": "nuScenes: A Multimodal Dataset for Autonomous Driving",
        "relation": "proposed_model",
        "tail": "nuScenes"
      },
      {
        "head": "nuScenes: A Multimodal Dataset for Autonomous Driving",
        "relation": "baseline_model",
        "tail": "KITTI"
      },
      {
        "head": "nuScenes: A Multimodal Dataset for Autonomous Driving",
        "relation": "evaluated_on",
        "tail": "nuScenes"
      },
      {
        "head": "nuScenes: A Multimodal Dataset for Autonomous Driving",
        "relation": "uses_metric",
        "tail": "3D detection and tracking metrics"
      },
      {
        "head": "CARLA: An Open Urban Driving Simulator",
        "relation": "proposed_model",
        "tail": "classic modular pipeline"
      },
      {
        "head": "CARLA: An Open Urban Driving Simulator",
        "relation": "proposed_model",
        "tail": "end-to-end model trained via imitation learning"
      },
      {
        "head": "CARLA: An Open Urban Driving Simulator",
        "relation": "proposed_model",
        "tail": "end-to-end model trained via reinforcement learning"
      },
      {
        "head": "classic modular pipeline",
        "relation": "baseline_model",
        "tail": "end-to-end model trained via imitation learning"
      },
      {
        "head": "classic modular pipeline",
        "relation": "baseline_model",
        "tail": "end-to-end model trained via reinforcement learning"
      },
      {
        "head": "classic modular pipeline",
        "relation": "evaluated_on",
        "tail": "CARLA"
      },
      {
        "head": "end-to-end model trained via imitation learning",
        "relation": "evaluated_on",
        "tail": "CARLA"
      },
      {
        "head": "end-to-end model trained via reinforcement learning",
        "relation": "evaluated_on",
        "tail": "CARLA"
      },
      {
        "head": "classic modular pipeline",
        "relation": "uses_metric",
        "tail": "metrics provided by CARLA"
      },
      {
        "head": "end-to-end model trained via imitation learning",
        "relation": "uses_metric",
        "tail": "metrics provided by CARLA"
      },
      {
        "head": "end-to-end model trained via reinforcement learning",
        "relation": "uses_metric",
        "tail": "metrics provided by CARLA"
      },
      {
        "head": "Adding Conditional Control to Text-to-Image Diffusion Models",
        "relation": "proposed_model",
        "tail": "ControlNet"
      },
      {
        "head": "ControlNet",
        "relation": "baseline_model",
        "tail": "Stable Diffusion"
      },
      {
        "head": "ControlNet",
        "relation": "evaluated_on",
        "tail": "small (<50k) and large (>1m) datasets"
      },
      {
        "head": "Scalable Diffusion Models with Transformers",
        "relation": "proposed_model",
        "tail": "Diffusion Transformers (DiTs)"
      },
      {
        "head": "Scalable Diffusion Models with Transformers",
        "relation": "proposed_model",
        "tail": "DiT-XL/2"
      },
      {
        "head": "Scalable Diffusion Models with Transformers",
        "relation": "baseline_model",
        "tail": "U-Net"
      },
      {
        "head": "Diffusion Transformers (DiTs)",
        "relation": "evaluated_on",
        "tail": "ImageNet 512x512"
      },
      {
        "head": "Diffusion Transformers (DiTs)",
        "relation": "evaluated_on",
        "tail": "ImageNet 256x256"
      },
      {
        "head": "DiT-XL/2",
        "relation": "evaluated_on",
        "tail": "ImageNet 512x512"
      },
      {
        "head": "DiT-XL/2",
        "relation": "evaluated_on",
        "tail": "ImageNet 256x256"
      },
      {
        "head": "Diffusion Transformers (DiTs)",
        "relation": "uses_metric",
        "tail": "FID"
      },
      {
        "head": "Diffusion Transformers (DiTs)",
        "relation": "uses_metric",
        "tail": "Gflops"
      },
      {
        "head": "DiT-XL/2",
        "relation": "uses_metric",
        "tail": "FID"
      },
      {
        "head": "SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis",
        "relation": "proposed_model",
        "tail": "SDXL"
      },
      {
        "head": "SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis",
        "relation": "baseline_model",
        "tail": "Stable Diffusion"
      },
      {
        "head": "SDXL",
        "relation": "proposed_model",
        "tail": "refinement model"
      },
      {
        "head": "Domain randomization for transferring deep neural networks from simulation to the real world",
        "relation": "proposed_model",
        "tail": "deep neural network"
      },
      {
        "head": "deep neural network",
        "relation": "evaluated_on",
        "tail": "simulated images"
      },
      {
        "head": "deep neural network",
        "relation": "evaluated_on",
        "tail": "real images"
      },
      {
        "head": "deep neural network",
        "relation": "uses_metric",
        "tail": "object localization"
      },
      {
        "head": "Domain randomization for transferring deep neural networks from simulation to the real world",
        "relation": "proposed_model",
        "tail": "object detector"
      },
      {
        "head": "object detector",
        "relation": "evaluated_on",
        "tail": "simulator with non-realistic random textures"
      },
      {
        "head": "object detector",
        "relation": "uses_metric",
        "tail": "grasping in a cluttered environment"
      },
      {
        "head": "The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes",
        "relation": "proposed_model",
        "tail": "SYNTHIA"
      },
      {
        "head": "Sora as a World Model? A Complete Survey on Text-to-Video Generation",
        "relation": "proposed_model",
        "tail": "Sora"
      },
      {
        "head": "Sora as a World Model? A Complete Survey on Text-to-Video Generation",
        "relation": "baseline_model",
        "tail": "MNIST"
      },
      {
        "head": "OmniNWM: Omniscient Driving Navigation World Models",
        "relation": "proposed_model",
        "tail": "OmniNWM"
      },
      {
        "head": "OmniNWM: Omniscient Driving Navigation World Models",
        "relation": "baseline_model",
        "tail": "Existing models"
      },
      {
        "head": "OmniNWM",
        "relation": "evaluated_on",
        "tail": "panoramic videos"
      },
      {
        "head": "OmniNWM",
        "relation": "uses_metric",
        "tail": "video generation"
      },
      {
        "head": "OmniNWM",
        "relation": "uses_metric",
        "tail": "control accuracy"
      },
      {
        "head": "OmniNWM",
        "relation": "uses_metric",
        "tail": "long-horizon stability"
      },
      {
        "head": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask",
        "relation": "proposed_model",
        "tail": "ConsisDrive"
      },
      {
        "head": "ConsisDrive",
        "relation": "baseline_model",
        "tail": "world models"
      },
      {
        "head": "ConsisDrive",
        "relation": "evaluated_on",
        "tail": "nuScenes"
      },
      {
        "head": "ConsisDrive",
        "relation": "uses_metric",
        "tail": "driving video generation quality"
      },
      {
        "head": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving",
        "relation": "proposed_model",
        "tail": "UniDriveDreamer"
      },
      {
        "head": "UniDriveDreamer",
        "relation": "baseline_model",
        "tail": "previous state-of-the-art methods"
      },
      {
        "head": "UniDriveDreamer",
        "relation": "evaluated_on",
        "tail": "multi-camera video"
      },
      {
        "head": "UniDriveDreamer",
        "relation": "evaluated_on",
        "tail": "LiDAR sequence"
      },
      {
        "head": "UniDriveDreamer",
        "relation": "uses_metric",
        "tail": "video generation"
      },
      {
        "head": "UniDriveDreamer",
        "relation": "uses_metric",
        "tail": "LiDAR generation"
      },
      {
        "head": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
        "relation": "proposed_model",
        "tail": "MAD-LTX"
      },
      {
        "head": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
        "relation": "baseline_model",
        "tail": "SVD"
      },
      {
        "head": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
        "relation": "baseline_model",
        "tail": "LTX"
      },
      {
        "head": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
        "relation": "baseline_model",
        "tail": "video diffusion models"
      },
      {
        "head": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
        "relation": "baseline_model",
        "tail": "driving world models"
      },
      {
        "head": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
        "relation": "evaluated_on",
        "tail": "autonomous driving"
      },
      {
        "head": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
        "relation": "evaluated_on",
        "tail": "driving domains"
      },
      {
        "head": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
        "relation": "uses_metric",
        "tail": "structured motion"
      },
      {
        "head": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
        "relation": "uses_metric",
        "tail": "physical and social plausibility"
      },
      {
        "head": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
        "relation": "uses_metric",
        "tail": "compute"
      },
      {
        "head": "View-Consistent Diffusion Representations for 3D-Consistent Video Generation",
        "relation": "proposed_model",
        "tail": "ViCoDR"
      },
      {
        "head": "View-Consistent Diffusion Representations for 3D-Consistent Video Generation",
        "relation": "baseline_model",
        "tail": "camera-controlled video diffusion models"
      },
      {
        "head": "ViCoDR",
        "relation": "evaluated_on",
        "tail": "camera controlled image-to-video, text-to-video, and multi-view generation models"
      },
      {
        "head": "Fully Convolutional Networks for Semantic Segmentation",
        "relation": "proposed_model",
        "tail": "Fully Convolutional Networks"
      },
      {
        "head": "Fully Convolutional Networks for Semantic Segmentation",
        "relation": "baseline_model",
        "tail": "AlexNet"
      },
      {
        "head": "Fully Convolutional Networks for Semantic Segmentation",
        "relation": "baseline_model",
        "tail": "VGG net"
      },
      {
        "head": "Fully Convolutional Networks for Semantic Segmentation",
        "relation": "baseline_model",
        "tail": "GoogLeNet"
      },
      {
        "head": "Fully Convolutional Networks",
        "relation": "evaluated_on",
        "tail": "PASCAL VOC"
      },
      {
        "head": "Fully Convolutional Networks",
        "relation": "evaluated_on",
        "tail": "NYUDv2"
      },
      {
        "head": "Fully Convolutional Networks",
        "relation": "evaluated_on",
        "tail": "SIFT Flow"
      },
      {
        "head": "Fully Convolutional Networks",
        "relation": "uses_metric",
        "tail": "mean IU"
      },
      {
        "head": "Speech Recognition with Deep Recurrent Neural Networks",
        "relation": "proposed_model",
        "tail": "deep recurrent neural networks"
      },
      {
        "head": "Speech Recognition with Deep Recurrent Neural Networks",
        "relation": "proposed_model",
        "tail": "deep Long Short-term Memory RNNs"
      },
      {
        "head": "Speech Recognition with Deep Recurrent Neural Networks",
        "relation": "baseline_model",
        "tail": "deep feedforward networks"
      },
      {
        "head": "Speech Recognition with Deep Recurrent Neural Networks",
        "relation": "evaluated_on",
        "tail": "TIMIT phoneme recognition benchmark"
      },
      {
        "head": "Speech Recognition with Deep Recurrent Neural Networks",
        "relation": "uses_metric",
        "tail": "test set error"
      },
      {
        "head": "nuScenes: A multimodal dataset for autonomous driving",
        "relation": "proposed_model",
        "tail": "nuScenes"
      },
      {
        "head": "nuScenes: A multimodal dataset for autonomous driving",
        "relation": "baseline_model",
        "tail": "lidar based detection and tracking"
      },
      {
        "head": "nuScenes: A multimodal dataset for autonomous driving",
        "relation": "baseline_model",
        "tail": "image based detection and tracking"
      },
      {
        "head": "nuScenes: A multimodal dataset for autonomous driving",
        "relation": "evaluated_on",
        "tail": "nuScenes"
      },
      {
        "head": "nuScenes: A multimodal dataset for autonomous driving",
        "relation": "evaluated_on",
        "tail": "KITTI dataset"
      },
      {
        "head": "nuScenes: A multimodal dataset for autonomous driving",
        "relation": "uses_metric",
        "tail": "novel 3D detection and tracking metrics"
      },
      {
        "head": "Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World",
        "relation": "proposed_model",
        "tail": "domain randomization"
      },
      {
        "head": "domain randomization",
        "relation": "evaluated_on",
        "tail": "simulated images"
      },
      {
        "head": "domain randomization",
        "relation": "evaluated_on",
        "tail": "real images"
      },
      {
        "head": "domain randomization",
        "relation": "uses_metric",
        "tail": "object localization"
      },
      {
        "head": "Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World",
        "relation": "proposed_model",
        "tail": "object detector"
      },
      {
        "head": "object detector",
        "relation": "uses_metric",
        "tail": "grasping"
      }
    ]
  }
}