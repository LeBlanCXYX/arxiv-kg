<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Top Citations KG - Attention Is All You Need</title>
    <script src="echarts.min.js"></script>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        html, body { height: 100%; }
        body { background: #f5f5f5; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; }
        #main { width: 100%; height: 100%; min-height: 400px; }
        .panel {
            position: absolute; background: white; border-radius: 8px;
            box-shadow: 0 2px 12px rgba(0,0,0,0.1); padding: 20px; font-size: 14px; z-index: 999; max-width: 420px;
        }
        .header { top: 20px; left: 20px; }
        .stats { top: 20px; right: 20px; }
        .header h2 { margin-bottom: 10px; color: #333; }
        .header p { color: #666; margin: 5px 0; line-height: 1.5; }
        .stat-item { margin: 8px 0; }
        .stat-label { font-weight: bold; color: #333; }
        .stat-value { color: #0066cc; }
    </style>
</head>
<body>
    <div id="main"></div>
    <div class="panel header">
        <h2>üìÑ Attention Is All You Need</h2>
        <p><strong>‰ΩúËÄÖ:</strong> Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit</p>
        <p><strong>ÂèëË°®Êó•Êúü:</strong> 2017-06-12</p>
        <p><strong>ArXiv ID:</strong> <code>1706.03762</code></p>
    </div>
    <div class="panel stats">
        <div class="stat-item"><span class="stat-label">ËÆ∫ÊñáËäÇÁÇπ:</span> <span class="stat-value">459</span></div>
        <div class="stat-item"><span class="stat-label">Á†îÁ©∂ËÄÖËäÇÁÇπ:</span> <span class="stat-value">3975</span></div>
        <div class="stat-item"><span class="stat-label">ÂÖ≥Á≥ªÊï∞:</span> <span class="stat-value">7917</span></div>
        <div class="stat-item"><span class="stat-label">ÂºïÁî®ÁöÑËÆ∫Êñá (top N):</span> <span class="stat-value">5</span></div>
        <div class="stat-item"><span class="stat-label">Ë¢´ÂºïÁî®ÁöÑËÆ∫Êñá (top N):</span> <span class="stat-value">5</span></div>
    </div>
    <script type="text/javascript">
        function initChart() {
            if (typeof echarts === 'undefined') {
                document.getElementById('main').innerHTML = '<p style="padding:20px">Êó†Ê≥ïÂä†ËΩΩ EChartsÔºåËØ∑Ê£ÄÊü•ÁΩëÁªúÊàñ CDN„ÄÇ</p>';
                return;
            }
            var chartDom = document.getElementById('main');
            var myChart = echarts.init(chartDom);
            var option = {
                tooltip: { formatter: function(params) {
                    if (params.dataType === 'node') return params.name + ' (' + (params.value || '') + ')';
                    return (params.source && params.source.name) + ' ' + (params.value || '') + ' ' + (params.target && params.target.name);
                }},
                legend: { data: ["Dataset", "Researcher", "Metric", "AIPaper", "AIModel"] },
                series: [{
                    type: 'graph', layout: 'force',
                    data: [{"name": "Attention Is All You Need", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Deep Residual Learning for Image Recognition", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Adam: A Method for Stochastic Optimization", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Long Short-Term Memory", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Dropout: a simple way to prevent neural networks from overfitting", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Rethinking the Inception Architecture for Computer Vision", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "A comprehensive review of recommender systems: Transitioning from theory to practice", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "DiffusionEngine: Diffusion model is scalable data engine for object detection", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "ImageNet classification with deep convolutional neural networks", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Et al", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Noise Perception Self-Supervised Learning for Unsupervised Person Re-Identification", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Auto-Encoding Variational Bayes", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Supporting Online Material for Reducing the Dimensionality of Data with Neural Networks", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Speech recognition with deep recurrent neural networks", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Weakly Supervised Image Dehazing via Physics-Based Decomposition", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Generating an annual 30\u00a0m rice cover product for monsoon Asia (2018\u20132023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Going deeper with convolutions", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "ImageNet Large Scale Visual Recognition Challenge", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Diffusion Transformers with Representation Autoencoders", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Scaling Large Language Models for Next-Generation Single-Cell Analysis", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "HybridVisionNet: An advanced hybrid deep learning framework for automated multi-class ocular disease diagnosis using fundus imaging", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Attention is All you Need", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "nuScenes: A Multimodal Dataset for Autonomous Driving", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "CARLA: An Open Urban Driving Simulator", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Sora as a World Model? A Complete Survey on Text-to-Video Generation", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "OmniNWM: Omniscient Driving Navigation World Models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "MAD: Motion Appearance Decoupling for efficient Driving World Models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "I and J", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "ImageNet: A large-scale hierarchical image database", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "A and V", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "What matters for Representation Alignment: Global Information or Spatial Structure?", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "SCB-DETR: Multiscale Deformable Transformers for Occlusion-Resilient Student Learning Behavior Detection in Smart Classroom", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Bio-Inspired Fine-Tuning for Selective Transfer Learning in Image Classification", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Hand Sign Language Detection Using Deep Learning", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "and as an in", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Microsoft COCO: Common Objects in Context", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "FANet: Frequency-Aware Attention-Based Tiny-Object Detection in Remote Sensing Images", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "UAV-based multimodal object detection via feature enhancement and dynamic gated fusion", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "An empirical analysis of deep learning methods for small object detection from satellite imagery", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Representation Learning: A Review and New Perspectives", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "In Advances in Neural Information Processing Systems", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "GenAD: Generative End-to-End Autonomous Driving", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Improving Video Generation with Human Feedback", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Speech Recognition with Deep Recurrent Neural Networks", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Learning representations by back-propagating errors", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Bidirectional recurrent neural networks", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Connectionist Temporal Classi\ufb01cation: Labelling Unsegmented Sequence Data with Recurrent Neural Networks", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "2005 Special Issue: Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "A high-performance neuroprosthesis for speech decoding and avatar control", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "A high-performance speech neuroprosthesis", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Loss of plasticity in deep continual learning", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "An analog-AI chip for energy-efficient speech recognition and transcription", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "A Mathematical Theory of Communication", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "L$^3$: Large Lookup Layers", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Going Deeper with Convolutions", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Gradient-based learning applied to document recognition", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Regression Shrinkage and Selection via the Lasso", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "LifeCLEF Plant Identification Task 2015", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Enhancing pine wilt disease detection with synthetic data and external attention-based transformers", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "A comprehensive review on YOLO versions for object detection", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Harnessing large vision and language models in agriculture: a review", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Multi-axis vision transformer for medical image segmentation", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "A comprehensive review of facial beauty prediction using deep learning techniques", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "A systematic comparison of predictive models on the retina", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Distinctive Image Features from Scale-Invariant Keypoints", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Aligning machine and human visual representations across abstraction levels", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Learning Transferable Visual Models From Natural Language Supervision", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "GENERATIVE ADVERSARIAL NETS", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Improved Mean Flows: On the Challenges of Fastforward Generative Models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "PixelDiT: Pixel Diffusion Transformers for Image Generation", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "A comprehensive review of object detection with traditional and deep learning methods", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Diffusion Language Models are Super Data Learners", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "FLEX: Continuous Agent Evolution via Forward Learning from Experience", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "nuScenes: A multimodal dataset for autonomous driving", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Histograms of oriented gradients for human detection", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Controllable Video Generation: A Survey", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Adversarial Attacks on Autonomous Driving Systems in the Physical World: A Survey", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Detect Anything via Next Point Prediction", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Human-level control through deep reinforcement learning", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Asynchronous Methods for Deep Reinforcement Learning", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Pseudo-Simulation for Autonomous Driving", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "A Survey on Vision-Language-Action Models for Autonomous Driving", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Understanding World or Predicting Future? A Comprehensive Survey of World Models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "A Survey of Multimodal Learning: Methods, Applications, and Future", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Effects of Generative AI in Tourism Industry", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Squeeze-and-Excitation Networks", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Decoupled Weight Decay Regularization", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "DVGT: Driving Visual Geometry Transformer", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Adding Conditional Control to Text-to-Image Diffusion Models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Scaling Instruction-Finetuned Language Models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "High-Resolution Image Synthesis with Latent Diffusion Models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "AUTO-ENCODING VARIATIONAL BAYES", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Generative Adversarial Networks", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Generalization of Diffusion Models Arises with a Balanced Representation Space", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Stable Velocity: A Variance Perspective on Flow Matching", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Laminating Representation Autoencoders for Efficient Diffusion", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Adaptive 1D Video Diffusion Autoencoder", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Test-Time Conditioning with Representation-Aligned Visual Features", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Bio-inspired fine-tuning for selective transfer learning in image classification", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Densely Connected Convolutional Networks", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "An extratropical cyclone center location method on satellite images based on transfer learning", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "An Advanced Convolutional Neural Network Architecture Utilizing Transfer Learning for Melanoma Detection", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Adaptive Transfer Learning for Surgical Tool Presence Detection in Laparoscopic Videos Through Gradual Freezing Fine\u2010Tuning", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "VGG Induced Deep Hand Sign Language Detection", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "MediaPipe: A Framework for Building Perception Pipelines", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Hand Pose Estimation and Hand Shape Classification Using Multi-layered Randomized Decision Forests", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "A robust static hand gesture recognition system using geometry based normalizations and Krawtchouk moments", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Hand signal classification system for sign language communication in Virtual Reality", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Real-Time Static Hand Sign Recognition System using MediaPipe and Fully Connected Neural Network", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Real-Time Gesture Recognition to Aid Communication in Children with Motor Impairments", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Learning Multiple Layers of Features from Tiny Images", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "MuMu-LLaMA: Multi-modal music understanding and generation via large language models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Visualizing Data using t-SNE", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "LLM Social Simulations Are a Promising Research Method", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Federated Contrastive Learning With Feature-Based Distillation for Human Activity Recognition", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Diffuse and Disperse: Image Generation with Representation Regularization", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Deep Autoencoder Neural Networks: A Comprehensive Review and New Perspectives", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "U-Net: Convolutional Networks for Biomedical Image Segmentation", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Improved Distribution Matching Distillation for Fast Image Synthesis", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Evolutionary optimization of model merging recipes", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Denoising Diffusion Probabilistic Models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Feature Pyramid Networks for Object Detection", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "How Far is Video Generation from World Model: A Physical Law Perspective", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "EMMA: End-to-End Multimodal Model for Autonomous Driving", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Language Models are Few-Shot Learners", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Proximal Policy Optimization Algorithms", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Flow-GRPO: Training Flow Matching Models via Online RL", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "DanceGRPO: Unleashing GRPO on Visual Generation", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "SkyReels-V2: Infinite-length Film Generative Model", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Unified Reward Model for Multimodal Understanding and Generation", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "GPT-4 Technical Report", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "LLaMA: Open and Efficient Foundation Language Models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Chain of Thought Prompting Elicits Reasoning in Large Language Models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Training Verifiers to Solve Math Word Problems", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Crystal structure of the nucleosome core particle at 2.8\u2009\u00c5 resolution", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "A catalogue of splice junction sequences.", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Origin of the Genetic Code", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "SGDR: Stochastic Gradient Descent with Warm Restarts", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Compression of individual sequences via variable-rate coding", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Pointer Sentinel Mixture Models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Measuring Massive Multitask Language Understanding", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Let's Verify Step by Step", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Pattern Recognition and Machine Learning", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Bagging Predictors", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Automatic Image-Based Plant Disease Severity Estimation Using Deep Learning", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Plant identification using deep neural networks via optimization of transfer learning parameters", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "New perspectives on plant disease characterization based on deep learning", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Deep Learning for Plant Identification in Natural Environment", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Going deeper in the automated identification of Herbarium specimens", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "MobileNetV2: Inverted Residuals and Linear Bottlenecks", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Communication-Efficient Learning of Deep Networks from Decentralized Data", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "A Comprehensive Survey on Transfer Learning", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Privacy Preserved and Decentralized Smartphone Recommendation System", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "A Survey on Heterogeneity Taxonomy, Security and Privacy Preservation in the Integration of IoT, Wireless Sensor Networks and Federated Learning", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "A privacy-preserving federated learning with a secure collaborative for malware detection models using Internet of Things resources", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Graph-based deep learning techniques for remote sensing applications: Techniques, taxonomy, and applications - A comprehensive review", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Federated Learning Architectures: A Performance Evaluation With Crop Yield Prediction Application", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Segment Anything", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Visual Instruction Tuning", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Improved Baselines with Visual Instruction Tuning", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Vision-Language-Action (VLA) Models: Concepts, Progress, Applications and Challenges", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Leave No Observation Behind: Real-time Correction for VLA Action Chunks", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Latent Chain-of-Thought World Modeling for End-to-End Driving", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "TransSIL: A Silhouette Cue-Aware Image Classification Framework for Bird Ecological Monitoring Systems", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "CBRFormer: rendering technology-based transformer for refinement segmentation of bridge crack images", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Deep contrastive learning enables genome-wide virtual screening.", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "LLaVA-based semantic feature modulation diffusion model for underwater image enhancement", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "3DGS-Drag: Dragging Gaussians for Intuitive Point-Based 3D Editing", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Attention mechanisms in neural networks", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Bidirectional Normalizing Flow: From Data to Noise and Back", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "One-step Latent-free Image Generation with Pixel Mean Flows", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Meta Flow Maps enable scalable reward alignment", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Generative Modeling via Drifting", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "A Simple Framework for Contrastive Learning of Visual Representations", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Simulating the Visual World with Artificial Intelligence: A Roadmap", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "RecTok: Reconstruction Distillation along Rectified Flow", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "RePack then Refine: Efficient Diffusion Transformer with Vision Foundation Model", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Diffusion Models Beat GANs on Image Synthesis", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "DINOv2: Learning Robust Visual Features without Supervision", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Distributed Representations of Words and Phrases and their Compositionality", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "GloVe: Global Vectors for Word Representation", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Deep Contextualized Word Representations", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Protein Language Models: Is Scaling Necessary?", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Generative Classifiers Avoid Shortcut Solutions", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Edge Large AI Model Agent-Empowered Cognitive Multimodal Semantic Communication", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Language Models are Unsupervised Multitask Learners", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "The Llama 3 Herd of Models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "A Survey on Diffusion Language Models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Training Optimal Large Diffusion Language Models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Encoder-Decoder Diffusion Language Models for Efficient Training and Inference", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Paper", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Qwen Technical Report", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Code Llama: Open Foundation Models for Code", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Set Block Decoding is a Language Model Inference Accelerator", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Constrained Decoding of Diffusion LLMs with Context-Free Grammars", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Paper2Video: Automatic Video Generation from Scientific Papers", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "LLaVA-OneVision: Easy Visual Task Transfer", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Spatial Reasoning in Multimodal Large Language Models: A Survey of Tasks, Benchmarks and Methods", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "SpatialTree: How Spatial Abilities Branch Out in MLLMs", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "You Only Look Once: Unified, Real-Time Object Detection", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "WeDetect: Fast Open-Vocabulary Object Detection as Retrieval", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "v1: Learning to Point Visual Tokens for Multimodal Grounded Reasoning", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Reinforcement Learning: An Introduction", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Continuous control with deep reinforcement learning", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Learning to Reason under Off-Policy Guidance", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement learning for LLMs", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Fully convolutional networks for semantic segmentation", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Diffusion Models in Vision: A Survey", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "SegNeXt: Rethinking Convolutional Attention Design for Semantic Segmentation", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation With Transformers", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Segment Anything in High Quality", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "PIDNet: A Real-time Semantic Segmentation Network Inspired by PID Controllers", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Qwen2 Technical Report", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "3D Gaussian Splatting for Real-Time Radiance Field Rendering", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Structure-from-Motion Revisited", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Generalized Trajectory Scoring for End-to-end Multimodal Planning", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "RAP: 3D Rasterization Augmented End-to-End Planning", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Flow Matching for Generative Modeling", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Diffusion policy: Visuomotor policy learning via action diffusion", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Large Language Models for Robotics: A Survey", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "ReSim: Reliable World Simulation for Autonomous Driving", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "A Survey on Vision-Language-Action Models for Embodied AI", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "AgentSquare: Automatic LLM Agent Search in Modular Design Space", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "CityGPT: Empowering Urban Spatial Cognition of Large Language Models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Epona: Autoregressive Diffusion World Model for Autonomous Driving", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Unlearning Concepts in Diffusion Model via Concept Domain Correction and Concept Preserving Gradient", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Large multimodal models evaluation: a survey", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Gradient-Guided Learning Network for Infrared Small Target Detection", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "A Retrieval-Augmented Generation Approach to Extracting Algorithmic Logic from Neural Networks", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "IMobileTransformer: A fusion-based lightweight model for rice disease identification", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Image quality assessment: from error visibility to structural similarity", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Scalable Diffusion Models with Transformers", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "RoFormer: Enhanced Transformer with Rotary Position Embedding", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Vision meets robotics: The KITTI dataset", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Least-Squares Estimation of Transformation Parameters Between Two Point Patterns", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Deep Learning", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Training language models to follow instructions with human feedback", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Toward expert-level medical question answering with large language models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Equivariant Diffusion for Crystal Structure Prediction", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Native and Compact Structured Latents for 3D Generation", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Progressive Learned Image Transmission for Semantic Communication Using Hierarchical VAE", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "STORM: Search-Guided Generative World Models for Robotic Manipulation", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "A New Approach to Linear Filtering and Prediction Problems", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Continuous 3D Perception Model with Persistent State", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Gen3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "FLARE: Feed-Forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "UniDepthV2: Universal Monocular Metric Depth Estimation Made Simpler", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Adaptive transfer learning for surgical tool presence detection in laparoscopic videos through gradual freezing fine-tuning", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Rich feature hierarchies for accurate object detection and semantic segmentation", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Evolutionary Optimization of Model Merging Recipes", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Federated Learning Architectures: A Performance Evaluation with Crop Yield Prediction Application", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Enhancing Descriptive Image Quality Assessment with A Large-scale Multi-modal Dataset", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Deep contextualized word representations", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Fully Convolutional Networks for Semantic Segmentation", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with Transformers", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "FLARE: Feed-forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Ashish Vaswani", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Noam Shazeer", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Niki Parmar", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jakob Uszkoreit", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Llion Jones", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Aidan N. Gomez", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lukasz Kaiser", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Illia Polosukhin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kaiming He", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiangyu Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shaoqing Ren", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jian Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Diederik P. Kingma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jimmy Ba", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sepp Hochreiter", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "J. Schmidhuber", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nitish Srivastava", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Geoffrey E. Hinton", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "A. Krizhevsky", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "I. Sutskever", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "R. Salakhutdinov", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Christian Szegedy", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Vincent Vanhoucke", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sergey Ioffe", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jonathon Shlens", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zbigniew Wojna", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shaina Raza", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mizanur Rahman", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Safiullah Kamawal", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Armin Toroghi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ananya Raval", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "F. Navah", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Amirmohammad Kazemeini", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhuoran Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xi Guo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chenjing Ding", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chiyu Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wei Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yanyong Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zisheng Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Junjie Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chisen Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Cong Peng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jianping Xuan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tielin Shi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ming J. Zuo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jinghuan Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wang Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jian Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Manlin Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jie Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuxi Ren", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiahong Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ming Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Andy J. Ma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Karen Simonyan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Andrew Zisserman", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "P. Cochat", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "L. Vaucoret", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "J. Sarles", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ross Girshick", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuqi Cheng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yunkang Cao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haiming Yao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wei Luo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Cheng Jiang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hui Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Weiming Shen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Naveen Kumar Srinivasa", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ajeet Rao Chalamala", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kumar Singh", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ieee Krishna Mohan Senior Member", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "K. Naveen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Srinivasa Rao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ajeet Kumar Singh", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hongbo Jiang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lei Ye", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jingyang Hu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaotian Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Siyu Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wei Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kehua Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jingya Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jianfeng Wen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Weiping Ding", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chunlin Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiatian Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhiyong Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Diederik P Kingma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Max Welling", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "John C. Duchi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Elad Hazan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Y. Singer", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Alex Graves", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Abdel-rahman Mohamed", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Geoffrey Hinton", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nian Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhigao Cui", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yanzhao Su", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yunwei Lan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuanliang Xue", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Cong Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Aihua Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Leong Kah Meng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ho Hooi Yi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ng Bo Wei", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lim Jia Xin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zailan Arabee Abdul Salam", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xin Cheng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wangding Zeng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Damai Dai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qinyu Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bingxuan Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhenda Xie", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kezhao Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xingkai Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhewen Hao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yukun Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Han Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Huishuai Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dongyan Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenfeng Liang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mostafa Saberian", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Vidya Samadi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ioana Popescu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Husheng Fang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shunlin Liang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenyuan Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yongzhe Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Han Ma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jianglei Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yichuan Ma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tao He", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Feng Tian", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fengjiao Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hui Liang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wei Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yangqing Jia", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Pierre Sermanet", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Scott Reed", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dragomir Anguelov", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dumitru Erhan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Andrew Rabinovich", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Olga Russakovsky", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jia Deng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hao Su", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jonathan Krause", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sanjeev Satheesh", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sean Ma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhiheng Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Andrej Karpathy", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Aditya Khosla", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Michael Bernstein", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Alexander C. Berg", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Li Fei-Fei", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Boyang Zheng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nanye Ma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shengbang Tong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Saining Xie", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "S. Rizvi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Daniel Levine", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Aakash Patel", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shiyang Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Eric Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Curtis Jamison Perry", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ivan Vrkic", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nicole Mayerli Constante", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zirui Fu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sizhuang He", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "David Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Cerise Tang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhuoyang Lyu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Rayyan Y Darji", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chang Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Emily Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "David Jeong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lawrence Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jennifer Kwan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "David Braun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Brian Hafler", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hattie Chung", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "R. M. Dhodapkar", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Paul F. Jaeger", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bryan Perozzi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jeffrey Ishizuka", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shekoofeh Azizi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "D. van Dijk", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhengyu Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hanwei Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Renjue Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "R. Sicre", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "L. Amsaleg", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Michael Backes", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qi Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chao Shen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "\u015eafak K\u0131l\u0131\u00e7", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yifei Ge", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhuo Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xuebin Yue", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hengyi Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lin Meng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Colin Raffel", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Adam Roberts", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Katherine Lee", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sharan Narang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Michael Matena", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yanqi Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wei Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Peter J. Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "M. Heusel", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hubert Ramsauer", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Thomas Unterthiner", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bernhard Nessler", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Holger Caesar", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Varun Bankiti", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Alex H. Lang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sourabh Vora", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Venice Erin Liong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qiang Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Anush Krishnan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yu Pan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Giancarlo Baldan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Oscar Beijbom", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Alexey Dosovitskiy", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "German Ros", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Felipe Codevilla", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Antonio Lopez", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Vladlen Koltun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fachrina Dewi Puspitasari", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chaoning Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Joseph Cho", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Adnan Haider", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Noor Ul Eman", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Omer Amin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Alexis Mankowski", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Muhammad Umair", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jingyao Zheng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sheng Zheng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lik-Hang Lee", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Caiyan Qin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tae-Ho Kim", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Choong Seon Hong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yang Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Heng Tao Shen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bohan Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhuang Ma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dalong Du", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Baorui Peng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhujin Liang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhenqiang Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chao Ma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yueming Jin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hao Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenjun Zeng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xin Jin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guosheng Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yaozeng Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaofeng Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zheng Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tingdong Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guan Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yongchen Zai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ji Jiao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Changliang Xue", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaole Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhen Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Futang Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xingang Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ahmad Rahimi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Valentin Gerard", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Eloi Zablocki", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Matthieu Cord", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Alexandre Alahi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "W. Marsden", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wei Dong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "R. Socher", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Li-Jia Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "K. Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "R. Stephenson", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jaskirat Singh", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xingjian Leng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zongze Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Liang Zheng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Richard Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Eli Shechtman", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhifeng Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Minghui Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chunyan Zeng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Longlong Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ehsan Zakeri", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Amanda Spilkin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hanae Elmekki", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Antonela Zanuttini", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "L. Kadem", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jamal Bentahar", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wen-Fang Xie", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Philippe Pibarot", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ana Davila", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jacinto Colan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yasuhisa Hasegawa", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Subham Sharma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sharmila Subudhi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tsung-Yi Lin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Michael Maire", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Serge Belongie", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lubomir Bourdev", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "James Hays", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Pietro Perona", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Deva Ramanan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "C. Lawrence Zitnick", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Piotr Doll\u00e1r", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zixiao Wen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Peifeng Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuhan Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jingming Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiantai Xiang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuan Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Huixian Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yongchao Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guangyao Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yu Gu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Weili Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dongliang Peng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaohui Yuan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Aniv Chakravarty", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Elinor M. Lichtenberg", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lichuan Gu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhenchun Wei", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tian Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yoshua Bengio", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Aaron Courville", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Pascal Vincent", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "H. Larochelle", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Isabelle Lajoie", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Pierre-Antoine Manzagol", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "D. Touretzky", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "M. C. Mozer", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "M. E. Hasselmo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "RegressionChristopher", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "I. K.", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "WilliamsNeural", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "GroupAston", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "UniversityBirmingham", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Danilo Jimenez Rezende", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "S. Mohamed", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Daan Wierstra", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shanchuan Lin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Anran Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiao Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhiyuan Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiajiong Cao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhiquan Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuming Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chenguang Ma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenzhao Zheng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ruiqi Song", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xianda Guo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chenming Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Long Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shiyin Lu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yang Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qing-Guo Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhao Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Weihua Luo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kaifu Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Han-Jia Ye", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jie Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Gongye Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiajun Liang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ziyang Yuan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaokun Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mingwu Zheng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiele Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qiulin Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Menghan Xia", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xintao Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaohong Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fei Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Pengfei Wan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Di Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kun Gai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yujiu Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wanli Ouyang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "D. Rumelhart", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ronald J. Williams", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "M. Schuster", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "K. Paliwal", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Santiago Fern\u00b4andez", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Faustino J. Gomez", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "J\u00a8urgen Schmidhuber", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sean L. Metzger", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "K. T. Littlejohn", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Alexander B. Silva", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "D. Moses", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Margaret P. Seaton", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ran Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Maximilian E. Dougherty", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jessie R. Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Peter Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "M. Berger", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Inga Zhuravleva", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "A. Tu-Chan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "K. Ganguly", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "G. Anumanchipalli", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Edward F. Chang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tianming Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bin Feng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jinpeng Huo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yu Xiao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wengan Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jin Peng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zehua Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chengjie Du", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenxian Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "G. Zou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lei Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Francis R. Willett", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Erin M. Kunz", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chaofei Fan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Donald T. Avansino", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "G. Wilson", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Eun Young Choi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Foram B. Kamdar", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "M. Glasser", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "L. Hochberg", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "S. Druckmann", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "K. Shenoy", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "J. Henderson", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shibhansh Dohare", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "J. F. Hernandez-Garcia", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qingfeng Lan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Parash Rahman", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "A. Mahmood", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "R. Sutton", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "S. Ambrogio", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "P. Narayanan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "A. Okazaki", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "A. Fasoli", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "C. Mackin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "K. Hosokawa", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "A. Nomura", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Takeo Yasuda", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "An Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "A. Friz", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "M. Ishii", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "J. Luquin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Y. Kohda", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "N. Saulnier", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "K. Brew", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Samuel Choi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "I. Ok", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Timothy Philip", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Victor Chan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "M. Silvestre", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ishtiaq Ahsan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Vijay Narayanan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "H. Tsai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Geoffrey W. Burr", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "J. Shin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sang Joon Kim", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yike Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haotong Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhouchen Lin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Muhan Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ning Ding", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fangcheng Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kyungrae Kim", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Linji Hao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kyeng-Hun Lee", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hyeonmok Ko", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yehui Tang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Huinan Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xuyang Feng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Junhong Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Junchen Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kaiwen Deng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kai Ding", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shengning Long", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiaxue Shuai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhaorong Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shiping Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guirong Xue", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhan Xiao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Albert Tseng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Christopher De Sa", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hong Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiaqi Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chao Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xing Hu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Linkun Lyu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiaqi Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xurui Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bo Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fengcun Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yulei Qian", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lingtong Si", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yerui Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Rumei Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Peng Pei", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuchen Xie", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xunliang Cai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "R. Tibshirani", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Herve Goeau", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Pierre Bonnet", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Alexis Joly", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Safa Ben Atitallah", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Maha Driss", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Henda Ben Ghezela", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sareer Ul Amin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yonghoon Jung", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Muhammad Fayaz", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bumsoo Kim", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sanghyun Seo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ay\u015fe Aybilge Murat", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "M. S. K\u0131ran", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hongyan Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shuai Qin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Min Su", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chengzhi Lin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Anjie Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Junfeng Gao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Abdul Rehman Khan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Asifullah Khan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "D. E. Boukhari", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "F. Dornaika", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "A. Chemsa", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Abdelmalik Taleb-Ahmed", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Michaela Vystr\u010dilov\u00e1", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shashwat Sridhar", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Max F. Burg", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "M. Khani", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dimokratis Karamanlis", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "H. Schreyer", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Varsha Ramakrishna", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Steffen Kr\u00fcppel", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "S\u00f6ren J. Zapp", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Matthias Mietsch", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "T. Gollisch", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Alexander S. Ecker", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "D. Lowe", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiang An", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yin Xie", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kaicheng Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenkang Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiuwei Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zheng Cheng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yirui Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Songcen Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Changrui Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Didi Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chunsheng Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Huajie Tan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chunyuan Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jing Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jie Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiyao Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bin Qin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yumeng Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zizhen Yan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ziyong Feng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ziwei Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bo Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiankang Deng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kento Kawaharazuka", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jihoon Oh", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jun Yamada", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ingmar Posner", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuke Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lukas Muttenthaler", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Klaus Greff", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Frieda Born", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bernhard Spitzer", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Simon Kornblith", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Klaus-Robert Muller", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Andrew Kyle Lampinen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lucas Beyer", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Alexander Kolesnikov", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dirk Weissenborn", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaohua Zhai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mostafa Dehghani", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Matthias Minderer", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Georg Heigold", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sylvain Gelly", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Neil Houlsby", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Alec Radford", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jong Wook Kim", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chris Hallacy", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Aditya Ramesh", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Gabriel Goh", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sandhini Agarwal", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Girish Sastry", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Amanda Askell", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Pamela Mishkin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jack Clark", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Gretchen Krueger", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ilya Sutskever", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Individualized Treat", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jinsung Yoon", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhengyang Geng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yiyang Lu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "J. Zico Kolter", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiachen Lei", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Keli Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Julius Berner", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haiming Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hongkai Zheng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiahong Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiangxiang Chu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Minglei Shi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haolin Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Borui Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bohan Zeng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaoshi Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuanxing Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Huan Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jie Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiwen Lu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yongsheng Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wei Xiong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Weili Nie", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yichen Sheng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shiqiu Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiebo Luo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhiheng Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Weiming Ren", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haozhe Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zijian Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shoufa Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haonan Qiu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaoke Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhaochong An", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fanny Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Aditya Patel", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Viktar Atliha", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tony Ng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiao Han", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chuyan Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chenyang Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ding Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Juan-Manuel Perez-Rua", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sen He", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "J\u00fcrgen Schmidhuber", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenhu Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ping Luo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tao Xiang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jonas Schult", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuren Cong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jacob Devlin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ming-Wei Chang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kenton Lee", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kristina Toutanova", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Vrushali Pagire", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "M. Chavali", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ashish Kale", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jinjie Ni", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qian Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Longxu Dou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chao Du", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zili Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hang Yan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tianyu Pang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Michael Qizhe Shieh", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zirui Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lin Zheng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhihui Xie", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiacheng Ye", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiahui Gao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shansan Gong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yansong Feng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhenguo Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wei Bi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guorui Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lingpeng Kong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhicheng Cai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xinyuan Guo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yu Pei", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiangtao Feng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jinsong Su", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiangjie Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ya-Qin Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wei-Ying Ma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mingxuan Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hao Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mingyue Cheng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jie Ouyang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shuo Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ruiran Yan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yucong Luo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zirui Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Daoyu Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qi Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Enhong Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Navneet Dalal", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "B. Triggs", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yue Ma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kunyu Feng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhongyuan Hu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xinyu Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yucheng Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mingzhe Zheng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bingyuan Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qinghe Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xuanhua He", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hongfa Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chenyang Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hongyu Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yingqing He", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zeyu Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhifeng Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiu Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sirui Han", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yike Guo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dan Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Linfeng Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qifeng Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lijun Chi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "M. Msahli", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qingjie Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Han Qiu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tianwei Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "G\u00e9rard Memmi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Meikang Qiu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "NVIDIA", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": ":", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yan Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenjie Luo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Junjie Bai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yulong Cao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tong Che", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ke Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuxiao Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jenna Diamond", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yifan Ding", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenhao Ding", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Liang Feng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Greg Heinrich", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jack Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Peter Karkus", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Boyi Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Pinyi Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dongran Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ming-Yu Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Langechuan Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhijian Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jason Lu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yunxiang Mao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Pavlo Molchanov", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lindsey Pavao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhenghao Peng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mike Ranzinger", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ed Schmerling", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shida Shen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yunfei Shi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sarah Tariq", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ran Tian", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tilman Wekel", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xinshuo Weng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tianjun Xiao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Eric Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaodong Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yurong You", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaohui Zeng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenyuan Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Boris Ivanovic", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Marco Pavone", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mohsen Gholami", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ahmad Rezaei", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhou Weimin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sitong Mao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shunbo Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yong Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mohammad Akbari", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qing Jiang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Junan Huo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xingyu Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuda Xiong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhaoyang Zeng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yihao Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tianhe Ren", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Junzhi Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lei Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Volodymyr Mnih", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "K. Kavukcuoglu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "David Silver", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Andrei A. Rusu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "J. Veness", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Marc G. Bellemare", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Martin A. Riedmiller", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "A. Fidjeland", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Georg Ostrovski", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Stig Petersen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Charlie Beattie", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Amir Sadik", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ioannis Antonoglou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Helen King", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "D. Kumaran", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "S. Legg", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "D. Hassabis", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Adri\u00e0 Puigdom\u00e8nech Badia", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mehdi Mirza", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Timothy P. Lillicrap", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tim Harley", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Koray Kavukcuoglu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guosheng Lin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Anton Milan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chunhua Shen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ian Reid", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhijie Qiao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haowei Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhong Cao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Henry X. Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yongkang Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kaixin Xiong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiangyu Guo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fang Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sixu Yan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Gangwei Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lijun Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haiyang Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bing Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kun Ma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guang Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hangjun Ye", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenyu Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xinggang Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wei Cao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Marcel Hallgarten", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tianyu Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Daniel Dauner", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xunjiang Gu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Caojun Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yakov Miron", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Marco Aiello", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hongyang Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Igor Gilitschenski", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Andreas Geiger", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kashyap Chitta", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhenjie Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yilin Chai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaosong Jia", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qifeng Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuqian Shao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xuekai Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haisheng Su", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Junchi Yan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sicong Jiang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zilin Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kangan Qian", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ziang Luo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tianze Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yang Zhong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yihong Tang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Menglin Kong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yunlong Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Siwen Jiao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hao Ye", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zihao Sheng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xin Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tuopu Wen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zheng Fu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sikai Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kun Jiang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Diange Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Seongjin Choi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lijun Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "M. Page", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "J. McKenzie", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "P. Bossuyt", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "I. Boutron", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "T. Hoffmann", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "C. Mulrow", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Larissa Shamseer", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "J. Tetzlaff", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "E. Akl", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "S. Brennan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "R. Chou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Julie May Glanville", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "J. Grimshaw", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "A. Hr\u00f5bjartsson", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "M. Lalu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tianjing Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "E. Loder", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "E. Mayo-Wilson", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Steve McDonald", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "L. McGuinness", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "L. Stewart", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "James Thomas", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "A. Tricco", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "V. Welch", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "P. Whiting", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "D. Moher", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wangbo Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chen Min", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nianchen Deng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Min Dou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuqi Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Botian Shi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kai Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chi Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yang You", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhaoxiang Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dawei Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Liang Xiao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jian Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jingtao Ding", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yunke Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yu Shang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jie Feng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuheng Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zefang Zong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuan Yuan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hongyuan Su", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nian Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jinghua Piao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yucheng Deng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nicholas Sukiennik", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chen Gao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fengli Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yong Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhaojian Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bin Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xuannan Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xing Cui", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Peipei Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zekun Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Huaibo Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shuhan Xia", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Miaoxuan Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yueying Zou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ran He", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Galina Ilieva", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tania Yankova", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Stanislava Klisarova-Belcheva", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jie Hu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Li Shen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Samuel Albanie", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Gang Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Enhua Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "I. Loshchilov", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "F. Hutter", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mingxing Tan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Quoc V. Le", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tianqi Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhaoxi Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zihao Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shaocong Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Saining Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chongjie Ye", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhiguo Cao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tianze Xia", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jingfeng Yao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sicheng Zuo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zixun Xie", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shaoqing Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shengyin Jiang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhi-Xin Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lvmin Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Anyi Rao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Maneesh Agrawala", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hyung Won Chung", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Le Hou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shayne Longpre", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Barret Zoph", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yi Tay", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "William Fedus", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yunxuan Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xuezhi Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Siddhartha Brahma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Albert Webson", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shixiang Shane Gu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhuyun Dai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mirac Suzgun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xinyun Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Aakanksha Chowdhery", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Alex Castro-Ros", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Marie Pellat", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kevin Robinson", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dasha Valter", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Gaurav Mishra", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Adams Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Vincent Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yanping Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Andrew Dai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hongkun Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Slav Petrov", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ed H. Chi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jeff Dean", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Denny Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jason Wei", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Robin Rombach", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Andreas Blattmann", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dominik Lorenz", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Patrick Esser", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bj\u00f6rn Ommer", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Romain Lopez", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Pierre Boyeau", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "N. Yosef", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Michael I. Jordan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "J. Regier", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Phillip Isola", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Alexei A. Efros", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Oliver Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Pei Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Henrik Kretzschmar", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xerxes Dotiwalla", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Aurelien Chouard", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Vijaysai Patnaik", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Paul Tsui", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "James Guo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yin Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuning Chai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Benjamin Caine", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Vijay Vasudevan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wei Han", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiquan Ngiam", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hang Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Aleksei Timofeev", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Scott Ettinger", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Maxim Krivokon", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Amy Gao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Aditya Joshi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sheng Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shuyang Cheng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yu Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhifeng Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Gilad Cohen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Raja Giryes", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zekai Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiao Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiang Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lianghe Shi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Meng Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Molei Tao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qing Qu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Donglin Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yongxing Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xin Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Liang Hou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xin Tao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaojuan Qi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Renjie Liao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ram\u00f3n Calvo-Gonz\u00e1lez", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fran\u00e7ois Fleuret", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yao Teng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Minxuan Lin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xian Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shuai Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xihui Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nicolas Sereyjol-Garros", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ellington Kirby", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Victor Letzelter", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Victor Besnier", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nermin Samet", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Gao Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhuang Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Laurens van der Maaten", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kilian Q. Weinberger", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhongyu Zeng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xuan Peng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "J. B. Awotunde", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Korede Israel Adeyanju", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kehinde Elisha Akerele", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Oluwatobi Akinlade", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "S. Folorunso", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "S. Ajagbe", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Camillo Lugaresi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiuqiang Tang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hadon Nash", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chris McClanahan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Esha Uboweja", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Michael Hays", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fan Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chuo-Ling Chang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ming Guang Yong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Juhyun Lee", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wan-Teh Chang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wei Hua", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Manfred Georg", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Matthias Grundmann", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Cem Keskin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mustafa Furkan K\u0131ra\u00e7", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yunus Emre Kara", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "L. Akarun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "S. P. Priyal", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "P. Bora", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Octavian Dudas", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "C. Nandra", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "C. Mocan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "D. Gorgan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Avinash Dhiran", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Anurag Kumbhare", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Achal Patil", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mrugank Vichare", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dhananjay Patel", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Saransh Mishra", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Pavan Nair", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Pushpalatha M", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Poornima S", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jeff Donahue", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Trevor Darrell", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jitendra Malik", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shansong Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Atin Sakkeer Hussain", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qilong Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chenshuo Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ying Shan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiahang Tu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ye Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yiming Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hanbin Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chao Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hui Qian", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haotian Lv", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuhui Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiangbo Dai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hanli Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiaji Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dawei Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mingxin Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yanzhao Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dingkun Long", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Keqin Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sibo Song", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shuai Bai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhibo Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Pengjun Xie", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "An Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dayiheng Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jingren Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Junyang Lin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mingyue Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xin Liao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Han Fang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jinlin Guo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yanxiang Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaoshuai Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "A. Dempster", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "N. Laird", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "D. Rubin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "L. Maaten", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jacy Reese Anthis", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ryan Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sean M. Richardson", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Austin C. Kozlowski", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bernard Koch", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "James Evans", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Erik Brynjolfsson", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhiwen Xiao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Huagang Tong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Runqian Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ibomoiye Domor Mienye", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Theo G. Swart", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jusheng Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zimeng Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yijia Fan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ningyuan Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mingyan Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhuojie Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiawei Yao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jian Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Keze Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Olaf Ronneberger", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Philipp Fischer", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Thomas Brox", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tianwei Yin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Micha\u00ebl Gharbi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Taesung Park", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fredo Durand", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "William T. Freeman", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Axel Sauer", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Frederic Boesel", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tim Dockhorn", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Takuya Akiba", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Makoto Shing", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yujin Tang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qi Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "David Ha", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qiang Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xun Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zinan Guo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yanze Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhuowei Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lang Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Peng Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qian He", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jonathan Ho", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ajay Jain", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Pieter Abbeel", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Weijie Kong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qi Tian", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zijian Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Rox Min", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zuozhuo Dai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jin Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiangfeng Xiong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xin Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bo Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jianwei Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kathrina Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qin Lin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Junkun Yuan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yanxin Long", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Aladdin Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Andong Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Changlin Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Duojun Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fang Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hao Tan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hongmei Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jacob Song", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiawang Bai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jianbing Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jinbao Xue", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Joey Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mengyang Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Pengyu Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shuai Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Weiyan Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenqing Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xinchi Deng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yi Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yutao Cui", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuanbo Peng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhentao Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhiyu He", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhiyong Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zixiang Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zunnan Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yangyu Tao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qinglin Lu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Songtao Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dax Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yong Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Di Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuhong Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jie Jiang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Caesar Zhong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jianwen Jiang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chao Liang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiaqi Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Gaojie Lin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tianyun Zhong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yanbo Zheng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zerong Zheng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiahao Cui", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hui Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yao Yao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hao Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hanlin Shang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kaihui Cheng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hang Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Siyu Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jingdong Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Rang Meng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xingyu Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bharath Hariharan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kyunghyun Cho", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bart van Merrienboer", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Caglar Gulcehre", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dzmitry Bahdanau", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fethi Bougares", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Holger Schwenk", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Erfei Cui", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenhai Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhiqi Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiangwei Xie", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haoming Zou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hanming Deng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Gen Luo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lewei Lu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xizhou Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jifeng Dai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shenyuan Gao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiazhi Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Li Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yihang Qiu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jun Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bencheng Liao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shaoyu Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haoran Yin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bo Jiang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Cheng Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xinbang Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiangyu Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ying Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qian Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bingyi Kang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yang Yue", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Rui Lu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhijie Lin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yang Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kaixin Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiashi Feng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jyh-Jing Hwang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Runsheng Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hubert Lin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wei-Chih Hung", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jingwei Ji", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kristy Choi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Di Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tong He", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Paul Covington", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Benjamin Sapp", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tom B. Brown", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Benjamin Mann", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nick Ryder", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Melanie Subbiah", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jared Kaplan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Prafulla Dhariwal", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Arvind Neelakantan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Pranav Shyam", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ariel Herbert-Voss", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tom Henighan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Rewon Child", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Daniel M. Ziegler", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jeffrey Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Clemens Winter", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Christopher Hesse", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mark Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Eric Sigler", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mateusz Litwin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Scott Gray", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Benjamin Chess", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Christopher Berner", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sam McCandlish", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dario Amodei", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhe Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Weiyun Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yue Cao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yangzhou Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhangwei Gao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jinguo Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shenglong Ye", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hao Tian", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhaoyang Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lixin Gu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xuehui Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qingyun Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yiming Ren", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zixuan Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiapeng Luo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiahao Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tan Jiang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Conghui He", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xingcheng Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Han Lv", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yi Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenqi Shao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Pei Chu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhongying Tu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhiyong Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Huipeng Deng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiaye Ge", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kai Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kaipeng Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Limin Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tong Lu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dahua Lin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yu Qiao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuchen Duan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Weijie Su", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jie Shao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xingguang Wei", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hongjie Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haomin Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Weiye Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hao Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Songze Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yinan He", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Junjun He", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yingtong Xiong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenwen Qu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Peng Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Penglong Jiao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lijun Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haodong Duan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xinyu Fang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Junming Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiangyu Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuxuan Qiao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mo Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Amit Agarwal", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lin Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuan Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yubo Ma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hailong Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yifan Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tack Hwa Wong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Peiheng Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaozhe Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chaoyou Fu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Junbo Cui", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jixuan Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Enxin Song", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Song Mao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shengyuan Ding", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tianhao Liang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zicheng Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaoyi Dong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuhang Zang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Pan Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiaqi Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guowei Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Peng Jin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ziang Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yibing Song", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lichao Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Li Yuan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hengjun Pu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Long Cui", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Linglin Jing", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhaokai Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ganlin Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qi Wei", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jinhui Yin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenhao Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guanzhou Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zichen Ding", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Changyao Tian", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhenyu Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jingjing Xie", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zehao Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bowen Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhi Hou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haoran Hao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tianyi Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bin Fu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Biqing Qi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qipeng Guo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenwei Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Songyang Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Maosong Cao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Junyao Lin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kexian Tang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jianfei Gao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haian Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuzhe Gu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chengqi Lyu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Huanze Tang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Rui Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haijun Lv", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bowen Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "John Schulman", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Filip Wolski", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Oleg Klimov", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yangguang Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiaheng Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zeyue Xue", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yu Gao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fangyuan Kong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lingting Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mengzhao Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qiushan Guo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Weilin Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haoyuan Guo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tuyen Hoang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lu Jiang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Huixia Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiashi Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Liang Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaojie Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xunsong Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yifu Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiawei Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shu Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaonan Nie", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhiwu Qing", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Li Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhi Tian", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sen Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guoqiang Wei", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guohong Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ruiqi Xia", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fei Xiao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xuefeng Xiao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiangqiao Yan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ceyuan Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jianchao Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Runkai Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tao Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yihang Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zilyu Ye", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xuejiao Zeng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yan Zeng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Heng Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaozheng Zheng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Peihao Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiaxin Zou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Feilong Zuo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guibin Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dixuan Lin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiangping Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chunze Lin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Junchen Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mingyuan Fan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hao Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sheng Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zheng Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chengcheng Ma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Weiming Xiong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wei Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nuo Pang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kang Kang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhiheng Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuzhe Jin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yupeng Liang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yubing Song", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Peng Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Boyuan Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Di Qiu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Debang Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhengcong Fei", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yahui Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yibin Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Cheng Jin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "OpenAI", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Josh Achiam", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Steven Adler", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lama Ahmad", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ilge Akkaya", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Florencia Leoni Aleman", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Diogo Almeida", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Janko Altenschmidt", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sam Altman", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shyamal Anadkat", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Red Avila", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Igor Babuschkin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Suchir Balaji", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Valerie Balcom", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Paul Baltescu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haiming Bao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mohammad Bavarian", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jeff Belgum", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Irwan Bello", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jake Berdine", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Gabriel Bernadett-Shapiro", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lenny Bogdonoff", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Oleg Boiko", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Madelaine Boyd", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Anna-Luisa Brakman", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Greg Brockman", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tim Brooks", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Miles Brundage", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kevin Button", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Trevor Cai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Rosie Campbell", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Andrew Cann", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Brittany Carey", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chelsea Carlson", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Rory Carmichael", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Brooke Chan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Che Chang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fotis Chantzis", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Derek Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sully Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ruby Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jason Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ben Chess", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chester Cho", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Casey Chu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dave Cummings", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jeremiah Currier", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yunxing Dai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Cory Decareaux", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Thomas Degry", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Noah Deutsch", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Damien Deville", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Arka Dhar", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "David Dohan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Steve Dowling", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sheila Dunning", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Adrien Ecoffet", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Atty Eleti", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tyna Eloundou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "David Farhi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Liam Fedus", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Niko Felix", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sim\u00f3n Posada Fishman", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Juston Forte", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Isabella Fulford", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Leo Gao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Elie Georges", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Christian Gibson", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Vik Goel", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tarun Gogineni", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Rapha Gontijo-Lopes", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jonathan Gordon", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Morgan Grafstein", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ryan Greene", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Joshua Gross", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yufei Guo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jesse Han", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jeff Harris", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuchen He", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mike Heaton", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Johannes Heidecke", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chris Hesse", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Alan Hickey", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wade Hickey", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Peter Hoeschele", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Brandon Houghton", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kenny Hsu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shengli Hu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xin Hu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Joost Huizinga", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shantanu Jain", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shawn Jain", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Joanne Jang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Angela Jiang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Roger Jiang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haozhun Jin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Denny Jin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shino Jomoto", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Billie Jonn", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Heewoo Jun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tomer Kaftan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "\u0141ukasz Kaiser", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ali Kamali", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ingmar Kanitscheider", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nitish Shirish Keskar", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tabarak Khan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Logan Kilpatrick", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Christina Kim", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yongjik Kim", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jan Hendrik Kirchner", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jamie Kiros", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Matt Knight", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Daniel Kokotajlo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "\u0141ukasz Kondraciuk", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Andrew Kondrich", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Aris Konstantinidis", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kyle Kosic", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Vishal Kuo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Michael Lampe", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ikai Lan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Teddy Lee", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jan Leike", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jade Leung", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Daniel Levy", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chak Ming Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Rachel Lim", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Molly Lin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Stephanie Lin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Theresa Lopez", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ryan Lowe", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Patricia Lue", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Anna Makanju", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kim Malfacini", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sam Manning", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Todor Markov", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yaniv Markovski", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bianca Martin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Katie Mayer", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Andrew Mayne", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bob McGrew", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Scott Mayer McKinney", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Christine McLeavey", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Paul McMillan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jake McNeil", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "David Medina", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Aalok Mehta", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jacob Menick", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Luke Metz", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Andrey Mishchenko", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Vinnie Monaco", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Evan Morikawa", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Daniel Mossing", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tong Mu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mira Murati", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Oleg Murk", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "David M\u00e9ly", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ashvin Nair", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Reiichiro Nakano", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Rajeev Nayak", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Richard Ngo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hyeonwoo Noh", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Long Ouyang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Cullen O'Keefe", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jakub Pachocki", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Alex Paino", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Joe Palermo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ashley Pantuliano", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Giambattista Parascandolo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Joel Parish", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Emy Parparita", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Alex Passos", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mikhail Pavlov", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Andrew Peng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Adam Perelman", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Filipe de Avila Belbute Peres", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Michael Petrov", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Henrique Ponde de Oliveira Pinto", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Michael", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Pokorny", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Michelle Pokrass", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Vitchyr H. Pong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tolly Powell", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Alethea Power", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Boris Power", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Elizabeth Proehl", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Raul Puri", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jack Rae", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Cameron Raymond", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Francis Real", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kendra Rimbach", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Carl Ross", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bob Rotsted", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Henri Roussez", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mario Saltarelli", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ted Sanders", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shibani Santurkar", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Heather Schmidt", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "David Schnurr", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Daniel Selsam", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kyla Sheppard", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Toki Sherbakov", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jessica Shieh", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sarah Shoker", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Szymon Sidor", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Maddie Simens", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jordan Sitkin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Katarina Slama", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ian Sohl", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Benjamin Sokolowsky", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yang Song", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Natalie Staudacher", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Felipe Petroski Such", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Natalie Summers", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jie Tang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nikolas Tezak", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Madeleine B. Thompson", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Phil Tillet", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Amin Tootoonchian", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Elizabeth Tseng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Preston Tuggle", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nick Turley", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jerry Tworek", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Juan Felipe Cer\u00f3n Uribe", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Andrea Vallone", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Arun Vijayvergiya", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chelsea Voss", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Carroll Wainwright", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Justin Jay Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Alvin Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ben Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jonathan Ward", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "CJ Weinmann", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Akila Welihinda", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Peter Welinder", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiayi Weng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lilian Weng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Matt Wiethoff", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dave Willner", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Samuel Wolrich", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hannah Wong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lauren Workman", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sherwin Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jeff Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Michael Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kai Xiao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tao Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sarah Yoo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kevin Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qiming Yuan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wojciech Zaremba", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Rowan Zellers", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chong Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Marvin Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shengjia Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tianhao Zheng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Juntang Zhuang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "William Zhuk", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hugo Touvron", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Thibaut Lavril", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Gautier Izacard", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xavier Martinet", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Marie-Anne Lachaux", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Timoth\u00e9e Lacroix", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Baptiste Rozi\u00e8re", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Naman Goyal", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Eric Hambro", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Faisal Azhar", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Aurelien Rodriguez", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Armand Joulin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Edouard Grave", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guillaume Lample", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dale Schuurmans", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Maarten Bosma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Brian Ichter", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fei Xia", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ed Chi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Quoc Le", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Karl Cobbe", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Vineet Kosaraju", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Matthias Plappert", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jacob Hilton", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "K. Luger", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "A. M\u00e4der", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "R. K. Richmond", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "D. Sargent", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "T. Richmond", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Albert Gu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tri Dao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Stephen M. Mount", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "F. Crick", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ilya Loshchilov", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Frank Hutter", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Azalia Mirhoseini", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Krzysztof Maziarz", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Andy Davis", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "J. Ziv", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "A. Lempel", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Stephen Merity", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Caiming Xiong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "James Bradbury", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Richard Socher", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dan Hendrycks", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Collin Burns", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Steven Basart", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Andy Zou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mantas Mazeika", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dawn Song", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jacob Steinhardt", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hunter Lightman", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yura Burda", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Harri Edwards", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bowen Baker", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "David Rein", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Betty Li Hou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Asa Cooper Stickland", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jackson Petty", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Richard Yuanzhe Pang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Julien Dirani", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Julian Michael", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Samuel R. Bowman", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dmitry Lepikhin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "HyoukJoong Lee", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuanzhong Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dehao Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Orhan Firat", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Maxim Krikun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Radford M. Neal", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "L. Breiman", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guan Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yu Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jianxin Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mostafa Mehdipour-Ghazi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "B. Yanikoglu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "E. Aptoula", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sue Han Lee", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "H. Go\u00ebau", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "P. Bonnet", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "A. Joly", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haiyan Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jose Carranza-Rojas", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Herv\u00e9 Goeau", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Erick Mata-Montero", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mark Sandler", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Andrew Howard", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Menglong Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Andrey Zhmoginov", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Liang-Chieh Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "H. Brendan McMahan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Eider Moore", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Daniel Ramage", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Seth Hampson", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Blaise Ag\u00fcera y Arcas", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fuzhen Zhuang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhiyuan Qi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Keyu Duan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dongbo Xi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yongchun Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hengshu Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hui Xiong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qing He", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "A. Khan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wadii Boulila", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "G. A. Sampedro", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sidra Abbas", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chitapong Wechtaisong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tesfahunegn Minwuyelet Mengistu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Taewoon Kim", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jenn-Wei Lin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "A. Alamer", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Manel Khazri Khlifi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "I. Farah", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Anwesha Mukherjee", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Rajkumar Buyya", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Alexander Kirillov", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Eric Mintun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nikhila Ravi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hanzi Mao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chloe Rolland", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Laura Gustafson", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tete Xiao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Spencer Whitehead", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wan-Yen Lo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haotian Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qingyang Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yong Jae Lee", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuheng Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhiyuan You", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jinjin Gu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xin Cai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zheyuan Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kaiwen Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chao Dong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tianfan Xue", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xintong Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhi Gao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bofei Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Pengxiang Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaowen Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yang Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tao Yuan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuwei Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yunde Jia", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Song-Chun Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qing Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhangquan Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Manyuan Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xinlei Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xufang Luo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mingze Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zihao Pan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yan Feng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ruqi Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tiancheng Gu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kaichen Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yueyi Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Weidong Cai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lidong Bing", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Keming Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zuhao Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kairui Hu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bin Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xingxuan Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ranjan Sapkota", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yang Cao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Konstantinos I. Roumeliotis", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Manoj Karkee", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kohei Sendai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Maxime Alvarez", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tatsuya Matsushima", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yutaka Matsuo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yusuke Iwasawa", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shuhan Tan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Philipp Krahenbuhl", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zheng Xiong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kang Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zilin Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Matthew Jackson", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jakob Foerster", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shimon Whiteson", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yifan Ye", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiaqi Ma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jun Cen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhihe Lu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhaohu Xing", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tian Ye", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yijun Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "D. Cai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Baowen Gai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiao-Jian Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Feng Gao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lei Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hai Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yu Song", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tingting Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhaoli Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaolan Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Neal N. Xiong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Honghu Chu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiahao Gai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Weiwei Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jun Ma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yinjun Jia", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bowen Gao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiaxin Tan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiqing Zheng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xin Hong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenyu Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haichuan Tan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuan Xiao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Liping Tan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hongyi Cai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yanwen Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhiheng Deng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiangwei Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yue Jin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yafei Yuan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiekang Tian", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wei He", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Weiying Ma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chuangye Yan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yanyan Lan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guodong Fan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shengning Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhen Hua", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jinjiang Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jingchun Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiahua Dong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yu-Xiong Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hasi Hays", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qiao Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xianbang Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhicheng Jiang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hanhong Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Susie Lu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tianhong Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Peter Potaptchik", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Adhi Saravanan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Abbas Mammadov", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Alvaro Prat", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Michael S. Albergo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yee Whye Teh", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yinan Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hans Hao-Hsun Hsu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Junran Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bo Dai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Pan Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mingyang Deng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "He Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yilun Du", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ting Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mohammad Norouzi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chubin Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sujie Hu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiashu Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Meiqi Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jintao Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yanxun Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nisha Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chengyu Fang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaokun Feng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chen Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bingze Song", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fangyuan Mao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kaiqi Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ziteng Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bingda Tang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ellis Brown", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jihan Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Rob Fergus", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yann LeCun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jingtong Yue", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ziqi Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kaixin Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Daili Hua", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bozhou Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chengzhuo Tong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuran Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xinyi Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yifan Dai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zixiang Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yifan Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhou Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hao Liang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaochen Ma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ruichuan An", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tianyi Bai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hongcheng Gao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Junbo Niu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yang Shi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xinlong Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yue Ding", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kai Zeng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yiwen Tang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wentao Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qingyu Shi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Size Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jinbin Bai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kaidong Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yujing Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yunhai Tong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiangtai Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xuelong Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guanfang Dong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Luke Schultz", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Negar Hassanpour", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chao Gao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Alex Nichol", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Maxime Oquab", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Timoth\u00e9e Darcet", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Th\u00e9o Moutakanni", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Huy Vo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Marc Szafraniec", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Vasil Khalidov", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Pierre Fernandez", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Daniel Haziza", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Francisco Massa", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Alaaeldin El-Nouby", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mahmoud Assran", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nicolas Ballas", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wojciech Galuba", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Russell Howes", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Po-Yao Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shang-Wen Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ishan Misra", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Michael Rabbat", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Vasu Sharma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Gabriel Synnaeve", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hu Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Herv\u00e9 Jegou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Julien Mairal", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Patrick Labatut", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Piotr Bojanowski", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zehong Ma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ruihan Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shiliang Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shanshan Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xinjie Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jintao Guo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiakui Hu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lunhao Duan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Minghao Fu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yong Xien Chng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guo-Hua Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jana Zeller", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Thadd\u00e4us Wiedemer", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fanfei Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Thomas Klein", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Prasanna Mayilvahanan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Matthias Bethge", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Felix Wichmann", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ryan Cotterell", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wieland Brendel", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Letian Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sucheng Ren", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yanqing Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xianhang Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuyin Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Huaxiu Yao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zeyu Zheng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guilin Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhiding Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Cihang Xie", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tomas Mikolov", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Greg Corrado", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jeffrey Dean", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jeffrey Pennington", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Christopher D. Manning", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Matthew E. Peters", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mark Neumann", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mohit Iyyer", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Matt Gardner", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Christopher Clark", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Luke Zettlemoyer", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Quentin Fournier", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Robert M. Vernon", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Almer M. van der Sloot", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Benjamin Schulz", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sarath Chandar", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "C. Langmead", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Scott Friedman", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sonja Schmer-Galunder", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Anthony Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jeffrey Rye", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Alexander C. Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ananya Kumar", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Deepak Pathak", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Y. Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yinqiu Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shaoyong Guo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xuesong Qiu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiewei Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiakai Hao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dusist Niyato", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "R. Child", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "D. Luan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Aaron Grattafiori", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Abhimanyu Dubey", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Abhinav Jauhri", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Abhinav Pandey", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Abhishek Kadian", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ahmad Al-Dahle", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Aiesha Letman", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Akhil Mathur", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Alan Schelten", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Alex Vaughan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Amy Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Angela Fan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Anirudh Goyal", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Anthony Hartshorn", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Aobo Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Archi Mitra", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Archie Sravankumar", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Artem Korenev", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Arthur Hinsvark", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Arun Rao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Aston Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Austen Gregerson", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ava Spataru", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Baptiste Roziere", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bethany Biron", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Binh Tang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bobbie Chern", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Charlotte Caucheteux", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chaya Nayak", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chloe Bi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chris Marra", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chris McConnell", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Christian Keller", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Christophe Touret", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chunyang Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Corinne Wong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Cristian Canton Ferrer", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Cyrus Nikolaidis", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Damien Allonsius", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Daniel Song", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Danielle Pintz", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Danny Livshits", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Danny Wyatt", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "David Esiobu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dhruv Choudhary", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dhruv Mahajan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Diego Garcia-Olano", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Diego Perino", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dieuwke Hupkes", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Egor Lakomkin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ehab AlBadawy", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Elina Lobanova", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Emily Dinan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Eric Michael Smith", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Filip Radenovic", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Francisco Guzm\u00e1n", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Frank Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Gabrielle Lee", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Georgia Lewis Anderson", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Govind Thattai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Graeme Nail", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Gregoire Mialon", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guan Pang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guillem Cucurell", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hailey Nguyen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hannah Korevaar", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Iliyan Zarov", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Imanol Arrieta Ibarra", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Isabel Kloumann", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ivan Evtimov", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jack Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jade Copet", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jaewon Lee", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jan Geffert", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jana Vranes", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jason Park", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jay Mahadeokar", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jeet Shah", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jelmer van der Linde", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jennifer Billock", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jenny Hong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jenya Lee", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jeremy Fu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jianfeng Chi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jianyu Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiawen Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jie Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiecao Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Joanna Bitton", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Joe Spisak", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jongsoo Park", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Joseph Rocca", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Joshua Johnstun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Joshua Saxe", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Junteng Jia", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kalyan Vasuden Alwala", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Karthik Prasad", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kartikeya Upasani", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kate Plawiak", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ke Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kenneth Heafield", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kevin Stone", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Khalid El-Arini", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Krithika Iyer", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kshitiz Malik", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kuenley Chiu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kunal Bhalla", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kushal Lakhotia", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lauren Rantala-Yeary", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lawrence Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Liang Tan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Liz Jenkins", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Louis Martin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lovish Madaan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lubo Malo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lukas Blecher", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lukas Landzaat", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Luke de Oliveira", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Madeline Muzzi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mahesh Pasupuleti", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mannat Singh", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Manohar Paluri", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Marcin Kardas", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Maria Tsimpoukelli", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mathew Oldham", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mathieu Rita", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Maya Pavlova", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Melanie Kambadur", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mike Lewis", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Min Si", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mitesh Kumar Singh", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mona Hassan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Narjes Torabi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nikolay Bashlykov", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nikolay Bogoychev", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Niladri Chatterji", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ning Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Olivier Duchenne", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Onur \u00c7elebi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Patrick Alrassy", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Pengchuan Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Pengwei Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Petar Vasic", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Peter Weng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Prajjwal Bhargava", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Pratik Dubal", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Praveen Krishnan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Punit Singh Koura", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Puxin Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qingxiao Dong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ragavan Srinivasan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Raj Ganapathy", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ramon Calderer", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ricardo Silveira Cabral", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Robert Stojnic", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Roberta Raileanu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Rohan Maheswari", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Rohit Girdhar", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Rohit Patel", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Romain Sauvestre", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ronnie Polidoro", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Roshan Sumbaly", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ross Taylor", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ruan Silva", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Rui Hou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Saghar Hosseini", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sahana Chennabasappa", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sanjay Singh", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sean Bell", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Seohyun Sonia Kim", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sergey Edunov", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shaoliang Nie", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sharath Raparthy", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sheng Shen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shengye Wan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shruti Bhosale", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shun Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Simon Vandenhende", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Soumya Batra", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Spencer Whitman", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sten Sootla", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Stephane Collot", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Suchin Gururangan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sydney Borodinsky", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tamar Herman", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tara Fowler", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tarek Sheasha", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Thomas Georgiou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Thomas Scialom", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tobias Speckbacher", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Todor Mihaylov", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tong Xiao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ujjwal Karn", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Vedanuj Goswami", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Vibhor Gupta", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Vignesh Ramanathan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Viktor Kerkez", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Vincent Gonguet", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Virginie Do", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Vish Vogeti", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "V\u00edtor Albiero", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Vladan Petrovic", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Weiwei Chu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenhan Xiong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenyin Fu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Whitney Meers", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaodong Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaofang Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaoqing Ellen Tan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xide Xia", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xinfeng Xie", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xuchao Jia", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xuewei Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yaelle Goldschlag", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yashesh Gaur", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yasmine Babaei", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yi Wen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yiwen Song", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuchen Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yue Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuning Mao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zacharie Delpierre Coudert", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zheng Yan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhengxing Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zoe Papakipos", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Aaditya Singh", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Aayushi Srivastava", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Abha Jain", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Adam Kelsey", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Adam Shajnfeld", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Adithya Gangidi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Adolfo Victoria", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ahuva Goldstand", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ajay Menon", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ajay Sharma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Alex Boesenberg", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Alexei Baevski", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Allie Feinstein", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Amanda Kallet", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Amit Sangani", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Amos Teo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Anam Yunus", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Andrei Lupu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Andres Alvarado", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Andrew Caples", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Andrew Gu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Andrew Ho", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Andrew Poulton", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Andrew Ryan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ankit Ramchandani", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Annie Dong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Annie Franco", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Anuj Goyal", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Aparajita Saraf", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Arkabandhu Chowdhury", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ashley Gabriel", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ashwin Bharambe", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Assaf Eisenman", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Azadeh Yazdan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Beau James", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ben Maurer", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Benjamin Leonhardi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bernie Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Beth Loyd", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Beto De Paola", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bhargavi Paranjape", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bing Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Boyu Ni", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Braden Hancock", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bram Wasti", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Brandon Spence", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Brani Stojkovic", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Brian Gamido", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Britt Montalvo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Carl Parker", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Carly Burton", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Catalina Mejia", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ce Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Changhan Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Changkyu Kim", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chao Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chester Hu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ching-Hsiang Chu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chris Cai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chris Tindal", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Christoph Feichtenhofer", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Cynthia Gao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Damon Civin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dana Beaty", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Daniel Kreymer", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Daniel Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "David Adkins", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "David Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Davide Testuggine", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Delia David", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Devi Parikh", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Diana Liskovich", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Didem Foss", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dingkang Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Duc Le", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dustin Holland", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Edward Dowling", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Eissa Jamil", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Elaine Montgomery", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Eleonora Presani", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Emily Hahn", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Emily Wood", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Eric-Tuan Le", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Erik Brinkman", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Esteban Arcaute", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Evan Dunbar", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Evan Smothers", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fei Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Felix Kreuk", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Filippos Kokkinos", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Firat Ozgenel", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Francesco Caggioni", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Frank Kanayet", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Frank Seide", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Gabriela Medina Florez", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Gabriella Schwarz", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Gada Badeer", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Georgia Swee", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Gil Halpern", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Grant Herman", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Grigory Sizov", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guangyi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guna Lakshminarayanan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hakan Inan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hamid Shojanazeri", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Han Zou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hannah Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hanwen Zha", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haroun Habeeb", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Harrison Rudolph", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Helen Suk", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Henry Aspegren", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hunter Goldman", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hongyuan Zhan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ibrahim Damlaj", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Igor Molybog", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Igor Tufanov", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ilias Leontiadis", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Irina-Elena Veliche", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Itai Gat", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jake Weissman", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "James Geboski", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "James Kohli", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Janice Lam", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Japhet Asher", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jean-Baptiste Gaya", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jeff Marcus", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jeff Tang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jennifer Chan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jenny Zhen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jeremy Reizenstein", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jeremy Teboul", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jessica Zhong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jian Jin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jingyi Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Joe Cummings", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jon Carvill", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jon Shepard", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jonathan McPhie", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jonathan Torres", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Josh Ginsburg", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Junjie Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kai Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kam Hou U", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Karan Saxena", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kartikay Khandelwal", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Katayoun Zand", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kathy Matosich", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kaushik Veeraraghavan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kelly Michelena", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Keqian Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kiran Jagadeesh", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kun Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kunal Chawla", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kyle Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lailin Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lakshya Garg", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lavender A", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Leandro Silva", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lee Bell", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Liangpeng Guo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Licheng Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Liron Moshkovich", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Luca Wehrstedt", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Madian Khabsa", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Manav Avalani", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Manish Bhatt", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Martynas Mankus", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Matan Hasson", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Matthew Lennie", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Matthias Reso", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Maxim Groshev", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Maxim Naumov", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Maya Lathi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Meghan Keneally", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Miao Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Michael L. Seltzer", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Michal Valko", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Michelle Restrepo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mihir Patel", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mik Vyatskov", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mikayel Samvelyan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mike Clark", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mike Macey", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mike Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Miquel Jubert Hermoso", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mo Metanat", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mohammad Rastegari", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Munish Bansal", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nandhini Santhanam", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Natascha Parks", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Natasha White", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Navyata Bawa", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nayan Singhal", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nick Egebo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nicolas Usunier", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nikhil Mehta", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nikolay Pavlovich Laptev", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ning Dong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Norman Cheng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Oleg Chernoguz", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Olivia Hart", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Omkar Salpekar", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ozlem Kalinli", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Parkin Kent", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Parth Parekh", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Paul Saab", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Pavan Balaji", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Pedro Rittner", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Philip Bontrager", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Pierre Roux", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Piotr Dollar", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Polina Zvyagina", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Prashant Ratanchandani", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Pritish Yuvraj", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qian Liang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Rachad Alao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Rachel Rodriguez", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Rafi Ayub", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Raghotham Murthy", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Raghu Nayani", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Rahul Mitra", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Rangaprabhu Parthasarathy", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Raymond Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Rebekkah Hogan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Robin Battey", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Rocky Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Russ Howes", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ruty Rinott", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sachin Mehta", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sachin Siby", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sai Jayesh Bondu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Samyak Datta", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sara Chugh", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sara Hunt", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sargun Dhillon", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sasha Sidorov", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Satadru Pan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Saurabh Mahajan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Saurabh Verma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Seiji Yamamoto", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sharadh Ramaswamy", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shaun Lindsay", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sheng Feng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shenghao Lin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shengxin Cindy Zha", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shishir Patil", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shiva Shankar", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shuqiang Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sinong Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sneha Agarwal", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Soji Sajuyigbe", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Soumith Chintala", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Stephanie Max", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Stephen Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Steve Kehoe", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Steve Satterfield", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sudarshan Govindaprasad", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sumit Gupta", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Summer Deng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sungmin Cho", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sunny Virk", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Suraj Subramanian", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sy Choudhury", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sydney Goldman", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tal Remez", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tamar Glaser", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tamara Best", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Thilo Koehler", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Thomas Robinson", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tianhe Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tianjun Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tim Matthews", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Timothy Chou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tzook Shaked", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Varun Vontimitta", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Victoria Ajayi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Victoria Montanez", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Vijai Mohan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Vinay Satish Kumar", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Vishal Mangla", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Vlad Ionescu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Vlad Poenaru", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Vlad Tiberiu Mihailescu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Vladimir Ivanov", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenchen Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenwen Jiang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wes Bouaziz", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Will Constable", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaocheng Tang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaojian Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaolan Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xilun Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xinbo Gao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yaniv Kleinman", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yanjun Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ye Hu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ye Jia", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ye Qi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yenda Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yilin Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yossi Adi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Youngjin Nam", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yu Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuchen Hao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yundi Qian", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yunlu Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuzi He", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zach Rait", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zachary DeVito", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zef Rosnbrick", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhaoduo Wen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhenyu Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhiwei Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhiyu Ma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tianyi Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mingda Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bowei Guo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhiqiang Shen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Siyan Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mengchen Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jing Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chenyu Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bo Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuandong Tian", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Aditya Grover", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Feiyu Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Marianne Arriola", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yair Schiff", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hao Phung", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Aaron Gokaslan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Volodymyr Kuleshov", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chenghao Fan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wen Heng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sichen Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuxuan Song", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jing Su", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaoye Qu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kai Shen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wei Wei", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhilin Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zihang Dai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yiming Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jaime Carbonell", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ruslan Salakhutdinov", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "N. Cambridge", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jinze Bai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yunfei Chu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zeyu Cui", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kai Dang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaodong Deng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yang Fan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenbin Ge", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yu Han", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fei Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Binyuan Hui", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Luo Ji", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mei Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Runji Lin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Gao Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chengqiang Lu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Keming Lu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jianxin Ma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Rui Men", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xingzhang Ren", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xuancheng Ren", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chuanqi Tan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sinan Tan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jianhong Tu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Peng Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shijie Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shengguang Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Benfeng Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jin Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hao Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jian Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shusheng Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yang Yao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bowen Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hongyi Yuan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zheng Yuan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xingxuan Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yichang Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhenru Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chang Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaohuan Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tianhang Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jonas Gehring", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fabian Gloeckle", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jingyu Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "J\u00e9r\u00e9my Rapin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Artyom Kozhevnikov", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Alexandre D\u00e9fossez", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yicun Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Cong Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shaobo Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zichen Wen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hanlin Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Heli Ben-Hamu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Marton Havasi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "David Lopez-Paz", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Brian Karrer", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yaron Lipman", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "John Nguyen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tariq Berrada", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ricky T. Q. Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Niels M\u00fcndler", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jasper Dekoninck", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Martin Vechev", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Julianna Piskorz", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Cristina Pinneri", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Alvaro Correia", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Motasem Alfarra", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Risheek Garrepalli", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Christos Louizos", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chang Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chuang Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yilin Xiao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Su Dong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Luyao Zhuang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yujing Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhu Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zijin Hong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhishang Xiang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shengyuan Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Huachi Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qinggang Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ninghao Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xinrun Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yi Chang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiao Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hao Lu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haoyuan Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yulin Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chen Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ningxin Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yu Cheng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiuan Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yongkang Hu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yihang Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Huichi Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mingang Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhizhong Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kun Shao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuan Xie", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhaoxia Yin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qirui Mi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhijian Ma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mengyue Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haoxuan Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yisen Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haifeng Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jun Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaoyu Tao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ze Guo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zexuan Yan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Heng Pan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ailing Zeng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chengfei Cai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Heung-Yeung Shum", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xinhua Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Boshi Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yikuang Yuluo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yinhan Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Runtao Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhiyuan Qin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shanhui Mo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xinyao Liao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xianfang Zeng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ziye Song", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhoujie Fu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Gang Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zeyu Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kevin Qinghong Lin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mike Zheng Shou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yiyang Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiujun Ma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "DeepSeek-AI", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Daya Guo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dejian Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haowei Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Junxiao Song", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Peiyi Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qihao Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Runxin Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ruoyu Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shirong Ma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiao Bi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaokang Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yu Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Z. F. Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhibin Gou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhihong Shao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhuoshu Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ziyi Gao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Aixin Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bing Xue", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bochao Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bei Feng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chengda Lu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chenggang Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chengqi Deng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chenyu Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chong Ruan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Deli Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dongjie Ji", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Erhang Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fangyun Lin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fucong Dai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fuli Luo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guangbo Hao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guanting Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guowei Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "H. Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Han Bao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hanwei Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haocheng Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Honghui Ding", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Huajian Xin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Huazuo Gao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hui Qu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jianzhong Guo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiawei Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jingchang Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jingyang Yuan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Junjie Qiu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Junlong Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "J. L. Cai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiaqi Ni", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jian Liang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jin Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kai Dong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kai Hu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kaige Gao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kang Guan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kexin Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kuai Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lean Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lecong Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Liang Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Litong Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Liyue Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lei Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Leyi Xia", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mingchuan Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Minghua Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Minghui Tang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Meng Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Miaojun Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mingming Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ning Tian", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Panpan Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qiancheng Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qiushi Du", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ruiqi Ge", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ruisong Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ruizhe Pan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Runji Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "R. J. Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "R. L. Jin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ruyi Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shanghao Lu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shangyan Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shanhuang Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shengfeng Ye", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shiyu Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shuiping Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shunfeng Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shuting Pan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "S. S. Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shuang Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shaoqing Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tao Yun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tian Pei", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tianyu Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "T. Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wanjia Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wen Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenjun Gao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenqin Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "W. L. Xiao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wei An", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaodong Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaohan Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaokang Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaotao Nie", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xin Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xin Xie", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xingchao Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xinyu Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xinyuan Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xuecheng Su", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xuheng Lin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "X. Q. Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiangyue Jin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaojin Shen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaosha Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaowen Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaoxiang Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xinnan Song", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xinyi Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xianzu Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xinxia Shan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Y. K. Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Y. Q. Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Y. X. Wei", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yang Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yanhong Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yao Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yao Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yaofeng Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yaohui Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yi Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yichao Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yifan Shi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yiliang Xiong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ying He", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yishi Piao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yisong Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yixuan Tan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yiyang Ma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yiyuan Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yongqiang Guo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuan Ou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuduan Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yue Gong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuheng Zou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yujia He", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yunfan Xiong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuxiang Luo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuxiang You", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuxuan Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuyang Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Y. X. Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yaohui Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yi Zheng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuchen Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yunxian Ma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ying Tang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yukun Zha", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuting Yan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Z. Z. Ren", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zehui Ren", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhangli Sha", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhe Fu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhean Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhengyan Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhicheng Ma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhigang Yan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhiyu Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zihui Gu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zijia Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zijun Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zilin Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ziwei Xie", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ziyang Song", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zizheng Pan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhen Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhipeng Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhongyu Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhen Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Marah Abdin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jyoti Aneja", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hany Awadalla", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ahmed Awadallah", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ammar Ahmad Awan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nguyen Bach", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Amit Bahree", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Arash Bakhtiari", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jianmin Bao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Harkirat Behl", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Alon Benhaim", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Misha Bilenko", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Johan Bjorck", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "S\u00e9bastien Bubeck", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Martin Cai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qin Cai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Vishrav Chaudhary", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dong Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dongdong Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Weizhu Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yen-Chun Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yi-Ling Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hao Cheng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Parul Chopra", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiyang Dai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Matthew Dixon", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ronen Eldan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Victor Fragoso", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jianfeng Gao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mei Gao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Min Gao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Amit Garg", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Allie Del Giorno", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Abhishek Goswami", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Suriya Gunasekar", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Emman Haider", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Junheng Hao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Russell J. Hewett", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenxiang Hu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jamie Huynh", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dan Iter", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sam Ade Jacobs", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mojan Javaheripi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nikos Karampatziakis", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Piero Kauffmann", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mahoud Khademi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dongwoo Kim", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Young Jin Kim", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lev Kurilenko", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "James R. Lee", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yin Tat Lee", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuanzhi Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yunsheng Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chen Liang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lars Liden", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xihui Lin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zeqi Lin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Liyuan Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Weishung Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chong Luo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Piyush Madan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ali Mahmoudzadeh", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "David Majercak", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Matt Mazzola", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Caio C\u00e9sar Teodoro Mendes", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Arindam Mitra", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hardik Modi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Anh Nguyen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Brandon Norick", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Barun Patra", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Daniel Perez-Becker", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Thomas Portet", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Reid Pryzant", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Heyang Qin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Marko Radmilac", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Liliang Ren", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Gustavo de Rosa", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Corby Rosset", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sambudha Roy", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Olatunji Ruwase", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Olli Saarikivi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Amin Saied", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Adil Salim", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Michael Santacroce", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shital Shah", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ning Shang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hiteshi Sharma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yelong Shen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Swadheen Shukla", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xia Song", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Masahiro Tanaka", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Andrea Tupini", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Praneetha Vaddamanu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chunyu Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guanhua Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lijuan Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shuohang Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xin Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yu Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Rachel Ward", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wen Wen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Philipp Witte", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haiping Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaoxia Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Michael Wyatt", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bin Xiao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Can Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiahang Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Weijian Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jilong Xue", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sonali Yadav", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fan Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jianwei Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ziyi Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Donghan Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lu Yuan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chenruidong Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Cyril Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jianwen Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Li Lyna Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yi Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yue Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yunan Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiren Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuanhan Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dong Guo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Renrui Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Feng Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Peiyuan Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yanwei Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Weichen Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qiyao Xue", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haoming Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiangyu Yin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Boyuan Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wei Gao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuxi Xiao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Longfei Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shen Yan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xinhang Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sida Peng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yunchao Wei", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaowei Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mingrui Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhaozhi Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fangjinhua Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiaolong Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Marc Pollefeys", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tong Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Meng Cao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xingyu Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xue Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaodan Liang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Joseph Redmon", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Santosh Divvala", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ali Farhadi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhichao Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yepeng Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhiling Su", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Huachao Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuliang Gu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuda Zou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zelong Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Gui-Song Xia", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bo Du", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yongchao Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shenghao Fu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yukun Su", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fengyun Rao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jing Lyu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaohua Xie", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wei-Shi Zheng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiwan Chung", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Junhyeok Kim", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Siyeol Kim", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jaeyoung Lee", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Min Soo Kim", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Youngjae Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiazhe Wei", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ken Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tianyu Lao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haofan Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Liang Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Caifeng Shan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chenyang Si", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jianhua Han", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Meng Tian", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiangtong Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fan He", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Huixin Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sitong Guo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dechang Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hao Tang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Pei Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuze Guo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Minzhe Niu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haojie Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qichao Dong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xuechao Yan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Siyuan Dong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lu Hou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qingqiu Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hang Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "R. S. Sutton", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "A. Barto", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jonathan J. Hunt", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Alexander Pritzel", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nicolas Heess", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tom Erez", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuval Tassa", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shenzhi Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Le Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chang Gao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chujie Zheng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shixuan Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xionghui Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jianxin Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuqiong Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Andrew Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shiji Song", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ganqu Cui", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiacheng Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lifan Yuan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhi Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuxin Zuo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haozhan Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuchen Fan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Huayu Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Weize Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhiyuan Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hao Peng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lei Bai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jianhao Yan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yafu Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zican Hu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Komal Kumar", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tajamul Ashraf", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Omkar Thawakar", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Rao Muhammad Anwer", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hisham Cholakkal", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mubarak Shah", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ming-Hsuan Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Phillip H. S. Torr", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fahad Shahbaz Khan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Salman Khan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nicolas Le Roux", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jonathan Lebensold", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Arnaud Bergeron", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Joshua Greaves", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Alex Fr\u00e9chette", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Carolyne Pelletier", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Eric Thibodeau-Laufer", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "S\u00e1ndor Toth", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sam Work", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Evan Shelhamer", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jonathan Long", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Florinel-Alin Croitoru", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Vlad Hondru", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Radu Tudor Ionescu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Meng-Hao Guo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Cheng-Ze Lu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qibin Hou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhengning Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ming-Ming Cheng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shi-Min Hu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiaming Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Huayao Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kailun Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xinxin Hu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ruiping Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Rainer Stiefelhagen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lei Ke", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mingqiao Ye", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Martin Danelljan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yifan Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yu-Wing Tai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chi-Keung Tang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fisher Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiacong Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zixiang Xiong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shankar P. Bhattacharyya", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Baosong Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bo Zheng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chengpeng Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chengyuan Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guanting Dong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haoran Wei", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Huan Lin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jialong Tang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jialin Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jinzheng He", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kexin Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mingfeng Xue", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Na Ni", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Pei Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ru Peng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ruize Gao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tianhao Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tianyu Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xinyu Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xipin Wei", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xuejing Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yu Wan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhifang Guo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhihao Fan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shuang Zeng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xinyuan Chang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mengwei Xie", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xinran Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yifan Bai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zheng Pan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mu Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xing Wei", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ning Guo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zewei Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tianhui Cai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Seth Z. Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yun Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhiyu Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bolei Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haohan Chi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Huan-ang Gao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ziming Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jianing Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chenyu Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jinwei Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kaisen Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yangcheng Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zeda Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenyi Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Leichen Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xingtao Hu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hao Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mengmeng Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jinyu Miao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yining Shi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "He Zhe Lim", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Li Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tianbao Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Huang Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yifei Hu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guang Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhenyu Lin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhiwei Xiong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xinhai Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Anqing Jiang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yiru Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhigang Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shuo Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuwen Heng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shichen Tang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lijuan Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jinhao Chai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jijun Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zichong Gu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hao Jiang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yingyan Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shuyao Shang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Weisong Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bing Zhan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haochen Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuntao Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaoman Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yasong An", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chufeng Tang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lue Fan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bernhard Kerbl", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Georgios Kopanas", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Thomas Leimk\u00fchler", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "George Drettakis", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Johannes L. Sch\u00f6nberger", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jan-Michael Frahm", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhenxin Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenhao Yao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zi Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xinglong Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Joshua Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nadine Chang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Maying Shen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zuxuan Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shiyi Lan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jose M. Alvarez", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lan Feng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yang Gao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Quanyi Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wuyang Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sichao Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Maciej K. Wozniak", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lianhang Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yixi Cai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Patric Jensfelt", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Junnan Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dongxu Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Silvio Savarese", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Steven Hoi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Maximilian Nickel", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Matt Le", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Cheng Chi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhenjia Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Siyuan Feng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Eric Cousineau", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Benjamin Burchfiel", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Russ Tedrake", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shuran Song", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fanlong Zeng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wensheng Gan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zezheng Huai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hechang Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yongheng Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ning Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Philip S. Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xue Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Maoqing Yao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dapeng Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jing Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chenghui Hu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaoyan Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhenlong Yuan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Rui Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fei Shen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qingguo Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiangyu Yue", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Peter Albert", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Amjad Almahairi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dan Bikel", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Moya Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guillem Cucurull", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jude Fernandes", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Brian Fuller", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yinghai Lu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Pushkar Mishra", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yixin Nie", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Rashi Rungta", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kalyan Saladi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ranjan Subramanian", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Adina Williams", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jian Xiang Kuan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Weifan Guan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qinghao Hu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Aosheng Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jian Cheng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuechen Luo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhiyi Lai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lei Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qimao Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiaxin Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhi-xin Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ruiyang Hao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haibao Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiaru Zhong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chuanye Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yiming Kan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenxian Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Siqi Fan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Huilin Yin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jianing Qiu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yao Mu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiankai Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Walter Zimmer", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dandan Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shanghang Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mac Schwager", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zaiqing Nie", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Weixing Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yongjie Bai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guanbin Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wen Gao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Liang Lin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yueen Ma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zixing Song", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuzheng Zhuang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jianye Hao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Irwin King", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chaojun Ni", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xueyang Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yida Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xinze Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Boyuan Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Youyi Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenjun Mei", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fanqing Meng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiaqi Liao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xinyu Tan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Quanfeng Lu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dianqi Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yu Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Keyu Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Likai Ma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiahe Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tianhui Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuwei Du", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Siqi Guo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuming Lin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kaiwen Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhenyu Tang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaotao Hu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xingang Pan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaoyang Guo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jingwei Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiao-Xiao Long", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xun Cao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wei Yin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yinhan Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Myle Ott", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jingfei Du", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mandar Joshi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Danqi Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Omer Levy", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Veselin Stoyanov", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yongliang Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shiji Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mingzhuo Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lianzhe Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Heng Chang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenbo Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xinting Hu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiao Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xu Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xingjun Ma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yifeng Gao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yixu Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ruofan Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ye Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hengyuan Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yunhao Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yunhan Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hanxun Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yige Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiang Zheng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yang Bai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Henghui Ding", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xipeng Qiu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jingfeng Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yiming Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jun Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jindong Gu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Baoyuan Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Siheng Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Min Gong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tongliang Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shirui Pan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yinpeng Dong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ruoxi Jia", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shi-jie Ma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Neil Gong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chaowei Xiao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sarah Erfani", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Masashi Sugiyama", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dacheng Tao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "James Bailey", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yu-Gang Jiang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Junying Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Farong Wen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yijin Guo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ziheng Jia", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiahao Xiao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ye Shen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yushuo Zheng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaorong Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yalun Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ziheng Jiao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wei Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zijian Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kaiwei Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kang Fu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuqin Cao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ming Hu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yue Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xuemei Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Juntai Cao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wei Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jinyu Cao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ronghui Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Donghao Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuan Tian", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiangyang Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chun-yuan Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haoning Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yu Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hui Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lin Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zesheng Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Huiyu Duan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yingjie Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiongkuo Min", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qi Jia", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dongzhan Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenlong Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiezhang Cao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guangtao Zhai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yangyang Guo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fangkai Jiao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Liqiang Nie", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mohan Kankanhalli", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zheqi He", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xi Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jinmiao Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chuang Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zelin Shi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yunpeng Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yingdi Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Waleed Khalid", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dmitry Ignatov", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Radu Timofte", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yu Tian", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhongheng Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chenshi Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yiyun Su", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ziwei Hong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zexi Gong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jingyuan Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yang Lu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haoyang Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Erzhi Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Gongfa Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tongjian Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jinhui Yi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Gina Lopez", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "S. Hadir", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jan Weyler", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lasse Klingbeil", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Marion Deichmann", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Juergen Gall", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "S. J. Seidel", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Isaac Robinson", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Peter Robicheaux", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Matvei Popov", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Neehar Peri", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhou Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "A. Bovik", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "H. Sheikh", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Eero P. Simoncelli", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fengrui Tian", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yulun Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yingying Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shenlong Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ning Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yaoyao Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuxue Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ziqi Shi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Junran Peng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Feng Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "William Peebles", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jianlin Su", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yu Lu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shengfeng Pan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ahmed Murtadha", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bo Wen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yunfeng Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sifan Tu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xin Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dingkang Liang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xingyu Jiang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yumeng Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaofan Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiang Bai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Philip Lenz", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "C. Stiller", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "R. Urtasun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "S. Umeyama", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Run Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chaoyi Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Amir Salarpour", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xi Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhi-Qi Cheng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Feng Luo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mert D. Pes\u00e9", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Siyu Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xingbang Hao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guigang Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shang Ma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nikhil Keetha", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Norman M\u00fcller", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Johannes Sch\u00f6nberger", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lorenzo Porzi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tobias Fischer", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Arno Knapitsch", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Duncan Zauss", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ethan Weber", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nelson Antunes", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jonathon Luiten", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Manuel Lopez-Antequera", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Samuel Rota Bul\u00f2", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Christian Richardt", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sebastian Scherer", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Peter Kontschieder", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Team Seedream", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yunpeng Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Lixue Gong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Meng Guo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhiyao Guo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaoxia Hou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yixuan Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaowen Jian", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Huafeng Kuang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhichao Lai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fanshi Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaochen Lian", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chao Liao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Liyang Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yanzuo Lu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhengxiong Luo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tongtong Ou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guang Shi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yichun Shi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shiqi Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xun Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ye Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guofeng Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenxu Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yonghui Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xin Xia", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shuang Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xin Yan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhonghua Zhai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chenlin Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qi Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuwei Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shijia Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenliang Zhao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenjia Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Junyan Ye", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dongzhi Jiang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zihao Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Leqi Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhenghao Hu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zilong Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jun He", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhiyuan Yan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jinghua Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hongsheng Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Weijia Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yi Xin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qi Qin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Siqi Luo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Juncheng Yan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yan Tai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiayi Lei", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuewen Cao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Keqi Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qian Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dengyang Jiang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuandong Pu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haoxing Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Le Zhuo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tianbin Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jin Ye", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bo Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chang Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yihao Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "NextStep Team", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chunrui Han", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guopeng Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jingwei Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Quan Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yan Cai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuang Peng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zheng Ge", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Deyu Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haomiao Tang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hongyu Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kenkun Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ailin Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Changxin Miao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Deshan Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "En Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fukun Yin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hao Nie", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haoran Lv", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hanpeng Hu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jia Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jian Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jianjian Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kaijun Tan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kang An", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kangheng Lin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mei Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Peng Xing", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shiyu Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shutao Xia", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tianhao You", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wei Ji", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xin Han", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xuelin Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yana Wei", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yanming Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yimin Jiang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yingming Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yucheng Han", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ziyang Meng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Binxing Jiao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Daxin Jiang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yibo Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xu Jiang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Carroll L. Wainwright", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Alex Ray", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Fraser Kelton", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Luke Miller", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Paul Christiano", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bowen Jin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hansi Zeng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhenrui Yue", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sercan Arik", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dong Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hamed Zamani", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiawei Han", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Karan Singhal", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tao Tu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Juraj Gottweis", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "R. Sayres", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ellery Wulczyn", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mohamed Amin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kevin Clark", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Stephen R. Pfohl", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Heather Cole-Lewis", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Darlene Neal", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Q. Rashid", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mike Schaekermann", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Amy Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dev Dash", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jonathan H. Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nigam H. Shah", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sami Lachgar", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "P. Mansfield", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sushant Prakash", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Bradley Green", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ewa Dominowska", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nenad Toma\u0161ev", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yun Liu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Renee Wong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Christopher Semturs", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "S. Mahdavi", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Joelle K. Barral", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dale R. Webster", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "G. Corrado", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yossi Matias", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "A. Karthikesalingam", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Vivek Natarajan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tianzhe Chu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuexiang Zhai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sergey Levine", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yi Ma", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Peijia Lin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Pin Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Rui Jiao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qing Mo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jianhuan Cen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenbing Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Dan Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yutong Lu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenqiang Sun", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haiyu Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Haoyuan Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Junta Wu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zehan Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhenwei Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yunhong Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tengfei Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chunchao Guo", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jianfeng Xiang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xiaoxue Chen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sicheng Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Ruicheng Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zelong Lv", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yu Deng", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hongyuan Zhu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yue Dong", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nicholas Jing Yuan", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Basile Terver", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tsung-Yen Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jean Ponce", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Adrien Bardes", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guangyi Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hanlei Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yunlong Cai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qiyu Hu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Guanding Yu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhijing Qin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wenjun Lin", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jensen Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kaitong Cai", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Qianqian Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yifei Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Aleksander Holynski", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Angjoo Kanazawa", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Xuanchi Ren", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Tianchang Shen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jiahui Huang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Huan Ling", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yifan Lu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Merlin Nimier-David", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Thomas M\u00fcller", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Alexander Keller", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Sanja Fidler", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jun Gao", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shangzhan Zhang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Jianyuan Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yinghao Xu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Nan Xue", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Christian Rupprecht", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yujun Shen", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Gordon Wetzstein", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Luigi Piccinelli", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Christos Sakaridis", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yung-Hsu Yang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Mattia Segu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Siyuan Li", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Wim Abbeloos", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Luc Van Gool", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Shuo Xing", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Chengyuan Qian", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yuping Wang", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Hongyuan Hua", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Kexin Tian", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Yang Zhou", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Zhengzhong Tu", "category": 1, "symbolSize": 25, "draggable": true, "value": "Researcher"}, {"name": "Transformer", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "WMT 2014 English-to-German translation task", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "WMT 2014 English-to-French translation task", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "English constituency parsing", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "BLEU", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "residual learning framework", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "VGG nets", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "ImageNet", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "CIFAR-10", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "COCO object detection dataset", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "error", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "relative improvement", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Adam", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "AdaMax", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Dropout", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Inception Architecture", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "ILSVRC 2012 classification challenge validation set", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "top-1 error", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "top-5 error", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "InstaDrive", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Instance Flow Guider", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Spatial Geometric Aligner", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "nuScenes", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "CARLA", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "video generation quality", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "safety evaluation", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "CNC-VLM", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "CNC fault detection dataset", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "accuracy", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "F1-score", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "mamba segmentation", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "four-point laser metric calibration", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "DiffusionEngine", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "deep convolutional neural network", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "ConvNet models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "ImageNet Challenge 2014", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "other datasets", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Region Proposal Network (RPN)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Fast R-CNN", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "SPPnet", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "VGG-16", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "PASCAL VOC 2007", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "PASCAL VOC 2012", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "MS COCO", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "ILSVRC", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "COCO 2015", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "frame rate", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "object detection accuracy", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Federated Learning Optimal Transport (FLOT)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "GTSRB", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "KBTS", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "CIFAR10", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "EMNIST", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "scalability", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "WarmGait", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Taylor Finite Difference (TFD)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "thermal array sensors", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "average recognition accuracy", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "NPSSL", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Duke dataset", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Unsupervised Domain Adaptation", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Noise Perception Self-Paced Learning", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "stochastic variational inference and learning algorithm", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "reparameterization of the variational lower bound", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "approximate inference model", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "i.i.d. datasets", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "variational lower bound", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "AdaGrad", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Online Learning", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Stochastic Optimization", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Convergence Rate", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Recurrent neural networks (RNNs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Connectionist Temporal Classification", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Long Short-term Memory RNN", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "deep recurrent neural networks", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "deep Long Short-term Memory RNNs", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "deep feedforward networks", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "TIMIT phoneme recognition benchmark", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "test set error of 17.7%", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "PBD", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "GAN", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "seven benchmarks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "reconstruction loss", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "AdamW", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "face mask detection model", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Engram", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Mixture-of-Experts (MoE)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "MMLU", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "CMMLU", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "BBH", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "ARC-Challenge", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "HumanEval", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "MATH", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Multi-Query NIAH", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Neural Hierarchical Interpolation for Time Series Forecasting (N-HiTS)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Network-Based Expansion Analysis for Interpretable Time Series Forecasting (N-BEATS)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "long short-term memory (LSTM)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "two headwater streams in Georgia and North Carolina, USA", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Multi-Quantile Loss", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "95th percentile prediction uncertainty (95 PPU)", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Generating an annual 30 m rice cover product for monsoon Asia (2018\u20132023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "NASA-IBM geospatial foundation model", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "harmonized Landsat and Sentinel-2 data", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Inception", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "GoogLeNet", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014)", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Batch Normalization", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "top-5 validation error", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "test error", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Diffusion Transformers (DiT)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Representation Autoencoders (RAEs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "VAE", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "DINO", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "SigLIP", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "MAE", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "DDT head", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "FID", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "C2S-Scale", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Cell2Sentence (C2S) framework", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Large Language Models (LLMs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "single-cell foundation models (scFMs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "corpus comprising over one billion tokens of transcriptomic data, biological text, and metadata", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "human cell models", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "predictive and generative capabilities", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "performance in perturbation response prediction, natural language interpretation, and complex biological reasoning", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "DI", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "DiffPure", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Google Cloud Vision", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Lp constraint", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "imperceptibility metrics", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "finer-grained measures", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "user study", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "HybridVisionNet", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "OGNet", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "YOLO-OG", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Dish-10", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Dish-20", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "mean Average Precision (mAP)", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Unified Text-to-Text Transformer", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Colossal Clean Crawled Corpus", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "dozens of language understanding tasks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "summarization", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "question answering", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "text classification", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "GANs", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Two Time-Scale Update Rule", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "KITTI", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "novel 3D detection and tracking metrics", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "lidar based detection and tracking", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "image based detection and tracking", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "classic modular pipeline", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "end-to-end model trained via imitation learning", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "end-to-end model trained via reinforcement learning", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "controlled scenarios of increasing difficulty", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "metrics provided by CARLA", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Sora", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "MNIST", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "text-to-video generation", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "world modeling", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "OmniNWM", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "existing models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "video generation", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "control accuracy", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "long-horizon stability", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "ConsisDrive", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Instance-Masked Attention", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Instance-Masked Loss", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "UniDriveDreamer", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "LiDAR-specific variational autoencoder (VAE)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "video VAE", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Unified Latent Anchoring (ULA)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "diffusion transformer", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "multi-camera video", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "LiDAR sequence", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "LiDAR generation", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "MAD-LTX", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "SVD", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "LTX", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "autonomous driving", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "driving domains", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "structured motion", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "physically consistent interactions", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "photorealistic, temporally coherent videos", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "text, ego, and object controls", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "REPA", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "iREPA", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "REPA-E", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Meanflow", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "JiT", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "ImageNet-1K", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "ImageNet-1K accuracy", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "SCB-DETR", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "baseline model", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "SCBehavior", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "AP50", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "ultrasound-cardiac-feature-net (UCF-Net)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "filtered integral quasi-super-twisting algorithm (FIQSTA)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "proportional (P) controller", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "sliding mode controller", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "super-twisting algorithm (STA)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "integral quasi-STA", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "cardiac phantom", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "parasternal short axis", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "parasternal long axis", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "subcostal", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "apical four chambers views", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "trajectory passing through the main views", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "BioTune", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "AutoRGN", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "LoRA", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "nine image classification datasets", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "medical imaging", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "efficiency", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "VGG-16 net", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "NUS dataset", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "PASCAL", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "SUN", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Deformable Parts Model", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "bounding box detection", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "segmentation detection", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "FANet", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Multi-Scale Frequency Feature Enhancement Module (MSFFEM)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Channel Attention-based RoI Enhancement Module (CAREM)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "AI-TOD", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "VisDrone2019", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "DOTA-v1.5", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "detection performance", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "probabilistic models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "auto-encoders", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "manifold learning", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "deep networks", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Stacked Denoising Autoencoders", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "SDXL-Lightning", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "SDXL", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "UNet", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "EchoMimic", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "various public datasets", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "our collected dataset", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "quantitative evaluations", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "qualitative evaluations", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "GenAD", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "state-of-the-art performance", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Ovis", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Multimodal Large Language Models (MLLMs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "LLM", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "vision transformer", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "MLP", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Qwen-VL-Plus", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "various multimodal benchmarks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "empirical evaluations", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "VideoReward", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Flow-DPO", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Flow-RWR", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Flow-NRG", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "large-scale human preference dataset", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "supervised fine-tuning methods", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "test set error", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Recurrent Neural Networks", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "bidirectional LSTM", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "other neural network architectures", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "machine learning", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "artificial synapses", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "flexible sensors", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "human activities", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "artificial sensory organs", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "soft robotics", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "data analysis", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "intelligent decision-making", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "speech-to-text BCI", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "50-word vocabulary", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "125,000-word vocabulary", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "word error rate", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "words per minute", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "deep-learning methods", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "backpropagation algorithm", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "continual backpropagation algorithm", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "plasticity", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "keyword-spotting network", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "MLPerf recurrent neural-network transducer (RNNT)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "speech-recognition tasks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "energy efficiency", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "TOPS/W chip-sustained performance", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "LiteToken", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "BPE tokenizers", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "commonly used tokenizers", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "token fragmentation", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "parameters", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "robustness to noisy or misspelled inputs", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "overall performance", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "MeKi", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "dense LLM baselines", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "edge devices", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "inference speed", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Gengram", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "genomic foundation models (GFMs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "functional genomics tasks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "L$^3$", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "dense models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "iso-sparse MoEs", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "language modeling", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "downstream tasks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "speed", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "quality", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "LongCat-Flash-Lite", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "agentic and coding domains", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Convolutional Neural Network (CNN)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Lasso", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "LifeCLEF plant identification challenge", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "FedMicro-IDA", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "MaleVis", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "detection and classification performance", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "external attention-based transformers", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "large language models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "large vision models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "multimodal large language models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "tailored models for agricultural question-answering", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "robotic automation", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "advanced image analysis from remote sensing and spectral data", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "traditional models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Web of Science", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "arXiv", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "bibliometric analysis", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "linear-nonlinear (LN) models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "convolutional neural networks (CNNs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "marmoset and salamander retinas datasets", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "predictive performance", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "cross-stimulus generalization", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "SIFT", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Image matching dataset", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Accuracy", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "LLaVA-OneVision-1.5", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "LLaVA-OneVision-1.5-8B", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "LLaVA-OneVision-1.5-4B", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Qwen2.5-VL-7B", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Qwen2.5-VL-3B", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "LLaVA-OneVision-1.5-Mid-Traning", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "LLaVA-OneVision-1.5-Instruct", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "27 benchmarks", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Vision-Language-Action (VLA) models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "large language models (LLMs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "vision-language models (VLMs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "publicly available datasets", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "evaluation benchmarks", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "teacher model", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "pretrained state-of-the-art vision foundation models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "human-aligned models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "dataset of human judgements spanning multiple levels of semantic abstractions", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Vision Transformer (ViT)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "CIFAR-100", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "VTAB", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "state-of-the-art convolutional networks", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "CLIP", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "ResNet-50", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "400 million (image, text) pairs", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "over 30 different existing computer vision datasets", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Generative Adversarial Network (GAN)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "MeanFlow (MF)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "improved MeanFlow (iMF)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "ImageNet 256\u00d7256", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "pixel-space diffusion and consistency models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "ImageNet-256", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "ImageNet-512", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "DiT", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "SVG-T2I", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "SVG (Self-supervised representations for Visual Generation)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "GenEval", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "DPG-Bench", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "0.75", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "85.78", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "PixelDiT", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Diffusion Transformers (DiTs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "ImageNet 256x256", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "DPG-bench", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "pixel generative models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "latent diffusion models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "TUNA", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "VAE encoder", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "representation encoder", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "multimodal understanding and generation benchmarks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "performance", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "BERT", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "GLUE", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "MultiNLI", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "SQuAD v1.1", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "SQuAD v2.0", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "GLUE score", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "MultiNLI accuracy", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "SQuAD v1.1 question answering Test F1", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "SQuAD v2.0 Test F1", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Diffusion language models (DLMs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "autoregressive (AR) models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "AR coder", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "1.7B DLM", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "1B-parameter DLM", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "10B unique Python tokens", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "HellaSwag", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "validation cross-entropy", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "DreamOn", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Diffusion Language Models (DLMs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Dream-Coder-7B", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "DiffuCoder-7B", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "HumanEval-Infilling", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "SantaCoder-FIM", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "FLEX", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "AIME25", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "USPTO50k", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "ProteinGym", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "mathematical reasoning", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "chemical retrosynthesis", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "protein fitness prediction", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Agent-R1", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Multihop QA benchmark tasks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "KITTI dataset", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "video generation foundation models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "video diffusion models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "open-source video generation models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "pretrained video generation models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "single-condition generation", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "multi-condition generation", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "universal controllable generation", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Awesome-Controllable-Video-Generation", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Autonomous Driving Systems (ADS)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "multi-label classification method", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Alpamayo-R1 (AR1)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Chain of Causation (CoC) dataset", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Cosmos-Reason", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "trajectory-only baseline", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "planning accuracy", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "close encounter rate", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "reasoning quality", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "reasoning-action consistency", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Ego3D-VLM", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "GPT-4o", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Gemini1.5-Pro", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "InternVL3", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Qwen2.5-VL", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Ego3D-Bench", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "multi-choice QA", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "absolute distance estimation", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Rex-Omni", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "YOLO", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "DETR", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Grounding DINO", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "COCO", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "LVIS", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Deep Q-Network (DQN)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Atari 2600 games", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "human-level performance", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "asynchronous gradient descent", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "asynchronous actor-critic", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Atari domain", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "continuous motor control problems", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "random 3D mazes", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "training time", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "RefineNet", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "intersection-over-union", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "LightEMMA", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Vision-Language Models (VLMs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "computational metrics", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "ReCogDrive", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "NAVSIM", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Bench2Drive", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "DriveBench", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Diffusion Group Relative Policy Optimization (DiffGRPO)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "pseudo-simulation", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "3D Gaussian Splatting", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "real datasets", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "closed-loop simulation", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "open-loop evaluation", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "R^2", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "DriveMoE", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Drive-\u03c0\u2080", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "state-of-the-art (SOTA) performance", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Vision-Language-Action (VLA) paradigms", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "multimodal large language models (MLLM)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "VLA for Autonomous Driving (VLA4AD)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "over 20 representative models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "existing datasets and benchmarks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "driving safety", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "explanation quality", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Sora model", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "General world models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "autonomous-driving world models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "world models deployed within autonomous agents", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "GPT-4", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "multimodal machine learning", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "state-of-the-art methods", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "multimodal learning", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "datasets covered in multimodal learning research", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "multimodal generative models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "multimodal foundation models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Any-to-Text", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Any-to-Vision", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Any-to-Any", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "new theoretical framework for decision making in the tourism industry", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Intelligent chatbots", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "generative artificial intelligence (GAI) tools", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "existing responsive AI instruments", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "tourism and hospitality scenarios", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Squeeze-and-Excitation (SE) block", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "SENet", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "ILSVRC 2017", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "EfficientNets", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "EfficientNet-B7", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "MobileNets", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "ResNet", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Flowers", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "top-1 accuracy", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Light-X", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Light-Syn", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Light-Syn dataset", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "baseline methods", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "prior video relighting methods", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "DriveLaW", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "DriveLaW-Video", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "DriveLaW-Act", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "FVD", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "DVGT", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "OpenScene", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Waymo", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "DDAD", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "ControlNet", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Stable Diffusion", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "small (<50k) and large (>1m) datasets", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Flan-PaLM 540B", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "PaLM 540B", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Flan-T5", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "PaLM 62B", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "TyDiQA", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "MGSM", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "five-shot MMLU", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "latent diffusion models (LDMs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "diffusion models (DMs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "image inpainting", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "unconditional image generation", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "semantic scene synthesis", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "super-resolution", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "visual fidelity", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "computational requirements", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Variational Autoencoder (VAE)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "VGG network", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "PSNR", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "SSIM", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "new dataset of human perceptual similarity judgments", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Waymo Open Dataset", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "diversity metric", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Data augmentation", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "face images generation", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "two-layer ReLU denoising autoencoder (DAE)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "unconditional and text-to-image diffusion models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "representation-based method for detecting memorization", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "training-free editing technique", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Stable Velocity", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Stable Velocity Matching (StableVM)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Variance-Aware Representation Alignment (VA-REPA)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Stable Velocity Sampling (StableVS)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "SD3.5", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Flux", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Qwen-Image", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Wan2.2", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "training efficiency", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "sample quality", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "FlatDINO", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "DiT-XL", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "DINOv2", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "gFID", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "One-Dimensional Diffusion Video Autoencoder (One-DVA)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "3D-CNN VAEs", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "REPA-G", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Dense Convolutional Network (DenseNet)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "SVHN", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "computation", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Convolutional Neural Network", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Transfer Learning", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "staged adaptive fine-tuning approach", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "DenseNet-121", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Cholec80", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "CATARACTS", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "mean average precision (mAP)", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "MediaPipe", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Multi-layered Randomized Decision Forests", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "geometry based normalizations", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Krawtchouk moments", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "machine learning model", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "user's hand signals", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "MediaPipe and a fully connected neural network (FCNN)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "American Sign Language (ASL) dataset", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "fast recognition", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "R-CNN", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "OverFeat", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "PASCAL VOC", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "VOC 2012", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "ILSVRC2013", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "MuMu-LLaMA", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "SuPLoRA", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "supertype-subtype concept hierarchy", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "group-wise suppression method", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "standard diffusion regularization", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "benchmark", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Multi-modal Chain and Global Attention Network (MCGA-Net)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "DCGAN-based data augmentation strategy", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Multi-modal Chain Feature Fusion (MCFF)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Global Attention Mechanism (GAM)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "GPR images", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Precision", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Recall", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "mAP@50", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Qwen3-VL-Embedding", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Qwen3-VL-Reranker", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Qwen3-VL", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "MMEB-V2", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "FPSMark", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "intrinsic signal localization network", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "partial screen-shooting scenarios", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "extraction accuracy", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "EM algorithm", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "t-SNE", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "LLM social simulations", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "social science datasets", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "empirical comparisons", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "FCLFD", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "CST", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "AWS", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "WISDM", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "PAMAP2", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "F1", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Dispersive Loss", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Knowledge-Aware Bayesian Bandits (KABB)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "multi-agent systems", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "cost-performance balance", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "U-Net", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "sliding-window convolutional network", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "ISBI challenge for segmentation of neuronal structures in electron microscopic stacks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "ISBI cell tracking challenge 2015", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "DMD2", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Distribution Matching Distillation (DMD)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "ImageNet-64x64", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "COCO 2014", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Latent Adversarial Diffusion Distillation (LADD)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "adversarial diffusion distillation (ADD)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Stable Diffusion 3 (8B)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "SD3-Turbo", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "state-of-the-art text-to-image generators", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "evolutionary approach", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Japanese LLM with Math reasoning capabilities", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "culturally-aware Japanese VLM", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Japanese LLM benchmarks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "bidirectional diffusion transformer", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "autoregressive transformer", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "distribution matching distillation (DMD)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "50-step diffusion model", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "4-step generator", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "VBench-Long", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "total score", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "PuLID", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Lightning T2I branch", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "standard diffusion branch", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "diffusion probabilistic models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "LSUN", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Inception score", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "FID score", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "ProgressiveGAN", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "HunyuanVideo", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Runway Gen-3", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Luma 1.6", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "three top-performing Chinese video generative models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Loopy", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "audio-driven portrait diffusion models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "OmniHuman", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "existing end-to-end audio-driven methods", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "highly realistic human video generation", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "more realistic videos", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "greater flexibility in inputs", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Hallo", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Hallo2", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "HDTF", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "CelebV", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Wild", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "EchoMimicV2", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Audio-Pose Dynamic Harmonization", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Pose Sampling", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Audio Diffusion", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Head Partial Attention", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Phase-specific Denoising Loss", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "half-body data", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "headshot data", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "novel benchmark for evaluating the effectiveness of half-body human animation", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Feature Pyramid Network (FPN)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Faster R-CNN", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "COCO detection benchmark", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "state-of-the-art single-model results", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "RNN Encoder-Decoder", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "statistical machine translation system", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "log-linear model", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "DriveMLM", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Autopilot", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Apollo", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "CARLA Town05 Long", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "multimodal LLM (MLLM)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Vista", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "most advanced general-purpose video generator", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "best-performing driving world model", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "multiple datasets", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "DiffusionDrive", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "vanilla diffusion policy", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "PDMS", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "ResNet-34", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "diffusion-based video generation models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "2D simulation testbed for object movement and collisions", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "physical laws adherence", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "EMMA", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Gemini", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Waymo Open Motion Dataset (WOMD)", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Waymo Open Dataset (WOD)", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "state-of-the-art performance in motion planning", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "competitive results", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "GPT-3", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "translation, question-answering, and cloze tasks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "few-shot performance", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "InternVL 2.5", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "InternVL 2.0", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Claude-3.5-Sonnet", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "MMMU benchmark", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Chain-of-Thought (CoT) reasoning", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "InternVL3-78B", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "MMMU", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "ChatGPT-4o", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Claude 3.5 Sonnet", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Gemini 2.5 Pro", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "VLMEvalKit", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "OpenVLM Leaderboard", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "LLaVA-CoT", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "LLaVA-CoT-100k", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Gemini-1.5-pro", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "GPT-4o-mini", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Llama-3.2-90B-Vision-Instruct", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "multimodal reasoning benchmarks", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "InternVL 3.5", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Cascade Reinforcement Learning (Cascade RL) framework", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Visual Resolution Router (ViR)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Decoupled Vision-Language Deployment (DvD) strategy", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "InternVL3.5-241B-A28B", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "GPT-5", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "MathVista", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "overall reasoning performance", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "inference speedup", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Proximal Policy Optimization (PPO)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Trust Region Policy Optimization (TRPO)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "simulated robotic locomotion", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Atari game playing", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "sample complexity", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Flow-GRPO", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "SD3.5-M", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "DanceGRPO", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Group Relative Policy Optimization (GRPO)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "DDPO", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "DPOK", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "diffusion models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "rectified flows", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "HPS-v2.1", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "CLIP Score", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "VideoAlign", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Seedance 1.0", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "state-of-the-art video generation models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "multi-source data curation", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "prompt following", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "motion plausibility", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "visual quality", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "spatiotemporal fluidity", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "structural stability", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "instruction adherence", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "narrative coherence", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "subject representation", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "SkyReels-V2", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Multi-modal Large Language Model (MLLM)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "SkyCaptioner-V1", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Multi-stage Pretraining", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Reinforcement Learning", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Diffusion Forcing Framework", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Supervised Fine-Tuning (SFT)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "video data", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "human-annotated and synthetic distortion data", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "prompt adherence", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "motion dynamics", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "duration", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "temporal visual quality", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "resolution", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "shot-aware generation", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "realistic long-form synthesis", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "professional film-style generation", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "dynamic artifacts", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "UnifiedReward", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Direct Preference Optimization (DPO)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Transformer-based model", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "simulated bar exam", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "factuality", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "adherence to desired behavior", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "LLaMA", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "LLaMA-13B", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "LLaMA-65B", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "GPT-3 (175B)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Chinchilla-70B", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "PaLM-540B", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "benchmarks", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "chain of thought prompting", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "GSM8K", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "verifiers", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "transformer models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "finetuning baseline", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "test performance", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Mamba", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Mamba-3B", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "linear attention", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "gated convolution", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "recurrent models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "structured state space models (SSMs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "language", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "audio", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "genomics", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "SGDR", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "EEG recordings dataset", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "downsampled ImageNet", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "error rate", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Sparsely-Gated Mixture-of-Experts layer (MoE)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "machine translation", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "large language modeling benchmarks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "machine translation benchmarks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "computational efficiency", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "pointer sentinel mixture architecture", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "pointer sentinel-LSTM model", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "standard softmax LSTM", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Penn Treebank", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "WikiText corpus", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "perplexity", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "massive multitask language understanding test", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "multitask accuracy", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "process-supervised model", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "PRM800K", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "GPQA", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "GShard", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "XLA compiler", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "prior art", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "translation from 100 languages to English", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "model quality", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "computation cost", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "ease of programming", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "efficient implementation on parallel devices", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Probability Distributions", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Linear Models for Regression", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Linear Models for Classification", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Neural Networks", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Kernel Methods", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Sparse Kernel Machines", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Graphical Models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Mixture Models and EM", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Approximate Inference", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Sampling Methods", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Continuous Latent Variables", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Sequential Data", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Combining Models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "deep convolutional neural networks", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "shallow networks", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "deep models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "VGG16", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "PlantVillage", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "deep neural networks", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "26-layer deep learning model consisting of 8 residual building blocks", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "BJFU100 dataset", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "recognition rate", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "herbarium images", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "big dataset with thousands of species from herbaria", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "different datasets from different herbaria", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "MobileNetV2", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "SSDLite", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Mobile DeepLabv3", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Imagenet", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "VOC", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "multiply-adds (MAdd)", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "number of parameters", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Federated Learning", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "five different model architectures", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "four datasets", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "synchronized stochastic gradient descent", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Amazon Reviews", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Reuters-21578", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Office-31", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "federated deep neural network (FDNN)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Flipkart dataset", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Federated learning (FL)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Internet of Things (IoT)", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Wireless Sensor Networks (WSNs)", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "privacy-preserving federated learning", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "malware detection models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Long Short-Term Memory Network", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "centralized federated learning framework", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "decentralized federated learning framework", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "crop yield prediction", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "prediction accuracy", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "precision", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "recall", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "F1-Score", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Segment Anything (SA) project", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Segment Anything Model (SAM)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "SA-1B", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "zero-shot performance", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "LLaVA", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Science QA", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "synthetic multimodal instruction-following dataset", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "relative score", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "CLIP-ViT-L-336px", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "MLP projection", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "academic-task-oriented VQA data", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "11 benchmarks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "state-of-the-art", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "DepictQA-Wild", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Vision Language Models (VLMs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "VLM-based Image Quality Assessment (IQA)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "traditional score-based methods", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "GPT-4V", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "DQ-495K", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "distortion identification", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "instant rating", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "reasoning tasks", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Chain-of-Focus (CoF)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "MM-CoF", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "V* benchmark", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "outcome accuracies", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "formats", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "3DThinker", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "VGGT", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "multiple benchmarks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "UniME-V2", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "MLLM-as-a-Judge mechanism", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "UniME-V2-Reranker", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "MMEB benchmark", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "retrieval tasks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "OpenMMReasoner", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Qwen2.5-VL-7B-Instruct", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "874K-sample cold-start dataset", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "74K-sample dataset", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "nine multimodal reasoning benchmarks", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "agentic AI", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "autonomous vehicles", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "medical and industrial robotics", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "precision agriculture", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "humanoid robotics", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "augmented reality", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "agentic adaptation", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "cross-embodiment planning", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Asynchronous Action Chunk Correction (A2C2)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Real Time Chunking (RTC)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "dynamic Kinetix task suite (12 tasks)", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "LIBERO Spatial", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "success rate", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Latent-CoT-Drive (LCDrive)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "large-scale end-to-end driving benchmark", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "trajectory quality", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "non-reasoning baselines", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "text-reasoning baselines", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "HyperVLA", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "OpenVLA", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "hypernetwork (HN)-based architecture", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "vision foundation models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "large-scale robotic data", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "inference costs", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "number of activated parameters", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "TEAM-VLA", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "LIBERO", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "task success rate", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "SegMamba-V2", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "CRC-2000", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "three other large-scale 3D medical image segmentation datasets", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "TransSIL", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "CUB200-2011", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "NABirds", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "CBRFormer", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Deep contrastive learning enables genome-wide virtual screening", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "DrugCLIP", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "GenomeScreenDB", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "AlphaFold2", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "docking", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "hit rate", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "LLaVA-based semantic feature modulation diffusion model", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "underwater image enhancement", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "3DGS-Drag", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "diffusion guidance", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "autoregressive transformers", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "bidirectional encoders for representation learning", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Vision Transformers", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "cross-modal attention", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "standard datasets", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "performance benchmarks", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "BiFlow", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Normalizing Flows", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "TARFlow", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "generation quality", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "sampling speed", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "pixel MeanFlow", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Meta Flow Maps (MFMs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "consistency models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "flow maps", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Best-of-1000", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Sequential Flow Matching", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "flow-matching models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "forecasting tasks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "decision-making tasks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "state estimation tasks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Drifting Models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "SimCLR", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "top-5 accuracy", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "AlexNet", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Directional Decoupling Alignment (D\u00b2-Align)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "DivGenBench", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "automated reward metrics", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "quantitative metrics for both quality and diversity", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "ImagerySearch", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "LDT-Bench", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "VBench", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "creative generation capabilities", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "FLUX VAE", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "web, synthetic, and text-rendering data", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "high-quality datasets", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "general fidelity", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "video foundation models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "implicit world model", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "video renderer", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "world model", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "video generation model", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "World models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "visual prediction", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "3D estimation", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "symbol grounding", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "RecTok", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "visual tokenizers", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "vision foundation models (VFMs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "flow matching", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "diffusion transformers", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "gFID-50K", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "RePack then Refine", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Repack then Refine", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "RePack-DiT-XL/1", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Vision Foundation Models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Latent Diffusion Models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Diffusion Transformers", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "RePack module", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Latent-Guided Refiner", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "BigGAN-deep", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "ImageNet 128\u00d7128", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "ImageNet 512\u00d7512", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "ViT model (Dosovitskiy et al., 2020)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "OpenCLIP (Ilharco et al., 2021)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "automatic pipeline to build a dedicated, diverse, and curated image dataset", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "PixelGen", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "diffusion-based models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "autoregressive-based architectures", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "hybrid approaches", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "datasets and benchmarks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "MentisOculi", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "multimodal large language models (MLLMs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "unified multimodal models (UMMs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "visual strategies", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "OpenVision 3", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "ViT encoder", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "ViT-VAE decoder", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "CLIP vision encoder", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "LLaVA-1.5 framework", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "RAE framework", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "SeedBench", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "POPE", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "continuous Skip-gram model", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "hierarchical softmax", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "negative sampling", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "global logbilinear regression model", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "global matrix factorization", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "local context window methods", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "word analogy task", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "similarity tasks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "named entity recognition", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "75%", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "deep bidirectional language model (biLM)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "large text corpus", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "textual entailment", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "sentiment analysis", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Protein Language Models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Modern models for common NLP tasks", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "2018 Twitter data spanning 51 U.S. regions and 99 countries", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "gender bias in word embeddings", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "statistical gender gaps in education, politics, economics, and health", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "generative classifiers", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "diffusion-based generative classifiers", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "autoregressive generative classifiers", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "five standard image and text distribution shift benchmarks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "medical datasets", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "satellite datasets", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Gaussian toy setting", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "impact of spurious correlations", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "GPT-2", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "WebText", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Llama 3", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Llama Guard 3", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "autoregressive (AR) paradigm", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "masked language models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Quokka", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Chinchilla", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "IGPO", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "GRPO", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Math500", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "AMC", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "E2D2", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "translation", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "inference throughput", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Stable-DiffCoder", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Seed-Coder", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Diffusion-based language models (DLLMs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "code benchmarks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "low-resource coding languages", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "XLNet", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Transformer-XL", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Qwen", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Qwen-Chat", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Code-Qwen", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Code-Qwen-Chat", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Math-Qwen-Chat", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Reinforcement Learning from Human Feedback (RLHF)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "code interpreter", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Code Llama", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Code Llama - Python", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Code Llama - Instruct", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Llama 2", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "MBPP", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "MultiPL-E", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "dLLM-Var", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "dLLMs", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "autoregressive models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Llama", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "standard benchmarks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "speedup", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Set Block Decoding (SBD)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "next token prediction (NTP)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "masked token prediction (MATP)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Llama-3.1 8B", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Qwen-3 8B", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "discrete diffusion literature", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "speedups", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "OneFlow", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "autoregressive baselines", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "diffusion-based approaches", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "generation and understanding tasks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "training FLOPs", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Large language models (LLMs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "diffusion LLMs", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "constrained decoding method for diffusion models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "C++ code infilling", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "structured data extraction in JSON", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "syntactic correctness", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "functional correctness", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Masked Diffusion Language Models (MDLMs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Autoregressive Language Models (ARLMs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "mask-agnostic loss function", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Large Language Model (LLM)-based agents", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "open-sourced libraries and benchmarks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "self-evolving agent memory", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Empirical-MCTS", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Pairwise-Experience-Evolutionary Meta-Prompting (PE-EMP)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Memory Optimization Agent", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Monte Carlo Tree Search (MCTS)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "ARC-AGI-2", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "MathArena Apex", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "TAME", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Trust-Memevo", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "trustworthiness", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "task performance", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "ProcMEM", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Non-Parametric PPO", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Skill-MDP", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "in-domain, cross-task, and cross-agent scenarios", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "reuse rates", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "performance gains", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "memory compression", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "agentic time series forecasting (ATSF)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "workflow-based design", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "agentic reinforcement learning", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "hybrid agentic workflow paradigm", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Follow-Your-Emoji-Faster", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "EmojiBench++", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "animation quality", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "controllability", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Follow-Your-Instruction", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "MLLM-Collector", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "MLLM-Generator", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "MLLM-Optimizer", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "MLLM-Planner", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "existing baseline models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "2D, 3D, and 4D generative tasks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "HunyuanVideoT2V", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "approximately 1M real video clips", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "fewer than 150k curated editing pairs", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "editing instruction following", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "editing quality", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "PaperTalker", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Paper2Video", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Meta Similarity", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "PresentArena", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "PresentQuiz", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "IP Memory", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "ContextFlow", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Rectified Flow", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Guidance Responsiveness Metric", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "DeepSeek-R1", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "chain-of-thought prompting", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "conventional supervised learning on human demonstrations", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "mathematics", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "coding competitions", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "STEM fields", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "phi-3-mini", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Mixtral 8x7B", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "GPT-3.5", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "MT-bench", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "phi-2", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "phi-3-small", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "phi-3-medium", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "phi-3.5-mini", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "phi-3.5-MoE", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "phi-3.5-Vision", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Llama 3.1", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Mixtral series", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Gemini-1.5-Flash", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "LLaVA-OneVision", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "LLaVA-NeXT blog series", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "single-image scenario", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "multi-image scenario", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "video scenario", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Multimodal large language models (MLLMs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "existing benchmarks across text only, vision language, and embodied settings", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "evaluation metrics and methodologies for assessing spatial reasoning ability", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "SpatialTree", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "mainstream MLLMs", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "capability-centric hierarchical benchmark", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "27 sub-abilities", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "large-scale benchmark built from pedestrian-perspective videos captured with synchronized stereo cameras, LiDAR, and IMU/GPS sensors", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "spatial reasoning questions", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "SpatialDreamer", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Geometric Policy Optimization (GeoPO)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Multi-modal Large Language Models (MLLMs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "multiple challenging benchmarks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "highly competitive results", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Fast YOLO", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "DPM", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Picasso Dataset", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "People-Art Dataset", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "mAP", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Number GroundingDINO (NGDINO)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "RefDrone", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "gRefCOCO", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "RDAgent (referring drone annotation framework with multi-agent system)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "state-of-the-art REC methods", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "WeDetect", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "WeDetect-Uni", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "WeDetect-Ref", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "15 benchmarks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "v1", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "multimodal mathematical reasoning benchmarks", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "PosterCopilot", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Large Multimodal Models (LMMs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "LMM-based design model", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "generative models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Perturbed Supervised Fine-Tuning", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Reinforcement Learning for Visual-Reality Alignment", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Reinforcement Learning from Aesthetic Feedback", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "existing methods", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Percept-WAM", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "vision-language models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "VLA systems", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "classical detectors and segmenters", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "PMDS", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Deep Q-Learning", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "actor-critic, model-free algorithm based on the deterministic policy gradient", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "simulated physics tasks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "cartpole swing-up", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "dexterous manipulation", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "legged locomotion", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "car driving", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "planning algorithm with full access to the dynamics of the domain and its derivatives", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Reinforcement Learning with Verifiable Rewards (RLVR)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Qwen3-8B", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Qwen3-32B", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Qwen3-14B", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "AIME'25", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "AIME'24", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Clip-Cov", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "KL-Cov", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Policy Gradient-like algorithms", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "entropy", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "downstream performance", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "covariance", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "LUFFY", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "RLVR", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Mixed-Policy GRPO", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "six math benchmarks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "out-of-distribution tasks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "average gain", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "advantage", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Fine-tuning", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Reinforcement learning", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Test-time scaling", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Model alignment", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Scalable adaptation", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Inference-time reasoning", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Pretraining", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Post-training", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Catastrophic forgetting", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Reward hacking", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Inference-time trade-offs", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Tapered Off-Policy REINFORCE (TOPR)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "test-time accuracy", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "training data efficiency", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "REINFORCE", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Fully convolutional network", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "VGG net", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "NYUDv2", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "SIFT Flow", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "mean IU", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "denoising diffusion probabilistic models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "noise conditioned score networks", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "stochastic differential equations", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "variational auto-encoders", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "generative adversarial networks", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "energy-based models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "normalizing flows", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "SegNeXt", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "EfficientNet-L2 w/ NAS-FPN", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "ADE20K", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Cityscapes", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "COCO-Stuff", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Pascal VOC", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Pascal Context", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "iSAID", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "mIoU", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "CMX", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Cross-Modal Feature Rectification Module (CM-FRM)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Feature Fusion Module (FFM)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "depth", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "thermal", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "polarization", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "event", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "LiDAR", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "RGB-Depth benchmarks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "RGB-Thermal", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "RGB-Polarization", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "RGB-LiDAR", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "EventScape dataset", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "RGB-Event semantic segmentation benchmark", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "state-of-the-art performances", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "HQ-SAM", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "dataset of 44K fine-grained masks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "suite of 10 diverse segmentation datasets", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "PIDNet", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "PIDNet-S", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "CamVid", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "mIOU", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Qwen2 series", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Qwen2-72B", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Qwen2-72B-Instruct", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Qwen1.5", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "MT-Bench", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Arena-Hard", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "LiveCodeBench", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "FSDrive", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "DriveLM", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "AutoVLA", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "nuPlan", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Impromptu VLA", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Impromptu VLA Dataset", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "NeuroNCAP", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "collision rates", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "L2 accuracy", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "AgentThink", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "DriveLMM-o1", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "overall reasoning scores", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "answer accuracy", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Drive-R1", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "DriveLM-nuScenes", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "IRL-VLA", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "VLA architecture", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "VLA policy", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "reward world model", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "PPO (Proximal Policy Optimization)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "NAVSIM v2 end-to-end driving benchmark", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "CVPR2025 Autonomous Grand Challenge", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "1st runner up", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "DriveVLA-W0", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "autoregressive world model", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "diffusion world model", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "BEV", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "VLA", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "NAVSIM v1/v2 benchmark", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "in-house dataset", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "several established datasets", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "GTRS (Generalized Trajectory Scoring)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Navsim v2 Challenge", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "3D Rasterization", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Rasterization Augmented Planning (RAP)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Raster-to-Real feature-space alignment", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "NAVSIM v1", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "NAVSIM v2", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Waymo Open Dataset Vision-based E2E Driving", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "closed-loop robustness", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "long-tail generalization", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "PRIX", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Context-aware Recalibration Transformer (CaRT)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "NavSim", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "multimodal diffusion planners", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "BLIP-2", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Flamingo80B", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "VQAv2", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "zero-shot VQAv2", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Flow Matching (FM)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Continuous Normalizing Flows (CNFs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "likelihood", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Diffusion Policy", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "existing state-of-the-art robot learning methods", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "4 different robot manipulation benchmarks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "average improvement of 46.9%", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "LLM-based robotic models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "robot control", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "perception", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "decision-making", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "planning", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "dexterity intelligence", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "human-robot interaction", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "autonomy", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Raw2Drive", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "CARLA Leaderboard 2.0", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Vision Language Action (VLA) models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "autoregression-based methods", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "diffusion-based methods", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "reinforcement-based methods", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "hybrid methods", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "specialized methods", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "foundational datasets", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "simulation platforms", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "ReSim", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Video2Reward", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "diffusion transformer architecture", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "planning performance", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "policy selection performance", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Llama 2-Chat", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "open-source chat models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "closed-source models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "human evaluations for helpfulness and safety", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "AdaThinkDrive", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Chain of Thought (CoT)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Navsim", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "never Think baseline", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "always Think baseline", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "vision only baseline", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "UniV2X framework", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "V2X-Seq-SPD dataset", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "End-to-End Autonomous Driving through V2X Cooperation Challenge", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "cooperative temporal perception", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "cooperative end-to-end planning", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Multi-modal Large Models (MLMs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "World Models (WMs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "embodied robots", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "simulators", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "embodied perception", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "embodied interaction", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "embodied agent", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "sim-to-real adaptation", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "comprehensive datasets", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "vision-language-action models (VLAs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "VLA-based control policies", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "high-level task planners", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "datasets", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "DriveDreamer4D", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "NeRF", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "3DGS", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "PVG", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "S3Gaussian", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Deformable-GS", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "4DGS", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "NTA-IoU", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "PhyGenBench", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "PhyGenEval", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "AgentSquare", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Modularized LLM Agent Search (MoLAS)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "six benchmarks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "performance gain", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "CityGPT", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "CityInstruction", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "SWFT", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "ChatGLM3-6B", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Llama3-8B", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Qwen2.5-7B", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "CityEval", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Epona", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "RoBERTa", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "RACE", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "SQuAD", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "DoCo", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Machine Unlearning (MU)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Text-to-image diffusion models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "various instances, styles, and offensive concepts", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "generalization", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "utility", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Vision Foundation Models (VFMs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Vision-Language Pre-training (VLP) models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Diffusion Models (DMs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "large-model-based Agents", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "commonly used datasets and benchmarks for safety research", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Vision Large Language Models (VLLMs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "LLM-Pipeline", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "benchmark datasets", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "benchmark evaluations", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "evaluation methods for jailbreak", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Video-SafetyBench", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "RJScore", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Large Vision-Language Models (LVLMs)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "GGL-Net", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "gradient supplementary module (GSM)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "two-way guidance fusion module (TGFM)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "NUAA-SIRST dataset", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "NUDT-SIRST dataset", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "NN-RAG", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "LEMUR", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "structural uniqueness", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "novel network structures", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "CenterMamba-SAM", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "CenterMamba encoder", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "memory-driven structural prompt generator", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "memory-augmented multi-scale decoder", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "public benchmarks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "IMobileTransformer", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "UAV-based RGB images", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "RF-DETR", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "RF-DETR (nano)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "RF-DETR (2x-large)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "D-FINE (nano)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "GroundingDINO (tiny)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Roboflow100-VL", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "AP", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "FreeOrbit4D", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "object-centric multi-view diffusion model", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "conditional video diffusion model", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "monocular video", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "redirected videos", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "faithful redirected videos", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "NeoVerse", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "standard reconstruction and generation benchmarks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "DiT-XL/2", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "ImageNet 512x512", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Gflops", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "RoPE", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "RoFormer", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "long text classification benchmark datasets", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Driving World Model (DWM)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "mainstream simulators", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "high-impact datasets", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "various metrics", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "FlexMap", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "multiple configurations", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "MapAnything", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "diverse datasets", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "specialist feed-forward models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Seedream 4.0", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Seedream 4.5", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "VLM model", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "billions of text-image pairs", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "T2I", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "multimodal image editing", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Echo-4o", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Bagel", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "OmniGen2", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "BLIP3-o", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Echo-4o-Image", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "GenEval++", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Imagine-Bench", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Lumina-DiMOO", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "autoregressive (AR) or hybrid AR-Diffusion paradigms", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "NextStep-1", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "vector quantization (VQ)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "flow matching head", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "text-to-image generation tasks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "high-fidelity image synthesis", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "image editing", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "InstructGPT", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "prompt distribution", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "public NLP datasets", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "human evaluations", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "truthfulness", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "toxic output generation", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Search-R1", "category": 3, "symbolSize": 50, "draggable": true, "value": "AIPaper"}, {"name": "Qwen2.5-3B", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "seven question-answering datasets", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "Med-PaLM", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Med-PaLM 2", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "MedQA", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "MedMCQA", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "PubMedQA", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "MMLU clinical topics", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "score", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "evaluation metrics", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Supervised fine-tuning (SFT)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "reinforcement learning (RL)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "GeneralPoints", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "V-IRL", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "outcome-based reward", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "EquiCSP", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "accurate structures", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "faster convergence", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "WorldPlay", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "existing techniques", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "diverse scenes", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "long-term geometric consistency", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "real-time speeds", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "24 FPS", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "720p video", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "O-Voxel", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Sparse Compression VAE", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "diverse public 3D asset datasets", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "JEPA-WMs", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "DINO-WM", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "V-JEPA-2-AC", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "simulated environments", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "real-world robotic data", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "planning success", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "PLIT", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "hierarchical VAE", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "rate-distortion performance", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "STORM", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "CogACT", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "SimplerEnv manipulation benchmark", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "average success rate", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "Frechet Video Distance", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "CUT3R (Continuous Updating Transformer for 3D Reconstruction)", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "various 3D/4D tasks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "GEN3C", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "sparse-view novel view synthesis", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "driving scenes", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "monocular dynamic video", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "camera control", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "temporal 3D consistency", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "FLARE", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "large-scale public datasets", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "pose estimation", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "geometry reconstruction", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "novel view synthesis", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "UniDepthV2", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "UniDepth", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "ten depth datasets", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "OpenEMMA", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Chain-of-Thought reasoning process", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "PASCAL VOC dataset", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "ILSVRC2013 detection dataset", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "cloud-only framework", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "crop yield prediction dataset", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "response time", "category": 2, "symbolSize": 25, "draggable": true, "value": "Metric"}, {"name": "prior VLM-based IQA models", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "Fully Convolutional Networks", "category": 4, "symbolSize": 25, "draggable": true, "value": "AIModel"}, {"name": "RGB-Thermal datasets", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "RGB-Polarization datasets", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "RGB-LiDAR datasets", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}, {"name": "robot manipulation benchmarks", "category": 0, "symbolSize": 25, "draggable": true, "value": "Dataset"}],
                    links: [{"source": "Ashish Vaswani", "target": "Attention Is All You Need", "value": "author_of"}, {"source": "Noam Shazeer", "target": "Attention Is All You Need", "value": "author_of"}, {"source": "Niki Parmar", "target": "Attention Is All You Need", "value": "author_of"}, {"source": "Jakob Uszkoreit", "target": "Attention Is All You Need", "value": "author_of"}, {"source": "Llion Jones", "target": "Attention Is All You Need", "value": "author_of"}, {"source": "Aidan N. Gomez", "target": "Attention Is All You Need", "value": "author_of"}, {"source": "Lukasz Kaiser", "target": "Attention Is All You Need", "value": "author_of"}, {"source": "Illia Polosukhin", "target": "Attention Is All You Need", "value": "author_of"}, {"source": "Kaiming He", "target": "Deep Residual Learning for Image Recognition", "value": "author_of"}, {"source": "Xiangyu Zhang", "target": "Deep Residual Learning for Image Recognition", "value": "author_of"}, {"source": "Shaoqing Ren", "target": "Deep Residual Learning for Image Recognition", "value": "author_of"}, {"source": "Jian Sun", "target": "Deep Residual Learning for Image Recognition", "value": "author_of"}, {"source": "Diederik P. Kingma", "target": "Adam: A Method for Stochastic Optimization", "value": "author_of"}, {"source": "Jimmy Ba", "target": "Adam: A Method for Stochastic Optimization", "value": "author_of"}, {"source": "Sepp Hochreiter", "target": "Long Short-Term Memory", "value": "author_of"}, {"source": "J. Schmidhuber", "target": "Long Short-Term Memory", "value": "author_of"}, {"source": "Nitish Srivastava", "target": "Dropout: a simple way to prevent neural networks from overfitting", "value": "author_of"}, {"source": "Geoffrey E. Hinton", "target": "Dropout: a simple way to prevent neural networks from overfitting", "value": "author_of"}, {"source": "A. Krizhevsky", "target": "Dropout: a simple way to prevent neural networks from overfitting", "value": "author_of"}, {"source": "I. Sutskever", "target": "Dropout: a simple way to prevent neural networks from overfitting", "value": "author_of"}, {"source": "R. Salakhutdinov", "target": "Dropout: a simple way to prevent neural networks from overfitting", "value": "author_of"}, {"source": "Christian Szegedy", "target": "Rethinking the Inception Architecture for Computer Vision", "value": "author_of"}, {"source": "Vincent Vanhoucke", "target": "Rethinking the Inception Architecture for Computer Vision", "value": "author_of"}, {"source": "Sergey Ioffe", "target": "Rethinking the Inception Architecture for Computer Vision", "value": "author_of"}, {"source": "Jonathon Shlens", "target": "Rethinking the Inception Architecture for Computer Vision", "value": "author_of"}, {"source": "Zbigniew Wojna", "target": "Rethinking the Inception Architecture for Computer Vision", "value": "author_of"}, {"source": "Shaina Raza", "target": "A comprehensive review of recommender systems: Transitioning from theory to practice", "value": "author_of"}, {"source": "Mizanur Rahman", "target": "A comprehensive review of recommender systems: Transitioning from theory to practice", "value": "author_of"}, {"source": "Safiullah Kamawal", "target": "A comprehensive review of recommender systems: Transitioning from theory to practice", "value": "author_of"}, {"source": "Armin Toroghi", "target": "A comprehensive review of recommender systems: Transitioning from theory to practice", "value": "author_of"}, {"source": "Ananya Raval", "target": "A comprehensive review of recommender systems: Transitioning from theory to practice", "value": "author_of"}, {"source": "F. Navah", "target": "A comprehensive review of recommender systems: Transitioning from theory to practice", "value": "author_of"}, {"source": "Amirmohammad Kazemeini", "target": "A comprehensive review of recommender systems: Transitioning from theory to practice", "value": "author_of"}, {"source": "Zhuoran Yang", "target": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation", "value": "author_of"}, {"source": "Xi Guo", "target": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation", "value": "author_of"}, {"source": "Chenjing Ding", "target": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation", "value": "author_of"}, {"source": "Chiyu Wang", "target": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation", "value": "author_of"}, {"source": "Wei Wu", "target": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation", "value": "author_of"}, {"source": "Yanyong Zhang", "target": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation", "value": "author_of"}, {"source": "Zisheng Wang", "target": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection", "value": "author_of"}, {"source": "Junjie Chen", "target": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection", "value": "author_of"}, {"source": "Chisen Wang", "target": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection", "value": "author_of"}, {"source": "Cong Peng", "target": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection", "value": "author_of"}, {"source": "Jianping Xuan", "target": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection", "value": "author_of"}, {"source": "Tielin Shi", "target": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection", "value": "author_of"}, {"source": "Ming J. Zuo", "target": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection", "value": "author_of"}, {"source": "Jinghuan Zhang", "target": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation", "value": "author_of"}, {"source": "Wang Chen", "target": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation", "value": "author_of"}, {"source": "Jian Zhang", "target": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation", "value": "author_of"}, {"source": "Manlin Zhang", "target": "DiffusionEngine: Diffusion model is scalable data engine for object detection", "value": "author_of"}, {"source": "Jie Wu", "target": "DiffusionEngine: Diffusion model is scalable data engine for object detection", "value": "author_of"}, {"source": "Yuxi Ren", "target": "DiffusionEngine: Diffusion model is scalable data engine for object detection", "value": "author_of"}, {"source": "Jiahong Yang", "target": "DiffusionEngine: Diffusion model is scalable data engine for object detection", "value": "author_of"}, {"source": "Ming Li", "target": "DiffusionEngine: Diffusion model is scalable data engine for object detection", "value": "author_of"}, {"source": "Andy J. Ma", "target": "DiffusionEngine: Diffusion model is scalable data engine for object detection", "value": "author_of"}, {"source": "A. Krizhevsky", "target": "ImageNet classification with deep convolutional neural networks", "value": "author_of"}, {"source": "I. Sutskever", "target": "ImageNet classification with deep convolutional neural networks", "value": "author_of"}, {"source": "Geoffrey E. Hinton", "target": "ImageNet classification with deep convolutional neural networks", "value": "author_of"}, {"source": "Karen Simonyan", "target": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "value": "author_of"}, {"source": "Andrew Zisserman", "target": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "value": "author_of"}, {"source": "P. Cochat", "target": "Et al", "value": "author_of"}, {"source": "L. Vaucoret", "target": "Et al", "value": "author_of"}, {"source": "J. Sarles", "target": "Et al", "value": "author_of"}, {"source": "Shaoqing Ren", "target": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "value": "author_of"}, {"source": "Kaiming He", "target": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "value": "author_of"}, {"source": "Ross Girshick", "target": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "value": "author_of"}, {"source": "Jian Sun", "target": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "value": "author_of"}, {"source": "Yuqi Cheng", "target": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects", "value": "author_of"}, {"source": "Yunkang Cao", "target": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects", "value": "author_of"}, {"source": "Haiming Yao", "target": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects", "value": "author_of"}, {"source": "Wei Luo", "target": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects", "value": "author_of"}, {"source": "Cheng Jiang", "target": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects", "value": "author_of"}, {"source": "Hui Zhang", "target": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects", "value": "author_of"}, {"source": "Weiming Shen", "target": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects", "value": "author_of"}, {"source": "Naveen Kumar Srinivasa", "target": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning", "value": "author_of"}, {"source": "Ajeet Rao Chalamala", "target": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning", "value": "author_of"}, {"source": "Kumar Singh", "target": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning", "value": "author_of"}, {"source": "Ieee Krishna Mohan Senior Member", "target": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning", "value": "author_of"}, {"source": "K. Naveen", "target": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning", "value": "author_of"}, {"source": "Srinivasa Rao", "target": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning", "value": "author_of"}, {"source": "Ajeet Kumar Singh", "target": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning", "value": "author_of"}, {"source": "Hongbo Jiang", "target": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID", "value": "author_of"}, {"source": "Lei Ye", "target": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID", "value": "author_of"}, {"source": "Jingyang Hu", "target": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID", "value": "author_of"}, {"source": "Xiaotian Chen", "target": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID", "value": "author_of"}, {"source": "Siyu Chen", "target": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID", "value": "author_of"}, {"source": "Wei Zhang", "target": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID", "value": "author_of"}, {"source": "Kehua Yang", "target": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID", "value": "author_of"}, {"source": "Jingya Wang", "target": "Noise Perception Self-Supervised Learning for Unsupervised Person Re-Identification", "value": "author_of"}, {"source": "Jianfeng Wen", "target": "Noise Perception Self-Supervised Learning for Unsupervised Person Re-Identification", "value": "author_of"}, {"source": "Weiping Ding", "target": "Noise Perception Self-Supervised Learning for Unsupervised Person Re-Identification", "value": "author_of"}, {"source": "Chunlin Yu", "target": "Noise Perception Self-Supervised Learning for Unsupervised Person Re-Identification", "value": "author_of"}, {"source": "Xiatian Zhu", "target": "Noise Perception Self-Supervised Learning for Unsupervised Person Re-Identification", "value": "author_of"}, {"source": "Zhiyong Wang", "target": "Noise Perception Self-Supervised Learning for Unsupervised Person Re-Identification", "value": "author_of"}, {"source": "Diederik P Kingma", "target": "Auto-Encoding Variational Bayes", "value": "author_of"}, {"source": "Max Welling", "target": "Auto-Encoding Variational Bayes", "value": "author_of"}, {"source": "Geoffrey E. Hinton", "target": "Supporting Online Material for Reducing the Dimensionality of Data with Neural Networks", "value": "author_of"}, {"source": "R. Salakhutdinov", "target": "Supporting Online Material for Reducing the Dimensionality of Data with Neural Networks", "value": "author_of"}, {"source": "John C. Duchi", "target": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "value": "author_of"}, {"source": "Elad Hazan", "target": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "value": "author_of"}, {"source": "Y. Singer", "target": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "value": "author_of"}, {"source": "Alex Graves", "target": "Speech recognition with deep recurrent neural networks", "value": "author_of"}, {"source": "Abdel-rahman Mohamed", "target": "Speech recognition with deep recurrent neural networks", "value": "author_of"}, {"source": "Geoffrey Hinton", "target": "Speech recognition with deep recurrent neural networks", "value": "author_of"}, {"source": "Nian Wang", "target": "Weakly Supervised Image Dehazing via Physics-Based Decomposition", "value": "author_of"}, {"source": "Zhigao Cui", "target": "Weakly Supervised Image Dehazing via Physics-Based Decomposition", "value": "author_of"}, {"source": "Yanzhao Su", "target": "Weakly Supervised Image Dehazing via Physics-Based Decomposition", "value": "author_of"}, {"source": "Yunwei Lan", "target": "Weakly Supervised Image Dehazing via Physics-Based Decomposition", "value": "author_of"}, {"source": "Yuanliang Xue", "target": "Weakly Supervised Image Dehazing via Physics-Based Decomposition", "value": "author_of"}, {"source": "Cong Zhang", "target": "Weakly Supervised Image Dehazing via Physics-Based Decomposition", "value": "author_of"}, {"source": "Aihua Li", "target": "Weakly Supervised Image Dehazing via Physics-Based Decomposition", "value": "author_of"}, {"source": "Leong Kah Meng", "target": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer", "value": "author_of"}, {"source": "Ho Hooi Yi", "target": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer", "value": "author_of"}, {"source": "Ng Bo Wei", "target": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer", "value": "author_of"}, {"source": "Lim Jia Xin", "target": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer", "value": "author_of"}, {"source": "Zailan Arabee Abdul Salam", "target": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer", "value": "author_of"}, {"source": "Xin Cheng", "target": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models", "value": "author_of"}, {"source": "Wangding Zeng", "target": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models", "value": "author_of"}, {"source": "Damai Dai", "target": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models", "value": "author_of"}, {"source": "Qinyu Chen", "target": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models", "value": "author_of"}, {"source": "Bingxuan Wang", "target": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models", "value": "author_of"}, {"source": "Zhenda Xie", "target": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models", "value": "author_of"}, {"source": "Kezhao Huang", "target": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models", "value": "author_of"}, {"source": "Xingkai Yu", "target": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models", "value": "author_of"}, {"source": "Zhewen Hao", "target": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models", "value": "author_of"}, {"source": "Yukun Li", "target": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models", "value": "author_of"}, {"source": "Han Zhang", "target": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models", "value": "author_of"}, {"source": "Huishuai Zhang", "target": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models", "value": "author_of"}, {"source": "Dongyan Zhao", "target": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models", "value": "author_of"}, {"source": "Wenfeng Liang", "target": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models", "value": "author_of"}, {"source": "Mostafa Saberian", "target": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction", "value": "author_of"}, {"source": "Vidya Samadi", "target": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction", "value": "author_of"}, {"source": "Ioana Popescu", "target": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction", "value": "author_of"}, {"source": "Husheng Fang", "target": "Generating an annual 30\u00a0m rice cover product for monsoon Asia (2018\u20132023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model", "value": "author_of"}, {"source": "Shunlin Liang", "target": "Generating an annual 30\u00a0m rice cover product for monsoon Asia (2018\u20132023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model", "value": "author_of"}, {"source": "Wenyuan Li", "target": "Generating an annual 30\u00a0m rice cover product for monsoon Asia (2018\u20132023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model", "value": "author_of"}, {"source": "Yongzhe Chen", "target": "Generating an annual 30\u00a0m rice cover product for monsoon Asia (2018\u20132023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model", "value": "author_of"}, {"source": "Han Ma", "target": "Generating an annual 30\u00a0m rice cover product for monsoon Asia (2018\u20132023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model", "value": "author_of"}, {"source": "Jianglei Xu", "target": "Generating an annual 30\u00a0m rice cover product for monsoon Asia (2018\u20132023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model", "value": "author_of"}, {"source": "Yichuan Ma", "target": "Generating an annual 30\u00a0m rice cover product for monsoon Asia (2018\u20132023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model", "value": "author_of"}, {"source": "Tao He", "target": "Generating an annual 30\u00a0m rice cover product for monsoon Asia (2018\u20132023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model", "value": "author_of"}, {"source": "Feng Tian", "target": "Generating an annual 30\u00a0m rice cover product for monsoon Asia (2018\u20132023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model", "value": "author_of"}, {"source": "Fengjiao Zhang", "target": "Generating an annual 30\u00a0m rice cover product for monsoon Asia (2018\u20132023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model", "value": "author_of"}, {"source": "Hui Liang", "target": "Generating an annual 30\u00a0m rice cover product for monsoon Asia (2018\u20132023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model", "value": "author_of"}, {"source": "Christian Szegedy", "target": "Going deeper with convolutions", "value": "author_of"}, {"source": "Wei Liu", "target": "Going deeper with convolutions", "value": "author_of"}, {"source": "Yangqing Jia", "target": "Going deeper with convolutions", "value": "author_of"}, {"source": "Pierre Sermanet", "target": "Going deeper with convolutions", "value": "author_of"}, {"source": "Scott Reed", "target": "Going deeper with convolutions", "value": "author_of"}, {"source": "Dragomir Anguelov", "target": "Going deeper with convolutions", "value": "author_of"}, {"source": "Dumitru Erhan", "target": "Going deeper with convolutions", "value": "author_of"}, {"source": "Vincent Vanhoucke", "target": "Going deeper with convolutions", "value": "author_of"}, {"source": "Andrew Rabinovich", "target": "Going deeper with convolutions", "value": "author_of"}, {"source": "Sergey Ioffe", "target": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "value": "author_of"}, {"source": "Christian Szegedy", "target": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "value": "author_of"}, {"source": "Olga Russakovsky", "target": "ImageNet Large Scale Visual Recognition Challenge", "value": "author_of"}, {"source": "Jia Deng", "target": "ImageNet Large Scale Visual Recognition Challenge", "value": "author_of"}, {"source": "Hao Su", "target": "ImageNet Large Scale Visual Recognition Challenge", "value": "author_of"}, {"source": "Jonathan Krause", "target": "ImageNet Large Scale Visual Recognition Challenge", "value": "author_of"}, {"source": "Sanjeev Satheesh", "target": "ImageNet Large Scale Visual Recognition Challenge", "value": "author_of"}, {"source": "Sean Ma", "target": "ImageNet Large Scale Visual Recognition Challenge", "value": "author_of"}, {"source": "Zhiheng Huang", "target": "ImageNet Large Scale Visual Recognition Challenge", "value": "author_of"}, {"source": "Andrej Karpathy", "target": "ImageNet Large Scale Visual Recognition Challenge", "value": "author_of"}, {"source": "Aditya Khosla", "target": "ImageNet Large Scale Visual Recognition Challenge", "value": "author_of"}, {"source": "Michael Bernstein", "target": "ImageNet Large Scale Visual Recognition Challenge", "value": "author_of"}, {"source": "Alexander C. Berg", "target": "ImageNet Large Scale Visual Recognition Challenge", "value": "author_of"}, {"source": "Li Fei-Fei", "target": "ImageNet Large Scale Visual Recognition Challenge", "value": "author_of"}, {"source": "Boyang Zheng", "target": "Diffusion Transformers with Representation Autoencoders", "value": "author_of"}, {"source": "Nanye Ma", "target": "Diffusion Transformers with Representation Autoencoders", "value": "author_of"}, {"source": "Shengbang Tong", "target": "Diffusion Transformers with Representation Autoencoders", "value": "author_of"}, {"source": "Saining Xie", "target": "Diffusion Transformers with Representation Autoencoders", "value": "author_of"}, {"source": "S. Rizvi", "target": "Scaling Large Language Models for Next-Generation Single-Cell Analysis", "value": "author_of"}, {"source": "Daniel Levine", "target": "Scaling Large Language Models for Next-Generation Single-Cell Analysis", "value": "author_of"}, {"source": "Aakash Patel", "target": "Scaling Large Language Models for Next-Generation Single-Cell Analysis", "value": "author_of"}, {"source": "Shiyang Zhang", "target": "Scaling Large Language Models for Next-Generation Single-Cell Analysis", "value": "author_of"}, {"source": "Eric Wang", "target": "Scaling Large Language Models for Next-Generation Single-Cell Analysis", "value": "author_of"}, {"source": "Curtis Jamison Perry", "target": "Scaling Large Language Models for Next-Generation Single-Cell Analysis", "value": "author_of"}, {"source": "Ivan Vrkic", "target": "Scaling Large Language Models for Next-Generation Single-Cell Analysis", "value": "author_of"}, {"source": "Nicole Mayerli Constante", "target": "Scaling Large Language Models for Next-Generation Single-Cell Analysis", "value": "author_of"}, {"source": "Zirui Fu", "target": "Scaling Large Language Models for Next-Generation Single-Cell Analysis", "value": "author_of"}, {"source": "Sizhuang He", "target": "Scaling Large Language Models for Next-Generation Single-Cell Analysis", "value": "author_of"}, {"source": "David Zhang", "target": "Scaling Large Language Models for Next-Generation Single-Cell Analysis", "value": "author_of"}, {"source": "Cerise Tang", "target": "Scaling Large Language Models for Next-Generation Single-Cell Analysis", "value": "author_of"}, {"source": "Zhuoyang Lyu", "target": "Scaling Large Language Models for Next-Generation Single-Cell Analysis", "value": "author_of"}, {"source": "Rayyan Y Darji", "target": "Scaling Large Language Models for Next-Generation Single-Cell Analysis", "value": "author_of"}, {"source": "Chang Li", "target": "Scaling Large Language Models for Next-Generation Single-Cell Analysis", "value": "author_of"}, {"source": "Emily Sun", "target": "Scaling Large Language Models for Next-Generation Single-Cell Analysis", "value": "author_of"}, {"source": "David Jeong", "target": "Scaling Large Language Models for Next-Generation Single-Cell Analysis", "value": "author_of"}, {"source": "Lawrence Zhao", "target": "Scaling Large Language Models for Next-Generation Single-Cell Analysis", "value": "author_of"}, {"source": "Jennifer Kwan", "target": "Scaling Large Language Models for Next-Generation Single-Cell Analysis", "value": "author_of"}, {"source": "David Braun", "target": "Scaling Large Language Models for Next-Generation Single-Cell Analysis", "value": "author_of"}, {"source": "Brian Hafler", "target": "Scaling Large Language Models for Next-Generation Single-Cell Analysis", "value": "author_of"}, {"source": "Hattie Chung", "target": "Scaling Large Language Models for Next-Generation Single-Cell Analysis", "value": "author_of"}, {"source": "R. M. Dhodapkar", "target": "Scaling Large Language Models for Next-Generation Single-Cell Analysis", "value": "author_of"}, {"source": "Paul F. Jaeger", "target": "Scaling Large Language Models for Next-Generation Single-Cell Analysis", "value": "author_of"}, {"source": "Bryan Perozzi", "target": "Scaling Large Language Models for Next-Generation Single-Cell Analysis", "value": "author_of"}, {"source": "Jeffrey Ishizuka", "target": "Scaling Large Language Models for Next-Generation Single-Cell Analysis", "value": "author_of"}, {"source": "Shekoofeh Azizi", "target": "Scaling Large Language Models for Next-Generation Single-Cell Analysis", "value": "author_of"}, {"source": "D. van Dijk", "target": "Scaling Large Language Models for Next-Generation Single-Cell Analysis", "value": "author_of"}, {"source": "Zhengyu Zhao", "target": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights", "value": "author_of"}, {"source": "Hanwei Zhang", "target": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights", "value": "author_of"}, {"source": "Renjue Li", "target": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights", "value": "author_of"}, {"source": "R. Sicre", "target": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights", "value": "author_of"}, {"source": "L. Amsaleg", "target": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights", "value": "author_of"}, {"source": "Michael Backes", "target": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights", "value": "author_of"}, {"source": "Qi Li", "target": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights", "value": "author_of"}, {"source": "Chao Shen", "target": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights", "value": "author_of"}, {"source": "\u015eafak K\u0131l\u0131\u00e7", "target": "HybridVisionNet: An advanced hybrid deep learning framework for automated multi-class ocular disease diagnosis using fundus imaging", "value": "author_of"}, {"source": "Yifei Ge", "target": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot", "value": "author_of"}, {"source": "Zhuo Li", "target": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot", "value": "author_of"}, {"source": "Xuebin Yue", "target": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot", "value": "author_of"}, {"source": "Hengyi Li", "target": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot", "value": "author_of"}, {"source": "Lin Meng", "target": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot", "value": "author_of"}, {"source": "Ashish Vaswani", "target": "Attention is All you Need", "value": "author_of"}, {"source": "Noam Shazeer", "target": "Attention is All you Need", "value": "author_of"}, {"source": "Niki Parmar", "target": "Attention is All you Need", "value": "author_of"}, {"source": "Jakob Uszkoreit", "target": "Attention is All you Need", "value": "author_of"}, {"source": "Llion Jones", "target": "Attention is All you Need", "value": "author_of"}, {"source": "Aidan N. Gomez", "target": "Attention is All you Need", "value": "author_of"}, {"source": "Lukasz Kaiser", "target": "Attention is All you Need", "value": "author_of"}, {"source": "Illia Polosukhin", "target": "Attention is All you Need", "value": "author_of"}, {"source": "Colin Raffel", "target": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "value": "author_of"}, {"source": "Noam Shazeer", "target": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "value": "author_of"}, {"source": "Adam Roberts", "target": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "value": "author_of"}, {"source": "Katherine Lee", "target": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "value": "author_of"}, {"source": "Sharan Narang", "target": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "value": "author_of"}, {"source": "Michael Matena", "target": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "value": "author_of"}, {"source": "Yanqi Zhou", "target": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "value": "author_of"}, {"source": "Wei Li", "target": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "value": "author_of"}, {"source": "Peter J. Liu", "target": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "value": "author_of"}, {"source": "M. Heusel", "target": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium", "value": "author_of"}, {"source": "Hubert Ramsauer", "target": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium", "value": "author_of"}, {"source": "Thomas Unterthiner", "target": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium", "value": "author_of"}, {"source": "Bernhard Nessler", "target": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium", "value": "author_of"}, {"source": "Sepp Hochreiter", "target": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium", "value": "author_of"}, {"source": "Holger Caesar", "target": "nuScenes: A Multimodal Dataset for Autonomous Driving", "value": "author_of"}, {"source": "Varun Bankiti", "target": "nuScenes: A Multimodal Dataset for Autonomous Driving", "value": "author_of"}, {"source": "Alex H. Lang", "target": "nuScenes: A Multimodal Dataset for Autonomous Driving", "value": "author_of"}, {"source": "Sourabh Vora", "target": "nuScenes: A Multimodal Dataset for Autonomous Driving", "value": "author_of"}, {"source": "Venice Erin Liong", "target": "nuScenes: A Multimodal Dataset for Autonomous Driving", "value": "author_of"}, {"source": "Qiang Xu", "target": "nuScenes: A Multimodal Dataset for Autonomous Driving", "value": "author_of"}, {"source": "Anush Krishnan", "target": "nuScenes: A Multimodal Dataset for Autonomous Driving", "value": "author_of"}, {"source": "Yu Pan", "target": "nuScenes: A Multimodal Dataset for Autonomous Driving", "value": "author_of"}, {"source": "Giancarlo Baldan", "target": "nuScenes: A Multimodal Dataset for Autonomous Driving", "value": "author_of"}, {"source": "Oscar Beijbom", "target": "nuScenes: A Multimodal Dataset for Autonomous Driving", "value": "author_of"}, {"source": "Alexey Dosovitskiy", "target": "CARLA: An Open Urban Driving Simulator", "value": "author_of"}, {"source": "German Ros", "target": "CARLA: An Open Urban Driving Simulator", "value": "author_of"}, {"source": "Felipe Codevilla", "target": "CARLA: An Open Urban Driving Simulator", "value": "author_of"}, {"source": "Antonio Lopez", "target": "CARLA: An Open Urban Driving Simulator", "value": "author_of"}, {"source": "Vladlen Koltun", "target": "CARLA: An Open Urban Driving Simulator", "value": "author_of"}, {"source": "Fachrina Dewi Puspitasari", "target": "Sora as a World Model? A Complete Survey on Text-to-Video Generation", "value": "author_of"}, {"source": "Chaoning Zhang", "target": "Sora as a World Model? A Complete Survey on Text-to-Video Generation", "value": "author_of"}, {"source": "Joseph Cho", "target": "Sora as a World Model? A Complete Survey on Text-to-Video Generation", "value": "author_of"}, {"source": "Adnan Haider", "target": "Sora as a World Model? A Complete Survey on Text-to-Video Generation", "value": "author_of"}, {"source": "Noor Ul Eman", "target": "Sora as a World Model? A Complete Survey on Text-to-Video Generation", "value": "author_of"}, {"source": "Omer Amin", "target": "Sora as a World Model? A Complete Survey on Text-to-Video Generation", "value": "author_of"}, {"source": "Alexis Mankowski", "target": "Sora as a World Model? A Complete Survey on Text-to-Video Generation", "value": "author_of"}, {"source": "Muhammad Umair", "target": "Sora as a World Model? A Complete Survey on Text-to-Video Generation", "value": "author_of"}, {"source": "Jingyao Zheng", "target": "Sora as a World Model? A Complete Survey on Text-to-Video Generation", "value": "author_of"}, {"source": "Sheng Zheng", "target": "Sora as a World Model? A Complete Survey on Text-to-Video Generation", "value": "author_of"}, {"source": "Lik-Hang Lee", "target": "Sora as a World Model? A Complete Survey on Text-to-Video Generation", "value": "author_of"}, {"source": "Caiyan Qin", "target": "Sora as a World Model? A Complete Survey on Text-to-Video Generation", "value": "author_of"}, {"source": "Tae-Ho Kim", "target": "Sora as a World Model? A Complete Survey on Text-to-Video Generation", "value": "author_of"}, {"source": "Choong Seon Hong", "target": "Sora as a World Model? A Complete Survey on Text-to-Video Generation", "value": "author_of"}, {"source": "Yang Yang", "target": "Sora as a World Model? A Complete Survey on Text-to-Video Generation", "value": "author_of"}, {"source": "Heng Tao Shen", "target": "Sora as a World Model? A Complete Survey on Text-to-Video Generation", "value": "author_of"}, {"source": "Bohan Li", "target": "OmniNWM: Omniscient Driving Navigation World Models", "value": "author_of"}, {"source": "Zhuang Ma", "target": "OmniNWM: Omniscient Driving Navigation World Models", "value": "author_of"}, {"source": "Dalong Du", "target": "OmniNWM: Omniscient Driving Navigation World Models", "value": "author_of"}, {"source": "Baorui Peng", "target": "OmniNWM: Omniscient Driving Navigation World Models", "value": "author_of"}, {"source": "Zhujin Liang", "target": "OmniNWM: Omniscient Driving Navigation World Models", "value": "author_of"}, {"source": "Zhenqiang Liu", "target": "OmniNWM: Omniscient Driving Navigation World Models", "value": "author_of"}, {"source": "Chao Ma", "target": "OmniNWM: Omniscient Driving Navigation World Models", "value": "author_of"}, {"source": "Yueming Jin", "target": "OmniNWM: Omniscient Driving Navigation World Models", "value": "author_of"}, {"source": "Hao Zhao", "target": "OmniNWM: Omniscient Driving Navigation World Models", "value": "author_of"}, {"source": "Wenjun Zeng", "target": "OmniNWM: Omniscient Driving Navigation World Models", "value": "author_of"}, {"source": "Xin Jin", "target": "OmniNWM: Omniscient Driving Navigation World Models", "value": "author_of"}, {"source": "Zhuoran Yang", "target": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask", "value": "author_of"}, {"source": "Yanyong Zhang", "target": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask", "value": "author_of"}, {"source": "Guosheng Zhao", "target": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving", "value": "author_of"}, {"source": "Yaozeng Wang", "target": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving", "value": "author_of"}, {"source": "Xiaofeng Wang", "target": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving", "value": "author_of"}, {"source": "Zheng Zhu", "target": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving", "value": "author_of"}, {"source": "Tingdong Yu", "target": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving", "value": "author_of"}, {"source": "Guan Huang", "target": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving", "value": "author_of"}, {"source": "Yongchen Zai", "target": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving", "value": "author_of"}, {"source": "Ji Jiao", "target": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving", "value": "author_of"}, {"source": "Changliang Xue", "target": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving", "value": "author_of"}, {"source": "Xiaole Wang", "target": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving", "value": "author_of"}, {"source": "Zhen Yang", "target": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving", "value": "author_of"}, {"source": "Futang Zhu", "target": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving", "value": "author_of"}, {"source": "Xingang Wang", "target": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving", "value": "author_of"}, {"source": "Ahmad Rahimi", "target": "MAD: Motion Appearance Decoupling for efficient Driving World Models", "value": "author_of"}, {"source": "Valentin Gerard", "target": "MAD: Motion Appearance Decoupling for efficient Driving World Models", "value": "author_of"}, {"source": "Eloi Zablocki", "target": "MAD: Motion Appearance Decoupling for efficient Driving World Models", "value": "author_of"}, {"source": "Matthieu Cord", "target": "MAD: Motion Appearance Decoupling for efficient Driving World Models", "value": "author_of"}, {"source": "Alexandre Alahi", "target": "MAD: Motion Appearance Decoupling for efficient Driving World Models", "value": "author_of"}, {"source": "W. Marsden", "target": "I and J", "value": "author_of"}, {"source": "Jia Deng", "target": "ImageNet: A large-scale hierarchical image database", "value": "author_of"}, {"source": "Wei Dong", "target": "ImageNet: A large-scale hierarchical image database", "value": "author_of"}, {"source": "R. Socher", "target": "ImageNet: A large-scale hierarchical image database", "value": "author_of"}, {"source": "Li-Jia Li", "target": "ImageNet: A large-scale hierarchical image database", "value": "author_of"}, {"source": "K. Li", "target": "ImageNet: A large-scale hierarchical image database", "value": "author_of"}, {"source": "Li Fei-Fei", "target": "ImageNet: A large-scale hierarchical image database", "value": "author_of"}, {"source": "R. Stephenson", "target": "A and V", "value": "author_of"}, {"source": "Jaskirat Singh", "target": "What matters for Representation Alignment: Global Information or Spatial Structure?", "value": "author_of"}, {"source": "Xingjian Leng", "target": "What matters for Representation Alignment: Global Information or Spatial Structure?", "value": "author_of"}, {"source": "Zongze Wu", "target": "What matters for Representation Alignment: Global Information or Spatial Structure?", "value": "author_of"}, {"source": "Liang Zheng", "target": "What matters for Representation Alignment: Global Information or Spatial Structure?", "value": "author_of"}, {"source": "Richard Zhang", "target": "What matters for Representation Alignment: Global Information or Spatial Structure?", "value": "author_of"}, {"source": "Eli Shechtman", "target": "What matters for Representation Alignment: Global Information or Spatial Structure?", "value": "author_of"}, {"source": "Saining Xie", "target": "What matters for Representation Alignment: Global Information or Spatial Structure?", "value": "author_of"}, {"source": "Zhifeng Wang", "target": "SCB-DETR: Multiscale Deformable Transformers for Occlusion-Resilient Student Learning Behavior Detection in Smart Classroom", "value": "author_of"}, {"source": "Minghui Wang", "target": "SCB-DETR: Multiscale Deformable Transformers for Occlusion-Resilient Student Learning Behavior Detection in Smart Classroom", "value": "author_of"}, {"source": "Chunyan Zeng", "target": "SCB-DETR: Multiscale Deformable Transformers for Occlusion-Resilient Student Learning Behavior Detection in Smart Classroom", "value": "author_of"}, {"source": "Longlong Li", "target": "SCB-DETR: Multiscale Deformable Transformers for Occlusion-Resilient Student Learning Behavior Detection in Smart Classroom", "value": "author_of"}, {"source": "Ehsan Zakeri", "target": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination", "value": "author_of"}, {"source": "Amanda Spilkin", "target": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination", "value": "author_of"}, {"source": "Hanae Elmekki", "target": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination", "value": "author_of"}, {"source": "Antonela Zanuttini", "target": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination", "value": "author_of"}, {"source": "L. Kadem", "target": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination", "value": "author_of"}, {"source": "Jamal Bentahar", "target": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination", "value": "author_of"}, {"source": "Wen-Fang Xie", "target": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination", "value": "author_of"}, {"source": "Philippe Pibarot", "target": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination", "value": "author_of"}, {"source": "Ana Davila", "target": "Bio-Inspired Fine-Tuning for Selective Transfer Learning in Image Classification", "value": "author_of"}, {"source": "Jacinto Colan", "target": "Bio-Inspired Fine-Tuning for Selective Transfer Learning in Image Classification", "value": "author_of"}, {"source": "Yasuhisa Hasegawa", "target": "Bio-Inspired Fine-Tuning for Selective Transfer Learning in Image Classification", "value": "author_of"}, {"source": "Subham Sharma", "target": "Hand Sign Language Detection Using Deep Learning", "value": "author_of"}, {"source": "Sharmila Subudhi", "target": "Hand Sign Language Detection Using Deep Learning", "value": "author_of"}, {"source": "Tsung-Yi Lin", "target": "Microsoft COCO: Common Objects in Context", "value": "author_of"}, {"source": "Michael Maire", "target": "Microsoft COCO: Common Objects in Context", "value": "author_of"}, {"source": "Serge Belongie", "target": "Microsoft COCO: Common Objects in Context", "value": "author_of"}, {"source": "Lubomir Bourdev", "target": "Microsoft COCO: Common Objects in Context", "value": "author_of"}, {"source": "Ross Girshick", "target": "Microsoft COCO: Common Objects in Context", "value": "author_of"}, {"source": "James Hays", "target": "Microsoft COCO: Common Objects in Context", "value": "author_of"}, {"source": "Pietro Perona", "target": "Microsoft COCO: Common Objects in Context", "value": "author_of"}, {"source": "Deva Ramanan", "target": "Microsoft COCO: Common Objects in Context", "value": "author_of"}, {"source": "C. Lawrence Zitnick", "target": "Microsoft COCO: Common Objects in Context", "value": "author_of"}, {"source": "Piotr Doll\u00e1r", "target": "Microsoft COCO: Common Objects in Context", "value": "author_of"}, {"source": "Zixiao Wen", "target": "FANet: Frequency-Aware Attention-Based Tiny-Object Detection in Remote Sensing Images", "value": "author_of"}, {"source": "Peifeng Li", "target": "FANet: Frequency-Aware Attention-Based Tiny-Object Detection in Remote Sensing Images", "value": "author_of"}, {"source": "Yuhan Liu", "target": "FANet: Frequency-Aware Attention-Based Tiny-Object Detection in Remote Sensing Images", "value": "author_of"}, {"source": "Jingming Chen", "target": "FANet: Frequency-Aware Attention-Based Tiny-Object Detection in Remote Sensing Images", "value": "author_of"}, {"source": "Xiantai Xiang", "target": "FANet: Frequency-Aware Attention-Based Tiny-Object Detection in Remote Sensing Images", "value": "author_of"}, {"source": "Yuan Li", "target": "FANet: Frequency-Aware Attention-Based Tiny-Object Detection in Remote Sensing Images", "value": "author_of"}, {"source": "Huixian Wang", "target": "FANet: Frequency-Aware Attention-Based Tiny-Object Detection in Remote Sensing Images", "value": "author_of"}, {"source": "Yongchao Zhao", "target": "FANet: Frequency-Aware Attention-Based Tiny-Object Detection in Remote Sensing Images", "value": "author_of"}, {"source": "Guangyao Zhou", "target": "FANet: Frequency-Aware Attention-Based Tiny-Object Detection in Remote Sensing Images", "value": "author_of"}, {"source": "Yu Gu", "target": "UAV-based multimodal object detection via feature enhancement and dynamic gated fusion", "value": "author_of"}, {"source": "Weili Chen", "target": "UAV-based multimodal object detection via feature enhancement and dynamic gated fusion", "value": "author_of"}, {"source": "Dongliang Peng", "target": "UAV-based multimodal object detection via feature enhancement and dynamic gated fusion", "value": "author_of"}, {"source": "Xiaohui Yuan", "target": "An empirical analysis of deep learning methods for small object detection from satellite imagery", "value": "author_of"}, {"source": "Aniv Chakravarty", "target": "An empirical analysis of deep learning methods for small object detection from satellite imagery", "value": "author_of"}, {"source": "Elinor M. Lichtenberg", "target": "An empirical analysis of deep learning methods for small object detection from satellite imagery", "value": "author_of"}, {"source": "Lichuan Gu", "target": "An empirical analysis of deep learning methods for small object detection from satellite imagery", "value": "author_of"}, {"source": "Zhenchun Wei", "target": "An empirical analysis of deep learning methods for small object detection from satellite imagery", "value": "author_of"}, {"source": "Tian Chen", "target": "An empirical analysis of deep learning methods for small object detection from satellite imagery", "value": "author_of"}, {"source": "Yoshua Bengio", "target": "Representation Learning: A Review and New Perspectives", "value": "author_of"}, {"source": "Aaron Courville", "target": "Representation Learning: A Review and New Perspectives", "value": "author_of"}, {"source": "Pascal Vincent", "target": "Representation Learning: A Review and New Perspectives", "value": "author_of"}, {"source": "Pascal Vincent", "target": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion", "value": "author_of"}, {"source": "H. Larochelle", "target": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion", "value": "author_of"}, {"source": "Isabelle Lajoie", "target": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion", "value": "author_of"}, {"source": "Yoshua Bengio", "target": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion", "value": "author_of"}, {"source": "Pierre-Antoine Manzagol", "target": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion", "value": "author_of"}, {"source": "D. Touretzky", "target": "In Advances in Neural Information Processing Systems", "value": "author_of"}, {"source": "M. C. Mozer", "target": "In Advances in Neural Information Processing Systems", "value": "author_of"}, {"source": "M. E. Hasselmo", "target": "In Advances in Neural Information Processing Systems", "value": "author_of"}, {"source": "RegressionChristopher", "target": "In Advances in Neural Information Processing Systems", "value": "author_of"}, {"source": "I. K.", "target": "In Advances in Neural Information Processing Systems", "value": "author_of"}, {"source": "WilliamsNeural", "target": "In Advances in Neural Information Processing Systems", "value": "author_of"}, {"source": "GroupAston", "target": "In Advances in Neural Information Processing Systems", "value": "author_of"}, {"source": "UniversityBirmingham", "target": "In Advances in Neural Information Processing Systems", "value": "author_of"}, {"source": "Danilo Jimenez Rezende", "target": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models", "value": "author_of"}, {"source": "S. Mohamed", "target": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models", "value": "author_of"}, {"source": "Daan Wierstra", "target": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models", "value": "author_of"}, {"source": "Shanchuan Lin", "target": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation", "value": "author_of"}, {"source": "Anran Wang", "target": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation", "value": "author_of"}, {"source": "Xiao Yang", "target": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation", "value": "author_of"}, {"source": "Zhiyuan Chen", "target": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions", "value": "author_of"}, {"source": "Jiajiong Cao", "target": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions", "value": "author_of"}, {"source": "Zhiquan Chen", "target": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions", "value": "author_of"}, {"source": "Yuming Li", "target": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions", "value": "author_of"}, {"source": "Chenguang Ma", "target": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions", "value": "author_of"}, {"source": "Wenzhao Zheng", "target": "GenAD: Generative End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Ruiqi Song", "target": "GenAD: Generative End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Xianda Guo", "target": "GenAD: Generative End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Chenming Zhang", "target": "GenAD: Generative End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Long Chen", "target": "GenAD: Generative End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Shiyin Lu", "target": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model", "value": "author_of"}, {"source": "Yang Li", "target": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model", "value": "author_of"}, {"source": "Qing-Guo Chen", "target": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model", "value": "author_of"}, {"source": "Zhao Xu", "target": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model", "value": "author_of"}, {"source": "Weihua Luo", "target": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model", "value": "author_of"}, {"source": "Kaifu Zhang", "target": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model", "value": "author_of"}, {"source": "Han-Jia Ye", "target": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model", "value": "author_of"}, {"source": "Jie Liu", "target": "Improving Video Generation with Human Feedback", "value": "author_of"}, {"source": "Gongye Liu", "target": "Improving Video Generation with Human Feedback", "value": "author_of"}, {"source": "Jiajun Liang", "target": "Improving Video Generation with Human Feedback", "value": "author_of"}, {"source": "Ziyang Yuan", "target": "Improving Video Generation with Human Feedback", "value": "author_of"}, {"source": "Xiaokun Liu", "target": "Improving Video Generation with Human Feedback", "value": "author_of"}, {"source": "Mingwu Zheng", "target": "Improving Video Generation with Human Feedback", "value": "author_of"}, {"source": "Xiele Wu", "target": "Improving Video Generation with Human Feedback", "value": "author_of"}, {"source": "Qiulin Wang", "target": "Improving Video Generation with Human Feedback", "value": "author_of"}, {"source": "Menghan Xia", "target": "Improving Video Generation with Human Feedback", "value": "author_of"}, {"source": "Xintao Wang", "target": "Improving Video Generation with Human Feedback", "value": "author_of"}, {"source": "Xiaohong Liu", "target": "Improving Video Generation with Human Feedback", "value": "author_of"}, {"source": "Fei Yang", "target": "Improving Video Generation with Human Feedback", "value": "author_of"}, {"source": "Pengfei Wan", "target": "Improving Video Generation with Human Feedback", "value": "author_of"}, {"source": "Di Zhang", "target": "Improving Video Generation with Human Feedback", "value": "author_of"}, {"source": "Kun Gai", "target": "Improving Video Generation with Human Feedback", "value": "author_of"}, {"source": "Yujiu Yang", "target": "Improving Video Generation with Human Feedback", "value": "author_of"}, {"source": "Wanli Ouyang", "target": "Improving Video Generation with Human Feedback", "value": "author_of"}, {"source": "Alex Graves", "target": "Speech Recognition with Deep Recurrent Neural Networks", "value": "author_of"}, {"source": "Abdel-rahman Mohamed", "target": "Speech Recognition with Deep Recurrent Neural Networks", "value": "author_of"}, {"source": "Geoffrey Hinton", "target": "Speech Recognition with Deep Recurrent Neural Networks", "value": "author_of"}, {"source": "D. Rumelhart", "target": "Learning representations by back-propagating errors", "value": "author_of"}, {"source": "Geoffrey E. Hinton", "target": "Learning representations by back-propagating errors", "value": "author_of"}, {"source": "Ronald J. Williams", "target": "Learning representations by back-propagating errors", "value": "author_of"}, {"source": "M. Schuster", "target": "Bidirectional recurrent neural networks", "value": "author_of"}, {"source": "K. Paliwal", "target": "Bidirectional recurrent neural networks", "value": "author_of"}, {"source": "Alex Graves", "target": "Connectionist Temporal Classi\ufb01cation: Labelling Unsegmented Sequence Data with Recurrent Neural Networks", "value": "author_of"}, {"source": "Santiago Fern\u00b4andez", "target": "Connectionist Temporal Classi\ufb01cation: Labelling Unsegmented Sequence Data with Recurrent Neural Networks", "value": "author_of"}, {"source": "Faustino J. Gomez", "target": "Connectionist Temporal Classi\ufb01cation: Labelling Unsegmented Sequence Data with Recurrent Neural Networks", "value": "author_of"}, {"source": "J\u00a8urgen Schmidhuber", "target": "Connectionist Temporal Classi\ufb01cation: Labelling Unsegmented Sequence Data with Recurrent Neural Networks", "value": "author_of"}, {"source": "Alex Graves", "target": "2005 Special Issue: Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "value": "author_of"}, {"source": "J. Schmidhuber", "target": "2005 Special Issue: Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "value": "author_of"}, {"source": "Sean L. Metzger", "target": "A high-performance neuroprosthesis for speech decoding and avatar control", "value": "author_of"}, {"source": "K. T. Littlejohn", "target": "A high-performance neuroprosthesis for speech decoding and avatar control", "value": "author_of"}, {"source": "Alexander B. Silva", "target": "A high-performance neuroprosthesis for speech decoding and avatar control", "value": "author_of"}, {"source": "D. Moses", "target": "A high-performance neuroprosthesis for speech decoding and avatar control", "value": "author_of"}, {"source": "Margaret P. Seaton", "target": "A high-performance neuroprosthesis for speech decoding and avatar control", "value": "author_of"}, {"source": "Ran Wang", "target": "A high-performance neuroprosthesis for speech decoding and avatar control", "value": "author_of"}, {"source": "Maximilian E. Dougherty", "target": "A high-performance neuroprosthesis for speech decoding and avatar control", "value": "author_of"}, {"source": "Jessie R. Liu", "target": "A high-performance neuroprosthesis for speech decoding and avatar control", "value": "author_of"}, {"source": "Peter Wu", "target": "A high-performance neuroprosthesis for speech decoding and avatar control", "value": "author_of"}, {"source": "M. Berger", "target": "A high-performance neuroprosthesis for speech decoding and avatar control", "value": "author_of"}, {"source": "Inga Zhuravleva", "target": "A high-performance neuroprosthesis for speech decoding and avatar control", "value": "author_of"}, {"source": "A. Tu-Chan", "target": "A high-performance neuroprosthesis for speech decoding and avatar control", "value": "author_of"}, {"source": "K. Ganguly", "target": "A high-performance neuroprosthesis for speech decoding and avatar control", "value": "author_of"}, {"source": "G. Anumanchipalli", "target": "A high-performance neuroprosthesis for speech decoding and avatar control", "value": "author_of"}, {"source": "Edward F. Chang", "target": "A high-performance neuroprosthesis for speech decoding and avatar control", "value": "author_of"}, {"source": "Tianming Sun", "target": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses", "value": "author_of"}, {"source": "Bin Feng", "target": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses", "value": "author_of"}, {"source": "Jinpeng Huo", "target": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses", "value": "author_of"}, {"source": "Yu Xiao", "target": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses", "value": "author_of"}, {"source": "Wengan Wang", "target": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses", "value": "author_of"}, {"source": "Jin Peng", "target": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses", "value": "author_of"}, {"source": "Zehua Li", "target": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses", "value": "author_of"}, {"source": "Chengjie Du", "target": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses", "value": "author_of"}, {"source": "Wenxian Wang", "target": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses", "value": "author_of"}, {"source": "G. Zou", "target": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses", "value": "author_of"}, {"source": "Lei Liu", "target": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses", "value": "author_of"}, {"source": "Francis R. Willett", "target": "A high-performance speech neuroprosthesis", "value": "author_of"}, {"source": "Erin M. Kunz", "target": "A high-performance speech neuroprosthesis", "value": "author_of"}, {"source": "Chaofei Fan", "target": "A high-performance speech neuroprosthesis", "value": "author_of"}, {"source": "Donald T. Avansino", "target": "A high-performance speech neuroprosthesis", "value": "author_of"}, {"source": "G. Wilson", "target": "A high-performance speech neuroprosthesis", "value": "author_of"}, {"source": "Eun Young Choi", "target": "A high-performance speech neuroprosthesis", "value": "author_of"}, {"source": "Foram B. Kamdar", "target": "A high-performance speech neuroprosthesis", "value": "author_of"}, {"source": "M. Glasser", "target": "A high-performance speech neuroprosthesis", "value": "author_of"}, {"source": "L. Hochberg", "target": "A high-performance speech neuroprosthesis", "value": "author_of"}, {"source": "S. Druckmann", "target": "A high-performance speech neuroprosthesis", "value": "author_of"}, {"source": "K. Shenoy", "target": "A high-performance speech neuroprosthesis", "value": "author_of"}, {"source": "J. Henderson", "target": "A high-performance speech neuroprosthesis", "value": "author_of"}, {"source": "Shibhansh Dohare", "target": "Loss of plasticity in deep continual learning", "value": "author_of"}, {"source": "J. F. Hernandez-Garcia", "target": "Loss of plasticity in deep continual learning", "value": "author_of"}, {"source": "Qingfeng Lan", "target": "Loss of plasticity in deep continual learning", "value": "author_of"}, {"source": "Parash Rahman", "target": "Loss of plasticity in deep continual learning", "value": "author_of"}, {"source": "A. Mahmood", "target": "Loss of plasticity in deep continual learning", "value": "author_of"}, {"source": "R. Sutton", "target": "Loss of plasticity in deep continual learning", "value": "author_of"}, {"source": "S. Ambrogio", "target": "An analog-AI chip for energy-efficient speech recognition and transcription", "value": "author_of"}, {"source": "P. Narayanan", "target": "An analog-AI chip for energy-efficient speech recognition and transcription", "value": "author_of"}, {"source": "A. Okazaki", "target": "An analog-AI chip for energy-efficient speech recognition and transcription", "value": "author_of"}, {"source": "A. Fasoli", "target": "An analog-AI chip for energy-efficient speech recognition and transcription", "value": "author_of"}, {"source": "C. Mackin", "target": "An analog-AI chip for energy-efficient speech recognition and transcription", "value": "author_of"}, {"source": "K. Hosokawa", "target": "An analog-AI chip for energy-efficient speech recognition and transcription", "value": "author_of"}, {"source": "A. Nomura", "target": "An analog-AI chip for energy-efficient speech recognition and transcription", "value": "author_of"}, {"source": "Takeo Yasuda", "target": "An analog-AI chip for energy-efficient speech recognition and transcription", "value": "author_of"}, {"source": "An Chen", "target": "An analog-AI chip for energy-efficient speech recognition and transcription", "value": "author_of"}, {"source": "A. Friz", "target": "An analog-AI chip for energy-efficient speech recognition and transcription", "value": "author_of"}, {"source": "M. Ishii", "target": "An analog-AI chip for energy-efficient speech recognition and transcription", "value": "author_of"}, {"source": "J. Luquin", "target": "An analog-AI chip for energy-efficient speech recognition and transcription", "value": "author_of"}, {"source": "Y. Kohda", "target": "An analog-AI chip for energy-efficient speech recognition and transcription", "value": "author_of"}, {"source": "N. Saulnier", "target": "An analog-AI chip for energy-efficient speech recognition and transcription", "value": "author_of"}, {"source": "K. Brew", "target": "An analog-AI chip for energy-efficient speech recognition and transcription", "value": "author_of"}, {"source": "Samuel Choi", "target": "An analog-AI chip for energy-efficient speech recognition and transcription", "value": "author_of"}, {"source": "I. Ok", "target": "An analog-AI chip for energy-efficient speech recognition and transcription", "value": "author_of"}, {"source": "Timothy Philip", "target": "An analog-AI chip for energy-efficient speech recognition and transcription", "value": "author_of"}, {"source": "Victor Chan", "target": "An analog-AI chip for energy-efficient speech recognition and transcription", "value": "author_of"}, {"source": "M. Silvestre", "target": "An analog-AI chip for energy-efficient speech recognition and transcription", "value": "author_of"}, {"source": "Ishtiaq Ahsan", "target": "An analog-AI chip for energy-efficient speech recognition and transcription", "value": "author_of"}, {"source": "Vijay Narayanan", "target": "An analog-AI chip for energy-efficient speech recognition and transcription", "value": "author_of"}, {"source": "H. Tsai", "target": "An analog-AI chip for energy-efficient speech recognition and transcription", "value": "author_of"}, {"source": "Geoffrey W. Burr", "target": "An analog-AI chip for energy-efficient speech recognition and transcription", "value": "author_of"}, {"source": "J. Shin", "target": "A Mathematical Theory of Communication", "value": "author_of"}, {"source": "Sang Joon Kim", "target": "A Mathematical Theory of Communication", "value": "author_of"}, {"source": "Yike Sun", "target": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers", "value": "author_of"}, {"source": "Haotong Yang", "target": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers", "value": "author_of"}, {"source": "Zhouchen Lin", "target": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers", "value": "author_of"}, {"source": "Muhan Zhang", "target": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers", "value": "author_of"}, {"source": "Ning Ding", "target": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling", "value": "author_of"}, {"source": "Fangcheng Liu", "target": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling", "value": "author_of"}, {"source": "Kyungrae Kim", "target": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling", "value": "author_of"}, {"source": "Linji Hao", "target": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling", "value": "author_of"}, {"source": "Kyeng-Hun Lee", "target": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling", "value": "author_of"}, {"source": "Hyeonmok Ko", "target": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling", "value": "author_of"}, {"source": "Yehui Tang", "target": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling", "value": "author_of"}, {"source": "Huinan Xu", "target": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram", "value": "author_of"}, {"source": "Xuyang Feng", "target": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram", "value": "author_of"}, {"source": "Junhong Chen", "target": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram", "value": "author_of"}, {"source": "Junchen Liu", "target": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram", "value": "author_of"}, {"source": "Kaiwen Deng", "target": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram", "value": "author_of"}, {"source": "Kai Ding", "target": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram", "value": "author_of"}, {"source": "Shengning Long", "target": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram", "value": "author_of"}, {"source": "Jiaxue Shuai", "target": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram", "value": "author_of"}, {"source": "Zhaorong Li", "target": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram", "value": "author_of"}, {"source": "Shiping Liu", "target": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram", "value": "author_of"}, {"source": "Guirong Xue", "target": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram", "value": "author_of"}, {"source": "Zhan Xiao", "target": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram", "value": "author_of"}, {"source": "Albert Tseng", "target": "L$^3$: Large Lookup Layers", "value": "author_of"}, {"source": "Christopher De Sa", "target": "L$^3$: Large Lookup Layers", "value": "author_of"}, {"source": "Hong Liu", "target": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "value": "author_of"}, {"source": "Jiaqi Zhang", "target": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "value": "author_of"}, {"source": "Chao Wang", "target": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "value": "author_of"}, {"source": "Xing Hu", "target": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "value": "author_of"}, {"source": "Linkun Lyu", "target": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "value": "author_of"}, {"source": "Jiaqi Sun", "target": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "value": "author_of"}, {"source": "Xurui Yang", "target": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "value": "author_of"}, {"source": "Bo Wang", "target": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "value": "author_of"}, {"source": "Fengcun Li", "target": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "value": "author_of"}, {"source": "Yulei Qian", "target": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "value": "author_of"}, {"source": "Lingtong Si", "target": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "value": "author_of"}, {"source": "Yerui Sun", "target": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "value": "author_of"}, {"source": "Rumei Li", "target": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "value": "author_of"}, {"source": "Peng Pei", "target": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "value": "author_of"}, {"source": "Yuchen Xie", "target": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "value": "author_of"}, {"source": "Xunliang Cai", "target": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "value": "author_of"}, {"source": "Christian Szegedy", "target": "Going Deeper with Convolutions", "value": "author_of"}, {"source": "Wei Liu", "target": "Going Deeper with Convolutions", "value": "author_of"}, {"source": "Yangqing Jia", "target": "Going Deeper with Convolutions", "value": "author_of"}, {"source": "Pierre Sermanet", "target": "Going Deeper with Convolutions", "value": "author_of"}, {"source": "Scott Reed", "target": "Going Deeper with Convolutions", "value": "author_of"}, {"source": "Dragomir Anguelov", "target": "Going Deeper with Convolutions", "value": "author_of"}, {"source": "Dumitru Erhan", "target": "Going Deeper with Convolutions", "value": "author_of"}, {"source": "Vincent Vanhoucke", "target": "Going Deeper with Convolutions", "value": "author_of"}, {"source": "Andrew Rabinovich", "target": "Going Deeper with Convolutions", "value": "author_of"}, {"source": "R. Tibshirani", "target": "Regression Shrinkage and Selection via the Lasso", "value": "author_of"}, {"source": "Herve Goeau", "target": "LifeCLEF Plant Identification Task 2015", "value": "author_of"}, {"source": "Pierre Bonnet", "target": "LifeCLEF Plant Identification Task 2015", "value": "author_of"}, {"source": "Alexis Joly", "target": "LifeCLEF Plant Identification Task 2015", "value": "author_of"}, {"source": "Safa Ben Atitallah", "target": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics", "value": "author_of"}, {"source": "Maha Driss", "target": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics", "value": "author_of"}, {"source": "Henda Ben Ghezela", "target": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics", "value": "author_of"}, {"source": "Sareer Ul Amin", "target": "Enhancing pine wilt disease detection with synthetic data and external attention-based transformers", "value": "author_of"}, {"source": "Yonghoon Jung", "target": "Enhancing pine wilt disease detection with synthetic data and external attention-based transformers", "value": "author_of"}, {"source": "Muhammad Fayaz", "target": "Enhancing pine wilt disease detection with synthetic data and external attention-based transformers", "value": "author_of"}, {"source": "Bumsoo Kim", "target": "Enhancing pine wilt disease detection with synthetic data and external attention-based transformers", "value": "author_of"}, {"source": "Sanghyun Seo", "target": "Enhancing pine wilt disease detection with synthetic data and external attention-based transformers", "value": "author_of"}, {"source": "Ay\u015fe Aybilge Murat", "target": "A comprehensive review on YOLO versions for object detection", "value": "author_of"}, {"source": "M. S. K\u0131ran", "target": "A comprehensive review on YOLO versions for object detection", "value": "author_of"}, {"source": "Hongyan Zhu", "target": "Harnessing large vision and language models in agriculture: a review", "value": "author_of"}, {"source": "Shuai Qin", "target": "Harnessing large vision and language models in agriculture: a review", "value": "author_of"}, {"source": "Min Su", "target": "Harnessing large vision and language models in agriculture: a review", "value": "author_of"}, {"source": "Chengzhi Lin", "target": "Harnessing large vision and language models in agriculture: a review", "value": "author_of"}, {"source": "Anjie Li", "target": "Harnessing large vision and language models in agriculture: a review", "value": "author_of"}, {"source": "Junfeng Gao", "target": "Harnessing large vision and language models in agriculture: a review", "value": "author_of"}, {"source": "Abdul Rehman Khan", "target": "Multi-axis vision transformer for medical image segmentation", "value": "author_of"}, {"source": "Asifullah Khan", "target": "Multi-axis vision transformer for medical image segmentation", "value": "author_of"}, {"source": "D. E. Boukhari", "target": "A comprehensive review of facial beauty prediction using deep learning techniques", "value": "author_of"}, {"source": "F. Dornaika", "target": "A comprehensive review of facial beauty prediction using deep learning techniques", "value": "author_of"}, {"source": "A. Chemsa", "target": "A comprehensive review of facial beauty prediction using deep learning techniques", "value": "author_of"}, {"source": "Abdelmalik Taleb-Ahmed", "target": "A comprehensive review of facial beauty prediction using deep learning techniques", "value": "author_of"}, {"source": "Michaela Vystr\u010dilov\u00e1", "target": "A systematic comparison of predictive models on the retina", "value": "author_of"}, {"source": "Shashwat Sridhar", "target": "A systematic comparison of predictive models on the retina", "value": "author_of"}, {"source": "Max F. Burg", "target": "A systematic comparison of predictive models on the retina", "value": "author_of"}, {"source": "M. Khani", "target": "A systematic comparison of predictive models on the retina", "value": "author_of"}, {"source": "Dimokratis Karamanlis", "target": "A systematic comparison of predictive models on the retina", "value": "author_of"}, {"source": "H. Schreyer", "target": "A systematic comparison of predictive models on the retina", "value": "author_of"}, {"source": "Varsha Ramakrishna", "target": "A systematic comparison of predictive models on the retina", "value": "author_of"}, {"source": "Steffen Kr\u00fcppel", "target": "A systematic comparison of predictive models on the retina", "value": "author_of"}, {"source": "S\u00f6ren J. Zapp", "target": "A systematic comparison of predictive models on the retina", "value": "author_of"}, {"source": "Matthias Mietsch", "target": "A systematic comparison of predictive models on the retina", "value": "author_of"}, {"source": "T. Gollisch", "target": "A systematic comparison of predictive models on the retina", "value": "author_of"}, {"source": "Alexander S. Ecker", "target": "A systematic comparison of predictive models on the retina", "value": "author_of"}, {"source": "D. Lowe", "target": "Distinctive Image Features from Scale-Invariant Keypoints", "value": "author_of"}, {"source": "Xiang An", "target": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training", "value": "author_of"}, {"source": "Yin Xie", "target": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training", "value": "author_of"}, {"source": "Kaicheng Yang", "target": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training", "value": "author_of"}, {"source": "Wenkang Zhang", "target": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training", "value": "author_of"}, {"source": "Xiuwei Zhao", "target": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training", "value": "author_of"}, {"source": "Zheng Cheng", "target": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training", "value": "author_of"}, {"source": "Yirui Wang", "target": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training", "value": "author_of"}, {"source": "Songcen Xu", "target": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training", "value": "author_of"}, {"source": "Changrui Chen", "target": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training", "value": "author_of"}, {"source": "Didi Zhu", "target": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training", "value": "author_of"}, {"source": "Chunsheng Wu", "target": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training", "value": "author_of"}, {"source": "Huajie Tan", "target": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training", "value": "author_of"}, {"source": "Chunyuan Li", "target": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training", "value": "author_of"}, {"source": "Jing Yang", "target": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training", "value": "author_of"}, {"source": "Jie Yu", "target": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training", "value": "author_of"}, {"source": "Xiyao Wang", "target": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training", "value": "author_of"}, {"source": "Bin Qin", "target": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training", "value": "author_of"}, {"source": "Yumeng Wang", "target": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training", "value": "author_of"}, {"source": "Zizhen Yan", "target": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training", "value": "author_of"}, {"source": "Ziyong Feng", "target": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training", "value": "author_of"}, {"source": "Ziwei Liu", "target": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training", "value": "author_of"}, {"source": "Bo Li", "target": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training", "value": "author_of"}, {"source": "Jiankang Deng", "target": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training", "value": "author_of"}, {"source": "Kento Kawaharazuka", "target": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications", "value": "author_of"}, {"source": "Jihoon Oh", "target": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications", "value": "author_of"}, {"source": "Jun Yamada", "target": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications", "value": "author_of"}, {"source": "Ingmar Posner", "target": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications", "value": "author_of"}, {"source": "Yuke Zhu", "target": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications", "value": "author_of"}, {"source": "Lukas Muttenthaler", "target": "Aligning machine and human visual representations across abstraction levels", "value": "author_of"}, {"source": "Klaus Greff", "target": "Aligning machine and human visual representations across abstraction levels", "value": "author_of"}, {"source": "Frieda Born", "target": "Aligning machine and human visual representations across abstraction levels", "value": "author_of"}, {"source": "Bernhard Spitzer", "target": "Aligning machine and human visual representations across abstraction levels", "value": "author_of"}, {"source": "Simon Kornblith", "target": "Aligning machine and human visual representations across abstraction levels", "value": "author_of"}, {"source": "M. C. Mozer", "target": "Aligning machine and human visual representations across abstraction levels", "value": "author_of"}, {"source": "Klaus-Robert Muller", "target": "Aligning machine and human visual representations across abstraction levels", "value": "author_of"}, {"source": "Thomas Unterthiner", "target": "Aligning machine and human visual representations across abstraction levels", "value": "author_of"}, {"source": "Andrew Kyle Lampinen", "target": "Aligning machine and human visual representations across abstraction levels", "value": "author_of"}, {"source": "Alexey Dosovitskiy", "target": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "value": "author_of"}, {"source": "Lucas Beyer", "target": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "value": "author_of"}, {"source": "Alexander Kolesnikov", "target": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "value": "author_of"}, {"source": "Dirk Weissenborn", "target": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "value": "author_of"}, {"source": "Xiaohua Zhai", "target": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "value": "author_of"}, {"source": "Thomas Unterthiner", "target": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "value": "author_of"}, {"source": "Mostafa Dehghani", "target": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "value": "author_of"}, {"source": "Matthias Minderer", "target": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "value": "author_of"}, {"source": "Georg Heigold", "target": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "value": "author_of"}, {"source": "Sylvain Gelly", "target": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "value": "author_of"}, {"source": "Jakob Uszkoreit", "target": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "value": "author_of"}, {"source": "Neil Houlsby", "target": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "value": "author_of"}, {"source": "Alec Radford", "target": "Learning Transferable Visual Models From Natural Language Supervision", "value": "author_of"}, {"source": "Jong Wook Kim", "target": "Learning Transferable Visual Models From Natural Language Supervision", "value": "author_of"}, {"source": "Chris Hallacy", "target": "Learning Transferable Visual Models From Natural Language Supervision", "value": "author_of"}, {"source": "Aditya Ramesh", "target": "Learning Transferable Visual Models From Natural Language Supervision", "value": "author_of"}, {"source": "Gabriel Goh", "target": "Learning Transferable Visual Models From Natural Language Supervision", "value": "author_of"}, {"source": "Sandhini Agarwal", "target": "Learning Transferable Visual Models From Natural Language Supervision", "value": "author_of"}, {"source": "Girish Sastry", "target": "Learning Transferable Visual Models From Natural Language Supervision", "value": "author_of"}, {"source": "Amanda Askell", "target": "Learning Transferable Visual Models From Natural Language Supervision", "value": "author_of"}, {"source": "Pamela Mishkin", "target": "Learning Transferable Visual Models From Natural Language Supervision", "value": "author_of"}, {"source": "Jack Clark", "target": "Learning Transferable Visual Models From Natural Language Supervision", "value": "author_of"}, {"source": "Gretchen Krueger", "target": "Learning Transferable Visual Models From Natural Language Supervision", "value": "author_of"}, {"source": "Ilya Sutskever", "target": "Learning Transferable Visual Models From Natural Language Supervision", "value": "author_of"}, {"source": "Individualized Treat", "target": "GENERATIVE ADVERSARIAL NETS", "value": "author_of"}, {"source": "Jinsung Yoon", "target": "GENERATIVE ADVERSARIAL NETS", "value": "author_of"}, {"source": "Zhengyang Geng", "target": "Improved Mean Flows: On the Challenges of Fastforward Generative Models", "value": "author_of"}, {"source": "Yiyang Lu", "target": "Improved Mean Flows: On the Challenges of Fastforward Generative Models", "value": "author_of"}, {"source": "Zongze Wu", "target": "Improved Mean Flows: On the Challenges of Fastforward Generative Models", "value": "author_of"}, {"source": "Eli Shechtman", "target": "Improved Mean Flows: On the Challenges of Fastforward Generative Models", "value": "author_of"}, {"source": "J. Zico Kolter", "target": "Improved Mean Flows: On the Challenges of Fastforward Generative Models", "value": "author_of"}, {"source": "Kaiming He", "target": "Improved Mean Flows: On the Challenges of Fastforward Generative Models", "value": "author_of"}, {"source": "Jiachen Lei", "target": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training", "value": "author_of"}, {"source": "Keli Liu", "target": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training", "value": "author_of"}, {"source": "Julius Berner", "target": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training", "value": "author_of"}, {"source": "Haiming Yu", "target": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training", "value": "author_of"}, {"source": "Hongkai Zheng", "target": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training", "value": "author_of"}, {"source": "Jiahong Wu", "target": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training", "value": "author_of"}, {"source": "Xiangxiang Chu", "target": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training", "value": "author_of"}, {"source": "Minglei Shi", "target": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder", "value": "author_of"}, {"source": "Haolin Wang", "target": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder", "value": "author_of"}, {"source": "Borui Zhang", "target": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder", "value": "author_of"}, {"source": "Wenzhao Zheng", "target": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder", "value": "author_of"}, {"source": "Bohan Zeng", "target": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder", "value": "author_of"}, {"source": "Ziyang Yuan", "target": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder", "value": "author_of"}, {"source": "Xiaoshi Wu", "target": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder", "value": "author_of"}, {"source": "Yuanxing Zhang", "target": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder", "value": "author_of"}, {"source": "Huan Yang", "target": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder", "value": "author_of"}, {"source": "Xintao Wang", "target": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder", "value": "author_of"}, {"source": "Pengfei Wan", "target": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder", "value": "author_of"}, {"source": "Kun Gai", "target": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder", "value": "author_of"}, {"source": "Jie Zhou", "target": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder", "value": "author_of"}, {"source": "Jiwen Lu", "target": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder", "value": "author_of"}, {"source": "Yongsheng Yu", "target": "PixelDiT: Pixel Diffusion Transformers for Image Generation", "value": "author_of"}, {"source": "Wei Xiong", "target": "PixelDiT: Pixel Diffusion Transformers for Image Generation", "value": "author_of"}, {"source": "Weili Nie", "target": "PixelDiT: Pixel Diffusion Transformers for Image Generation", "value": "author_of"}, {"source": "Yichen Sheng", "target": "PixelDiT: Pixel Diffusion Transformers for Image Generation", "value": "author_of"}, {"source": "Shiqiu Liu", "target": "PixelDiT: Pixel Diffusion Transformers for Image Generation", "value": "author_of"}, {"source": "Jiebo Luo", "target": "PixelDiT: Pixel Diffusion Transformers for Image Generation", "value": "author_of"}, {"source": "Zhiheng Liu", "target": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "value": "author_of"}, {"source": "Weiming Ren", "target": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "value": "author_of"}, {"source": "Haozhe Liu", "target": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "value": "author_of"}, {"source": "Zijian Zhou", "target": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "value": "author_of"}, {"source": "Shoufa Chen", "target": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "value": "author_of"}, {"source": "Haonan Qiu", "target": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "value": "author_of"}, {"source": "Xiaoke Huang", "target": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "value": "author_of"}, {"source": "Zhaochong An", "target": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "value": "author_of"}, {"source": "Fanny Yang", "target": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "value": "author_of"}, {"source": "Aditya Patel", "target": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "value": "author_of"}, {"source": "Viktar Atliha", "target": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "value": "author_of"}, {"source": "Tony Ng", "target": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "value": "author_of"}, {"source": "Xiao Han", "target": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "value": "author_of"}, {"source": "Chuyan Zhu", "target": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "value": "author_of"}, {"source": "Chenyang Zhang", "target": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "value": "author_of"}, {"source": "Ding Liu", "target": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "value": "author_of"}, {"source": "Juan-Manuel Perez-Rua", "target": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "value": "author_of"}, {"source": "Sen He", "target": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "value": "author_of"}, {"source": "J\u00fcrgen Schmidhuber", "target": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "value": "author_of"}, {"source": "Wenhu Chen", "target": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "value": "author_of"}, {"source": "Ping Luo", "target": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "value": "author_of"}, {"source": "Wei Liu", "target": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "value": "author_of"}, {"source": "Tao Xiang", "target": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "value": "author_of"}, {"source": "Jonas Schult", "target": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "value": "author_of"}, {"source": "Yuren Cong", "target": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "value": "author_of"}, {"source": "Jacob Devlin", "target": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "value": "author_of"}, {"source": "Ming-Wei Chang", "target": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "value": "author_of"}, {"source": "Kenton Lee", "target": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "value": "author_of"}, {"source": "Kristina Toutanova", "target": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "value": "author_of"}, {"source": "Vrushali Pagire", "target": "A comprehensive review of object detection with traditional and deep learning methods", "value": "author_of"}, {"source": "M. Chavali", "target": "A comprehensive review of object detection with traditional and deep learning methods", "value": "author_of"}, {"source": "Ashish Kale", "target": "A comprehensive review of object detection with traditional and deep learning methods", "value": "author_of"}, {"source": "Jinjie Ni", "target": "Diffusion Language Models are Super Data Learners", "value": "author_of"}, {"source": "Qian Liu", "target": "Diffusion Language Models are Super Data Learners", "value": "author_of"}, {"source": "Longxu Dou", "target": "Diffusion Language Models are Super Data Learners", "value": "author_of"}, {"source": "Chao Du", "target": "Diffusion Language Models are Super Data Learners", "value": "author_of"}, {"source": "Zili Wang", "target": "Diffusion Language Models are Super Data Learners", "value": "author_of"}, {"source": "Hang Yan", "target": "Diffusion Language Models are Super Data Learners", "value": "author_of"}, {"source": "Tianyu Pang", "target": "Diffusion Language Models are Super Data Learners", "value": "author_of"}, {"source": "Michael Qizhe Shieh", "target": "Diffusion Language Models are Super Data Learners", "value": "author_of"}, {"source": "Zirui Wu", "target": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas", "value": "author_of"}, {"source": "Lin Zheng", "target": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas", "value": "author_of"}, {"source": "Zhihui Xie", "target": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas", "value": "author_of"}, {"source": "Jiacheng Ye", "target": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas", "value": "author_of"}, {"source": "Jiahui Gao", "target": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas", "value": "author_of"}, {"source": "Shansan Gong", "target": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas", "value": "author_of"}, {"source": "Yansong Feng", "target": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas", "value": "author_of"}, {"source": "Zhenguo Li", "target": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas", "value": "author_of"}, {"source": "Wei Bi", "target": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas", "value": "author_of"}, {"source": "Guorui Zhou", "target": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas", "value": "author_of"}, {"source": "Lingpeng Kong", "target": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas", "value": "author_of"}, {"source": "Zhicheng Cai", "target": "FLEX: Continuous Agent Evolution via Forward Learning from Experience", "value": "author_of"}, {"source": "Xinyuan Guo", "target": "FLEX: Continuous Agent Evolution via Forward Learning from Experience", "value": "author_of"}, {"source": "Yu Pei", "target": "FLEX: Continuous Agent Evolution via Forward Learning from Experience", "value": "author_of"}, {"source": "Jiangtao Feng", "target": "FLEX: Continuous Agent Evolution via Forward Learning from Experience", "value": "author_of"}, {"source": "Jinsong Su", "target": "FLEX: Continuous Agent Evolution via Forward Learning from Experience", "value": "author_of"}, {"source": "Jiangjie Chen", "target": "FLEX: Continuous Agent Evolution via Forward Learning from Experience", "value": "author_of"}, {"source": "Ya-Qin Zhang", "target": "FLEX: Continuous Agent Evolution via Forward Learning from Experience", "value": "author_of"}, {"source": "Wei-Ying Ma", "target": "FLEX: Continuous Agent Evolution via Forward Learning from Experience", "value": "author_of"}, {"source": "Mingxuan Wang", "target": "FLEX: Continuous Agent Evolution via Forward Learning from Experience", "value": "author_of"}, {"source": "Hao Zhou", "target": "FLEX: Continuous Agent Evolution via Forward Learning from Experience", "value": "author_of"}, {"source": "Mingyue Cheng", "target": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning", "value": "author_of"}, {"source": "Jie Ouyang", "target": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning", "value": "author_of"}, {"source": "Shuo Yu", "target": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning", "value": "author_of"}, {"source": "Ruiran Yan", "target": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning", "value": "author_of"}, {"source": "Yucong Luo", "target": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning", "value": "author_of"}, {"source": "Zirui Liu", "target": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning", "value": "author_of"}, {"source": "Daoyu Wang", "target": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning", "value": "author_of"}, {"source": "Qi Liu", "target": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning", "value": "author_of"}, {"source": "Enhong Chen", "target": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning", "value": "author_of"}, {"source": "Holger Caesar", "target": "nuScenes: A multimodal dataset for autonomous driving", "value": "author_of"}, {"source": "Varun Bankiti", "target": "nuScenes: A multimodal dataset for autonomous driving", "value": "author_of"}, {"source": "Alex H. Lang", "target": "nuScenes: A multimodal dataset for autonomous driving", "value": "author_of"}, {"source": "Sourabh Vora", "target": "nuScenes: A multimodal dataset for autonomous driving", "value": "author_of"}, {"source": "Venice Erin Liong", "target": "nuScenes: A multimodal dataset for autonomous driving", "value": "author_of"}, {"source": "Qiang Xu", "target": "nuScenes: A multimodal dataset for autonomous driving", "value": "author_of"}, {"source": "Anush Krishnan", "target": "nuScenes: A multimodal dataset for autonomous driving", "value": "author_of"}, {"source": "Yu Pan", "target": "nuScenes: A multimodal dataset for autonomous driving", "value": "author_of"}, {"source": "Giancarlo Baldan", "target": "nuScenes: A multimodal dataset for autonomous driving", "value": "author_of"}, {"source": "Oscar Beijbom", "target": "nuScenes: A multimodal dataset for autonomous driving", "value": "author_of"}, {"source": "Navneet Dalal", "target": "Histograms of oriented gradients for human detection", "value": "author_of"}, {"source": "B. Triggs", "target": "Histograms of oriented gradients for human detection", "value": "author_of"}, {"source": "Yue Ma", "target": "Controllable Video Generation: A Survey", "value": "author_of"}, {"source": "Kunyu Feng", "target": "Controllable Video Generation: A Survey", "value": "author_of"}, {"source": "Zhongyuan Hu", "target": "Controllable Video Generation: A Survey", "value": "author_of"}, {"source": "Xinyu Wang", "target": "Controllable Video Generation: A Survey", "value": "author_of"}, {"source": "Yucheng Wang", "target": "Controllable Video Generation: A Survey", "value": "author_of"}, {"source": "Mingzhe Zheng", "target": "Controllable Video Generation: A Survey", "value": "author_of"}, {"source": "Bingyuan Wang", "target": "Controllable Video Generation: A Survey", "value": "author_of"}, {"source": "Qinghe Wang", "target": "Controllable Video Generation: A Survey", "value": "author_of"}, {"source": "Xuanhua He", "target": "Controllable Video Generation: A Survey", "value": "author_of"}, {"source": "Hongfa Wang", "target": "Controllable Video Generation: A Survey", "value": "author_of"}, {"source": "Chenyang Zhu", "target": "Controllable Video Generation: A Survey", "value": "author_of"}, {"source": "Hongyu Liu", "target": "Controllable Video Generation: A Survey", "value": "author_of"}, {"source": "Yingqing He", "target": "Controllable Video Generation: A Survey", "value": "author_of"}, {"source": "Zeyu Wang", "target": "Controllable Video Generation: A Survey", "value": "author_of"}, {"source": "Zhifeng Li", "target": "Controllable Video Generation: A Survey", "value": "author_of"}, {"source": "Xiu Li", "target": "Controllable Video Generation: A Survey", "value": "author_of"}, {"source": "Sirui Han", "target": "Controllable Video Generation: A Survey", "value": "author_of"}, {"source": "Yike Guo", "target": "Controllable Video Generation: A Survey", "value": "author_of"}, {"source": "Wei Liu", "target": "Controllable Video Generation: A Survey", "value": "author_of"}, {"source": "Dan Xu", "target": "Controllable Video Generation: A Survey", "value": "author_of"}, {"source": "Linfeng Zhang", "target": "Controllable Video Generation: A Survey", "value": "author_of"}, {"source": "Qifeng Chen", "target": "Controllable Video Generation: A Survey", "value": "author_of"}, {"source": "Lijun Chi", "target": "Adversarial Attacks on Autonomous Driving Systems in the Physical World: A Survey", "value": "author_of"}, {"source": "M. Msahli", "target": "Adversarial Attacks on Autonomous Driving Systems in the Physical World: A Survey", "value": "author_of"}, {"source": "Qingjie Zhang", "target": "Adversarial Attacks on Autonomous Driving Systems in the Physical World: A Survey", "value": "author_of"}, {"source": "Han Qiu", "target": "Adversarial Attacks on Autonomous Driving Systems in the Physical World: A Survey", "value": "author_of"}, {"source": "Tianwei Zhang", "target": "Adversarial Attacks on Autonomous Driving Systems in the Physical World: A Survey", "value": "author_of"}, {"source": "G\u00e9rard Memmi", "target": "Adversarial Attacks on Autonomous Driving Systems in the Physical World: A Survey", "value": "author_of"}, {"source": "Meikang Qiu", "target": "Adversarial Attacks on Autonomous Driving Systems in the Physical World: A Survey", "value": "author_of"}, {"source": "NVIDIA", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": ":", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Yan Wang", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Wenjie Luo", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Junjie Bai", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Yulong Cao", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Tong Che", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Ke Chen", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Yuxiao Chen", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Jenna Diamond", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Yifan Ding", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Wenhao Ding", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Liang Feng", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Greg Heinrich", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Jack Huang", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Peter Karkus", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Boyi Li", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Pinyi Li", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Tsung-Yi Lin", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Dongran Liu", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Ming-Yu Liu", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Langechuan Liu", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Zhijian Liu", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Jason Lu", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Yunxiang Mao", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Pavlo Molchanov", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Lindsey Pavao", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Zhenghao Peng", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Mike Ranzinger", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Ed Schmerling", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Shida Shen", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Yunfei Shi", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Sarah Tariq", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Ran Tian", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Tilman Wekel", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Xinshuo Weng", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Tianjun Xiao", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Eric Yang", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Xiaodong Yang", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Yurong You", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Xiaohui Zeng", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Wenyuan Zhang", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Boris Ivanovic", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Marco Pavone", "target": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "value": "author_of"}, {"source": "Mohsen Gholami", "target": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes", "value": "author_of"}, {"source": "Ahmad Rezaei", "target": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes", "value": "author_of"}, {"source": "Zhou Weimin", "target": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes", "value": "author_of"}, {"source": "Sitong Mao", "target": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes", "value": "author_of"}, {"source": "Shunbo Zhou", "target": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes", "value": "author_of"}, {"source": "Yong Zhang", "target": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes", "value": "author_of"}, {"source": "Mohammad Akbari", "target": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes", "value": "author_of"}, {"source": "Qing Jiang", "target": "Detect Anything via Next Point Prediction", "value": "author_of"}, {"source": "Junan Huo", "target": "Detect Anything via Next Point Prediction", "value": "author_of"}, {"source": "Xingyu Chen", "target": "Detect Anything via Next Point Prediction", "value": "author_of"}, {"source": "Yuda Xiong", "target": "Detect Anything via Next Point Prediction", "value": "author_of"}, {"source": "Zhaoyang Zeng", "target": "Detect Anything via Next Point Prediction", "value": "author_of"}, {"source": "Yihao Chen", "target": "Detect Anything via Next Point Prediction", "value": "author_of"}, {"source": "Tianhe Ren", "target": "Detect Anything via Next Point Prediction", "value": "author_of"}, {"source": "Junzhi Yu", "target": "Detect Anything via Next Point Prediction", "value": "author_of"}, {"source": "Lei Zhang", "target": "Detect Anything via Next Point Prediction", "value": "author_of"}, {"source": "Volodymyr Mnih", "target": "Human-level control through deep reinforcement learning", "value": "author_of"}, {"source": "K. Kavukcuoglu", "target": "Human-level control through deep reinforcement learning", "value": "author_of"}, {"source": "David Silver", "target": "Human-level control through deep reinforcement learning", "value": "author_of"}, {"source": "Andrei A. Rusu", "target": "Human-level control through deep reinforcement learning", "value": "author_of"}, {"source": "J. Veness", "target": "Human-level control through deep reinforcement learning", "value": "author_of"}, {"source": "Marc G. Bellemare", "target": "Human-level control through deep reinforcement learning", "value": "author_of"}, {"source": "Alex Graves", "target": "Human-level control through deep reinforcement learning", "value": "author_of"}, {"source": "Martin A. Riedmiller", "target": "Human-level control through deep reinforcement learning", "value": "author_of"}, {"source": "A. Fidjeland", "target": "Human-level control through deep reinforcement learning", "value": "author_of"}, {"source": "Georg Ostrovski", "target": "Human-level control through deep reinforcement learning", "value": "author_of"}, {"source": "Stig Petersen", "target": "Human-level control through deep reinforcement learning", "value": "author_of"}, {"source": "Charlie Beattie", "target": "Human-level control through deep reinforcement learning", "value": "author_of"}, {"source": "Amir Sadik", "target": "Human-level control through deep reinforcement learning", "value": "author_of"}, {"source": "Ioannis Antonoglou", "target": "Human-level control through deep reinforcement learning", "value": "author_of"}, {"source": "Helen King", "target": "Human-level control through deep reinforcement learning", "value": "author_of"}, {"source": "D. Kumaran", "target": "Human-level control through deep reinforcement learning", "value": "author_of"}, {"source": "Daan Wierstra", "target": "Human-level control through deep reinforcement learning", "value": "author_of"}, {"source": "S. Legg", "target": "Human-level control through deep reinforcement learning", "value": "author_of"}, {"source": "D. Hassabis", "target": "Human-level control through deep reinforcement learning", "value": "author_of"}, {"source": "Volodymyr Mnih", "target": "Asynchronous Methods for Deep Reinforcement Learning", "value": "author_of"}, {"source": "Adri\u00e0 Puigdom\u00e8nech Badia", "target": "Asynchronous Methods for Deep Reinforcement Learning", "value": "author_of"}, {"source": "Mehdi Mirza", "target": "Asynchronous Methods for Deep Reinforcement Learning", "value": "author_of"}, {"source": "Alex Graves", "target": "Asynchronous Methods for Deep Reinforcement Learning", "value": "author_of"}, {"source": "Timothy P. Lillicrap", "target": "Asynchronous Methods for Deep Reinforcement Learning", "value": "author_of"}, {"source": "Tim Harley", "target": "Asynchronous Methods for Deep Reinforcement Learning", "value": "author_of"}, {"source": "David Silver", "target": "Asynchronous Methods for Deep Reinforcement Learning", "value": "author_of"}, {"source": "Koray Kavukcuoglu", "target": "Asynchronous Methods for Deep Reinforcement Learning", "value": "author_of"}, {"source": "Guosheng Lin", "target": "RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation", "value": "author_of"}, {"source": "Anton Milan", "target": "RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation", "value": "author_of"}, {"source": "Chunhua Shen", "target": "RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation", "value": "author_of"}, {"source": "Ian Reid", "target": "RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation", "value": "author_of"}, {"source": "Zhijie Qiao", "target": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving", "value": "author_of"}, {"source": "Haowei Li", "target": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving", "value": "author_of"}, {"source": "Zhong Cao", "target": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving", "value": "author_of"}, {"source": "Henry X. Liu", "target": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving", "value": "author_of"}, {"source": "Yongkang Li", "target": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Kaixin Xiong", "target": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Xiangyu Guo", "target": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Fang Li", "target": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Sixu Yan", "target": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Gangwei Xu", "target": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Lijun Zhou", "target": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Long Chen", "target": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Haiyang Sun", "target": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Bing Wang", "target": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Kun Ma", "target": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Guang Chen", "target": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Hangjun Ye", "target": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Wenyu Liu", "target": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Xinggang Wang", "target": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Wei Cao", "target": "Pseudo-Simulation for Autonomous Driving", "value": "author_of"}, {"source": "Marcel Hallgarten", "target": "Pseudo-Simulation for Autonomous Driving", "value": "author_of"}, {"source": "Tianyu Li", "target": "Pseudo-Simulation for Autonomous Driving", "value": "author_of"}, {"source": "Daniel Dauner", "target": "Pseudo-Simulation for Autonomous Driving", "value": "author_of"}, {"source": "Xunjiang Gu", "target": "Pseudo-Simulation for Autonomous Driving", "value": "author_of"}, {"source": "Caojun Wang", "target": "Pseudo-Simulation for Autonomous Driving", "value": "author_of"}, {"source": "Yakov Miron", "target": "Pseudo-Simulation for Autonomous Driving", "value": "author_of"}, {"source": "Marco Aiello", "target": "Pseudo-Simulation for Autonomous Driving", "value": "author_of"}, {"source": "Hongyang Li", "target": "Pseudo-Simulation for Autonomous Driving", "value": "author_of"}, {"source": "Igor Gilitschenski", "target": "Pseudo-Simulation for Autonomous Driving", "value": "author_of"}, {"source": "Boris Ivanovic", "target": "Pseudo-Simulation for Autonomous Driving", "value": "author_of"}, {"source": "Marco Pavone", "target": "Pseudo-Simulation for Autonomous Driving", "value": "author_of"}, {"source": "Andreas Geiger", "target": "Pseudo-Simulation for Autonomous Driving", "value": "author_of"}, {"source": "Kashyap Chitta", "target": "Pseudo-Simulation for Autonomous Driving", "value": "author_of"}, {"source": "Zhenjie Yang", "target": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Yilin Chai", "target": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Xiaosong Jia", "target": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Qifeng Li", "target": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Yuqian Shao", "target": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Xuekai Zhu", "target": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Haisheng Su", "target": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Junchi Yan", "target": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Sicong Jiang", "target": "A Survey on Vision-Language-Action Models for Autonomous Driving", "value": "author_of"}, {"source": "Zilin Huang", "target": "A Survey on Vision-Language-Action Models for Autonomous Driving", "value": "author_of"}, {"source": "Kangan Qian", "target": "A Survey on Vision-Language-Action Models for Autonomous Driving", "value": "author_of"}, {"source": "Ziang Luo", "target": "A Survey on Vision-Language-Action Models for Autonomous Driving", "value": "author_of"}, {"source": "Tianze Zhu", "target": "A Survey on Vision-Language-Action Models for Autonomous Driving", "value": "author_of"}, {"source": "Yang Zhong", "target": "A Survey on Vision-Language-Action Models for Autonomous Driving", "value": "author_of"}, {"source": "Yihong Tang", "target": "A Survey on Vision-Language-Action Models for Autonomous Driving", "value": "author_of"}, {"source": "Menglin Kong", "target": "A Survey on Vision-Language-Action Models for Autonomous Driving", "value": "author_of"}, {"source": "Yunlong Wang", "target": "A Survey on Vision-Language-Action Models for Autonomous Driving", "value": "author_of"}, {"source": "Siwen Jiao", "target": "A Survey on Vision-Language-Action Models for Autonomous Driving", "value": "author_of"}, {"source": "Hao Ye", "target": "A Survey on Vision-Language-Action Models for Autonomous Driving", "value": "author_of"}, {"source": "Zihao Sheng", "target": "A Survey on Vision-Language-Action Models for Autonomous Driving", "value": "author_of"}, {"source": "Xin Zhao", "target": "A Survey on Vision-Language-Action Models for Autonomous Driving", "value": "author_of"}, {"source": "Tuopu Wen", "target": "A Survey on Vision-Language-Action Models for Autonomous Driving", "value": "author_of"}, {"source": "Zheng Fu", "target": "A Survey on Vision-Language-Action Models for Autonomous Driving", "value": "author_of"}, {"source": "Sikai Chen", "target": "A Survey on Vision-Language-Action Models for Autonomous Driving", "value": "author_of"}, {"source": "Kun Jiang", "target": "A Survey on Vision-Language-Action Models for Autonomous Driving", "value": "author_of"}, {"source": "Diange Yang", "target": "A Survey on Vision-Language-Action Models for Autonomous Driving", "value": "author_of"}, {"source": "Seongjin Choi", "target": "A Survey on Vision-Language-Action Models for Autonomous Driving", "value": "author_of"}, {"source": "Lijun Sun", "target": "A Survey on Vision-Language-Action Models for Autonomous Driving", "value": "author_of"}, {"source": "M. Page", "target": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis", "value": "author_of"}, {"source": "J. McKenzie", "target": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis", "value": "author_of"}, {"source": "P. Bossuyt", "target": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis", "value": "author_of"}, {"source": "I. Boutron", "target": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis", "value": "author_of"}, {"source": "T. Hoffmann", "target": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis", "value": "author_of"}, {"source": "C. Mulrow", "target": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis", "value": "author_of"}, {"source": "Larissa Shamseer", "target": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis", "value": "author_of"}, {"source": "J. Tetzlaff", "target": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis", "value": "author_of"}, {"source": "E. Akl", "target": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis", "value": "author_of"}, {"source": "S. Brennan", "target": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis", "value": "author_of"}, {"source": "R. Chou", "target": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis", "value": "author_of"}, {"source": "Julie May Glanville", "target": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis", "value": "author_of"}, {"source": "J. Grimshaw", "target": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis", "value": "author_of"}, {"source": "A. Hr\u00f5bjartsson", "target": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis", "value": "author_of"}, {"source": "M. Lalu", "target": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis", "value": "author_of"}, {"source": "Tianjing Li", "target": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis", "value": "author_of"}, {"source": "E. Loder", "target": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis", "value": "author_of"}, {"source": "E. Mayo-Wilson", "target": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis", "value": "author_of"}, {"source": "Steve McDonald", "target": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis", "value": "author_of"}, {"source": "L. McGuinness", "target": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis", "value": "author_of"}, {"source": "L. Stewart", "target": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis", "value": "author_of"}, {"source": "James Thomas", "target": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis", "value": "author_of"}, {"source": "A. Tricco", "target": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis", "value": "author_of"}, {"source": "V. Welch", "target": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis", "value": "author_of"}, {"source": "P. Whiting", "target": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis", "value": "author_of"}, {"source": "D. Moher", "target": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis", "value": "author_of"}, {"source": "Zheng Zhu", "target": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond", "value": "author_of"}, {"source": "Xiaofeng Wang", "target": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond", "value": "author_of"}, {"source": "Wangbo Zhao", "target": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond", "value": "author_of"}, {"source": "Chen Min", "target": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond", "value": "author_of"}, {"source": "Bohan Li", "target": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond", "value": "author_of"}, {"source": "Nianchen Deng", "target": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond", "value": "author_of"}, {"source": "Min Dou", "target": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond", "value": "author_of"}, {"source": "Yuqi Wang", "target": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond", "value": "author_of"}, {"source": "Botian Shi", "target": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond", "value": "author_of"}, {"source": "Kai Wang", "target": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond", "value": "author_of"}, {"source": "Chi Zhang", "target": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond", "value": "author_of"}, {"source": "Yang You", "target": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond", "value": "author_of"}, {"source": "Zhaoxiang Zhang", "target": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond", "value": "author_of"}, {"source": "Dawei Zhao", "target": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond", "value": "author_of"}, {"source": "Liang Xiao", "target": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond", "value": "author_of"}, {"source": "Jian Zhao", "target": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond", "value": "author_of"}, {"source": "Jiwen Lu", "target": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond", "value": "author_of"}, {"source": "Guan Huang", "target": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond", "value": "author_of"}, {"source": "Jingtao Ding", "target": "Understanding World or Predicting Future? A Comprehensive Survey of World Models", "value": "author_of"}, {"source": "Yunke Zhang", "target": "Understanding World or Predicting Future? A Comprehensive Survey of World Models", "value": "author_of"}, {"source": "Yu Shang", "target": "Understanding World or Predicting Future? A Comprehensive Survey of World Models", "value": "author_of"}, {"source": "Jie Feng", "target": "Understanding World or Predicting Future? A Comprehensive Survey of World Models", "value": "author_of"}, {"source": "Yuheng Zhang", "target": "Understanding World or Predicting Future? A Comprehensive Survey of World Models", "value": "author_of"}, {"source": "Zefang Zong", "target": "Understanding World or Predicting Future? A Comprehensive Survey of World Models", "value": "author_of"}, {"source": "Yuan Yuan", "target": "Understanding World or Predicting Future? A Comprehensive Survey of World Models", "value": "author_of"}, {"source": "Hongyuan Su", "target": "Understanding World or Predicting Future? A Comprehensive Survey of World Models", "value": "author_of"}, {"source": "Nian Li", "target": "Understanding World or Predicting Future? A Comprehensive Survey of World Models", "value": "author_of"}, {"source": "Jinghua Piao", "target": "Understanding World or Predicting Future? A Comprehensive Survey of World Models", "value": "author_of"}, {"source": "Yucheng Deng", "target": "Understanding World or Predicting Future? A Comprehensive Survey of World Models", "value": "author_of"}, {"source": "Nicholas Sukiennik", "target": "Understanding World or Predicting Future? A Comprehensive Survey of World Models", "value": "author_of"}, {"source": "Chen Gao", "target": "Understanding World or Predicting Future? A Comprehensive Survey of World Models", "value": "author_of"}, {"source": "Fengli Xu", "target": "Understanding World or Predicting Future? A Comprehensive Survey of World Models", "value": "author_of"}, {"source": "Yong Li", "target": "Understanding World or Predicting Future? A Comprehensive Survey of World Models", "value": "author_of"}, {"source": "Yuan Yuan", "target": "A Survey of Multimodal Learning: Methods, Applications, and Future", "value": "author_of"}, {"source": "Zhaojian Li", "target": "A Survey of Multimodal Learning: Methods, Applications, and Future", "value": "author_of"}, {"source": "Bin Zhao", "target": "A Survey of Multimodal Learning: Methods, Applications, and Future", "value": "author_of"}, {"source": "Xuannan Liu", "target": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey", "value": "author_of"}, {"source": "Xing Cui", "target": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey", "value": "author_of"}, {"source": "Peipei Li", "target": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey", "value": "author_of"}, {"source": "Zekun Li", "target": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey", "value": "author_of"}, {"source": "Huaibo Huang", "target": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey", "value": "author_of"}, {"source": "Shuhan Xia", "target": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey", "value": "author_of"}, {"source": "Miaoxuan Zhang", "target": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey", "value": "author_of"}, {"source": "Yueying Zou", "target": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey", "value": "author_of"}, {"source": "Ran He", "target": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey", "value": "author_of"}, {"source": "Galina Ilieva", "target": "Effects of Generative AI in Tourism Industry", "value": "author_of"}, {"source": "Tania Yankova", "target": "Effects of Generative AI in Tourism Industry", "value": "author_of"}, {"source": "Stanislava Klisarova-Belcheva", "target": "Effects of Generative AI in Tourism Industry", "value": "author_of"}, {"source": "Jie Hu", "target": "Squeeze-and-Excitation Networks", "value": "author_of"}, {"source": "Li Shen", "target": "Squeeze-and-Excitation Networks", "value": "author_of"}, {"source": "Samuel Albanie", "target": "Squeeze-and-Excitation Networks", "value": "author_of"}, {"source": "Gang Sun", "target": "Squeeze-and-Excitation Networks", "value": "author_of"}, {"source": "Enhua Wu", "target": "Squeeze-and-Excitation Networks", "value": "author_of"}, {"source": "I. Loshchilov", "target": "Decoupled Weight Decay Regularization", "value": "author_of"}, {"source": "F. Hutter", "target": "Decoupled Weight Decay Regularization", "value": "author_of"}, {"source": "Mingxing Tan", "target": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks", "value": "author_of"}, {"source": "Quoc V. Le", "target": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks", "value": "author_of"}, {"source": "Tianqi Liu", "target": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control", "value": "author_of"}, {"source": "Zhaoxi Chen", "target": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control", "value": "author_of"}, {"source": "Zihao Huang", "target": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control", "value": "author_of"}, {"source": "Shaocong Xu", "target": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control", "value": "author_of"}, {"source": "Saining Zhang", "target": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control", "value": "author_of"}, {"source": "Chongjie Ye", "target": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control", "value": "author_of"}, {"source": "Bohan Li", "target": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control", "value": "author_of"}, {"source": "Zhiguo Cao", "target": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control", "value": "author_of"}, {"source": "Wei Li", "target": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control", "value": "author_of"}, {"source": "Hao Zhao", "target": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control", "value": "author_of"}, {"source": "Ziwei Liu", "target": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control", "value": "author_of"}, {"source": "Tianze Xia", "target": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World", "value": "author_of"}, {"source": "Yongkang Li", "target": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World", "value": "author_of"}, {"source": "Lijun Zhou", "target": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World", "value": "author_of"}, {"source": "Jingfeng Yao", "target": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World", "value": "author_of"}, {"source": "Kaixin Xiong", "target": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World", "value": "author_of"}, {"source": "Haiyang Sun", "target": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World", "value": "author_of"}, {"source": "Bing Wang", "target": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World", "value": "author_of"}, {"source": "Kun Ma", "target": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World", "value": "author_of"}, {"source": "Guang Chen", "target": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World", "value": "author_of"}, {"source": "Hangjun Ye", "target": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World", "value": "author_of"}, {"source": "Wenyu Liu", "target": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World", "value": "author_of"}, {"source": "Xinggang Wang", "target": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World", "value": "author_of"}, {"source": "Sicheng Zuo", "target": "DVGT: Driving Visual Geometry Transformer", "value": "author_of"}, {"source": "Zixun Xie", "target": "DVGT: Driving Visual Geometry Transformer", "value": "author_of"}, {"source": "Wenzhao Zheng", "target": "DVGT: Driving Visual Geometry Transformer", "value": "author_of"}, {"source": "Shaoqing Xu", "target": "DVGT: Driving Visual Geometry Transformer", "value": "author_of"}, {"source": "Fang Li", "target": "DVGT: Driving Visual Geometry Transformer", "value": "author_of"}, {"source": "Shengyin Jiang", "target": "DVGT: Driving Visual Geometry Transformer", "value": "author_of"}, {"source": "Long Chen", "target": "DVGT: Driving Visual Geometry Transformer", "value": "author_of"}, {"source": "Zhi-Xin Yang", "target": "DVGT: Driving Visual Geometry Transformer", "value": "author_of"}, {"source": "Jiwen Lu", "target": "DVGT: Driving Visual Geometry Transformer", "value": "author_of"}, {"source": "Lvmin Zhang", "target": "Adding Conditional Control to Text-to-Image Diffusion Models", "value": "author_of"}, {"source": "Anyi Rao", "target": "Adding Conditional Control to Text-to-Image Diffusion Models", "value": "author_of"}, {"source": "Maneesh Agrawala", "target": "Adding Conditional Control to Text-to-Image Diffusion Models", "value": "author_of"}, {"source": "Hyung Won Chung", "target": "Scaling Instruction-Finetuned Language Models", "value": "author_of"}, {"source": "Le Hou", "target": "Scaling Instruction-Finetuned Language Models", "value": "author_of"}, {"source": "Shayne Longpre", "target": "Scaling Instruction-Finetuned Language Models", "value": "author_of"}, {"source": "Barret Zoph", "target": "Scaling Instruction-Finetuned Language Models", "value": "author_of"}, {"source": "Yi Tay", "target": "Scaling Instruction-Finetuned Language Models", "value": "author_of"}, {"source": "William Fedus", "target": "Scaling Instruction-Finetuned Language Models", "value": "author_of"}, {"source": "Yunxuan Li", "target": "Scaling Instruction-Finetuned Language Models", "value": "author_of"}, {"source": "Xuezhi Wang", "target": "Scaling Instruction-Finetuned Language Models", "value": "author_of"}, {"source": "Mostafa Dehghani", "target": "Scaling Instruction-Finetuned Language Models", "value": "author_of"}, {"source": "Siddhartha Brahma", "target": "Scaling Instruction-Finetuned Language Models", "value": "author_of"}, {"source": "Albert Webson", "target": "Scaling Instruction-Finetuned Language Models", "value": "author_of"}, {"source": "Shixiang Shane Gu", "target": "Scaling Instruction-Finetuned Language Models", "value": "author_of"}, {"source": "Zhuyun Dai", "target": "Scaling Instruction-Finetuned Language Models", "value": "author_of"}, {"source": "Mirac Suzgun", "target": "Scaling Instruction-Finetuned Language Models", "value": "author_of"}, {"source": "Xinyun Chen", "target": "Scaling Instruction-Finetuned Language Models", "value": "author_of"}, {"source": "Aakanksha Chowdhery", "target": "Scaling Instruction-Finetuned Language Models", "value": "author_of"}, {"source": "Alex Castro-Ros", "target": "Scaling Instruction-Finetuned Language Models", "value": "author_of"}, {"source": "Marie Pellat", "target": "Scaling Instruction-Finetuned Language Models", "value": "author_of"}, {"source": "Kevin Robinson", "target": "Scaling Instruction-Finetuned Language Models", "value": "author_of"}, {"source": "Dasha Valter", "target": "Scaling Instruction-Finetuned Language Models", "value": "author_of"}, {"source": "Sharan Narang", "target": "Scaling Instruction-Finetuned Language Models", "value": "author_of"}, {"source": "Gaurav Mishra", "target": "Scaling Instruction-Finetuned Language Models", "value": "author_of"}, {"source": "Adams Yu", "target": "Scaling Instruction-Finetuned Language Models", "value": "author_of"}, {"source": "Vincent Zhao", "target": "Scaling Instruction-Finetuned Language Models", "value": "author_of"}, {"source": "Yanping Huang", "target": "Scaling Instruction-Finetuned Language Models", "value": "author_of"}, {"source": "Andrew Dai", "target": "Scaling Instruction-Finetuned Language Models", "value": "author_of"}, {"source": "Hongkun Yu", "target": "Scaling Instruction-Finetuned Language Models", "value": "author_of"}, {"source": "Slav Petrov", "target": "Scaling Instruction-Finetuned Language Models", "value": "author_of"}, {"source": "Ed H. Chi", "target": "Scaling Instruction-Finetuned Language Models", "value": "author_of"}, {"source": "Jeff Dean", "target": "Scaling Instruction-Finetuned Language Models", "value": "author_of"}, {"source": "Jacob Devlin", "target": "Scaling Instruction-Finetuned Language Models", "value": "author_of"}, {"source": "Adam Roberts", "target": "Scaling Instruction-Finetuned Language Models", "value": "author_of"}, {"source": "Denny Zhou", "target": "Scaling Instruction-Finetuned Language Models", "value": "author_of"}, {"source": "Quoc V. Le", "target": "Scaling Instruction-Finetuned Language Models", "value": "author_of"}, {"source": "Jason Wei", "target": "Scaling Instruction-Finetuned Language Models", "value": "author_of"}, {"source": "Robin Rombach", "target": "High-Resolution Image Synthesis with Latent Diffusion Models", "value": "author_of"}, {"source": "Andreas Blattmann", "target": "High-Resolution Image Synthesis with Latent Diffusion Models", "value": "author_of"}, {"source": "Dominik Lorenz", "target": "High-Resolution Image Synthesis with Latent Diffusion Models", "value": "author_of"}, {"source": "Patrick Esser", "target": "High-Resolution Image Synthesis with Latent Diffusion Models", "value": "author_of"}, {"source": "Bj\u00f6rn Ommer", "target": "High-Resolution Image Synthesis with Latent Diffusion Models", "value": "author_of"}, {"source": "Romain Lopez", "target": "AUTO-ENCODING VARIATIONAL BAYES", "value": "author_of"}, {"source": "Pierre Boyeau", "target": "AUTO-ENCODING VARIATIONAL BAYES", "value": "author_of"}, {"source": "N. Yosef", "target": "AUTO-ENCODING VARIATIONAL BAYES", "value": "author_of"}, {"source": "Michael I. Jordan", "target": "AUTO-ENCODING VARIATIONAL BAYES", "value": "author_of"}, {"source": "J. Regier", "target": "AUTO-ENCODING VARIATIONAL BAYES", "value": "author_of"}, {"source": "Richard Zhang", "target": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric", "value": "author_of"}, {"source": "Phillip Isola", "target": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric", "value": "author_of"}, {"source": "Alexei A. Efros", "target": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric", "value": "author_of"}, {"source": "Eli Shechtman", "target": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric", "value": "author_of"}, {"source": "Oliver Wang", "target": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric", "value": "author_of"}, {"source": "Pei Sun", "target": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "value": "author_of"}, {"source": "Henrik Kretzschmar", "target": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "value": "author_of"}, {"source": "Xerxes Dotiwalla", "target": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "value": "author_of"}, {"source": "Aurelien Chouard", "target": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "value": "author_of"}, {"source": "Vijaysai Patnaik", "target": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "value": "author_of"}, {"source": "Paul Tsui", "target": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "value": "author_of"}, {"source": "James Guo", "target": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "value": "author_of"}, {"source": "Yin Zhou", "target": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "value": "author_of"}, {"source": "Yuning Chai", "target": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "value": "author_of"}, {"source": "Benjamin Caine", "target": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "value": "author_of"}, {"source": "Vijay Vasudevan", "target": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "value": "author_of"}, {"source": "Wei Han", "target": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "value": "author_of"}, {"source": "Jiquan Ngiam", "target": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "value": "author_of"}, {"source": "Hang Zhao", "target": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "value": "author_of"}, {"source": "Aleksei Timofeev", "target": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "value": "author_of"}, {"source": "Scott Ettinger", "target": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "value": "author_of"}, {"source": "Maxim Krivokon", "target": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "value": "author_of"}, {"source": "Amy Gao", "target": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "value": "author_of"}, {"source": "Aditya Joshi", "target": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "value": "author_of"}, {"source": "Sheng Zhao", "target": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "value": "author_of"}, {"source": "Shuyang Cheng", "target": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "value": "author_of"}, {"source": "Yu Zhang", "target": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "value": "author_of"}, {"source": "Jonathon Shlens", "target": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "value": "author_of"}, {"source": "Zhifeng Chen", "target": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "value": "author_of"}, {"source": "Dragomir Anguelov", "target": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "value": "author_of"}, {"source": "Gilad Cohen", "target": "Generative Adversarial Networks", "value": "author_of"}, {"source": "Raja Giryes", "target": "Generative Adversarial Networks", "value": "author_of"}, {"source": "Zekai Zhang", "target": "Generalization of Diffusion Models Arises with a Balanced Representation Space", "value": "author_of"}, {"source": "Xiao Li", "target": "Generalization of Diffusion Models Arises with a Balanced Representation Space", "value": "author_of"}, {"source": "Xiang Li", "target": "Generalization of Diffusion Models Arises with a Balanced Representation Space", "value": "author_of"}, {"source": "Lianghe Shi", "target": "Generalization of Diffusion Models Arises with a Balanced Representation Space", "value": "author_of"}, {"source": "Meng Wu", "target": "Generalization of Diffusion Models Arises with a Balanced Representation Space", "value": "author_of"}, {"source": "Molei Tao", "target": "Generalization of Diffusion Models Arises with a Balanced Representation Space", "value": "author_of"}, {"source": "Qing Qu", "target": "Generalization of Diffusion Models Arises with a Balanced Representation Space", "value": "author_of"}, {"source": "Donglin Yang", "target": "Stable Velocity: A Variance Perspective on Flow Matching", "value": "author_of"}, {"source": "Yongxing Zhang", "target": "Stable Velocity: A Variance Perspective on Flow Matching", "value": "author_of"}, {"source": "Xin Yu", "target": "Stable Velocity: A Variance Perspective on Flow Matching", "value": "author_of"}, {"source": "Liang Hou", "target": "Stable Velocity: A Variance Perspective on Flow Matching", "value": "author_of"}, {"source": "Xin Tao", "target": "Stable Velocity: A Variance Perspective on Flow Matching", "value": "author_of"}, {"source": "Pengfei Wan", "target": "Stable Velocity: A Variance Perspective on Flow Matching", "value": "author_of"}, {"source": "Xiaojuan Qi", "target": "Stable Velocity: A Variance Perspective on Flow Matching", "value": "author_of"}, {"source": "Renjie Liao", "target": "Stable Velocity: A Variance Perspective on Flow Matching", "value": "author_of"}, {"source": "Ram\u00f3n Calvo-Gonz\u00e1lez", "target": "Laminating Representation Autoencoders for Efficient Diffusion", "value": "author_of"}, {"source": "Fran\u00e7ois Fleuret", "target": "Laminating Representation Autoencoders for Efficient Diffusion", "value": "author_of"}, {"source": "Yao Teng", "target": "Adaptive 1D Video Diffusion Autoencoder", "value": "author_of"}, {"source": "Minxuan Lin", "target": "Adaptive 1D Video Diffusion Autoencoder", "value": "author_of"}, {"source": "Xian Liu", "target": "Adaptive 1D Video Diffusion Autoencoder", "value": "author_of"}, {"source": "Shuai Wang", "target": "Adaptive 1D Video Diffusion Autoencoder", "value": "author_of"}, {"source": "Xiao Yang", "target": "Adaptive 1D Video Diffusion Autoencoder", "value": "author_of"}, {"source": "Xihui Liu", "target": "Adaptive 1D Video Diffusion Autoencoder", "value": "author_of"}, {"source": "Nicolas Sereyjol-Garros", "target": "Test-Time Conditioning with Representation-Aligned Visual Features", "value": "author_of"}, {"source": "Ellington Kirby", "target": "Test-Time Conditioning with Representation-Aligned Visual Features", "value": "author_of"}, {"source": "Victor Letzelter", "target": "Test-Time Conditioning with Representation-Aligned Visual Features", "value": "author_of"}, {"source": "Victor Besnier", "target": "Test-Time Conditioning with Representation-Aligned Visual Features", "value": "author_of"}, {"source": "Nermin Samet", "target": "Test-Time Conditioning with Representation-Aligned Visual Features", "value": "author_of"}, {"source": "Ana Davila", "target": "Bio-inspired fine-tuning for selective transfer learning in image classification", "value": "author_of"}, {"source": "Jacinto Colan", "target": "Bio-inspired fine-tuning for selective transfer learning in image classification", "value": "author_of"}, {"source": "Yasuhisa Hasegawa", "target": "Bio-inspired fine-tuning for selective transfer learning in image classification", "value": "author_of"}, {"source": "Gao Huang", "target": "Densely Connected Convolutional Networks", "value": "author_of"}, {"source": "Zhuang Liu", "target": "Densely Connected Convolutional Networks", "value": "author_of"}, {"source": "Laurens van der Maaten", "target": "Densely Connected Convolutional Networks", "value": "author_of"}, {"source": "Kilian Q. Weinberger", "target": "Densely Connected Convolutional Networks", "value": "author_of"}, {"source": "Zhongyu Zeng", "target": "An extratropical cyclone center location method on satellite images based on transfer learning", "value": "author_of"}, {"source": "Xuan Peng", "target": "An extratropical cyclone center location method on satellite images based on transfer learning", "value": "author_of"}, {"source": "J. B. Awotunde", "target": "An Advanced Convolutional Neural Network Architecture Utilizing Transfer Learning for Melanoma Detection", "value": "author_of"}, {"source": "Korede Israel Adeyanju", "target": "An Advanced Convolutional Neural Network Architecture Utilizing Transfer Learning for Melanoma Detection", "value": "author_of"}, {"source": "Kehinde Elisha Akerele", "target": "An Advanced Convolutional Neural Network Architecture Utilizing Transfer Learning for Melanoma Detection", "value": "author_of"}, {"source": "Oluwatobi Akinlade", "target": "An Advanced Convolutional Neural Network Architecture Utilizing Transfer Learning for Melanoma Detection", "value": "author_of"}, {"source": "S. Folorunso", "target": "An Advanced Convolutional Neural Network Architecture Utilizing Transfer Learning for Melanoma Detection", "value": "author_of"}, {"source": "S. Ajagbe", "target": "An Advanced Convolutional Neural Network Architecture Utilizing Transfer Learning for Melanoma Detection", "value": "author_of"}, {"source": "Ana Davila", "target": "Adaptive Transfer Learning for Surgical Tool Presence Detection in Laparoscopic Videos Through Gradual Freezing Fine\u2010Tuning", "value": "author_of"}, {"source": "Jacinto Colan", "target": "Adaptive Transfer Learning for Surgical Tool Presence Detection in Laparoscopic Videos Through Gradual Freezing Fine\u2010Tuning", "value": "author_of"}, {"source": "Yasuhisa Hasegawa", "target": "Adaptive Transfer Learning for Surgical Tool Presence Detection in Laparoscopic Videos Through Gradual Freezing Fine\u2010Tuning", "value": "author_of"}, {"source": "Subham Sharma", "target": "VGG Induced Deep Hand Sign Language Detection", "value": "author_of"}, {"source": "Sharmila Subudhi", "target": "VGG Induced Deep Hand Sign Language Detection", "value": "author_of"}, {"source": "Camillo Lugaresi", "target": "MediaPipe: A Framework for Building Perception Pipelines", "value": "author_of"}, {"source": "Jiuqiang Tang", "target": "MediaPipe: A Framework for Building Perception Pipelines", "value": "author_of"}, {"source": "Hadon Nash", "target": "MediaPipe: A Framework for Building Perception Pipelines", "value": "author_of"}, {"source": "Chris McClanahan", "target": "MediaPipe: A Framework for Building Perception Pipelines", "value": "author_of"}, {"source": "Esha Uboweja", "target": "MediaPipe: A Framework for Building Perception Pipelines", "value": "author_of"}, {"source": "Michael Hays", "target": "MediaPipe: A Framework for Building Perception Pipelines", "value": "author_of"}, {"source": "Fan Zhang", "target": "MediaPipe: A Framework for Building Perception Pipelines", "value": "author_of"}, {"source": "Chuo-Ling Chang", "target": "MediaPipe: A Framework for Building Perception Pipelines", "value": "author_of"}, {"source": "Ming Guang Yong", "target": "MediaPipe: A Framework for Building Perception Pipelines", "value": "author_of"}, {"source": "Juhyun Lee", "target": "MediaPipe: A Framework for Building Perception Pipelines", "value": "author_of"}, {"source": "Wan-Teh Chang", "target": "MediaPipe: A Framework for Building Perception Pipelines", "value": "author_of"}, {"source": "Wei Hua", "target": "MediaPipe: A Framework for Building Perception Pipelines", "value": "author_of"}, {"source": "Manfred Georg", "target": "MediaPipe: A Framework for Building Perception Pipelines", "value": "author_of"}, {"source": "Matthias Grundmann", "target": "MediaPipe: A Framework for Building Perception Pipelines", "value": "author_of"}, {"source": "Cem Keskin", "target": "Hand Pose Estimation and Hand Shape Classification Using Multi-layered Randomized Decision Forests", "value": "author_of"}, {"source": "Mustafa Furkan K\u0131ra\u00e7", "target": "Hand Pose Estimation and Hand Shape Classification Using Multi-layered Randomized Decision Forests", "value": "author_of"}, {"source": "Yunus Emre Kara", "target": "Hand Pose Estimation and Hand Shape Classification Using Multi-layered Randomized Decision Forests", "value": "author_of"}, {"source": "L. Akarun", "target": "Hand Pose Estimation and Hand Shape Classification Using Multi-layered Randomized Decision Forests", "value": "author_of"}, {"source": "S. P. Priyal", "target": "A robust static hand gesture recognition system using geometry based normalizations and Krawtchouk moments", "value": "author_of"}, {"source": "P. Bora", "target": "A robust static hand gesture recognition system using geometry based normalizations and Krawtchouk moments", "value": "author_of"}, {"source": "Octavian Dudas", "target": "Hand signal classification system for sign language communication in Virtual Reality", "value": "author_of"}, {"source": "C. Nandra", "target": "Hand signal classification system for sign language communication in Virtual Reality", "value": "author_of"}, {"source": "C. Mocan", "target": "Hand signal classification system for sign language communication in Virtual Reality", "value": "author_of"}, {"source": "D. Gorgan", "target": "Hand signal classification system for sign language communication in Virtual Reality", "value": "author_of"}, {"source": "Avinash Dhiran", "target": "Real-Time Static Hand Sign Recognition System using MediaPipe and Fully Connected Neural Network", "value": "author_of"}, {"source": "Anurag Kumbhare", "target": "Real-Time Static Hand Sign Recognition System using MediaPipe and Fully Connected Neural Network", "value": "author_of"}, {"source": "Achal Patil", "target": "Real-Time Static Hand Sign Recognition System using MediaPipe and Fully Connected Neural Network", "value": "author_of"}, {"source": "Mrugank Vichare", "target": "Real-Time Static Hand Sign Recognition System using MediaPipe and Fully Connected Neural Network", "value": "author_of"}, {"source": "Dhananjay Patel", "target": "Real-Time Static Hand Sign Recognition System using MediaPipe and Fully Connected Neural Network", "value": "author_of"}, {"source": "Saransh Mishra", "target": "Real-Time Gesture Recognition to Aid Communication in Children with Motor Impairments", "value": "author_of"}, {"source": "Pavan Nair", "target": "Real-Time Gesture Recognition to Aid Communication in Children with Motor Impairments", "value": "author_of"}, {"source": "Pushpalatha M", "target": "Real-Time Gesture Recognition to Aid Communication in Children with Motor Impairments", "value": "author_of"}, {"source": "Poornima S", "target": "Real-Time Gesture Recognition to Aid Communication in Children with Motor Impairments", "value": "author_of"}, {"source": "A. Krizhevsky", "target": "Learning Multiple Layers of Features from Tiny Images", "value": "author_of"}, {"source": "Ross Girshick", "target": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation", "value": "author_of"}, {"source": "Jeff Donahue", "target": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation", "value": "author_of"}, {"source": "Trevor Darrell", "target": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation", "value": "author_of"}, {"source": "Jitendra Malik", "target": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation", "value": "author_of"}, {"source": "Shansong Liu", "target": "MuMu-LLaMA: Multi-modal music understanding and generation via large language models", "value": "author_of"}, {"source": "Atin Sakkeer Hussain", "target": "MuMu-LLaMA: Multi-modal music understanding and generation via large language models", "value": "author_of"}, {"source": "Qilong Wu", "target": "MuMu-LLaMA: Multi-modal music understanding and generation via large language models", "value": "author_of"}, {"source": "Chenshuo Sun", "target": "MuMu-LLaMA: Multi-modal music understanding and generation via large language models", "value": "author_of"}, {"source": "Ying Shan", "target": "MuMu-LLaMA: Multi-modal music understanding and generation via large language models", "value": "author_of"}, {"source": "Jiahang Tu", "target": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy", "value": "author_of"}, {"source": "Ye Li", "target": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy", "value": "author_of"}, {"source": "Yiming Wu", "target": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy", "value": "author_of"}, {"source": "Hanbin Zhao", "target": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy", "value": "author_of"}, {"source": "Chao Zhang", "target": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy", "value": "author_of"}, {"source": "Hui Qian", "target": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy", "value": "author_of"}, {"source": "Haotian Lv", "target": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism", "value": "author_of"}, {"source": "Yuhui Zhang", "target": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism", "value": "author_of"}, {"source": "Jiangbo Dai", "target": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism", "value": "author_of"}, {"source": "Hanli Wu", "target": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism", "value": "author_of"}, {"source": "Jiaji Wang", "target": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism", "value": "author_of"}, {"source": "Dawei Wang", "target": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism", "value": "author_of"}, {"source": "Mingxin Li", "target": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking", "value": "author_of"}, {"source": "Yanzhao Zhang", "target": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking", "value": "author_of"}, {"source": "Dingkun Long", "target": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking", "value": "author_of"}, {"source": "Keqin Chen", "target": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking", "value": "author_of"}, {"source": "Sibo Song", "target": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking", "value": "author_of"}, {"source": "Shuai Bai", "target": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking", "value": "author_of"}, {"source": "Zhibo Yang", "target": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking", "value": "author_of"}, {"source": "Pengjun Xie", "target": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking", "value": "author_of"}, {"source": "An Yang", "target": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking", "value": "author_of"}, {"source": "Dayiheng Liu", "target": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking", "value": "author_of"}, {"source": "Jingren Zhou", "target": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking", "value": "author_of"}, {"source": "Junyang Lin", "target": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking", "value": "author_of"}, {"source": "Mingyue Chen", "target": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness", "value": "author_of"}, {"source": "Xin Liao", "target": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness", "value": "author_of"}, {"source": "Han Fang", "target": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness", "value": "author_of"}, {"source": "Jinlin Guo", "target": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness", "value": "author_of"}, {"source": "Yanxiang Chen", "target": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness", "value": "author_of"}, {"source": "Xiaoshuai Wu", "target": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness", "value": "author_of"}, {"source": "A. Dempster", "target": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper", "value": "author_of"}, {"source": "N. Laird", "target": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper", "value": "author_of"}, {"source": "D. Rubin", "target": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper", "value": "author_of"}, {"source": "L. Maaten", "target": "Visualizing Data using t-SNE", "value": "author_of"}, {"source": "Geoffrey E. Hinton", "target": "Visualizing Data using t-SNE", "value": "author_of"}, {"source": "Jacy Reese Anthis", "target": "LLM Social Simulations Are a Promising Research Method", "value": "author_of"}, {"source": "Ryan Liu", "target": "LLM Social Simulations Are a Promising Research Method", "value": "author_of"}, {"source": "Sean M. Richardson", "target": "LLM Social Simulations Are a Promising Research Method", "value": "author_of"}, {"source": "Austin C. Kozlowski", "target": "LLM Social Simulations Are a Promising Research Method", "value": "author_of"}, {"source": "Bernard Koch", "target": "LLM Social Simulations Are a Promising Research Method", "value": "author_of"}, {"source": "James Evans", "target": "LLM Social Simulations Are a Promising Research Method", "value": "author_of"}, {"source": "Erik Brynjolfsson", "target": "LLM Social Simulations Are a Promising Research Method", "value": "author_of"}, {"source": "Michael Bernstein", "target": "LLM Social Simulations Are a Promising Research Method", "value": "author_of"}, {"source": "Zhiwen Xiao", "target": "Federated Contrastive Learning With Feature-Based Distillation for Human Activity Recognition", "value": "author_of"}, {"source": "Huagang Tong", "target": "Federated Contrastive Learning With Feature-Based Distillation for Human Activity Recognition", "value": "author_of"}, {"source": "Runqian Wang", "target": "Diffuse and Disperse: Image Generation with Representation Regularization", "value": "author_of"}, {"source": "Kaiming He", "target": "Diffuse and Disperse: Image Generation with Representation Regularization", "value": "author_of"}, {"source": "Ibomoiye Domor Mienye", "target": "Deep Autoencoder Neural Networks: A Comprehensive Review and New Perspectives", "value": "author_of"}, {"source": "Theo G. Swart", "target": "Deep Autoencoder Neural Networks: A Comprehensive Review and New Perspectives", "value": "author_of"}, {"source": "Jusheng Zhang", "target": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems", "value": "author_of"}, {"source": "Zimeng Huang", "target": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems", "value": "author_of"}, {"source": "Yijia Fan", "target": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems", "value": "author_of"}, {"source": "Ningyuan Liu", "target": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems", "value": "author_of"}, {"source": "Mingyan Li", "target": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems", "value": "author_of"}, {"source": "Zhuojie Yang", "target": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems", "value": "author_of"}, {"source": "Jiawei Yao", "target": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems", "value": "author_of"}, {"source": "Jian Wang", "target": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems", "value": "author_of"}, {"source": "Keze Wang", "target": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems", "value": "author_of"}, {"source": "Olaf Ronneberger", "target": "U-Net: Convolutional Networks for Biomedical Image Segmentation", "value": "author_of"}, {"source": "Philipp Fischer", "target": "U-Net: Convolutional Networks for Biomedical Image Segmentation", "value": "author_of"}, {"source": "Thomas Brox", "target": "U-Net: Convolutional Networks for Biomedical Image Segmentation", "value": "author_of"}, {"source": "Tianwei Yin", "target": "Improved Distribution Matching Distillation for Fast Image Synthesis", "value": "author_of"}, {"source": "Micha\u00ebl Gharbi", "target": "Improved Distribution Matching Distillation for Fast Image Synthesis", "value": "author_of"}, {"source": "Taesung Park", "target": "Improved Distribution Matching Distillation for Fast Image Synthesis", "value": "author_of"}, {"source": "Richard Zhang", "target": "Improved Distribution Matching Distillation for Fast Image Synthesis", "value": "author_of"}, {"source": "Eli Shechtman", "target": "Improved Distribution Matching Distillation for Fast Image Synthesis", "value": "author_of"}, {"source": "Fredo Durand", "target": "Improved Distribution Matching Distillation for Fast Image Synthesis", "value": "author_of"}, {"source": "William T. Freeman", "target": "Improved Distribution Matching Distillation for Fast Image Synthesis", "value": "author_of"}, {"source": "Axel Sauer", "target": "Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation", "value": "author_of"}, {"source": "Frederic Boesel", "target": "Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation", "value": "author_of"}, {"source": "Tim Dockhorn", "target": "Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation", "value": "author_of"}, {"source": "Andreas Blattmann", "target": "Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation", "value": "author_of"}, {"source": "Patrick Esser", "target": "Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation", "value": "author_of"}, {"source": "Robin Rombach", "target": "Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation", "value": "author_of"}, {"source": "Takuya Akiba", "target": "Evolutionary optimization of model merging recipes", "value": "author_of"}, {"source": "Makoto Shing", "target": "Evolutionary optimization of model merging recipes", "value": "author_of"}, {"source": "Yujin Tang", "target": "Evolutionary optimization of model merging recipes", "value": "author_of"}, {"source": "Qi Sun", "target": "Evolutionary optimization of model merging recipes", "value": "author_of"}, {"source": "David Ha", "target": "Evolutionary optimization of model merging recipes", "value": "author_of"}, {"source": "Tianwei Yin", "target": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models", "value": "author_of"}, {"source": "Qiang Zhang", "target": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models", "value": "author_of"}, {"source": "Richard Zhang", "target": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models", "value": "author_of"}, {"source": "William T. Freeman", "target": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models", "value": "author_of"}, {"source": "Fredo Durand", "target": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models", "value": "author_of"}, {"source": "Eli Shechtman", "target": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models", "value": "author_of"}, {"source": "Xun Huang", "target": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models", "value": "author_of"}, {"source": "Zinan Guo", "target": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment", "value": "author_of"}, {"source": "Yanze Wu", "target": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment", "value": "author_of"}, {"source": "Zhuowei Chen", "target": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment", "value": "author_of"}, {"source": "Lang Chen", "target": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment", "value": "author_of"}, {"source": "Peng Zhang", "target": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment", "value": "author_of"}, {"source": "Qian He", "target": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment", "value": "author_of"}, {"source": "Jonathan Ho", "target": "Denoising Diffusion Probabilistic Models", "value": "author_of"}, {"source": "Ajay Jain", "target": "Denoising Diffusion Probabilistic Models", "value": "author_of"}, {"source": "Pieter Abbeel", "target": "Denoising Diffusion Probabilistic Models", "value": "author_of"}, {"source": "Weijie Kong", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Qi Tian", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Zijian Zhang", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Rox Min", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Zuozhuo Dai", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Jin Zhou", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Jiangfeng Xiong", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Xin Li", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Bo Wu", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Jianwei Zhang", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Kathrina Wu", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Qin Lin", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Junkun Yuan", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Yanxin Long", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Aladdin Wang", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Andong Wang", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Changlin Li", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Duojun Huang", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Fang Yang", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Hao Tan", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Hongmei Wang", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Jacob Song", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Jiawang Bai", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Jianbing Wu", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Jinbao Xue", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Joey Wang", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Kai Wang", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Mengyang Liu", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Pengyu Li", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Shuai Li", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Weiyan Wang", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Wenqing Yu", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Xinchi Deng", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Yang Li", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Yi Chen", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Yutao Cui", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Yuanbo Peng", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Zhentao Yu", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Zhiyu He", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Zhiyong Xu", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Zixiang Zhou", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Zunnan Xu", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Yangyu Tao", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Qinglin Lu", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Songtao Liu", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Dax Zhou", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Hongfa Wang", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Yong Yang", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Di Wang", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Yuhong Liu", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Jie Jiang", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Caesar Zhong", "target": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "value": "author_of"}, {"source": "Jianwen Jiang", "target": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency", "value": "author_of"}, {"source": "Chao Liang", "target": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency", "value": "author_of"}, {"source": "Jiaqi Yang", "target": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency", "value": "author_of"}, {"source": "Gaojie Lin", "target": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency", "value": "author_of"}, {"source": "Tianyun Zhong", "target": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency", "value": "author_of"}, {"source": "Yanbo Zheng", "target": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency", "value": "author_of"}, {"source": "Gaojie Lin", "target": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models", "value": "author_of"}, {"source": "Jianwen Jiang", "target": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models", "value": "author_of"}, {"source": "Jiaqi Yang", "target": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models", "value": "author_of"}, {"source": "Zerong Zheng", "target": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models", "value": "author_of"}, {"source": "Chao Liang", "target": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models", "value": "author_of"}, {"source": "Jiahao Cui", "target": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation", "value": "author_of"}, {"source": "Hui Li", "target": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation", "value": "author_of"}, {"source": "Yao Yao", "target": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation", "value": "author_of"}, {"source": "Hao Zhu", "target": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation", "value": "author_of"}, {"source": "Hanlin Shang", "target": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation", "value": "author_of"}, {"source": "Kaihui Cheng", "target": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation", "value": "author_of"}, {"source": "Hang Zhou", "target": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation", "value": "author_of"}, {"source": "Siyu Zhu", "target": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation", "value": "author_of"}, {"source": "Jingdong Wang", "target": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation", "value": "author_of"}, {"source": "Rang Meng", "target": "EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation", "value": "author_of"}, {"source": "Xingyu Zhang", "target": "EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation", "value": "author_of"}, {"source": "Yuming Li", "target": "EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation", "value": "author_of"}, {"source": "Chenguang Ma", "target": "EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation", "value": "author_of"}, {"source": "Tsung-Yi Lin", "target": "Feature Pyramid Networks for Object Detection", "value": "author_of"}, {"source": "Piotr Doll\u00e1r", "target": "Feature Pyramid Networks for Object Detection", "value": "author_of"}, {"source": "Ross Girshick", "target": "Feature Pyramid Networks for Object Detection", "value": "author_of"}, {"source": "Kaiming He", "target": "Feature Pyramid Networks for Object Detection", "value": "author_of"}, {"source": "Bharath Hariharan", "target": "Feature Pyramid Networks for Object Detection", "value": "author_of"}, {"source": "Serge Belongie", "target": "Feature Pyramid Networks for Object Detection", "value": "author_of"}, {"source": "Kyunghyun Cho", "target": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation", "value": "author_of"}, {"source": "Bart van Merrienboer", "target": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation", "value": "author_of"}, {"source": "Caglar Gulcehre", "target": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation", "value": "author_of"}, {"source": "Dzmitry Bahdanau", "target": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation", "value": "author_of"}, {"source": "Fethi Bougares", "target": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation", "value": "author_of"}, {"source": "Holger Schwenk", "target": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation", "value": "author_of"}, {"source": "Yoshua Bengio", "target": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation", "value": "author_of"}, {"source": "Erfei Cui", "target": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving", "value": "author_of"}, {"source": "Wenhai Wang", "target": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving", "value": "author_of"}, {"source": "Zhiqi Li", "target": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving", "value": "author_of"}, {"source": "Jiangwei Xie", "target": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving", "value": "author_of"}, {"source": "Haoming Zou", "target": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving", "value": "author_of"}, {"source": "Hanming Deng", "target": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving", "value": "author_of"}, {"source": "Gen Luo", "target": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving", "value": "author_of"}, {"source": "Lewei Lu", "target": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving", "value": "author_of"}, {"source": "Xizhou Zhu", "target": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving", "value": "author_of"}, {"source": "Jifeng Dai", "target": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving", "value": "author_of"}, {"source": "Shenyuan Gao", "target": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability", "value": "author_of"}, {"source": "Jiazhi Yang", "target": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability", "value": "author_of"}, {"source": "Li Chen", "target": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability", "value": "author_of"}, {"source": "Kashyap Chitta", "target": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability", "value": "author_of"}, {"source": "Yihang Qiu", "target": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability", "value": "author_of"}, {"source": "Andreas Geiger", "target": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability", "value": "author_of"}, {"source": "Jun Zhang", "target": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability", "value": "author_of"}, {"source": "Hongyang Li", "target": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability", "value": "author_of"}, {"source": "Bencheng Liao", "target": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Shaoyu Chen", "target": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Haoran Yin", "target": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Bo Jiang", "target": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Cheng Wang", "target": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Sixu Yan", "target": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Xinbang Zhang", "target": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Xiangyu Li", "target": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Ying Zhang", "target": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Qian Zhang", "target": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Xinggang Wang", "target": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Bingyi Kang", "target": "How Far is Video Generation from World Model: A Physical Law Perspective", "value": "author_of"}, {"source": "Yang Yue", "target": "How Far is Video Generation from World Model: A Physical Law Perspective", "value": "author_of"}, {"source": "Rui Lu", "target": "How Far is Video Generation from World Model: A Physical Law Perspective", "value": "author_of"}, {"source": "Zhijie Lin", "target": "How Far is Video Generation from World Model: A Physical Law Perspective", "value": "author_of"}, {"source": "Yang Zhao", "target": "How Far is Video Generation from World Model: A Physical Law Perspective", "value": "author_of"}, {"source": "Kaixin Wang", "target": "How Far is Video Generation from World Model: A Physical Law Perspective", "value": "author_of"}, {"source": "Gao Huang", "target": "How Far is Video Generation from World Model: A Physical Law Perspective", "value": "author_of"}, {"source": "Jiashi Feng", "target": "How Far is Video Generation from World Model: A Physical Law Perspective", "value": "author_of"}, {"source": "Jyh-Jing Hwang", "target": "EMMA: End-to-End Multimodal Model for Autonomous Driving", "value": "author_of"}, {"source": "Runsheng Xu", "target": "EMMA: End-to-End Multimodal Model for Autonomous Driving", "value": "author_of"}, {"source": "Hubert Lin", "target": "EMMA: End-to-End Multimodal Model for Autonomous Driving", "value": "author_of"}, {"source": "Wei-Chih Hung", "target": "EMMA: End-to-End Multimodal Model for Autonomous Driving", "value": "author_of"}, {"source": "Jingwei Ji", "target": "EMMA: End-to-End Multimodal Model for Autonomous Driving", "value": "author_of"}, {"source": "Kristy Choi", "target": "EMMA: End-to-End Multimodal Model for Autonomous Driving", "value": "author_of"}, {"source": "Di Huang", "target": "EMMA: End-to-End Multimodal Model for Autonomous Driving", "value": "author_of"}, {"source": "Tong He", "target": "EMMA: End-to-End Multimodal Model for Autonomous Driving", "value": "author_of"}, {"source": "Paul Covington", "target": "EMMA: End-to-End Multimodal Model for Autonomous Driving", "value": "author_of"}, {"source": "Benjamin Sapp", "target": "EMMA: End-to-End Multimodal Model for Autonomous Driving", "value": "author_of"}, {"source": "Yin Zhou", "target": "EMMA: End-to-End Multimodal Model for Autonomous Driving", "value": "author_of"}, {"source": "James Guo", "target": "EMMA: End-to-End Multimodal Model for Autonomous Driving", "value": "author_of"}, {"source": "Dragomir Anguelov", "target": "EMMA: End-to-End Multimodal Model for Autonomous Driving", "value": "author_of"}, {"source": "Mingxing Tan", "target": "EMMA: End-to-End Multimodal Model for Autonomous Driving", "value": "author_of"}, {"source": "Tom B. Brown", "target": "Language Models are Few-Shot Learners", "value": "author_of"}, {"source": "Benjamin Mann", "target": "Language Models are Few-Shot Learners", "value": "author_of"}, {"source": "Nick Ryder", "target": "Language Models are Few-Shot Learners", "value": "author_of"}, {"source": "Melanie Subbiah", "target": "Language Models are Few-Shot Learners", "value": "author_of"}, {"source": "Jared Kaplan", "target": "Language Models are Few-Shot Learners", "value": "author_of"}, {"source": "Prafulla Dhariwal", "target": "Language Models are Few-Shot Learners", "value": "author_of"}, {"source": "Arvind Neelakantan", "target": "Language Models are Few-Shot Learners", "value": "author_of"}, {"source": "Pranav Shyam", "target": "Language Models are Few-Shot Learners", "value": "author_of"}, {"source": "Girish Sastry", "target": "Language Models are Few-Shot Learners", "value": "author_of"}, {"source": "Amanda Askell", "target": "Language Models are Few-Shot Learners", "value": "author_of"}, {"source": "Sandhini Agarwal", "target": "Language Models are Few-Shot Learners", "value": "author_of"}, {"source": "Ariel Herbert-Voss", "target": "Language Models are Few-Shot Learners", "value": "author_of"}, {"source": "Gretchen Krueger", "target": "Language Models are Few-Shot Learners", "value": "author_of"}, {"source": "Tom Henighan", "target": "Language Models are Few-Shot Learners", "value": "author_of"}, {"source": "Rewon Child", "target": "Language Models are Few-Shot Learners", "value": "author_of"}, {"source": "Aditya Ramesh", "target": "Language Models are Few-Shot Learners", "value": "author_of"}, {"source": "Daniel M. Ziegler", "target": "Language Models are Few-Shot Learners", "value": "author_of"}, {"source": "Jeffrey Wu", "target": "Language Models are Few-Shot Learners", "value": "author_of"}, {"source": "Clemens Winter", "target": "Language Models are Few-Shot Learners", "value": "author_of"}, {"source": "Christopher Hesse", "target": "Language Models are Few-Shot Learners", "value": "author_of"}, {"source": "Mark Chen", "target": "Language Models are Few-Shot Learners", "value": "author_of"}, {"source": "Eric Sigler", "target": "Language Models are Few-Shot Learners", "value": "author_of"}, {"source": "Mateusz Litwin", "target": "Language Models are Few-Shot Learners", "value": "author_of"}, {"source": "Scott Gray", "target": "Language Models are Few-Shot Learners", "value": "author_of"}, {"source": "Benjamin Chess", "target": "Language Models are Few-Shot Learners", "value": "author_of"}, {"source": "Jack Clark", "target": "Language Models are Few-Shot Learners", "value": "author_of"}, {"source": "Christopher Berner", "target": "Language Models are Few-Shot Learners", "value": "author_of"}, {"source": "Sam McCandlish", "target": "Language Models are Few-Shot Learners", "value": "author_of"}, {"source": "Alec Radford", "target": "Language Models are Few-Shot Learners", "value": "author_of"}, {"source": "Ilya Sutskever", "target": "Language Models are Few-Shot Learners", "value": "author_of"}, {"source": "Dario Amodei", "target": "Language Models are Few-Shot Learners", "value": "author_of"}, {"source": "Zhe Chen", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Weiyun Wang", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Yue Cao", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Yangzhou Liu", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Zhangwei Gao", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Erfei Cui", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Jinguo Zhu", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Shenglong Ye", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Hao Tian", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Zhaoyang Liu", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Lixin Gu", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Xuehui Wang", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Qingyun Li", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Yiming Ren", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Zixuan Chen", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Jiapeng Luo", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Jiahao Wang", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Tan Jiang", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Bo Wang", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Conghui He", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Botian Shi", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Xingcheng Zhang", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Han Lv", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Yi Wang", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Wenqi Shao", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Pei Chu", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Zhongying Tu", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Tong He", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Zhiyong Wu", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Huipeng Deng", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Jiaye Ge", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Kai Chen", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Kaipeng Zhang", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Limin Wang", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Min Dou", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Lewei Lu", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Xizhou Zhu", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Tong Lu", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Dahua Lin", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Yu Qiao", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Jifeng Dai", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Wenhai Wang", "target": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "value": "author_of"}, {"source": "Jinguo Zhu", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Weiyun Wang", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Zhe Chen", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Zhaoyang Liu", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Shenglong Ye", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Lixin Gu", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Hao Tian", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Yuchen Duan", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Weijie Su", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Jie Shao", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Zhangwei Gao", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Erfei Cui", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Xuehui Wang", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Yue Cao", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Yangzhou Liu", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Xingguang Wei", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Hongjie Zhang", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Haomin Wang", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Weiye Xu", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Hao Li", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Jiahao Wang", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Nianchen Deng", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Songze Li", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Yinan He", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Tan Jiang", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Jiapeng Luo", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Yi Wang", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Conghui He", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Botian Shi", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Xingcheng Zhang", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Wenqi Shao", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Junjun He", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Yingtong Xiong", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Wenwen Qu", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Peng Sun", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Penglong Jiao", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Han Lv", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Lijun Wu", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Kaipeng Zhang", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Huipeng Deng", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Jiaye Ge", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Kai Chen", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Limin Wang", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Min Dou", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Lewei Lu", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Xizhou Zhu", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Tong Lu", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Dahua Lin", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Yu Qiao", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Jifeng Dai", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Wenhai Wang", "target": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "value": "author_of"}, {"source": "Haodong Duan", "target": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Xinyu Fang", "target": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Junming Yang", "target": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Xiangyu Zhao", "target": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Yuxuan Qiao", "target": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Mo Li", "target": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Amit Agarwal", "target": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Zhe Chen", "target": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Lin Chen", "target": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Yuan Liu", "target": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Yubo Ma", "target": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Hailong Sun", "target": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Yifan Zhang", "target": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Shiyin Lu", "target": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Tack Hwa Wong", "target": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Weiyun Wang", "target": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Peiheng Zhou", "target": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Xiaozhe Li", "target": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Chaoyou Fu", "target": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Junbo Cui", "target": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Jixuan Chen", "target": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Enxin Song", "target": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Song Mao", "target": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Shengyuan Ding", "target": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Tianhao Liang", "target": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Zicheng Zhang", "target": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Xiaoyi Dong", "target": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Yuhang Zang", "target": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Pan Zhang", "target": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Jiaqi Wang", "target": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Dahua Lin", "target": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Kai Chen", "target": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Guowei Xu", "target": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step", "value": "author_of"}, {"source": "Peng Jin", "target": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step", "value": "author_of"}, {"source": "Ziang Wu", "target": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step", "value": "author_of"}, {"source": "Hao Li", "target": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step", "value": "author_of"}, {"source": "Yibing Song", "target": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step", "value": "author_of"}, {"source": "Lichao Sun", "target": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step", "value": "author_of"}, {"source": "Li Yuan", "target": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step", "value": "author_of"}, {"source": "Weiyun Wang", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Zhangwei Gao", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Lixin Gu", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Hengjun Pu", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Long Cui", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Xingguang Wei", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Zhaoyang Liu", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Linglin Jing", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Shenglong Ye", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Jie Shao", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Zhaokai Wang", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Zhe Chen", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Hongjie Zhang", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Ganlin Yang", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Haomin Wang", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Qi Wei", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Jinhui Yin", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Wenhao Li", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Erfei Cui", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Guanzhou Chen", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Zichen Ding", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Changyao Tian", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Zhenyu Wu", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Jingjing Xie", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Zehao Li", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Bowen Yang", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Yuchen Duan", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Xuehui Wang", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Zhi Hou", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Haoran Hao", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Tianyi Zhang", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Songze Li", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Xiangyu Zhao", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Haodong Duan", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Nianchen Deng", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Bin Fu", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Yinan He", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Yi Wang", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Conghui He", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Botian Shi", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Junjun He", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Yingtong Xiong", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Han Lv", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Lijun Wu", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Wenqi Shao", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Kaipeng Zhang", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Huipeng Deng", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Biqing Qi", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Jiaye Ge", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Qipeng Guo", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Wenwei Zhang", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Songyang Zhang", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Maosong Cao", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Junyao Lin", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Kexian Tang", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Jianfei Gao", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Haian Huang", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Yuzhe Gu", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Chengqi Lyu", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Huanze Tang", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Rui Wang", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Haijun Lv", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Wanli Ouyang", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Limin Wang", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Min Dou", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Xizhou Zhu", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Tong Lu", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Dahua Lin", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Jifeng Dai", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Weijie Su", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Bowen Zhou", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Kai Chen", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Yu Qiao", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Wenhai Wang", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "Gen Luo", "target": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "value": "author_of"}, {"source": "John Schulman", "target": "Proximal Policy Optimization Algorithms", "value": "author_of"}, {"source": "Filip Wolski", "target": "Proximal Policy Optimization Algorithms", "value": "author_of"}, {"source": "Prafulla Dhariwal", "target": "Proximal Policy Optimization Algorithms", "value": "author_of"}, {"source": "Alec Radford", "target": "Proximal Policy Optimization Algorithms", "value": "author_of"}, {"source": "Oleg Klimov", "target": "Proximal Policy Optimization Algorithms", "value": "author_of"}, {"source": "Jie Liu", "target": "Flow-GRPO: Training Flow Matching Models via Online RL", "value": "author_of"}, {"source": "Gongye Liu", "target": "Flow-GRPO: Training Flow Matching Models via Online RL", "value": "author_of"}, {"source": "Jiajun Liang", "target": "Flow-GRPO: Training Flow Matching Models via Online RL", "value": "author_of"}, {"source": "Yangguang Li", "target": "Flow-GRPO: Training Flow Matching Models via Online RL", "value": "author_of"}, {"source": "Jiaheng Liu", "target": "Flow-GRPO: Training Flow Matching Models via Online RL", "value": "author_of"}, {"source": "Xintao Wang", "target": "Flow-GRPO: Training Flow Matching Models via Online RL", "value": "author_of"}, {"source": "Pengfei Wan", "target": "Flow-GRPO: Training Flow Matching Models via Online RL", "value": "author_of"}, {"source": "Di Zhang", "target": "Flow-GRPO: Training Flow Matching Models via Online RL", "value": "author_of"}, {"source": "Wanli Ouyang", "target": "Flow-GRPO: Training Flow Matching Models via Online RL", "value": "author_of"}, {"source": "Zeyue Xue", "target": "DanceGRPO: Unleashing GRPO on Visual Generation", "value": "author_of"}, {"source": "Jie Wu", "target": "DanceGRPO: Unleashing GRPO on Visual Generation", "value": "author_of"}, {"source": "Yu Gao", "target": "DanceGRPO: Unleashing GRPO on Visual Generation", "value": "author_of"}, {"source": "Fangyuan Kong", "target": "DanceGRPO: Unleashing GRPO on Visual Generation", "value": "author_of"}, {"source": "Lingting Zhu", "target": "DanceGRPO: Unleashing GRPO on Visual Generation", "value": "author_of"}, {"source": "Mengzhao Chen", "target": "DanceGRPO: Unleashing GRPO on Visual Generation", "value": "author_of"}, {"source": "Zhiheng Liu", "target": "DanceGRPO: Unleashing GRPO on Visual Generation", "value": "author_of"}, {"source": "Wei Liu", "target": "DanceGRPO: Unleashing GRPO on Visual Generation", "value": "author_of"}, {"source": "Qiushan Guo", "target": "DanceGRPO: Unleashing GRPO on Visual Generation", "value": "author_of"}, {"source": "Weilin Huang", "target": "DanceGRPO: Unleashing GRPO on Visual Generation", "value": "author_of"}, {"source": "Ping Luo", "target": "DanceGRPO: Unleashing GRPO on Visual Generation", "value": "author_of"}, {"source": "Yu Gao", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Haoyuan Guo", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Tuyen Hoang", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Weilin Huang", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Lu Jiang", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Fangyuan Kong", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Huixia Li", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Jiashi Li", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Liang Li", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Xiaojie Li", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Xunsong Li", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Yifu Li", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Shanchuan Lin", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Zhijie Lin", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Jiawei Liu", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Shu Liu", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Xiaonan Nie", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Zhiwu Qing", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Yuxi Ren", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Li Sun", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Zhi Tian", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Rui Wang", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Sen Wang", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Guoqiang Wei", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Guohong Wu", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Jie Wu", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Ruiqi Xia", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Fei Xiao", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Xuefeng Xiao", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Jiangqiao Yan", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Ceyuan Yang", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Jianchao Yang", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Runkai Yang", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Tao Yang", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Yihang Yang", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Zilyu Ye", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Xuejiao Zeng", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Yan Zeng", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Heng Zhang", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Yang Zhao", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Xiaozheng Zheng", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Peihao Zhu", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Jiaxin Zou", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Feilong Zuo", "target": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "value": "author_of"}, {"source": "Guibin Chen", "target": "SkyReels-V2: Infinite-length Film Generative Model", "value": "author_of"}, {"source": "Dixuan Lin", "target": "SkyReels-V2: Infinite-length Film Generative Model", "value": "author_of"}, {"source": "Jiangping Yang", "target": "SkyReels-V2: Infinite-length Film Generative Model", "value": "author_of"}, {"source": "Chunze Lin", "target": "SkyReels-V2: Infinite-length Film Generative Model", "value": "author_of"}, {"source": "Junchen Zhu", "target": "SkyReels-V2: Infinite-length Film Generative Model", "value": "author_of"}, {"source": "Mingyuan Fan", "target": "SkyReels-V2: Infinite-length Film Generative Model", "value": "author_of"}, {"source": "Hao Zhang", "target": "SkyReels-V2: Infinite-length Film Generative Model", "value": "author_of"}, {"source": "Sheng Chen", "target": "SkyReels-V2: Infinite-length Film Generative Model", "value": "author_of"}, {"source": "Zheng Chen", "target": "SkyReels-V2: Infinite-length Film Generative Model", "value": "author_of"}, {"source": "Chengcheng Ma", "target": "SkyReels-V2: Infinite-length Film Generative Model", "value": "author_of"}, {"source": "Weiming Xiong", "target": "SkyReels-V2: Infinite-length Film Generative Model", "value": "author_of"}, {"source": "Wei Wang", "target": "SkyReels-V2: Infinite-length Film Generative Model", "value": "author_of"}, {"source": "Nuo Pang", "target": "SkyReels-V2: Infinite-length Film Generative Model", "value": "author_of"}, {"source": "Kang Kang", "target": "SkyReels-V2: Infinite-length Film Generative Model", "value": "author_of"}, {"source": "Zhiheng Xu", "target": "SkyReels-V2: Infinite-length Film Generative Model", "value": "author_of"}, {"source": "Yuzhe Jin", "target": "SkyReels-V2: Infinite-length Film Generative Model", "value": "author_of"}, {"source": "Yupeng Liang", "target": "SkyReels-V2: Infinite-length Film Generative Model", "value": "author_of"}, {"source": "Yubing Song", "target": "SkyReels-V2: Infinite-length Film Generative Model", "value": "author_of"}, {"source": "Peng Zhao", "target": "SkyReels-V2: Infinite-length Film Generative Model", "value": "author_of"}, {"source": "Boyuan Xu", "target": "SkyReels-V2: Infinite-length Film Generative Model", "value": "author_of"}, {"source": "Di Qiu", "target": "SkyReels-V2: Infinite-length Film Generative Model", "value": "author_of"}, {"source": "Debang Li", "target": "SkyReels-V2: Infinite-length Film Generative Model", "value": "author_of"}, {"source": "Zhengcong Fei", "target": "SkyReels-V2: Infinite-length Film Generative Model", "value": "author_of"}, {"source": "Yang Li", "target": "SkyReels-V2: Infinite-length Film Generative Model", "value": "author_of"}, {"source": "Yahui Zhou", "target": "SkyReels-V2: Infinite-length Film Generative Model", "value": "author_of"}, {"source": "Yibin Wang", "target": "Unified Reward Model for Multimodal Understanding and Generation", "value": "author_of"}, {"source": "Yuhang Zang", "target": "Unified Reward Model for Multimodal Understanding and Generation", "value": "author_of"}, {"source": "Hao Li", "target": "Unified Reward Model for Multimodal Understanding and Generation", "value": "author_of"}, {"source": "Cheng Jin", "target": "Unified Reward Model for Multimodal Understanding and Generation", "value": "author_of"}, {"source": "Jiaqi Wang", "target": "Unified Reward Model for Multimodal Understanding and Generation", "value": "author_of"}, {"source": "OpenAI", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Josh Achiam", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Steven Adler", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Sandhini Agarwal", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Lama Ahmad", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Ilge Akkaya", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Florencia Leoni Aleman", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Diogo Almeida", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Janko Altenschmidt", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Sam Altman", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Shyamal Anadkat", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Red Avila", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Igor Babuschkin", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Suchir Balaji", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Valerie Balcom", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Paul Baltescu", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Haiming Bao", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Mohammad Bavarian", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Jeff Belgum", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Irwan Bello", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Jake Berdine", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Gabriel Bernadett-Shapiro", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Christopher Berner", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Lenny Bogdonoff", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Oleg Boiko", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Madelaine Boyd", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Anna-Luisa Brakman", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Greg Brockman", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Tim Brooks", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Miles Brundage", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Kevin Button", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Trevor Cai", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Rosie Campbell", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Andrew Cann", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Brittany Carey", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Chelsea Carlson", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Rory Carmichael", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Brooke Chan", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Che Chang", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Fotis Chantzis", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Derek Chen", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Sully Chen", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Ruby Chen", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Jason Chen", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Mark Chen", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Ben Chess", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Chester Cho", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Casey Chu", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Hyung Won Chung", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Dave Cummings", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Jeremiah Currier", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Yunxing Dai", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Cory Decareaux", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Thomas Degry", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Noah Deutsch", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Damien Deville", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Arka Dhar", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "David Dohan", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Steve Dowling", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Sheila Dunning", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Adrien Ecoffet", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Atty Eleti", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Tyna Eloundou", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "David Farhi", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Liam Fedus", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Niko Felix", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Sim\u00f3n Posada Fishman", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Juston Forte", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Isabella Fulford", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Leo Gao", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Elie Georges", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Christian Gibson", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Vik Goel", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Tarun Gogineni", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Gabriel Goh", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Rapha Gontijo-Lopes", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Jonathan Gordon", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Morgan Grafstein", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Scott Gray", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Ryan Greene", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Joshua Gross", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Shixiang Shane Gu", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Yufei Guo", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Chris Hallacy", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Jesse Han", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Jeff Harris", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Yuchen He", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Mike Heaton", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Johannes Heidecke", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Chris Hesse", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Alan Hickey", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Wade Hickey", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Peter Hoeschele", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Brandon Houghton", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Kenny Hsu", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Shengli Hu", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Xin Hu", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Joost Huizinga", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Shantanu Jain", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Shawn Jain", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Joanne Jang", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Angela Jiang", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Roger Jiang", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Haozhun Jin", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Denny Jin", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Shino Jomoto", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Billie Jonn", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Heewoo Jun", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Tomer Kaftan", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "\u0141ukasz Kaiser", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Ali Kamali", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Ingmar Kanitscheider", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Nitish Shirish Keskar", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Tabarak Khan", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Logan Kilpatrick", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Jong Wook Kim", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Christina Kim", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Yongjik Kim", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Jan Hendrik Kirchner", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Jamie Kiros", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Matt Knight", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Daniel Kokotajlo", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "\u0141ukasz Kondraciuk", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Andrew Kondrich", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Aris Konstantinidis", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Kyle Kosic", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Gretchen Krueger", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Vishal Kuo", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Michael Lampe", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Ikai Lan", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Teddy Lee", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Jan Leike", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Jade Leung", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Daniel Levy", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Chak Ming Li", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Rachel Lim", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Molly Lin", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Stephanie Lin", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Mateusz Litwin", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Theresa Lopez", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Ryan Lowe", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Patricia Lue", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Anna Makanju", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Kim Malfacini", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Sam Manning", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Todor Markov", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Yaniv Markovski", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Bianca Martin", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Katie Mayer", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Andrew Mayne", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Bob McGrew", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Scott Mayer McKinney", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Christine McLeavey", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Paul McMillan", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Jake McNeil", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "David Medina", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Aalok Mehta", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Jacob Menick", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Luke Metz", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Andrey Mishchenko", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Pamela Mishkin", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Vinnie Monaco", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Evan Morikawa", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Daniel Mossing", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Tong Mu", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Mira Murati", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Oleg Murk", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "David M\u00e9ly", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Ashvin Nair", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Reiichiro Nakano", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Rajeev Nayak", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Arvind Neelakantan", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Richard Ngo", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Hyeonwoo Noh", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Long Ouyang", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Cullen O'Keefe", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Jakub Pachocki", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Alex Paino", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Joe Palermo", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Ashley Pantuliano", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Giambattista Parascandolo", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Joel Parish", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Emy Parparita", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Alex Passos", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Mikhail Pavlov", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Andrew Peng", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Adam Perelman", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Filipe de Avila Belbute Peres", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Michael Petrov", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Henrique Ponde de Oliveira Pinto", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Michael", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Pokorny", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Michelle Pokrass", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Vitchyr H. Pong", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Tolly Powell", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Alethea Power", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Boris Power", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Elizabeth Proehl", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Raul Puri", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Alec Radford", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Jack Rae", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Aditya Ramesh", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Cameron Raymond", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Francis Real", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Kendra Rimbach", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Carl Ross", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Bob Rotsted", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Henri Roussez", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Nick Ryder", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Mario Saltarelli", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Ted Sanders", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Shibani Santurkar", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Girish Sastry", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Heather Schmidt", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "David Schnurr", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "John Schulman", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Daniel Selsam", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Kyla Sheppard", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Toki Sherbakov", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Jessica Shieh", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Sarah Shoker", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Pranav Shyam", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Szymon Sidor", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Eric Sigler", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Maddie Simens", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Jordan Sitkin", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Katarina Slama", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Ian Sohl", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Benjamin Sokolowsky", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Yang Song", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Natalie Staudacher", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Felipe Petroski Such", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Natalie Summers", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Ilya Sutskever", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Jie Tang", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Nikolas Tezak", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Madeleine B. Thompson", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Phil Tillet", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Amin Tootoonchian", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Elizabeth Tseng", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Preston Tuggle", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Nick Turley", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Jerry Tworek", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Juan Felipe Cer\u00f3n Uribe", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Andrea Vallone", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Arun Vijayvergiya", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Chelsea Voss", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Carroll Wainwright", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Justin Jay Wang", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Alvin Wang", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Ben Wang", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Jonathan Ward", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Jason Wei", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "CJ Weinmann", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Akila Welihinda", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Peter Welinder", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Jiayi Weng", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Lilian Weng", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Matt Wiethoff", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Dave Willner", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Clemens Winter", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Samuel Wolrich", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Hannah Wong", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Lauren Workman", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Sherwin Wu", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Jeff Wu", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Michael Wu", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Kai Xiao", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Tao Xu", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Sarah Yoo", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Kevin Yu", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Qiming Yuan", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Wojciech Zaremba", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Rowan Zellers", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Chong Zhang", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Marvin Zhang", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Shengjia Zhao", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Tianhao Zheng", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Juntang Zhuang", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "William Zhuk", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Barret Zoph", "target": "GPT-4 Technical Report", "value": "author_of"}, {"source": "Hugo Touvron", "target": "LLaMA: Open and Efficient Foundation Language Models", "value": "author_of"}, {"source": "Thibaut Lavril", "target": "LLaMA: Open and Efficient Foundation Language Models", "value": "author_of"}, {"source": "Gautier Izacard", "target": "LLaMA: Open and Efficient Foundation Language Models", "value": "author_of"}, {"source": "Xavier Martinet", "target": "LLaMA: Open and Efficient Foundation Language Models", "value": "author_of"}, {"source": "Marie-Anne Lachaux", "target": "LLaMA: Open and Efficient Foundation Language Models", "value": "author_of"}, {"source": "Timoth\u00e9e Lacroix", "target": "LLaMA: Open and Efficient Foundation Language Models", "value": "author_of"}, {"source": "Baptiste Rozi\u00e8re", "target": "LLaMA: Open and Efficient Foundation Language Models", "value": "author_of"}, {"source": "Naman Goyal", "target": "LLaMA: Open and Efficient Foundation Language Models", "value": "author_of"}, {"source": "Eric Hambro", "target": "LLaMA: Open and Efficient Foundation Language Models", "value": "author_of"}, {"source": "Faisal Azhar", "target": "LLaMA: Open and Efficient Foundation Language Models", "value": "author_of"}, {"source": "Aurelien Rodriguez", "target": "LLaMA: Open and Efficient Foundation Language Models", "value": "author_of"}, {"source": "Armand Joulin", "target": "LLaMA: Open and Efficient Foundation Language Models", "value": "author_of"}, {"source": "Edouard Grave", "target": "LLaMA: Open and Efficient Foundation Language Models", "value": "author_of"}, {"source": "Guillaume Lample", "target": "LLaMA: Open and Efficient Foundation Language Models", "value": "author_of"}, {"source": "Jason Wei", "target": "Chain of Thought Prompting Elicits Reasoning in Large Language Models", "value": "author_of"}, {"source": "Xuezhi Wang", "target": "Chain of Thought Prompting Elicits Reasoning in Large Language Models", "value": "author_of"}, {"source": "Dale Schuurmans", "target": "Chain of Thought Prompting Elicits Reasoning in Large Language Models", "value": "author_of"}, {"source": "Maarten Bosma", "target": "Chain of Thought Prompting Elicits Reasoning in Large Language Models", "value": "author_of"}, {"source": "Brian Ichter", "target": "Chain of Thought Prompting Elicits Reasoning in Large Language Models", "value": "author_of"}, {"source": "Fei Xia", "target": "Chain of Thought Prompting Elicits Reasoning in Large Language Models", "value": "author_of"}, {"source": "Ed Chi", "target": "Chain of Thought Prompting Elicits Reasoning in Large Language Models", "value": "author_of"}, {"source": "Quoc Le", "target": "Chain of Thought Prompting Elicits Reasoning in Large Language Models", "value": "author_of"}, {"source": "Denny Zhou", "target": "Chain of Thought Prompting Elicits Reasoning in Large Language Models", "value": "author_of"}, {"source": "Karl Cobbe", "target": "Training Verifiers to Solve Math Word Problems", "value": "author_of"}, {"source": "Vineet Kosaraju", "target": "Training Verifiers to Solve Math Word Problems", "value": "author_of"}, {"source": "Mohammad Bavarian", "target": "Training Verifiers to Solve Math Word Problems", "value": "author_of"}, {"source": "Mark Chen", "target": "Training Verifiers to Solve Math Word Problems", "value": "author_of"}, {"source": "Heewoo Jun", "target": "Training Verifiers to Solve Math Word Problems", "value": "author_of"}, {"source": "Lukasz Kaiser", "target": "Training Verifiers to Solve Math Word Problems", "value": "author_of"}, {"source": "Matthias Plappert", "target": "Training Verifiers to Solve Math Word Problems", "value": "author_of"}, {"source": "Jerry Tworek", "target": "Training Verifiers to Solve Math Word Problems", "value": "author_of"}, {"source": "Jacob Hilton", "target": "Training Verifiers to Solve Math Word Problems", "value": "author_of"}, {"source": "Reiichiro Nakano", "target": "Training Verifiers to Solve Math Word Problems", "value": "author_of"}, {"source": "Christopher Hesse", "target": "Training Verifiers to Solve Math Word Problems", "value": "author_of"}, {"source": "John Schulman", "target": "Training Verifiers to Solve Math Word Problems", "value": "author_of"}, {"source": "K. Luger", "target": "Crystal structure of the nucleosome core particle at 2.8\u2009\u00c5 resolution", "value": "author_of"}, {"source": "A. M\u00e4der", "target": "Crystal structure of the nucleosome core particle at 2.8\u2009\u00c5 resolution", "value": "author_of"}, {"source": "R. K. Richmond", "target": "Crystal structure of the nucleosome core particle at 2.8\u2009\u00c5 resolution", "value": "author_of"}, {"source": "D. Sargent", "target": "Crystal structure of the nucleosome core particle at 2.8\u2009\u00c5 resolution", "value": "author_of"}, {"source": "T. Richmond", "target": "Crystal structure of the nucleosome core particle at 2.8\u2009\u00c5 resolution", "value": "author_of"}, {"source": "Albert Gu", "target": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "value": "author_of"}, {"source": "Tri Dao", "target": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "value": "author_of"}, {"source": "Stephen M. Mount", "target": "A catalogue of splice junction sequences.", "value": "author_of"}, {"source": "F. Crick", "target": "Origin of the Genetic Code", "value": "author_of"}, {"source": "Ilya Loshchilov", "target": "SGDR: Stochastic Gradient Descent with Warm Restarts", "value": "author_of"}, {"source": "Frank Hutter", "target": "SGDR: Stochastic Gradient Descent with Warm Restarts", "value": "author_of"}, {"source": "Noam Shazeer", "target": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "value": "author_of"}, {"source": "Azalia Mirhoseini", "target": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "value": "author_of"}, {"source": "Krzysztof Maziarz", "target": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "value": "author_of"}, {"source": "Andy Davis", "target": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "value": "author_of"}, {"source": "Quoc Le", "target": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "value": "author_of"}, {"source": "Geoffrey Hinton", "target": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "value": "author_of"}, {"source": "Jeff Dean", "target": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "value": "author_of"}, {"source": "J. Ziv", "target": "Compression of individual sequences via variable-rate coding", "value": "author_of"}, {"source": "A. Lempel", "target": "Compression of individual sequences via variable-rate coding", "value": "author_of"}, {"source": "Stephen Merity", "target": "Pointer Sentinel Mixture Models", "value": "author_of"}, {"source": "Caiming Xiong", "target": "Pointer Sentinel Mixture Models", "value": "author_of"}, {"source": "James Bradbury", "target": "Pointer Sentinel Mixture Models", "value": "author_of"}, {"source": "Richard Socher", "target": "Pointer Sentinel Mixture Models", "value": "author_of"}, {"source": "Dan Hendrycks", "target": "Measuring Massive Multitask Language Understanding", "value": "author_of"}, {"source": "Collin Burns", "target": "Measuring Massive Multitask Language Understanding", "value": "author_of"}, {"source": "Steven Basart", "target": "Measuring Massive Multitask Language Understanding", "value": "author_of"}, {"source": "Andy Zou", "target": "Measuring Massive Multitask Language Understanding", "value": "author_of"}, {"source": "Mantas Mazeika", "target": "Measuring Massive Multitask Language Understanding", "value": "author_of"}, {"source": "Dawn Song", "target": "Measuring Massive Multitask Language Understanding", "value": "author_of"}, {"source": "Jacob Steinhardt", "target": "Measuring Massive Multitask Language Understanding", "value": "author_of"}, {"source": "Hunter Lightman", "target": "Let's Verify Step by Step", "value": "author_of"}, {"source": "Vineet Kosaraju", "target": "Let's Verify Step by Step", "value": "author_of"}, {"source": "Yura Burda", "target": "Let's Verify Step by Step", "value": "author_of"}, {"source": "Harri Edwards", "target": "Let's Verify Step by Step", "value": "author_of"}, {"source": "Bowen Baker", "target": "Let's Verify Step by Step", "value": "author_of"}, {"source": "Teddy Lee", "target": "Let's Verify Step by Step", "value": "author_of"}, {"source": "Jan Leike", "target": "Let's Verify Step by Step", "value": "author_of"}, {"source": "John Schulman", "target": "Let's Verify Step by Step", "value": "author_of"}, {"source": "Ilya Sutskever", "target": "Let's Verify Step by Step", "value": "author_of"}, {"source": "Karl Cobbe", "target": "Let's Verify Step by Step", "value": "author_of"}, {"source": "David Rein", "target": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark", "value": "author_of"}, {"source": "Betty Li Hou", "target": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark", "value": "author_of"}, {"source": "Asa Cooper Stickland", "target": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark", "value": "author_of"}, {"source": "Jackson Petty", "target": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark", "value": "author_of"}, {"source": "Richard Yuanzhe Pang", "target": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark", "value": "author_of"}, {"source": "Julien Dirani", "target": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark", "value": "author_of"}, {"source": "Julian Michael", "target": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark", "value": "author_of"}, {"source": "Samuel R. Bowman", "target": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark", "value": "author_of"}, {"source": "Dmitry Lepikhin", "target": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding", "value": "author_of"}, {"source": "HyoukJoong Lee", "target": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding", "value": "author_of"}, {"source": "Yuanzhong Xu", "target": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding", "value": "author_of"}, {"source": "Dehao Chen", "target": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding", "value": "author_of"}, {"source": "Orhan Firat", "target": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding", "value": "author_of"}, {"source": "Yanping Huang", "target": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding", "value": "author_of"}, {"source": "Maxim Krikun", "target": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding", "value": "author_of"}, {"source": "Noam Shazeer", "target": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding", "value": "author_of"}, {"source": "Zhifeng Chen", "target": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding", "value": "author_of"}, {"source": "Radford M. Neal", "target": "Pattern Recognition and Machine Learning", "value": "author_of"}, {"source": "L. Breiman", "target": "Bagging Predictors", "value": "author_of"}, {"source": "Guan Wang", "target": "Automatic Image-Based Plant Disease Severity Estimation Using Deep Learning", "value": "author_of"}, {"source": "Yu Sun", "target": "Automatic Image-Based Plant Disease Severity Estimation Using Deep Learning", "value": "author_of"}, {"source": "Jianxin Wang", "target": "Automatic Image-Based Plant Disease Severity Estimation Using Deep Learning", "value": "author_of"}, {"source": "Mostafa Mehdipour-Ghazi", "target": "Plant identification using deep neural networks via optimization of transfer learning parameters", "value": "author_of"}, {"source": "B. Yanikoglu", "target": "Plant identification using deep neural networks via optimization of transfer learning parameters", "value": "author_of"}, {"source": "E. Aptoula", "target": "Plant identification using deep neural networks via optimization of transfer learning parameters", "value": "author_of"}, {"source": "Sue Han Lee", "target": "New perspectives on plant disease characterization based on deep learning", "value": "author_of"}, {"source": "H. Go\u00ebau", "target": "New perspectives on plant disease characterization based on deep learning", "value": "author_of"}, {"source": "P. Bonnet", "target": "New perspectives on plant disease characterization based on deep learning", "value": "author_of"}, {"source": "A. Joly", "target": "New perspectives on plant disease characterization based on deep learning", "value": "author_of"}, {"source": "Yu Sun", "target": "Deep Learning for Plant Identification in Natural Environment", "value": "author_of"}, {"source": "Yuan Liu", "target": "Deep Learning for Plant Identification in Natural Environment", "value": "author_of"}, {"source": "Guan Wang", "target": "Deep Learning for Plant Identification in Natural Environment", "value": "author_of"}, {"source": "Haiyan Zhang", "target": "Deep Learning for Plant Identification in Natural Environment", "value": "author_of"}, {"source": "Jose Carranza-Rojas", "target": "Going deeper in the automated identification of Herbarium specimens", "value": "author_of"}, {"source": "Herv\u00e9 Goeau", "target": "Going deeper in the automated identification of Herbarium specimens", "value": "author_of"}, {"source": "P. Bonnet", "target": "Going deeper in the automated identification of Herbarium specimens", "value": "author_of"}, {"source": "Erick Mata-Montero", "target": "Going deeper in the automated identification of Herbarium specimens", "value": "author_of"}, {"source": "A. Joly", "target": "Going deeper in the automated identification of Herbarium specimens", "value": "author_of"}, {"source": "Mark Sandler", "target": "MobileNetV2: Inverted Residuals and Linear Bottlenecks", "value": "author_of"}, {"source": "Andrew Howard", "target": "MobileNetV2: Inverted Residuals and Linear Bottlenecks", "value": "author_of"}, {"source": "Menglong Zhu", "target": "MobileNetV2: Inverted Residuals and Linear Bottlenecks", "value": "author_of"}, {"source": "Andrey Zhmoginov", "target": "MobileNetV2: Inverted Residuals and Linear Bottlenecks", "value": "author_of"}, {"source": "Liang-Chieh Chen", "target": "MobileNetV2: Inverted Residuals and Linear Bottlenecks", "value": "author_of"}, {"source": "H. Brendan McMahan", "target": "Communication-Efficient Learning of Deep Networks from Decentralized Data", "value": "author_of"}, {"source": "Eider Moore", "target": "Communication-Efficient Learning of Deep Networks from Decentralized Data", "value": "author_of"}, {"source": "Daniel Ramage", "target": "Communication-Efficient Learning of Deep Networks from Decentralized Data", "value": "author_of"}, {"source": "Seth Hampson", "target": "Communication-Efficient Learning of Deep Networks from Decentralized Data", "value": "author_of"}, {"source": "Blaise Ag\u00fcera y Arcas", "target": "Communication-Efficient Learning of Deep Networks from Decentralized Data", "value": "author_of"}, {"source": "Fuzhen Zhuang", "target": "A Comprehensive Survey on Transfer Learning", "value": "author_of"}, {"source": "Zhiyuan Qi", "target": "A Comprehensive Survey on Transfer Learning", "value": "author_of"}, {"source": "Keyu Duan", "target": "A Comprehensive Survey on Transfer Learning", "value": "author_of"}, {"source": "Dongbo Xi", "target": "A Comprehensive Survey on Transfer Learning", "value": "author_of"}, {"source": "Yongchun Zhu", "target": "A Comprehensive Survey on Transfer Learning", "value": "author_of"}, {"source": "Hengshu Zhu", "target": "A Comprehensive Survey on Transfer Learning", "value": "author_of"}, {"source": "Hui Xiong", "target": "A Comprehensive Survey on Transfer Learning", "value": "author_of"}, {"source": "Qing He", "target": "A Comprehensive Survey on Transfer Learning", "value": "author_of"}, {"source": "A. Khan", "target": "Privacy Preserved and Decentralized Smartphone Recommendation System", "value": "author_of"}, {"source": "Maha Driss", "target": "Privacy Preserved and Decentralized Smartphone Recommendation System", "value": "author_of"}, {"source": "Wadii Boulila", "target": "Privacy Preserved and Decentralized Smartphone Recommendation System", "value": "author_of"}, {"source": "G. A. Sampedro", "target": "Privacy Preserved and Decentralized Smartphone Recommendation System", "value": "author_of"}, {"source": "Sidra Abbas", "target": "Privacy Preserved and Decentralized Smartphone Recommendation System", "value": "author_of"}, {"source": "Chitapong Wechtaisong", "target": "Privacy Preserved and Decentralized Smartphone Recommendation System", "value": "author_of"}, {"source": "Tesfahunegn Minwuyelet Mengistu", "target": "A Survey on Heterogeneity Taxonomy, Security and Privacy Preservation in the Integration of IoT, Wireless Sensor Networks and Federated Learning", "value": "author_of"}, {"source": "Taewoon Kim", "target": "A Survey on Heterogeneity Taxonomy, Security and Privacy Preservation in the Integration of IoT, Wireless Sensor Networks and Federated Learning", "value": "author_of"}, {"source": "Jenn-Wei Lin", "target": "A Survey on Heterogeneity Taxonomy, Security and Privacy Preservation in the Integration of IoT, Wireless Sensor Networks and Federated Learning", "value": "author_of"}, {"source": "A. Alamer", "target": "A privacy-preserving federated learning with a secure collaborative for malware detection models using Internet of Things resources", "value": "author_of"}, {"source": "Manel Khazri Khlifi", "target": "Graph-based deep learning techniques for remote sensing applications: Techniques, taxonomy, and applications - A comprehensive review", "value": "author_of"}, {"source": "Wadii Boulila", "target": "Graph-based deep learning techniques for remote sensing applications: Techniques, taxonomy, and applications - A comprehensive review", "value": "author_of"}, {"source": "I. Farah", "target": "Graph-based deep learning techniques for remote sensing applications: Techniques, taxonomy, and applications - A comprehensive review", "value": "author_of"}, {"source": "Anwesha Mukherjee", "target": "Federated Learning Architectures: A Performance Evaluation With Crop Yield Prediction Application", "value": "author_of"}, {"source": "Rajkumar Buyya", "target": "Federated Learning Architectures: A Performance Evaluation With Crop Yield Prediction Application", "value": "author_of"}, {"source": "Alexander Kirillov", "target": "Segment Anything", "value": "author_of"}, {"source": "Eric Mintun", "target": "Segment Anything", "value": "author_of"}, {"source": "Nikhila Ravi", "target": "Segment Anything", "value": "author_of"}, {"source": "Hanzi Mao", "target": "Segment Anything", "value": "author_of"}, {"source": "Chloe Rolland", "target": "Segment Anything", "value": "author_of"}, {"source": "Laura Gustafson", "target": "Segment Anything", "value": "author_of"}, {"source": "Tete Xiao", "target": "Segment Anything", "value": "author_of"}, {"source": "Spencer Whitehead", "target": "Segment Anything", "value": "author_of"}, {"source": "Alexander C. Berg", "target": "Segment Anything", "value": "author_of"}, {"source": "Wan-Yen Lo", "target": "Segment Anything", "value": "author_of"}, {"source": "Piotr Doll\u00e1r", "target": "Segment Anything", "value": "author_of"}, {"source": "Ross Girshick", "target": "Segment Anything", "value": "author_of"}, {"source": "Haotian Liu", "target": "Visual Instruction Tuning", "value": "author_of"}, {"source": "Chunyuan Li", "target": "Visual Instruction Tuning", "value": "author_of"}, {"source": "Qingyang Wu", "target": "Visual Instruction Tuning", "value": "author_of"}, {"source": "Yong Jae Lee", "target": "Visual Instruction Tuning", "value": "author_of"}, {"source": "Haotian Liu", "target": "Improved Baselines with Visual Instruction Tuning", "value": "author_of"}, {"source": "Chunyuan Li", "target": "Improved Baselines with Visual Instruction Tuning", "value": "author_of"}, {"source": "Yuheng Li", "target": "Improved Baselines with Visual Instruction Tuning", "value": "author_of"}, {"source": "Yong Jae Lee", "target": "Improved Baselines with Visual Instruction Tuning", "value": "author_of"}, {"source": "Zhiyuan You", "target": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset", "value": "author_of"}, {"source": "Jinjin Gu", "target": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset", "value": "author_of"}, {"source": "Xin Cai", "target": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset", "value": "author_of"}, {"source": "Zheyuan Li", "target": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset", "value": "author_of"}, {"source": "Kaiwen Zhu", "target": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset", "value": "author_of"}, {"source": "Chao Dong", "target": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset", "value": "author_of"}, {"source": "Tianfan Xue", "target": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset", "value": "author_of"}, {"source": "Xintong Zhang", "target": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs", "value": "author_of"}, {"source": "Zhi Gao", "target": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs", "value": "author_of"}, {"source": "Bofei Zhang", "target": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs", "value": "author_of"}, {"source": "Pengxiang Li", "target": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs", "value": "author_of"}, {"source": "Xiaowen Zhang", "target": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs", "value": "author_of"}, {"source": "Yang Liu", "target": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs", "value": "author_of"}, {"source": "Tao Yuan", "target": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs", "value": "author_of"}, {"source": "Yuwei Wu", "target": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs", "value": "author_of"}, {"source": "Yunde Jia", "target": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs", "value": "author_of"}, {"source": "Song-Chun Zhu", "target": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs", "value": "author_of"}, {"source": "Qing Li", "target": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs", "value": "author_of"}, {"source": "Zhangquan Chen", "target": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views", "value": "author_of"}, {"source": "Manyuan Zhang", "target": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views", "value": "author_of"}, {"source": "Xinlei Yu", "target": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views", "value": "author_of"}, {"source": "Xufang Luo", "target": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views", "value": "author_of"}, {"source": "Mingze Sun", "target": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views", "value": "author_of"}, {"source": "Zihao Pan", "target": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views", "value": "author_of"}, {"source": "Yan Feng", "target": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views", "value": "author_of"}, {"source": "Peng Pei", "target": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views", "value": "author_of"}, {"source": "Xunliang Cai", "target": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views", "value": "author_of"}, {"source": "Ruqi Huang", "target": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views", "value": "author_of"}, {"source": "Tiancheng Gu", "target": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning", "value": "author_of"}, {"source": "Kaicheng Yang", "target": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning", "value": "author_of"}, {"source": "Kaichen Zhang", "target": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning", "value": "author_of"}, {"source": "Xiang An", "target": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning", "value": "author_of"}, {"source": "Ziyong Feng", "target": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning", "value": "author_of"}, {"source": "Yueyi Zhang", "target": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning", "value": "author_of"}, {"source": "Weidong Cai", "target": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning", "value": "author_of"}, {"source": "Jiankang Deng", "target": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning", "value": "author_of"}, {"source": "Lidong Bing", "target": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning", "value": "author_of"}, {"source": "Kaichen Zhang", "target": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe", "value": "author_of"}, {"source": "Keming Wu", "target": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe", "value": "author_of"}, {"source": "Zuhao Yang", "target": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe", "value": "author_of"}, {"source": "Bo Li", "target": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe", "value": "author_of"}, {"source": "Kairui Hu", "target": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe", "value": "author_of"}, {"source": "Bin Wang", "target": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe", "value": "author_of"}, {"source": "Ziwei Liu", "target": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe", "value": "author_of"}, {"source": "Xingxuan Li", "target": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe", "value": "author_of"}, {"source": "Lidong Bing", "target": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe", "value": "author_of"}, {"source": "Ranjan Sapkota", "target": "Vision-Language-Action (VLA) Models: Concepts, Progress, Applications and Challenges", "value": "author_of"}, {"source": "Yang Cao", "target": "Vision-Language-Action (VLA) Models: Concepts, Progress, Applications and Challenges", "value": "author_of"}, {"source": "Konstantinos I. Roumeliotis", "target": "Vision-Language-Action (VLA) Models: Concepts, Progress, Applications and Challenges", "value": "author_of"}, {"source": "Manoj Karkee", "target": "Vision-Language-Action (VLA) Models: Concepts, Progress, Applications and Challenges", "value": "author_of"}, {"source": "Kohei Sendai", "target": "Leave No Observation Behind: Real-time Correction for VLA Action Chunks", "value": "author_of"}, {"source": "Maxime Alvarez", "target": "Leave No Observation Behind: Real-time Correction for VLA Action Chunks", "value": "author_of"}, {"source": "Tatsuya Matsushima", "target": "Leave No Observation Behind: Real-time Correction for VLA Action Chunks", "value": "author_of"}, {"source": "Yutaka Matsuo", "target": "Leave No Observation Behind: Real-time Correction for VLA Action Chunks", "value": "author_of"}, {"source": "Yusuke Iwasawa", "target": "Leave No Observation Behind: Real-time Correction for VLA Action Chunks", "value": "author_of"}, {"source": "Shuhan Tan", "target": "Latent Chain-of-Thought World Modeling for End-to-End Driving", "value": "author_of"}, {"source": "Kashyap Chitta", "target": "Latent Chain-of-Thought World Modeling for End-to-End Driving", "value": "author_of"}, {"source": "Yuxiao Chen", "target": "Latent Chain-of-Thought World Modeling for End-to-End Driving", "value": "author_of"}, {"source": "Ran Tian", "target": "Latent Chain-of-Thought World Modeling for End-to-End Driving", "value": "author_of"}, {"source": "Yurong You", "target": "Latent Chain-of-Thought World Modeling for End-to-End Driving", "value": "author_of"}, {"source": "Yan Wang", "target": "Latent Chain-of-Thought World Modeling for End-to-End Driving", "value": "author_of"}, {"source": "Wenjie Luo", "target": "Latent Chain-of-Thought World Modeling for End-to-End Driving", "value": "author_of"}, {"source": "Yulong Cao", "target": "Latent Chain-of-Thought World Modeling for End-to-End Driving", "value": "author_of"}, {"source": "Philipp Krahenbuhl", "target": "Latent Chain-of-Thought World Modeling for End-to-End Driving", "value": "author_of"}, {"source": "Marco Pavone", "target": "Latent Chain-of-Thought World Modeling for End-to-End Driving", "value": "author_of"}, {"source": "Boris Ivanovic", "target": "Latent Chain-of-Thought World Modeling for End-to-End Driving", "value": "author_of"}, {"source": "Zheng Xiong", "target": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks", "value": "author_of"}, {"source": "Kang Li", "target": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks", "value": "author_of"}, {"source": "Zilin Wang", "target": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks", "value": "author_of"}, {"source": "Matthew Jackson", "target": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks", "value": "author_of"}, {"source": "Jakob Foerster", "target": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks", "value": "author_of"}, {"source": "Shimon Whiteson", "target": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks", "value": "author_of"}, {"source": "Yifan Ye", "target": "Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models", "value": "author_of"}, {"source": "Jiaqi Ma", "target": "Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models", "value": "author_of"}, {"source": "Jun Cen", "target": "Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models", "value": "author_of"}, {"source": "Zhihe Lu", "target": "Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models", "value": "author_of"}, {"source": "Zhaohu Xing", "target": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation", "value": "author_of"}, {"source": "Tian Ye", "target": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation", "value": "author_of"}, {"source": "Yijun Yang", "target": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation", "value": "author_of"}, {"source": "D. Cai", "target": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation", "value": "author_of"}, {"source": "Baowen Gai", "target": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation", "value": "author_of"}, {"source": "Xiao-Jian Wu", "target": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation", "value": "author_of"}, {"source": "Feng Gao", "target": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation", "value": "author_of"}, {"source": "Lei Zhu", "target": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation", "value": "author_of"}, {"source": "Hai Liu", "target": "TransSIL: A Silhouette Cue-Aware Image Classification Framework for Bird Ecological Monitoring Systems", "value": "author_of"}, {"source": "Yu Song", "target": "TransSIL: A Silhouette Cue-Aware Image Classification Framework for Bird Ecological Monitoring Systems", "value": "author_of"}, {"source": "Tingting Liu", "target": "TransSIL: A Silhouette Cue-Aware Image Classification Framework for Bird Ecological Monitoring Systems", "value": "author_of"}, {"source": "Lin Chen", "target": "TransSIL: A Silhouette Cue-Aware Image Classification Framework for Bird Ecological Monitoring Systems", "value": "author_of"}, {"source": "Zhaoli Zhang", "target": "TransSIL: A Silhouette Cue-Aware Image Classification Framework for Bird Ecological Monitoring Systems", "value": "author_of"}, {"source": "Xiaolan Yang", "target": "TransSIL: A Silhouette Cue-Aware Image Classification Framework for Bird Ecological Monitoring Systems", "value": "author_of"}, {"source": "Neal N. Xiong", "target": "TransSIL: A Silhouette Cue-Aware Image Classification Framework for Bird Ecological Monitoring Systems", "value": "author_of"}, {"source": "Honghu Chu", "target": "CBRFormer: rendering technology-based transformer for refinement segmentation of bridge crack images", "value": "author_of"}, {"source": "Jiahao Gai", "target": "CBRFormer: rendering technology-based transformer for refinement segmentation of bridge crack images", "value": "author_of"}, {"source": "Weiwei Chen", "target": "CBRFormer: rendering technology-based transformer for refinement segmentation of bridge crack images", "value": "author_of"}, {"source": "Jun Ma", "target": "CBRFormer: rendering technology-based transformer for refinement segmentation of bridge crack images", "value": "author_of"}, {"source": "Yinjun Jia", "target": "Deep contrastive learning enables genome-wide virtual screening.", "value": "author_of"}, {"source": "Bowen Gao", "target": "Deep contrastive learning enables genome-wide virtual screening.", "value": "author_of"}, {"source": "Jiaxin Tan", "target": "Deep contrastive learning enables genome-wide virtual screening.", "value": "author_of"}, {"source": "Jiqing Zheng", "target": "Deep contrastive learning enables genome-wide virtual screening.", "value": "author_of"}, {"source": "Xin Hong", "target": "Deep contrastive learning enables genome-wide virtual screening.", "value": "author_of"}, {"source": "Wenyu Zhu", "target": "Deep contrastive learning enables genome-wide virtual screening.", "value": "author_of"}, {"source": "Haichuan Tan", "target": "Deep contrastive learning enables genome-wide virtual screening.", "value": "author_of"}, {"source": "Yuan Xiao", "target": "Deep contrastive learning enables genome-wide virtual screening.", "value": "author_of"}, {"source": "Liping Tan", "target": "Deep contrastive learning enables genome-wide virtual screening.", "value": "author_of"}, {"source": "Hongyi Cai", "target": "Deep contrastive learning enables genome-wide virtual screening.", "value": "author_of"}, {"source": "Yanwen Huang", "target": "Deep contrastive learning enables genome-wide virtual screening.", "value": "author_of"}, {"source": "Zhiheng Deng", "target": "Deep contrastive learning enables genome-wide virtual screening.", "value": "author_of"}, {"source": "Xiangwei Wu", "target": "Deep contrastive learning enables genome-wide virtual screening.", "value": "author_of"}, {"source": "Yue Jin", "target": "Deep contrastive learning enables genome-wide virtual screening.", "value": "author_of"}, {"source": "Yafei Yuan", "target": "Deep contrastive learning enables genome-wide virtual screening.", "value": "author_of"}, {"source": "Jiekang Tian", "target": "Deep contrastive learning enables genome-wide virtual screening.", "value": "author_of"}, {"source": "Wei He", "target": "Deep contrastive learning enables genome-wide virtual screening.", "value": "author_of"}, {"source": "Weiying Ma", "target": "Deep contrastive learning enables genome-wide virtual screening.", "value": "author_of"}, {"source": "Ya-Qin Zhang", "target": "Deep contrastive learning enables genome-wide virtual screening.", "value": "author_of"}, {"source": "Lei Liu", "target": "Deep contrastive learning enables genome-wide virtual screening.", "value": "author_of"}, {"source": "Chuangye Yan", "target": "Deep contrastive learning enables genome-wide virtual screening.", "value": "author_of"}, {"source": "Wei Zhang", "target": "Deep contrastive learning enables genome-wide virtual screening.", "value": "author_of"}, {"source": "Yanyan Lan", "target": "Deep contrastive learning enables genome-wide virtual screening.", "value": "author_of"}, {"source": "Guodong Fan", "target": "LLaVA-based semantic feature modulation diffusion model for underwater image enhancement", "value": "author_of"}, {"source": "Shengning Zhou", "target": "LLaVA-based semantic feature modulation diffusion model for underwater image enhancement", "value": "author_of"}, {"source": "Zhen Hua", "target": "LLaVA-based semantic feature modulation diffusion model for underwater image enhancement", "value": "author_of"}, {"source": "Jinjiang Li", "target": "LLaVA-based semantic feature modulation diffusion model for underwater image enhancement", "value": "author_of"}, {"source": "Jingchun Zhou", "target": "LLaVA-based semantic feature modulation diffusion model for underwater image enhancement", "value": "author_of"}, {"source": "Jiahua Dong", "target": "3DGS-Drag: Dragging Gaussians for Intuitive Point-Based 3D Editing", "value": "author_of"}, {"source": "Yu-Xiong Wang", "target": "3DGS-Drag: Dragging Gaussians for Intuitive Point-Based 3D Editing", "value": "author_of"}, {"source": "Hasi Hays", "target": "Attention mechanisms in neural networks", "value": "author_of"}, {"source": "Yiyang Lu", "target": "Bidirectional Normalizing Flow: From Data to Noise and Back", "value": "author_of"}, {"source": "Qiao Sun", "target": "Bidirectional Normalizing Flow: From Data to Noise and Back", "value": "author_of"}, {"source": "Xianbang Wang", "target": "Bidirectional Normalizing Flow: From Data to Noise and Back", "value": "author_of"}, {"source": "Zhicheng Jiang", "target": "Bidirectional Normalizing Flow: From Data to Noise and Back", "value": "author_of"}, {"source": "Hanhong Zhao", "target": "Bidirectional Normalizing Flow: From Data to Noise and Back", "value": "author_of"}, {"source": "Kaiming He", "target": "Bidirectional Normalizing Flow: From Data to Noise and Back", "value": "author_of"}, {"source": "Yiyang Lu", "target": "One-step Latent-free Image Generation with Pixel Mean Flows", "value": "author_of"}, {"source": "Susie Lu", "target": "One-step Latent-free Image Generation with Pixel Mean Flows", "value": "author_of"}, {"source": "Qiao Sun", "target": "One-step Latent-free Image Generation with Pixel Mean Flows", "value": "author_of"}, {"source": "Hanhong Zhao", "target": "One-step Latent-free Image Generation with Pixel Mean Flows", "value": "author_of"}, {"source": "Zhicheng Jiang", "target": "One-step Latent-free Image Generation with Pixel Mean Flows", "value": "author_of"}, {"source": "Xianbang Wang", "target": "One-step Latent-free Image Generation with Pixel Mean Flows", "value": "author_of"}, {"source": "Tianhong Li", "target": "One-step Latent-free Image Generation with Pixel Mean Flows", "value": "author_of"}, {"source": "Zhengyang Geng", "target": "One-step Latent-free Image Generation with Pixel Mean Flows", "value": "author_of"}, {"source": "Kaiming He", "target": "One-step Latent-free Image Generation with Pixel Mean Flows", "value": "author_of"}, {"source": "Peter Potaptchik", "target": "Meta Flow Maps enable scalable reward alignment", "value": "author_of"}, {"source": "Adhi Saravanan", "target": "Meta Flow Maps enable scalable reward alignment", "value": "author_of"}, {"source": "Abbas Mammadov", "target": "Meta Flow Maps enable scalable reward alignment", "value": "author_of"}, {"source": "Alvaro Prat", "target": "Meta Flow Maps enable scalable reward alignment", "value": "author_of"}, {"source": "Michael S. Albergo", "target": "Meta Flow Maps enable scalable reward alignment", "value": "author_of"}, {"source": "Yee Whye Teh", "target": "Meta Flow Maps enable scalable reward alignment", "value": "author_of"}, {"source": "Yinan Huang", "target": "Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective", "value": "author_of"}, {"source": "Hans Hao-Hsun Hsu", "target": "Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective", "value": "author_of"}, {"source": "Junran Wang", "target": "Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective", "value": "author_of"}, {"source": "Bo Dai", "target": "Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective", "value": "author_of"}, {"source": "Pan Li", "target": "Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective", "value": "author_of"}, {"source": "Mingyang Deng", "target": "Generative Modeling via Drifting", "value": "author_of"}, {"source": "He Li", "target": "Generative Modeling via Drifting", "value": "author_of"}, {"source": "Tianhong Li", "target": "Generative Modeling via Drifting", "value": "author_of"}, {"source": "Yilun Du", "target": "Generative Modeling via Drifting", "value": "author_of"}, {"source": "Kaiming He", "target": "Generative Modeling via Drifting", "value": "author_of"}, {"source": "Ting Chen", "target": "A Simple Framework for Contrastive Learning of Visual Representations", "value": "author_of"}, {"source": "Simon Kornblith", "target": "A Simple Framework for Contrastive Learning of Visual Representations", "value": "author_of"}, {"source": "Mohammad Norouzi", "target": "A Simple Framework for Contrastive Learning of Visual Representations", "value": "author_of"}, {"source": "Geoffrey Hinton", "target": "A Simple Framework for Contrastive Learning of Visual Representations", "value": "author_of"}, {"source": "Chubin Chen", "target": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning", "value": "author_of"}, {"source": "Sujie Hu", "target": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning", "value": "author_of"}, {"source": "Jiashu Zhu", "target": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning", "value": "author_of"}, {"source": "Meiqi Wu", "target": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning", "value": "author_of"}, {"source": "Jintao Chen", "target": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning", "value": "author_of"}, {"source": "Yanxun Li", "target": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning", "value": "author_of"}, {"source": "Nisha Huang", "target": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning", "value": "author_of"}, {"source": "Chengyu Fang", "target": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning", "value": "author_of"}, {"source": "Jiahong Wu", "target": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning", "value": "author_of"}, {"source": "Xiangxiang Chu", "target": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning", "value": "author_of"}, {"source": "Xiu Li", "target": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning", "value": "author_of"}, {"source": "Meiqi Wu", "target": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints", "value": "author_of"}, {"source": "Jiashu Zhu", "target": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints", "value": "author_of"}, {"source": "Xiaokun Feng", "target": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints", "value": "author_of"}, {"source": "Chubin Chen", "target": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints", "value": "author_of"}, {"source": "Chen Zhu", "target": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints", "value": "author_of"}, {"source": "Bingze Song", "target": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints", "value": "author_of"}, {"source": "Fangyuan Mao", "target": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints", "value": "author_of"}, {"source": "Jiahong Wu", "target": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints", "value": "author_of"}, {"source": "Xiangxiang Chu", "target": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints", "value": "author_of"}, {"source": "Kaiqi Huang", "target": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints", "value": "author_of"}, {"source": "Shengbang Tong", "target": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders", "value": "author_of"}, {"source": "Boyang Zheng", "target": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders", "value": "author_of"}, {"source": "Ziteng Wang", "target": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders", "value": "author_of"}, {"source": "Bingda Tang", "target": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders", "value": "author_of"}, {"source": "Nanye Ma", "target": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders", "value": "author_of"}, {"source": "Ellis Brown", "target": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders", "value": "author_of"}, {"source": "Jihan Yang", "target": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders", "value": "author_of"}, {"source": "Rob Fergus", "target": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders", "value": "author_of"}, {"source": "Yann LeCun", "target": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders", "value": "author_of"}, {"source": "Saining Xie", "target": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders", "value": "author_of"}, {"source": "Jingtong Yue", "target": "Simulating the Visual World with Artificial Intelligence: A Roadmap", "value": "author_of"}, {"source": "Ziqi Huang", "target": "Simulating the Visual World with Artificial Intelligence: A Roadmap", "value": "author_of"}, {"source": "Zhaoxi Chen", "target": "Simulating the Visual World with Artificial Intelligence: A Roadmap", "value": "author_of"}, {"source": "Xintao Wang", "target": "Simulating the Visual World with Artificial Intelligence: A Roadmap", "value": "author_of"}, {"source": "Pengfei Wan", "target": "Simulating the Visual World with Artificial Intelligence: A Roadmap", "value": "author_of"}, {"source": "Ziwei Liu", "target": "Simulating the Visual World with Artificial Intelligence: A Roadmap", "value": "author_of"}, {"source": "Bohan Zeng", "target": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks", "value": "author_of"}, {"source": "Kaixin Zhu", "target": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks", "value": "author_of"}, {"source": "Daili Hua", "target": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks", "value": "author_of"}, {"source": "Bozhou Li", "target": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks", "value": "author_of"}, {"source": "Chengzhuo Tong", "target": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks", "value": "author_of"}, {"source": "Yuran Wang", "target": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks", "value": "author_of"}, {"source": "Xinyi Huang", "target": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks", "value": "author_of"}, {"source": "Yifan Dai", "target": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks", "value": "author_of"}, {"source": "Zixiang Zhang", "target": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks", "value": "author_of"}, {"source": "Yifan Yang", "target": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks", "value": "author_of"}, {"source": "Zhou Liu", "target": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks", "value": "author_of"}, {"source": "Hao Liang", "target": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks", "value": "author_of"}, {"source": "Xiaochen Ma", "target": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks", "value": "author_of"}, {"source": "Ruichuan An", "target": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks", "value": "author_of"}, {"source": "Tianyi Bai", "target": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks", "value": "author_of"}, {"source": "Hongcheng Gao", "target": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks", "value": "author_of"}, {"source": "Junbo Niu", "target": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks", "value": "author_of"}, {"source": "Yang Shi", "target": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks", "value": "author_of"}, {"source": "Xinlong Chen", "target": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks", "value": "author_of"}, {"source": "Yue Ding", "target": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks", "value": "author_of"}, {"source": "Minglei Shi", "target": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks", "value": "author_of"}, {"source": "Kai Zeng", "target": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks", "value": "author_of"}, {"source": "Yiwen Tang", "target": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks", "value": "author_of"}, {"source": "Yuanxing Zhang", "target": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks", "value": "author_of"}, {"source": "Pengfei Wan", "target": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks", "value": "author_of"}, {"source": "Xintao Wang", "target": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks", "value": "author_of"}, {"source": "Wentao Zhang", "target": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks", "value": "author_of"}, {"source": "Qingyu Shi", "target": "RecTok: Reconstruction Distillation along Rectified Flow", "value": "author_of"}, {"source": "Size Wu", "target": "RecTok: Reconstruction Distillation along Rectified Flow", "value": "author_of"}, {"source": "Jinbin Bai", "target": "RecTok: Reconstruction Distillation along Rectified Flow", "value": "author_of"}, {"source": "Kaidong Yu", "target": "RecTok: Reconstruction Distillation along Rectified Flow", "value": "author_of"}, {"source": "Yujing Wang", "target": "RecTok: Reconstruction Distillation along Rectified Flow", "value": "author_of"}, {"source": "Yunhai Tong", "target": "RecTok: Reconstruction Distillation along Rectified Flow", "value": "author_of"}, {"source": "Xiangtai Li", "target": "RecTok: Reconstruction Distillation along Rectified Flow", "value": "author_of"}, {"source": "Xuelong Li", "target": "RecTok: Reconstruction Distillation along Rectified Flow", "value": "author_of"}, {"source": "Guanfang Dong", "target": "RePack then Refine: Efficient Diffusion Transformer with Vision Foundation Model", "value": "author_of"}, {"source": "Luke Schultz", "target": "RePack then Refine: Efficient Diffusion Transformer with Vision Foundation Model", "value": "author_of"}, {"source": "Negar Hassanpour", "target": "RePack then Refine: Efficient Diffusion Transformer with Vision Foundation Model", "value": "author_of"}, {"source": "Chao Gao", "target": "RePack then Refine: Efficient Diffusion Transformer with Vision Foundation Model", "value": "author_of"}, {"source": "Prafulla Dhariwal", "target": "Diffusion Models Beat GANs on Image Synthesis", "value": "author_of"}, {"source": "Alex Nichol", "target": "Diffusion Models Beat GANs on Image Synthesis", "value": "author_of"}, {"source": "Maxime Oquab", "target": "DINOv2: Learning Robust Visual Features without Supervision", "value": "author_of"}, {"source": "Timoth\u00e9e Darcet", "target": "DINOv2: Learning Robust Visual Features without Supervision", "value": "author_of"}, {"source": "Th\u00e9o Moutakanni", "target": "DINOv2: Learning Robust Visual Features without Supervision", "value": "author_of"}, {"source": "Huy Vo", "target": "DINOv2: Learning Robust Visual Features without Supervision", "value": "author_of"}, {"source": "Marc Szafraniec", "target": "DINOv2: Learning Robust Visual Features without Supervision", "value": "author_of"}, {"source": "Vasil Khalidov", "target": "DINOv2: Learning Robust Visual Features without Supervision", "value": "author_of"}, {"source": "Pierre Fernandez", "target": "DINOv2: Learning Robust Visual Features without Supervision", "value": "author_of"}, {"source": "Daniel Haziza", "target": "DINOv2: Learning Robust Visual Features without Supervision", "value": "author_of"}, {"source": "Francisco Massa", "target": "DINOv2: Learning Robust Visual Features without Supervision", "value": "author_of"}, {"source": "Alaaeldin El-Nouby", "target": "DINOv2: Learning Robust Visual Features without Supervision", "value": "author_of"}, {"source": "Mahmoud Assran", "target": "DINOv2: Learning Robust Visual Features without Supervision", "value": "author_of"}, {"source": "Nicolas Ballas", "target": "DINOv2: Learning Robust Visual Features without Supervision", "value": "author_of"}, {"source": "Wojciech Galuba", "target": "DINOv2: Learning Robust Visual Features without Supervision", "value": "author_of"}, {"source": "Russell Howes", "target": "DINOv2: Learning Robust Visual Features without Supervision", "value": "author_of"}, {"source": "Po-Yao Huang", "target": "DINOv2: Learning Robust Visual Features without Supervision", "value": "author_of"}, {"source": "Shang-Wen Li", "target": "DINOv2: Learning Robust Visual Features without Supervision", "value": "author_of"}, {"source": "Ishan Misra", "target": "DINOv2: Learning Robust Visual Features without Supervision", "value": "author_of"}, {"source": "Michael Rabbat", "target": "DINOv2: Learning Robust Visual Features without Supervision", "value": "author_of"}, {"source": "Vasu Sharma", "target": "DINOv2: Learning Robust Visual Features without Supervision", "value": "author_of"}, {"source": "Gabriel Synnaeve", "target": "DINOv2: Learning Robust Visual Features without Supervision", "value": "author_of"}, {"source": "Hu Xu", "target": "DINOv2: Learning Robust Visual Features without Supervision", "value": "author_of"}, {"source": "Herv\u00e9 Jegou", "target": "DINOv2: Learning Robust Visual Features without Supervision", "value": "author_of"}, {"source": "Julien Mairal", "target": "DINOv2: Learning Robust Visual Features without Supervision", "value": "author_of"}, {"source": "Patrick Labatut", "target": "DINOv2: Learning Robust Visual Features without Supervision", "value": "author_of"}, {"source": "Armand Joulin", "target": "DINOv2: Learning Robust Visual Features without Supervision", "value": "author_of"}, {"source": "Piotr Bojanowski", "target": "DINOv2: Learning Robust Visual Features without Supervision", "value": "author_of"}, {"source": "Zehong Ma", "target": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss", "value": "author_of"}, {"source": "Ruihan Xu", "target": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss", "value": "author_of"}, {"source": "Shiliang Zhang", "target": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss", "value": "author_of"}, {"source": "Shanshan Zhao", "target": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities", "value": "author_of"}, {"source": "Xinjie Zhang", "target": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities", "value": "author_of"}, {"source": "Jintao Guo", "target": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities", "value": "author_of"}, {"source": "Jiakui Hu", "target": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities", "value": "author_of"}, {"source": "Lunhao Duan", "target": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities", "value": "author_of"}, {"source": "Minghao Fu", "target": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities", "value": "author_of"}, {"source": "Yong Xien Chng", "target": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities", "value": "author_of"}, {"source": "Guo-Hua Wang", "target": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities", "value": "author_of"}, {"source": "Qing-Guo Chen", "target": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities", "value": "author_of"}, {"source": "Zhao Xu", "target": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities", "value": "author_of"}, {"source": "Weihua Luo", "target": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities", "value": "author_of"}, {"source": "Kaifu Zhang", "target": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities", "value": "author_of"}, {"source": "Jana Zeller", "target": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery", "value": "author_of"}, {"source": "Thadd\u00e4us Wiedemer", "target": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery", "value": "author_of"}, {"source": "Fanfei Li", "target": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery", "value": "author_of"}, {"source": "Thomas Klein", "target": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery", "value": "author_of"}, {"source": "Prasanna Mayilvahanan", "target": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery", "value": "author_of"}, {"source": "Matthias Bethge", "target": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery", "value": "author_of"}, {"source": "Felix Wichmann", "target": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery", "value": "author_of"}, {"source": "Ryan Cotterell", "target": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery", "value": "author_of"}, {"source": "Wieland Brendel", "target": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery", "value": "author_of"}, {"source": "Letian Zhang", "target": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation", "value": "author_of"}, {"source": "Sucheng Ren", "target": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation", "value": "author_of"}, {"source": "Yanqing Liu", "target": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation", "value": "author_of"}, {"source": "Xianhang Li", "target": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation", "value": "author_of"}, {"source": "Zeyu Wang", "target": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation", "value": "author_of"}, {"source": "Yuyin Zhou", "target": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation", "value": "author_of"}, {"source": "Huaxiu Yao", "target": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation", "value": "author_of"}, {"source": "Zeyu Zheng", "target": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation", "value": "author_of"}, {"source": "Weili Nie", "target": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation", "value": "author_of"}, {"source": "Guilin Liu", "target": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation", "value": "author_of"}, {"source": "Zhiding Yu", "target": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation", "value": "author_of"}, {"source": "Cihang Xie", "target": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation", "value": "author_of"}, {"source": "Tomas Mikolov", "target": "Distributed Representations of Words and Phrases and their Compositionality", "value": "author_of"}, {"source": "Ilya Sutskever", "target": "Distributed Representations of Words and Phrases and their Compositionality", "value": "author_of"}, {"source": "Kai Chen", "target": "Distributed Representations of Words and Phrases and their Compositionality", "value": "author_of"}, {"source": "Greg Corrado", "target": "Distributed Representations of Words and Phrases and their Compositionality", "value": "author_of"}, {"source": "Jeffrey Dean", "target": "Distributed Representations of Words and Phrases and their Compositionality", "value": "author_of"}, {"source": "Jeffrey Pennington", "target": "GloVe: Global Vectors for Word Representation", "value": "author_of"}, {"source": "R. Socher", "target": "GloVe: Global Vectors for Word Representation", "value": "author_of"}, {"source": "Christopher D. Manning", "target": "GloVe: Global Vectors for Word Representation", "value": "author_of"}, {"source": "Matthew E. Peters", "target": "Deep Contextualized Word Representations", "value": "author_of"}, {"source": "Mark Neumann", "target": "Deep Contextualized Word Representations", "value": "author_of"}, {"source": "Mohit Iyyer", "target": "Deep Contextualized Word Representations", "value": "author_of"}, {"source": "Matt Gardner", "target": "Deep Contextualized Word Representations", "value": "author_of"}, {"source": "Christopher Clark", "target": "Deep Contextualized Word Representations", "value": "author_of"}, {"source": "Kenton Lee", "target": "Deep Contextualized Word Representations", "value": "author_of"}, {"source": "Luke Zettlemoyer", "target": "Deep Contextualized Word Representations", "value": "author_of"}, {"source": "Quentin Fournier", "target": "Protein Language Models: Is Scaling Necessary?", "value": "author_of"}, {"source": "Robert M. Vernon", "target": "Protein Language Models: Is Scaling Necessary?", "value": "author_of"}, {"source": "Almer M. van der Sloot", "target": "Protein Language Models: Is Scaling Necessary?", "value": "author_of"}, {"source": "Benjamin Schulz", "target": "Protein Language Models: Is Scaling Necessary?", "value": "author_of"}, {"source": "Sarath Chandar", "target": "Protein Language Models: Is Scaling Necessary?", "value": "author_of"}, {"source": "C. Langmead", "target": "Protein Language Models: Is Scaling Necessary?", "value": "author_of"}, {"source": "Scott Friedman", "target": "Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis", "value": "author_of"}, {"source": "Sonja Schmer-Galunder", "target": "Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis", "value": "author_of"}, {"source": "Anthony Chen", "target": "Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis", "value": "author_of"}, {"source": "Jeffrey Rye", "target": "Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis", "value": "author_of"}, {"source": "Alexander C. Li", "target": "Generative Classifiers Avoid Shortcut Solutions", "value": "author_of"}, {"source": "Ananya Kumar", "target": "Generative Classifiers Avoid Shortcut Solutions", "value": "author_of"}, {"source": "Deepak Pathak", "target": "Generative Classifiers Avoid Shortcut Solutions", "value": "author_of"}, {"source": "Y. Sun", "target": "Edge Large AI Model Agent-Empowered Cognitive Multimodal Semantic Communication", "value": "author_of"}, {"source": "Yinqiu Liu", "target": "Edge Large AI Model Agent-Empowered Cognitive Multimodal Semantic Communication", "value": "author_of"}, {"source": "Shaoyong Guo", "target": "Edge Large AI Model Agent-Empowered Cognitive Multimodal Semantic Communication", "value": "author_of"}, {"source": "Xuesong Qiu", "target": "Edge Large AI Model Agent-Empowered Cognitive Multimodal Semantic Communication", "value": "author_of"}, {"source": "Jiewei Chen", "target": "Edge Large AI Model Agent-Empowered Cognitive Multimodal Semantic Communication", "value": "author_of"}, {"source": "Jiakai Hao", "target": "Edge Large AI Model Agent-Empowered Cognitive Multimodal Semantic Communication", "value": "author_of"}, {"source": "Dusist Niyato", "target": "Edge Large AI Model Agent-Empowered Cognitive Multimodal Semantic Communication", "value": "author_of"}, {"source": "Alec Radford", "target": "Language Models are Unsupervised Multitask Learners", "value": "author_of"}, {"source": "Jeff Wu", "target": "Language Models are Unsupervised Multitask Learners", "value": "author_of"}, {"source": "R. Child", "target": "Language Models are Unsupervised Multitask Learners", "value": "author_of"}, {"source": "D. Luan", "target": "Language Models are Unsupervised Multitask Learners", "value": "author_of"}, {"source": "Dario Amodei", "target": "Language Models are Unsupervised Multitask Learners", "value": "author_of"}, {"source": "I. Sutskever", "target": "Language Models are Unsupervised Multitask Learners", "value": "author_of"}, {"source": "Aaron Grattafiori", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Abhimanyu Dubey", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Abhinav Jauhri", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Abhinav Pandey", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Abhishek Kadian", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Ahmad Al-Dahle", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Aiesha Letman", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Akhil Mathur", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Alan Schelten", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Alex Vaughan", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Amy Yang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Angela Fan", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Anirudh Goyal", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Anthony Hartshorn", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Aobo Yang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Archi Mitra", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Archie Sravankumar", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Artem Korenev", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Arthur Hinsvark", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Arun Rao", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Aston Zhang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Aurelien Rodriguez", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Austen Gregerson", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Ava Spataru", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Baptiste Roziere", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Bethany Biron", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Binh Tang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Bobbie Chern", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Charlotte Caucheteux", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Chaya Nayak", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Chloe Bi", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Chris Marra", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Chris McConnell", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Christian Keller", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Christophe Touret", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Chunyang Wu", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Corinne Wong", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Cristian Canton Ferrer", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Cyrus Nikolaidis", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Damien Allonsius", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Daniel Song", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Danielle Pintz", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Danny Livshits", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Danny Wyatt", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "David Esiobu", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Dhruv Choudhary", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Dhruv Mahajan", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Diego Garcia-Olano", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Diego Perino", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Dieuwke Hupkes", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Egor Lakomkin", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Ehab AlBadawy", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Elina Lobanova", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Emily Dinan", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Eric Michael Smith", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Filip Radenovic", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Francisco Guzm\u00e1n", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Frank Zhang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Gabriel Synnaeve", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Gabrielle Lee", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Georgia Lewis Anderson", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Govind Thattai", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Graeme Nail", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Gregoire Mialon", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Guan Pang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Guillem Cucurell", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Hailey Nguyen", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Hannah Korevaar", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Hu Xu", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Hugo Touvron", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Iliyan Zarov", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Imanol Arrieta Ibarra", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Isabel Kloumann", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Ishan Misra", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Ivan Evtimov", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Jack Zhang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Jade Copet", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Jaewon Lee", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Jan Geffert", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Jana Vranes", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Jason Park", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Jay Mahadeokar", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Jeet Shah", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Jelmer van der Linde", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Jennifer Billock", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Jenny Hong", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Jenya Lee", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Jeremy Fu", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Jianfeng Chi", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Jianyu Huang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Jiawen Liu", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Jie Wang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Jiecao Yu", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Joanna Bitton", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Joe Spisak", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Jongsoo Park", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Joseph Rocca", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Joshua Johnstun", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Joshua Saxe", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Junteng Jia", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Kalyan Vasuden Alwala", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Karthik Prasad", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Kartikeya Upasani", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Kate Plawiak", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Ke Li", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Kenneth Heafield", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Kevin Stone", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Khalid El-Arini", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Krithika Iyer", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Kshitiz Malik", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Kuenley Chiu", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Kunal Bhalla", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Kushal Lakhotia", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Lauren Rantala-Yeary", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Laurens van der Maaten", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Lawrence Chen", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Liang Tan", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Liz Jenkins", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Louis Martin", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Lovish Madaan", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Lubo Malo", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Lukas Blecher", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Lukas Landzaat", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Luke de Oliveira", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Madeline Muzzi", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Mahesh Pasupuleti", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Mannat Singh", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Manohar Paluri", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Marcin Kardas", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Maria Tsimpoukelli", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Mathew Oldham", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Mathieu Rita", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Maya Pavlova", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Melanie Kambadur", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Mike Lewis", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Min Si", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Mitesh Kumar Singh", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Mona Hassan", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Naman Goyal", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Narjes Torabi", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Nikolay Bashlykov", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Nikolay Bogoychev", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Niladri Chatterji", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Ning Zhang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Olivier Duchenne", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Onur \u00c7elebi", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Patrick Alrassy", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Pengchuan Zhang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Pengwei Li", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Petar Vasic", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Peter Weng", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Prajjwal Bhargava", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Pratik Dubal", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Praveen Krishnan", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Punit Singh Koura", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Puxin Xu", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Qing He", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Qingxiao Dong", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Ragavan Srinivasan", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Raj Ganapathy", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Ramon Calderer", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Ricardo Silveira Cabral", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Robert Stojnic", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Roberta Raileanu", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Rohan Maheswari", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Rohit Girdhar", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Rohit Patel", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Romain Sauvestre", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Ronnie Polidoro", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Roshan Sumbaly", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Ross Taylor", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Ruan Silva", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Rui Hou", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Rui Wang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Saghar Hosseini", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Sahana Chennabasappa", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Sanjay Singh", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Sean Bell", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Seohyun Sonia Kim", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Sergey Edunov", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Shaoliang Nie", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Sharan Narang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Sharath Raparthy", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Sheng Shen", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Shengye Wan", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Shruti Bhosale", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Shun Zhang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Simon Vandenhende", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Soumya Batra", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Spencer Whitman", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Sten Sootla", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Stephane Collot", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Suchin Gururangan", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Sydney Borodinsky", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Tamar Herman", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Tara Fowler", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Tarek Sheasha", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Thomas Georgiou", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Thomas Scialom", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Tobias Speckbacher", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Todor Mihaylov", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Tong Xiao", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Ujjwal Karn", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Vedanuj Goswami", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Vibhor Gupta", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Vignesh Ramanathan", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Viktor Kerkez", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Vincent Gonguet", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Virginie Do", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Vish Vogeti", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "V\u00edtor Albiero", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Vladan Petrovic", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Weiwei Chu", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Wenhan Xiong", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Wenyin Fu", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Whitney Meers", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Xavier Martinet", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Xiaodong Wang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Xiaofang Wang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Xiaoqing Ellen Tan", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Xide Xia", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Xinfeng Xie", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Xuchao Jia", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Xuewei Wang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Yaelle Goldschlag", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Yashesh Gaur", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Yasmine Babaei", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Yi Wen", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Yiwen Song", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Yuchen Zhang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Yue Li", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Yuning Mao", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Zacharie Delpierre Coudert", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Zheng Yan", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Zhengxing Chen", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Zoe Papakipos", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Aaditya Singh", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Aayushi Srivastava", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Abha Jain", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Adam Kelsey", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Adam Shajnfeld", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Adithya Gangidi", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Adolfo Victoria", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Ahuva Goldstand", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Ajay Menon", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Ajay Sharma", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Alex Boesenberg", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Alexei Baevski", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Allie Feinstein", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Amanda Kallet", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Amit Sangani", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Amos Teo", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Anam Yunus", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Andrei Lupu", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Andres Alvarado", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Andrew Caples", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Andrew Gu", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Andrew Ho", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Andrew Poulton", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Andrew Ryan", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Ankit Ramchandani", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Annie Dong", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Annie Franco", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Anuj Goyal", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Aparajita Saraf", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Arkabandhu Chowdhury", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Ashley Gabriel", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Ashwin Bharambe", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Assaf Eisenman", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Azadeh Yazdan", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Beau James", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Ben Maurer", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Benjamin Leonhardi", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Bernie Huang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Beth Loyd", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Beto De Paola", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Bhargavi Paranjape", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Bing Liu", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Bo Wu", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Boyu Ni", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Braden Hancock", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Bram Wasti", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Brandon Spence", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Brani Stojkovic", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Brian Gamido", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Britt Montalvo", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Carl Parker", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Carly Burton", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Catalina Mejia", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Ce Liu", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Changhan Wang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Changkyu Kim", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Chao Zhou", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Chester Hu", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Ching-Hsiang Chu", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Chris Cai", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Chris Tindal", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Christoph Feichtenhofer", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Cynthia Gao", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Damon Civin", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Dana Beaty", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Daniel Kreymer", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Daniel Li", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "David Adkins", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "David Xu", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Davide Testuggine", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Delia David", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Devi Parikh", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Diana Liskovich", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Didem Foss", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Dingkang Wang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Duc Le", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Dustin Holland", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Edward Dowling", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Eissa Jamil", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Elaine Montgomery", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Eleonora Presani", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Emily Hahn", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Emily Wood", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Eric-Tuan Le", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Erik Brinkman", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Esteban Arcaute", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Evan Dunbar", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Evan Smothers", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Fei Sun", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Felix Kreuk", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Feng Tian", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Filippos Kokkinos", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Firat Ozgenel", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Francesco Caggioni", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Frank Kanayet", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Frank Seide", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Gabriela Medina Florez", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Gabriella Schwarz", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Gada Badeer", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Georgia Swee", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Gil Halpern", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Grant Herman", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Grigory Sizov", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Guangyi", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Zhang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Guna Lakshminarayanan", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Hakan Inan", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Hamid Shojanazeri", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Han Zou", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Hannah Wang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Hanwen Zha", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Haroun Habeeb", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Harrison Rudolph", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Helen Suk", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Henry Aspegren", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Hunter Goldman", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Hongyuan Zhan", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Ibrahim Damlaj", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Igor Molybog", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Igor Tufanov", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Ilias Leontiadis", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Irina-Elena Veliche", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Itai Gat", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Jake Weissman", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "James Geboski", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "James Kohli", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Janice Lam", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Japhet Asher", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Jean-Baptiste Gaya", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Jeff Marcus", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Jeff Tang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Jennifer Chan", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Jenny Zhen", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Jeremy Reizenstein", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Jeremy Teboul", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Jessica Zhong", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Jian Jin", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Jingyi Yang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Joe Cummings", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Jon Carvill", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Jon Shepard", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Jonathan McPhie", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Jonathan Torres", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Josh Ginsburg", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Junjie Wang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Kai Wu", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Kam Hou U", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Karan Saxena", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Kartikay Khandelwal", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Katayoun Zand", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Kathy Matosich", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Kaushik Veeraraghavan", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Kelly Michelena", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Keqian Li", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Kiran Jagadeesh", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Kun Huang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Kunal Chawla", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Kyle Huang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Lailin Chen", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Lakshya Garg", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Lavender A", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Leandro Silva", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Lee Bell", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Lei Zhang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Liangpeng Guo", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Licheng Yu", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Liron Moshkovich", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Luca Wehrstedt", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Madian Khabsa", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Manav Avalani", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Manish Bhatt", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Martynas Mankus", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Matan Hasson", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Matthew Lennie", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Matthias Reso", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Maxim Groshev", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Maxim Naumov", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Maya Lathi", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Meghan Keneally", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Miao Liu", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Michael L. Seltzer", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Michal Valko", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Michelle Restrepo", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Mihir Patel", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Mik Vyatskov", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Mikayel Samvelyan", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Mike Clark", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Mike Macey", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Mike Wang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Miquel Jubert Hermoso", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Mo Metanat", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Mohammad Rastegari", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Munish Bansal", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Nandhini Santhanam", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Natascha Parks", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Natasha White", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Navyata Bawa", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Nayan Singhal", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Nick Egebo", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Nicolas Usunier", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Nikhil Mehta", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Nikolay Pavlovich Laptev", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Ning Dong", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Norman Cheng", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Oleg Chernoguz", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Olivia Hart", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Omkar Salpekar", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Ozlem Kalinli", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Parkin Kent", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Parth Parekh", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Paul Saab", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Pavan Balaji", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Pedro Rittner", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Philip Bontrager", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Pierre Roux", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Piotr Dollar", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Polina Zvyagina", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Prashant Ratanchandani", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Pritish Yuvraj", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Qian Liang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Rachad Alao", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Rachel Rodriguez", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Rafi Ayub", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Raghotham Murthy", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Raghu Nayani", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Rahul Mitra", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Rangaprabhu Parthasarathy", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Raymond Li", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Rebekkah Hogan", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Robin Battey", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Rocky Wang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Russ Howes", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Ruty Rinott", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Sachin Mehta", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Sachin Siby", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Sai Jayesh Bondu", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Samyak Datta", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Sara Chugh", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Sara Hunt", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Sargun Dhillon", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Sasha Sidorov", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Satadru Pan", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Saurabh Mahajan", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Saurabh Verma", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Seiji Yamamoto", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Sharadh Ramaswamy", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Shaun Lindsay", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Shaun Lindsay", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Sheng Feng", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Shenghao Lin", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Shengxin Cindy Zha", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Shishir Patil", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Shiva Shankar", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Shuqiang Zhang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Shuqiang Zhang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Sinong Wang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Sneha Agarwal", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Soji Sajuyigbe", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Soumith Chintala", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Stephanie Max", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Stephen Chen", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Steve Kehoe", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Steve Satterfield", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Sudarshan Govindaprasad", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Sumit Gupta", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Summer Deng", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Sungmin Cho", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Sunny Virk", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Suraj Subramanian", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Sy Choudhury", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Sydney Goldman", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Tal Remez", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Tamar Glaser", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Tamara Best", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Thilo Koehler", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Thomas Robinson", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Tianhe Li", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Tianjun Zhang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Tim Matthews", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Timothy Chou", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Tzook Shaked", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Varun Vontimitta", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Victoria Ajayi", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Victoria Montanez", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Vijai Mohan", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Vinay Satish Kumar", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Vishal Mangla", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Vlad Ionescu", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Vlad Poenaru", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Vlad Tiberiu Mihailescu", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Vladimir Ivanov", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Wei Li", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Wenchen Wang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Wenwen Jiang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Wes Bouaziz", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Will Constable", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Xiaocheng Tang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Xiaojian Wu", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Xiaolan Wang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Xilun Wu", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Xinbo Gao", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Yaniv Kleinman", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Yanjun Chen", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Ye Hu", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Ye Jia", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Ye Qi", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Yenda Li", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Yilin Zhang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Ying Zhang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Yossi Adi", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Youngjin Nam", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Yu", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Wang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Yu Zhao", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Yuchen Hao", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Yundi Qian", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Yunlu Li", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Yuzi He", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Zach Rait", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Zachary DeVito", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Zef Rosnbrick", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Zhaoduo Wen", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Zhenyu Yang", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Zhiwei Zhao", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Zhiyu Ma", "target": "The Llama 3 Herd of Models", "value": "author_of"}, {"source": "Tianyi Li", "target": "A Survey on Diffusion Language Models", "value": "author_of"}, {"source": "Mingda Chen", "target": "A Survey on Diffusion Language Models", "value": "author_of"}, {"source": "Bowei Guo", "target": "A Survey on Diffusion Language Models", "value": "author_of"}, {"source": "Zhiqiang Shen", "target": "A Survey on Diffusion Language Models", "value": "author_of"}, {"source": "Jinjie Ni", "target": "Training Optimal Large Diffusion Language Models", "value": "author_of"}, {"source": "Qian Liu", "target": "Training Optimal Large Diffusion Language Models", "value": "author_of"}, {"source": "Chao Du", "target": "Training Optimal Large Diffusion Language Models", "value": "author_of"}, {"source": "Longxu Dou", "target": "Training Optimal Large Diffusion Language Models", "value": "author_of"}, {"source": "Hang Yan", "target": "Training Optimal Large Diffusion Language Models", "value": "author_of"}, {"source": "Zili Wang", "target": "Training Optimal Large Diffusion Language Models", "value": "author_of"}, {"source": "Tianyu Pang", "target": "Training Optimal Large Diffusion Language Models", "value": "author_of"}, {"source": "Michael Qizhe Shieh", "target": "Training Optimal Large Diffusion Language Models", "value": "author_of"}, {"source": "Siyan Zhao", "target": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models", "value": "author_of"}, {"source": "Mengchen Liu", "target": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models", "value": "author_of"}, {"source": "Jing Huang", "target": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models", "value": "author_of"}, {"source": "Miao Liu", "target": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models", "value": "author_of"}, {"source": "Chenyu Wang", "target": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models", "value": "author_of"}, {"source": "Bo Liu", "target": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models", "value": "author_of"}, {"source": "Yuandong Tian", "target": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models", "value": "author_of"}, {"source": "Guan Pang", "target": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models", "value": "author_of"}, {"source": "Sean Bell", "target": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models", "value": "author_of"}, {"source": "Aditya Grover", "target": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models", "value": "author_of"}, {"source": "Feiyu Chen", "target": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models", "value": "author_of"}, {"source": "Marianne Arriola", "target": "Encoder-Decoder Diffusion Language Models for Efficient Training and Inference", "value": "author_of"}, {"source": "Yair Schiff", "target": "Encoder-Decoder Diffusion Language Models for Efficient Training and Inference", "value": "author_of"}, {"source": "Hao Phung", "target": "Encoder-Decoder Diffusion Language Models for Efficient Training and Inference", "value": "author_of"}, {"source": "Aaron Gokaslan", "target": "Encoder-Decoder Diffusion Language Models for Efficient Training and Inference", "value": "author_of"}, {"source": "Volodymyr Kuleshov", "target": "Encoder-Decoder Diffusion Language Models for Efficient Training and Inference", "value": "author_of"}, {"source": "Chenghao Fan", "target": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model", "value": "author_of"}, {"source": "Wen Heng", "target": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model", "value": "author_of"}, {"source": "Bo Li", "target": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model", "value": "author_of"}, {"source": "Sichen Liu", "target": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model", "value": "author_of"}, {"source": "Yuxuan Song", "target": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model", "value": "author_of"}, {"source": "Jing Su", "target": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model", "value": "author_of"}, {"source": "Xiaoye Qu", "target": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model", "value": "author_of"}, {"source": "Kai Shen", "target": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model", "value": "author_of"}, {"source": "Wei Wei", "target": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model", "value": "author_of"}, {"source": "Zhilin Yang", "target": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "value": "author_of"}, {"source": "Zihang Dai", "target": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "value": "author_of"}, {"source": "Yiming Yang", "target": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "value": "author_of"}, {"source": "Jaime Carbonell", "target": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "value": "author_of"}, {"source": "Ruslan Salakhutdinov", "target": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "value": "author_of"}, {"source": "Quoc V. Le", "target": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "value": "author_of"}, {"source": "N. Cambridge", "target": "Paper", "value": "author_of"}, {"source": "Jinze Bai", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Shuai Bai", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Yunfei Chu", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Zeyu Cui", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Kai Dang", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Xiaodong Deng", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Yang Fan", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Wenbin Ge", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Yu Han", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Fei Huang", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Binyuan Hui", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Luo Ji", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Mei Li", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Junyang Lin", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Runji Lin", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Dayiheng Liu", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Gao Liu", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Chengqiang Lu", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Keming Lu", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Jianxin Ma", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Rui Men", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Xingzhang Ren", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Xuancheng Ren", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Chuanqi Tan", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Sinan Tan", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Jianhong Tu", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Peng Wang", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Shijie Wang", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Wei Wang", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Shengguang Wu", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Benfeng Xu", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Jin Xu", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "An Yang", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Hao Yang", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Jian Yang", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Shusheng Yang", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Yang Yao", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Bowen Yu", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Hongyi Yuan", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Zheng Yuan", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Jianwei Zhang", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Xingxuan Zhang", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Yichang Zhang", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Zhenru Zhang", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Chang Zhou", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Jingren Zhou", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Xiaohuan Zhou", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Tianhang Zhu", "target": "Qwen Technical Report", "value": "author_of"}, {"source": "Baptiste Rozi\u00e8re", "target": "Code Llama: Open Foundation Models for Code", "value": "author_of"}, {"source": "Jonas Gehring", "target": "Code Llama: Open Foundation Models for Code", "value": "author_of"}, {"source": "Fabian Gloeckle", "target": "Code Llama: Open Foundation Models for Code", "value": "author_of"}, {"source": "Sten Sootla", "target": "Code Llama: Open Foundation Models for Code", "value": "author_of"}, {"source": "Itai Gat", "target": "Code Llama: Open Foundation Models for Code", "value": "author_of"}, {"source": "Xiaoqing Ellen Tan", "target": "Code Llama: Open Foundation Models for Code", "value": "author_of"}, {"source": "Yossi Adi", "target": "Code Llama: Open Foundation Models for Code", "value": "author_of"}, {"source": "Jingyu Liu", "target": "Code Llama: Open Foundation Models for Code", "value": "author_of"}, {"source": "Romain Sauvestre", "target": "Code Llama: Open Foundation Models for Code", "value": "author_of"}, {"source": "Tal Remez", "target": "Code Llama: Open Foundation Models for Code", "value": "author_of"}, {"source": "J\u00e9r\u00e9my Rapin", "target": "Code Llama: Open Foundation Models for Code", "value": "author_of"}, {"source": "Artyom Kozhevnikov", "target": "Code Llama: Open Foundation Models for Code", "value": "author_of"}, {"source": "Ivan Evtimov", "target": "Code Llama: Open Foundation Models for Code", "value": "author_of"}, {"source": "Joanna Bitton", "target": "Code Llama: Open Foundation Models for Code", "value": "author_of"}, {"source": "Manish Bhatt", "target": "Code Llama: Open Foundation Models for Code", "value": "author_of"}, {"source": "Cristian Canton Ferrer", "target": "Code Llama: Open Foundation Models for Code", "value": "author_of"}, {"source": "Aaron Grattafiori", "target": "Code Llama: Open Foundation Models for Code", "value": "author_of"}, {"source": "Wenhan Xiong", "target": "Code Llama: Open Foundation Models for Code", "value": "author_of"}, {"source": "Alexandre D\u00e9fossez", "target": "Code Llama: Open Foundation Models for Code", "value": "author_of"}, {"source": "Jade Copet", "target": "Code Llama: Open Foundation Models for Code", "value": "author_of"}, {"source": "Faisal Azhar", "target": "Code Llama: Open Foundation Models for Code", "value": "author_of"}, {"source": "Hugo Touvron", "target": "Code Llama: Open Foundation Models for Code", "value": "author_of"}, {"source": "Louis Martin", "target": "Code Llama: Open Foundation Models for Code", "value": "author_of"}, {"source": "Nicolas Usunier", "target": "Code Llama: Open Foundation Models for Code", "value": "author_of"}, {"source": "Thomas Scialom", "target": "Code Llama: Open Foundation Models for Code", "value": "author_of"}, {"source": "Gabriel Synnaeve", "target": "Code Llama: Open Foundation Models for Code", "value": "author_of"}, {"source": "Yicun Yang", "target": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way", "value": "author_of"}, {"source": "Cong Wang", "target": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way", "value": "author_of"}, {"source": "Shaobo Wang", "target": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way", "value": "author_of"}, {"source": "Zichen Wen", "target": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way", "value": "author_of"}, {"source": "Biqing Qi", "target": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way", "value": "author_of"}, {"source": "Hanlin Xu", "target": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way", "value": "author_of"}, {"source": "Linfeng Zhang", "target": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way", "value": "author_of"}, {"source": "Itai Gat", "target": "Set Block Decoding is a Language Model Inference Accelerator", "value": "author_of"}, {"source": "Heli Ben-Hamu", "target": "Set Block Decoding is a Language Model Inference Accelerator", "value": "author_of"}, {"source": "Marton Havasi", "target": "Set Block Decoding is a Language Model Inference Accelerator", "value": "author_of"}, {"source": "Daniel Haziza", "target": "Set Block Decoding is a Language Model Inference Accelerator", "value": "author_of"}, {"source": "Jeremy Reizenstein", "target": "Set Block Decoding is a Language Model Inference Accelerator", "value": "author_of"}, {"source": "Gabriel Synnaeve", "target": "Set Block Decoding is a Language Model Inference Accelerator", "value": "author_of"}, {"source": "David Lopez-Paz", "target": "Set Block Decoding is a Language Model Inference Accelerator", "value": "author_of"}, {"source": "Brian Karrer", "target": "Set Block Decoding is a Language Model Inference Accelerator", "value": "author_of"}, {"source": "Yaron Lipman", "target": "Set Block Decoding is a Language Model Inference Accelerator", "value": "author_of"}, {"source": "John Nguyen", "target": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows", "value": "author_of"}, {"source": "Marton Havasi", "target": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows", "value": "author_of"}, {"source": "Tariq Berrada", "target": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows", "value": "author_of"}, {"source": "Luke Zettlemoyer", "target": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows", "value": "author_of"}, {"source": "Ricky T. Q. Chen", "target": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows", "value": "author_of"}, {"source": "Niels M\u00fcndler", "target": "Constrained Decoding of Diffusion LLMs with Context-Free Grammars", "value": "author_of"}, {"source": "Jasper Dekoninck", "target": "Constrained Decoding of Diffusion LLMs with Context-Free Grammars", "value": "author_of"}, {"source": "Martin Vechev", "target": "Constrained Decoding of Diffusion LLMs with Context-Free Grammars", "value": "author_of"}, {"source": "Julianna Piskorz", "target": "Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models", "value": "author_of"}, {"source": "Cristina Pinneri", "target": "Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models", "value": "author_of"}, {"source": "Alvaro Correia", "target": "Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models", "value": "author_of"}, {"source": "Motasem Alfarra", "target": "Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models", "value": "author_of"}, {"source": "Risheek Garrepalli", "target": "Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models", "value": "author_of"}, {"source": "Christos Louizos", "target": "Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models", "value": "author_of"}, {"source": "Chang Yang", "target": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications", "value": "author_of"}, {"source": "Chuang Zhou", "target": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications", "value": "author_of"}, {"source": "Yilin Xiao", "target": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications", "value": "author_of"}, {"source": "Su Dong", "target": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications", "value": "author_of"}, {"source": "Luyao Zhuang", "target": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications", "value": "author_of"}, {"source": "Yujing Zhang", "target": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications", "value": "author_of"}, {"source": "Zhu Wang", "target": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications", "value": "author_of"}, {"source": "Zijin Hong", "target": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications", "value": "author_of"}, {"source": "Zheng Yuan", "target": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications", "value": "author_of"}, {"source": "Zhishang Xiang", "target": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications", "value": "author_of"}, {"source": "Shengyuan Chen", "target": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications", "value": "author_of"}, {"source": "Huachi Zhou", "target": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications", "value": "author_of"}, {"source": "Qinggang Zhang", "target": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications", "value": "author_of"}, {"source": "Ninghao Liu", "target": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications", "value": "author_of"}, {"source": "Jinsong Su", "target": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications", "value": "author_of"}, {"source": "Xinrun Wang", "target": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications", "value": "author_of"}, {"source": "Yi Chang", "target": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications", "value": "author_of"}, {"source": "Xiao Huang", "target": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications", "value": "author_of"}, {"source": "Hao Lu", "target": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search", "value": "author_of"}, {"source": "Haoyuan Huang", "target": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search", "value": "author_of"}, {"source": "Yulin Zhou", "target": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search", "value": "author_of"}, {"source": "Chen Li", "target": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search", "value": "author_of"}, {"source": "Ningxin Zhu", "target": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search", "value": "author_of"}, {"source": "Yu Cheng", "target": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking", "value": "author_of"}, {"source": "Jiuan Zhou", "target": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking", "value": "author_of"}, {"source": "Yongkang Hu", "target": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking", "value": "author_of"}, {"source": "Yihang Chen", "target": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking", "value": "author_of"}, {"source": "Huichi Zhou", "target": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking", "value": "author_of"}, {"source": "Mingang Chen", "target": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking", "value": "author_of"}, {"source": "Zhizhong Zhang", "target": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking", "value": "author_of"}, {"source": "Kun Shao", "target": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking", "value": "author_of"}, {"source": "Yuan Xie", "target": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking", "value": "author_of"}, {"source": "Zhaoxia Yin", "target": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking", "value": "author_of"}, {"source": "Qirui Mi", "target": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents", "value": "author_of"}, {"source": "Zhijian Ma", "target": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents", "value": "author_of"}, {"source": "Mengyue Yang", "target": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents", "value": "author_of"}, {"source": "Haoxuan Li", "target": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents", "value": "author_of"}, {"source": "Yisen Wang", "target": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents", "value": "author_of"}, {"source": "Haifeng Zhang", "target": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents", "value": "author_of"}, {"source": "Jun Wang", "target": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents", "value": "author_of"}, {"source": "Mingyue Cheng", "target": "Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting", "value": "author_of"}, {"source": "Xiaoyu Tao", "target": "Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting", "value": "author_of"}, {"source": "Qi Liu", "target": "Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting", "value": "author_of"}, {"source": "Ze Guo", "target": "Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting", "value": "author_of"}, {"source": "Enhong Chen", "target": "Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting", "value": "author_of"}, {"source": "Yue Ma", "target": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation", "value": "author_of"}, {"source": "Zexuan Yan", "target": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation", "value": "author_of"}, {"source": "Hongyu Liu", "target": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation", "value": "author_of"}, {"source": "Hongfa Wang", "target": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation", "value": "author_of"}, {"source": "Heng Pan", "target": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation", "value": "author_of"}, {"source": "Yingqing He", "target": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation", "value": "author_of"}, {"source": "Junkun Yuan", "target": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation", "value": "author_of"}, {"source": "Ailing Zeng", "target": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation", "value": "author_of"}, {"source": "Chengfei Cai", "target": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation", "value": "author_of"}, {"source": "Heung-Yeung Shum", "target": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation", "value": "author_of"}, {"source": "Zhifeng Li", "target": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation", "value": "author_of"}, {"source": "Wei Liu", "target": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation", "value": "author_of"}, {"source": "Linfeng Zhang", "target": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation", "value": "author_of"}, {"source": "Qifeng Chen", "target": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation", "value": "author_of"}, {"source": "Kunyu Feng", "target": "Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis", "value": "author_of"}, {"source": "Yue Ma", "target": "Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis", "value": "author_of"}, {"source": "Xinhua Zhang", "target": "Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis", "value": "author_of"}, {"source": "Boshi Liu", "target": "Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis", "value": "author_of"}, {"source": "Yikuang Yuluo", "target": "Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis", "value": "author_of"}, {"source": "Yinhan Zhang", "target": "Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis", "value": "author_of"}, {"source": "Runtao Liu", "target": "Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis", "value": "author_of"}, {"source": "Hongyu Liu", "target": "Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis", "value": "author_of"}, {"source": "Zhiyuan Qin", "target": "Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis", "value": "author_of"}, {"source": "Shanhui Mo", "target": "Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis", "value": "author_of"}, {"source": "Qifeng Chen", "target": "Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis", "value": "author_of"}, {"source": "Zeyu Wang", "target": "Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis", "value": "author_of"}, {"source": "Xinyao Liao", "target": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing", "value": "author_of"}, {"source": "Xianfang Zeng", "target": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing", "value": "author_of"}, {"source": "Ziye Song", "target": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing", "value": "author_of"}, {"source": "Zhoujie Fu", "target": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing", "value": "author_of"}, {"source": "Gang Yu", "target": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing", "value": "author_of"}, {"source": "Guosheng Lin", "target": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing", "value": "author_of"}, {"source": "Zeyu Zhu", "target": "Paper2Video: Automatic Video Generation from Scientific Papers", "value": "author_of"}, {"source": "Kevin Qinghong Lin", "target": "Paper2Video: Automatic Video Generation from Scientific Papers", "value": "author_of"}, {"source": "Mike Zheng Shou", "target": "Paper2Video: Automatic Video Generation from Scientific Papers", "value": "author_of"}, {"source": "Yiyang Chen", "target": "ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment", "value": "author_of"}, {"source": "Xuanhua He", "target": "ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment", "value": "author_of"}, {"source": "Xiujun Ma", "target": "ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment", "value": "author_of"}, {"source": "Yue Ma", "target": "ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment", "value": "author_of"}, {"source": "DeepSeek-AI", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Daya Guo", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Dejian Yang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Haowei Zhang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Junxiao Song", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Peiyi Wang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Qihao Zhu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Runxin Xu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Ruoyu Zhang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Shirong Ma", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Xiao Bi", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Xiaokang Zhang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Xingkai Yu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Yu Wu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Z. F. Wu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Zhibin Gou", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Zhihong Shao", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Zhuoshu Li", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Ziyi Gao", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Aixin Liu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Bing Xue", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Bingxuan Wang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Bochao Wu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Bei Feng", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Chengda Lu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Chenggang Zhao", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Chengqi Deng", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Chenyu Zhang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Chong Ruan", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Damai Dai", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Deli Chen", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Dongjie Ji", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Erhang Li", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Fangyun Lin", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Fucong Dai", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Fuli Luo", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Guangbo Hao", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Guanting Chen", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Guowei Li", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "H. Zhang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Han Bao", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Hanwei Xu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Haocheng Wang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Honghui Ding", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Huajian Xin", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Huazuo Gao", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Hui Qu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Hui Li", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Jianzhong Guo", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Jiashi Li", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Jiawei Wang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Jingchang Chen", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Jingyang Yuan", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Junjie Qiu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Junlong Li", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "J. L. Cai", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Jiaqi Ni", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Jian Liang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Jin Chen", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Kai Dong", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Kai Hu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Kaige Gao", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Kang Guan", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Kexin Huang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Kuai Yu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Lean Wang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Lecong Zhang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Liang Zhao", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Litong Wang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Liyue Zhang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Lei Xu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Leyi Xia", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Mingchuan Zhang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Minghua Zhang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Minghui Tang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Meng Li", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Miaojun Wang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Mingming Li", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Ning Tian", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Panpan Huang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Peng Zhang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Qiancheng Wang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Qinyu Chen", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Qiushi Du", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Ruiqi Ge", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Ruisong Zhang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Ruizhe Pan", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Runji Wang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "R. J. Chen", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "R. L. Jin", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Ruyi Chen", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Shanghao Lu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Shangyan Zhou", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Shanhuang Chen", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Shengfeng Ye", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Shiyu Wang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Shuiping Yu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Shunfeng Zhou", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Shuting Pan", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "S. S. Li", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Shuang Zhou", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Shaoqing Wu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Shengfeng Ye", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Tao Yun", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Tian Pei", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Tianyu Sun", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "T. Wang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Wangding Zeng", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Wanjia Zhao", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Wen Liu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Wenfeng Liang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Wenjun Gao", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Wenqin Yu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Wentao Zhang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "W. L. Xiao", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Wei An", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Xiaodong Liu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Xiaohan Wang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Xiaokang Chen", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Xiaotao Nie", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Xin Cheng", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Xin Liu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Xin Xie", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Xingchao Liu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Xinyu Yang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Xinyuan Li", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Xuecheng Su", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Xuheng Lin", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "X. Q. Li", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Xiangyue Jin", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Xiaojin Shen", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Xiaosha Chen", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Xiaowen Sun", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Xiaoxiang Wang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Xinnan Song", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Xinyi Zhou", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Xianzu Wang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Xinxia Shan", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Y. K. Li", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Y. Q. Wang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Y. X. Wei", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Yang Zhang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Yanhong Xu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Yao Li", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Yao Zhao", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Yaofeng Sun", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Yaohui Wang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Yi Yu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Yichao Zhang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Yifan Shi", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Yiliang Xiong", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Ying He", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Yishi Piao", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Yisong Wang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Yixuan Tan", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Yiyang Ma", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Yiyuan Liu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Yongqiang Guo", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Yuan Ou", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Yuduan Wang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Yue Gong", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Yuheng Zou", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Yujia He", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Yunfan Xiong", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Yuxiang Luo", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Yuxiang You", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Yuxuan Liu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Yuyang Zhou", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Y. X. Zhu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Yanhong Xu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Yanping Huang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Yaohui Li", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Yi Zheng", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Yuchen Zhu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Yunxian Ma", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Ying Tang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Yukun Zha", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Yuting Yan", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Z. Z. Ren", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Zehui Ren", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Zhangli Sha", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Zhe Fu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Zhean Xu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Zhenda Xie", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Zhengyan Zhang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Zhewen Hao", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Zhicheng Ma", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Zhigang Yan", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Zhiyu Wu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Zihui Gu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Zijia Zhu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Zijun Liu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Zilin Li", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Ziwei Xie", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Ziyang Song", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Zizheng Pan", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Zhen Huang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Zhipeng Xu", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Zhongyu Zhang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Zhen Zhang", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "author_of"}, {"source": "Marah Abdin", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Jyoti Aneja", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Hany Awadalla", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Ahmed Awadallah", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Ammar Ahmad Awan", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Nguyen Bach", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Amit Bahree", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Arash Bakhtiari", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Jianmin Bao", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Harkirat Behl", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Alon Benhaim", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Misha Bilenko", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Johan Bjorck", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "S\u00e9bastien Bubeck", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Martin Cai", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Qin Cai", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Vishrav Chaudhary", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Dong Chen", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Dongdong Chen", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Weizhu Chen", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Yen-Chun Chen", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Yi-Ling Chen", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Hao Cheng", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Parul Chopra", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Xiyang Dai", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Matthew Dixon", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Ronen Eldan", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Victor Fragoso", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Jianfeng Gao", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Mei Gao", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Min Gao", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Amit Garg", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Allie Del Giorno", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Abhishek Goswami", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Suriya Gunasekar", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Emman Haider", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Junheng Hao", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Russell J. Hewett", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Wenxiang Hu", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Jamie Huynh", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Dan Iter", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Sam Ade Jacobs", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Mojan Javaheripi", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Xin Jin", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Nikos Karampatziakis", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Piero Kauffmann", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Mahoud Khademi", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Dongwoo Kim", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Young Jin Kim", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Lev Kurilenko", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "James R. Lee", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Yin Tat Lee", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Yuanzhi Li", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Yunsheng Li", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Chen Liang", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Lars Liden", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Xihui Lin", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Zeqi Lin", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Ce Liu", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Liyuan Liu", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Mengchen Liu", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Weishung Liu", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Xiaodong Liu", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Chong Luo", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Piyush Madan", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Ali Mahmoudzadeh", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "David Majercak", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Matt Mazzola", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Caio C\u00e9sar Teodoro Mendes", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Arindam Mitra", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Hardik Modi", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Anh Nguyen", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Brandon Norick", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Barun Patra", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Daniel Perez-Becker", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Thomas Portet", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Reid Pryzant", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Heyang Qin", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Marko Radmilac", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Liliang Ren", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Gustavo de Rosa", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Corby Rosset", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Sambudha Roy", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Olatunji Ruwase", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Olli Saarikivi", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Amin Saied", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Adil Salim", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Michael Santacroce", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Shital Shah", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Ning Shang", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Hiteshi Sharma", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Yelong Shen", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Swadheen Shukla", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Xia Song", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Masahiro Tanaka", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Andrea Tupini", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Praneetha Vaddamanu", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Chunyu Wang", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Guanhua Wang", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Lijuan Wang", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Shuohang Wang", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Xin Wang", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Yu Wang", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Rachel Ward", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Wen Wen", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Philipp Witte", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Haiping Wu", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Xiaoxia Wu", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Michael Wyatt", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Bin Xiao", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Can Xu", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Jiahang Xu", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Weijian Xu", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Jilong Xue", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Sonali Yadav", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Fan Yang", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Jianwei Yang", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Yifan Yang", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Ziyi Yang", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Donghan Yu", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Lu Yuan", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Chenruidong Zhang", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Cyril Zhang", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Jianwen Zhang", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Li Lyna Zhang", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Yi Zhang", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Yue Zhang", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Yunan Zhang", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Xiren Zhou", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "author_of"}, {"source": "Bo Li", "target": "LLaVA-OneVision: Easy Visual Task Transfer", "value": "author_of"}, {"source": "Yuanhan Zhang", "target": "LLaVA-OneVision: Easy Visual Task Transfer", "value": "author_of"}, {"source": "Dong Guo", "target": "LLaVA-OneVision: Easy Visual Task Transfer", "value": "author_of"}, {"source": "Renrui Zhang", "target": "LLaVA-OneVision: Easy Visual Task Transfer", "value": "author_of"}, {"source": "Feng Li", "target": "LLaVA-OneVision: Easy Visual Task Transfer", "value": "author_of"}, {"source": "Hao Zhang", "target": "LLaVA-OneVision: Easy Visual Task Transfer", "value": "author_of"}, {"source": "Kaichen Zhang", "target": "LLaVA-OneVision: Easy Visual Task Transfer", "value": "author_of"}, {"source": "Peiyuan Zhang", "target": "LLaVA-OneVision: Easy Visual Task Transfer", "value": "author_of"}, {"source": "Yanwei Li", "target": "LLaVA-OneVision: Easy Visual Task Transfer", "value": "author_of"}, {"source": "Ziwei Liu", "target": "LLaVA-OneVision: Easy Visual Task Transfer", "value": "author_of"}, {"source": "Chunyuan Li", "target": "LLaVA-OneVision: Easy Visual Task Transfer", "value": "author_of"}, {"source": "Weichen Liu", "target": "Spatial Reasoning in Multimodal Large Language Models: A Survey of Tasks, Benchmarks and Methods", "value": "author_of"}, {"source": "Qiyao Xue", "target": "Spatial Reasoning in Multimodal Large Language Models: A Survey of Tasks, Benchmarks and Methods", "value": "author_of"}, {"source": "Haoming Wang", "target": "Spatial Reasoning in Multimodal Large Language Models: A Survey of Tasks, Benchmarks and Methods", "value": "author_of"}, {"source": "Xiangyu Yin", "target": "Spatial Reasoning in Multimodal Large Language Models: A Survey of Tasks, Benchmarks and Methods", "value": "author_of"}, {"source": "Boyuan Yang", "target": "Spatial Reasoning in Multimodal Large Language Models: A Survey of Tasks, Benchmarks and Methods", "value": "author_of"}, {"source": "Wei Gao", "target": "Spatial Reasoning in Multimodal Large Language Models: A Survey of Tasks, Benchmarks and Methods", "value": "author_of"}, {"source": "Yuxi Xiao", "target": "SpatialTree: How Spatial Abilities Branch Out in MLLMs", "value": "author_of"}, {"source": "Longfei Li", "target": "SpatialTree: How Spatial Abilities Branch Out in MLLMs", "value": "author_of"}, {"source": "Shen Yan", "target": "SpatialTree: How Spatial Abilities Branch Out in MLLMs", "value": "author_of"}, {"source": "Xinhang Liu", "target": "SpatialTree: How Spatial Abilities Branch Out in MLLMs", "value": "author_of"}, {"source": "Sida Peng", "target": "SpatialTree: How Spatial Abilities Branch Out in MLLMs", "value": "author_of"}, {"source": "Yunchao Wei", "target": "SpatialTree: How Spatial Abilities Branch Out in MLLMs", "value": "author_of"}, {"source": "Xiaowei Zhou", "target": "SpatialTree: How Spatial Abilities Branch Out in MLLMs", "value": "author_of"}, {"source": "Bingyi Kang", "target": "SpatialTree: How Spatial Abilities Branch Out in MLLMs", "value": "author_of"}, {"source": "Mingrui Wu", "target": "From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs", "value": "author_of"}, {"source": "Zhaozhi Wang", "target": "From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs", "value": "author_of"}, {"source": "Fangjinhua Wang", "target": "From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs", "value": "author_of"}, {"source": "Jiaolong Yang", "target": "From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs", "value": "author_of"}, {"source": "Marc Pollefeys", "target": "From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs", "value": "author_of"}, {"source": "Tong Zhang", "target": "From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs", "value": "author_of"}, {"source": "Meng Cao", "target": "SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery", "value": "author_of"}, {"source": "Xingyu Li", "target": "SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery", "value": "author_of"}, {"source": "Xue Liu", "target": "SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery", "value": "author_of"}, {"source": "Ian Reid", "target": "SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery", "value": "author_of"}, {"source": "Xiaodan Liang", "target": "SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery", "value": "author_of"}, {"source": "Joseph Redmon", "target": "You Only Look Once: Unified, Real-Time Object Detection", "value": "author_of"}, {"source": "Santosh Divvala", "target": "You Only Look Once: Unified, Real-Time Object Detection", "value": "author_of"}, {"source": "Ross Girshick", "target": "You Only Look Once: Unified, Real-Time Object Detection", "value": "author_of"}, {"source": "Ali Farhadi", "target": "You Only Look Once: Unified, Real-Time Object Detection", "value": "author_of"}, {"source": "Zhichao Sun", "target": "RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes", "value": "author_of"}, {"source": "Yepeng Liu", "target": "RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes", "value": "author_of"}, {"source": "Zhiling Su", "target": "RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes", "value": "author_of"}, {"source": "Huachao Zhu", "target": "RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes", "value": "author_of"}, {"source": "Yuliang Gu", "target": "RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes", "value": "author_of"}, {"source": "Yuda Zou", "target": "RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes", "value": "author_of"}, {"source": "Zelong Liu", "target": "RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes", "value": "author_of"}, {"source": "Gui-Song Xia", "target": "RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes", "value": "author_of"}, {"source": "Bo Du", "target": "RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes", "value": "author_of"}, {"source": "Yongchao Xu", "target": "RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes", "value": "author_of"}, {"source": "Shenghao Fu", "target": "WeDetect: Fast Open-Vocabulary Object Detection as Retrieval", "value": "author_of"}, {"source": "Yukun Su", "target": "WeDetect: Fast Open-Vocabulary Object Detection as Retrieval", "value": "author_of"}, {"source": "Fengyun Rao", "target": "WeDetect: Fast Open-Vocabulary Object Detection as Retrieval", "value": "author_of"}, {"source": "Jing Lyu", "target": "WeDetect: Fast Open-Vocabulary Object Detection as Retrieval", "value": "author_of"}, {"source": "Xiaohua Xie", "target": "WeDetect: Fast Open-Vocabulary Object Detection as Retrieval", "value": "author_of"}, {"source": "Wei-Shi Zheng", "target": "WeDetect: Fast Open-Vocabulary Object Detection as Retrieval", "value": "author_of"}, {"source": "Jiwan Chung", "target": "v1: Learning to Point Visual Tokens for Multimodal Grounded Reasoning", "value": "author_of"}, {"source": "Junhyeok Kim", "target": "v1: Learning to Point Visual Tokens for Multimodal Grounded Reasoning", "value": "author_of"}, {"source": "Siyeol Kim", "target": "v1: Learning to Point Visual Tokens for Multimodal Grounded Reasoning", "value": "author_of"}, {"source": "Jaeyoung Lee", "target": "v1: Learning to Point Visual Tokens for Multimodal Grounded Reasoning", "value": "author_of"}, {"source": "Min Soo Kim", "target": "v1: Learning to Point Visual Tokens for Multimodal Grounded Reasoning", "value": "author_of"}, {"source": "Youngjae Yu", "target": "v1: Learning to Point Visual Tokens for Multimodal Grounded Reasoning", "value": "author_of"}, {"source": "Jiazhe Wei", "target": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design", "value": "author_of"}, {"source": "Ken Li", "target": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design", "value": "author_of"}, {"source": "Tianyu Lao", "target": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design", "value": "author_of"}, {"source": "Haofan Wang", "target": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design", "value": "author_of"}, {"source": "Liang Wang", "target": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design", "value": "author_of"}, {"source": "Caifeng Shan", "target": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design", "value": "author_of"}, {"source": "Chenyang Si", "target": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design", "value": "author_of"}, {"source": "Jianhua Han", "target": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Meng Tian", "target": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Jiangtong Zhu", "target": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Fan He", "target": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Huixin Zhang", "target": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Sitong Guo", "target": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Dechang Zhu", "target": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Hao Tang", "target": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Pei Xu", "target": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Yuze Guo", "target": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Minzhe Niu", "target": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Haojie Zhu", "target": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Qichao Dong", "target": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Xuechao Yan", "target": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Siyuan Dong", "target": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Lu Hou", "target": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Qingqiu Huang", "target": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Xiaosong Jia", "target": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Hang Xu", "target": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving", "value": "author_of"}, {"source": "R. S. Sutton", "target": "Reinforcement Learning: An Introduction", "value": "author_of"}, {"source": "A. Barto", "target": "Reinforcement Learning: An Introduction", "value": "author_of"}, {"source": "Timothy P. Lillicrap", "target": "Continuous control with deep reinforcement learning", "value": "author_of"}, {"source": "Jonathan J. Hunt", "target": "Continuous control with deep reinforcement learning", "value": "author_of"}, {"source": "Alexander Pritzel", "target": "Continuous control with deep reinforcement learning", "value": "author_of"}, {"source": "Nicolas Heess", "target": "Continuous control with deep reinforcement learning", "value": "author_of"}, {"source": "Tom Erez", "target": "Continuous control with deep reinforcement learning", "value": "author_of"}, {"source": "Yuval Tassa", "target": "Continuous control with deep reinforcement learning", "value": "author_of"}, {"source": "David Silver", "target": "Continuous control with deep reinforcement learning", "value": "author_of"}, {"source": "Daan Wierstra", "target": "Continuous control with deep reinforcement learning", "value": "author_of"}, {"source": "Shenzhi Wang", "target": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning", "value": "author_of"}, {"source": "Le Yu", "target": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning", "value": "author_of"}, {"source": "Chang Gao", "target": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning", "value": "author_of"}, {"source": "Chujie Zheng", "target": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning", "value": "author_of"}, {"source": "Shixuan Liu", "target": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning", "value": "author_of"}, {"source": "Rui Lu", "target": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning", "value": "author_of"}, {"source": "Kai Dang", "target": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning", "value": "author_of"}, {"source": "Xionghui Chen", "target": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning", "value": "author_of"}, {"source": "Jianxin Yang", "target": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning", "value": "author_of"}, {"source": "Zhenru Zhang", "target": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning", "value": "author_of"}, {"source": "Yuqiong Liu", "target": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning", "value": "author_of"}, {"source": "An Yang", "target": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning", "value": "author_of"}, {"source": "Andrew Zhao", "target": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning", "value": "author_of"}, {"source": "Yang Yue", "target": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning", "value": "author_of"}, {"source": "Shiji Song", "target": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning", "value": "author_of"}, {"source": "Bowen Yu", "target": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning", "value": "author_of"}, {"source": "Gao Huang", "target": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning", "value": "author_of"}, {"source": "Junyang Lin", "target": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning", "value": "author_of"}, {"source": "Ganqu Cui", "target": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models", "value": "author_of"}, {"source": "Yuchen Zhang", "target": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models", "value": "author_of"}, {"source": "Jiacheng Chen", "target": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models", "value": "author_of"}, {"source": "Lifan Yuan", "target": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models", "value": "author_of"}, {"source": "Zhi Wang", "target": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models", "value": "author_of"}, {"source": "Yuxin Zuo", "target": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models", "value": "author_of"}, {"source": "Haozhan Li", "target": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models", "value": "author_of"}, {"source": "Yuchen Fan", "target": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models", "value": "author_of"}, {"source": "Huayu Chen", "target": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models", "value": "author_of"}, {"source": "Weize Chen", "target": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models", "value": "author_of"}, {"source": "Zhiyuan Liu", "target": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models", "value": "author_of"}, {"source": "Hao Peng", "target": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models", "value": "author_of"}, {"source": "Lei Bai", "target": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models", "value": "author_of"}, {"source": "Wanli Ouyang", "target": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models", "value": "author_of"}, {"source": "Yu Cheng", "target": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models", "value": "author_of"}, {"source": "Bowen Zhou", "target": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models", "value": "author_of"}, {"source": "Ning Ding", "target": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models", "value": "author_of"}, {"source": "Jianhao Yan", "target": "Learning to Reason under Off-Policy Guidance", "value": "author_of"}, {"source": "Yafu Li", "target": "Learning to Reason under Off-Policy Guidance", "value": "author_of"}, {"source": "Zican Hu", "target": "Learning to Reason under Off-Policy Guidance", "value": "author_of"}, {"source": "Zhi Wang", "target": "Learning to Reason under Off-Policy Guidance", "value": "author_of"}, {"source": "Ganqu Cui", "target": "Learning to Reason under Off-Policy Guidance", "value": "author_of"}, {"source": "Xiaoye Qu", "target": "Learning to Reason under Off-Policy Guidance", "value": "author_of"}, {"source": "Yu Cheng", "target": "Learning to Reason under Off-Policy Guidance", "value": "author_of"}, {"source": "Yue Zhang", "target": "Learning to Reason under Off-Policy Guidance", "value": "author_of"}, {"source": "Komal Kumar", "target": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models", "value": "author_of"}, {"source": "Tajamul Ashraf", "target": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models", "value": "author_of"}, {"source": "Omkar Thawakar", "target": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models", "value": "author_of"}, {"source": "Rao Muhammad Anwer", "target": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models", "value": "author_of"}, {"source": "Hisham Cholakkal", "target": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models", "value": "author_of"}, {"source": "Mubarak Shah", "target": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models", "value": "author_of"}, {"source": "Ming-Hsuan Yang", "target": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models", "value": "author_of"}, {"source": "Phillip H. S. Torr", "target": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models", "value": "author_of"}, {"source": "Fahad Shahbaz Khan", "target": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models", "value": "author_of"}, {"source": "Salman Khan", "target": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models", "value": "author_of"}, {"source": "Nicolas Le Roux", "target": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement learning for LLMs", "value": "author_of"}, {"source": "Marc G. Bellemare", "target": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement learning for LLMs", "value": "author_of"}, {"source": "Jonathan Lebensold", "target": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement learning for LLMs", "value": "author_of"}, {"source": "Arnaud Bergeron", "target": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement learning for LLMs", "value": "author_of"}, {"source": "Joshua Greaves", "target": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement learning for LLMs", "value": "author_of"}, {"source": "Alex Fr\u00e9chette", "target": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement learning for LLMs", "value": "author_of"}, {"source": "Carolyne Pelletier", "target": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement learning for LLMs", "value": "author_of"}, {"source": "Eric Thibodeau-Laufer", "target": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement learning for LLMs", "value": "author_of"}, {"source": "S\u00e1ndor Toth", "target": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement learning for LLMs", "value": "author_of"}, {"source": "Sam Work", "target": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement learning for LLMs", "value": "author_of"}, {"source": "Guosheng Lin", "target": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation", "value": "author_of"}, {"source": "Anton Milan", "target": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation", "value": "author_of"}, {"source": "Chunhua Shen", "target": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation", "value": "author_of"}, {"source": "Ian Reid", "target": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation", "value": "author_of"}, {"source": "Evan Shelhamer", "target": "Fully convolutional networks for semantic segmentation", "value": "author_of"}, {"source": "Jonathan Long", "target": "Fully convolutional networks for semantic segmentation", "value": "author_of"}, {"source": "Trevor Darrell", "target": "Fully convolutional networks for semantic segmentation", "value": "author_of"}, {"source": "Florinel-Alin Croitoru", "target": "Diffusion Models in Vision: A Survey", "value": "author_of"}, {"source": "Vlad Hondru", "target": "Diffusion Models in Vision: A Survey", "value": "author_of"}, {"source": "Radu Tudor Ionescu", "target": "Diffusion Models in Vision: A Survey", "value": "author_of"}, {"source": "Mubarak Shah", "target": "Diffusion Models in Vision: A Survey", "value": "author_of"}, {"source": "Meng-Hao Guo", "target": "SegNeXt: Rethinking Convolutional Attention Design for Semantic Segmentation", "value": "author_of"}, {"source": "Cheng-Ze Lu", "target": "SegNeXt: Rethinking Convolutional Attention Design for Semantic Segmentation", "value": "author_of"}, {"source": "Qibin Hou", "target": "SegNeXt: Rethinking Convolutional Attention Design for Semantic Segmentation", "value": "author_of"}, {"source": "Zhengning Liu", "target": "SegNeXt: Rethinking Convolutional Attention Design for Semantic Segmentation", "value": "author_of"}, {"source": "Ming-Ming Cheng", "target": "SegNeXt: Rethinking Convolutional Attention Design for Semantic Segmentation", "value": "author_of"}, {"source": "Shi-Min Hu", "target": "SegNeXt: Rethinking Convolutional Attention Design for Semantic Segmentation", "value": "author_of"}, {"source": "Jiaming Zhang", "target": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation With Transformers", "value": "author_of"}, {"source": "Huayao Liu", "target": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation With Transformers", "value": "author_of"}, {"source": "Kailun Yang", "target": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation With Transformers", "value": "author_of"}, {"source": "Xinxin Hu", "target": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation With Transformers", "value": "author_of"}, {"source": "Ruiping Liu", "target": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation With Transformers", "value": "author_of"}, {"source": "Rainer Stiefelhagen", "target": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation With Transformers", "value": "author_of"}, {"source": "Lei Ke", "target": "Segment Anything in High Quality", "value": "author_of"}, {"source": "Mingqiao Ye", "target": "Segment Anything in High Quality", "value": "author_of"}, {"source": "Martin Danelljan", "target": "Segment Anything in High Quality", "value": "author_of"}, {"source": "Yifan Liu", "target": "Segment Anything in High Quality", "value": "author_of"}, {"source": "Yu-Wing Tai", "target": "Segment Anything in High Quality", "value": "author_of"}, {"source": "Chi-Keung Tang", "target": "Segment Anything in High Quality", "value": "author_of"}, {"source": "Fisher Yu", "target": "Segment Anything in High Quality", "value": "author_of"}, {"source": "Jiacong Xu", "target": "PIDNet: A Real-time Semantic Segmentation Network Inspired by PID Controllers", "value": "author_of"}, {"source": "Zixiang Xiong", "target": "PIDNet: A Real-time Semantic Segmentation Network Inspired by PID Controllers", "value": "author_of"}, {"source": "Shankar P. Bhattacharyya", "target": "PIDNet: A Real-time Semantic Segmentation Network Inspired by PID Controllers", "value": "author_of"}, {"source": "An Yang", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Baosong Yang", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Binyuan Hui", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Bo Zheng", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Bowen Yu", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Chang Zhou", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Chengpeng Li", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Chengyuan Li", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Dayiheng Liu", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Fei Huang", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Guanting Dong", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Haoran Wei", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Huan Lin", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Jialong Tang", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Jialin Wang", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Jian Yang", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Jianhong Tu", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Jianwei Zhang", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Jianxin Ma", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Jianxin Yang", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Jin Xu", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Jingren Zhou", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Jinze Bai", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Jinzheng He", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Junyang Lin", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Kai Dang", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Keming Lu", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Keqin Chen", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Kexin Yang", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Mei Li", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Mingfeng Xue", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Na Ni", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Pei Zhang", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Peng Wang", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Ru Peng", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Rui Men", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Ruize Gao", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Runji Lin", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Shijie Wang", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Shuai Bai", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Sinan Tan", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Tianhang Zhu", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Tianhao Li", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Tianyu Liu", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Wenbin Ge", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Xiaodong Deng", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Xiaohuan Zhou", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Xingzhang Ren", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Xinyu Zhang", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Xipin Wei", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Xuancheng Ren", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Xuejing Liu", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Yang Fan", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Yang Yao", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Yichang Zhang", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Yu Wan", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Yunfei Chu", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Yuqiong Liu", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Zeyu Cui", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Zhenru Zhang", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Zhifang Guo", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Zhihao Fan", "target": "Qwen2 Technical Report", "value": "author_of"}, {"source": "Shuang Zeng", "target": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving", "value": "author_of"}, {"source": "Xinyuan Chang", "target": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving", "value": "author_of"}, {"source": "Mengwei Xie", "target": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving", "value": "author_of"}, {"source": "Xinran Liu", "target": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving", "value": "author_of"}, {"source": "Yifan Bai", "target": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving", "value": "author_of"}, {"source": "Zheng Pan", "target": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving", "value": "author_of"}, {"source": "Mu Xu", "target": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving", "value": "author_of"}, {"source": "Xing Wei", "target": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving", "value": "author_of"}, {"source": "Ning Guo", "target": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving", "value": "author_of"}, {"source": "Zewei Zhou", "target": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning", "value": "author_of"}, {"source": "Tianhui Cai", "target": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning", "value": "author_of"}, {"source": "Seth Z. Zhao", "target": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning", "value": "author_of"}, {"source": "Yun Zhang", "target": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning", "value": "author_of"}, {"source": "Zhiyu Huang", "target": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning", "value": "author_of"}, {"source": "Bolei Zhou", "target": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning", "value": "author_of"}, {"source": "Jiaqi Ma", "target": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning", "value": "author_of"}, {"source": "Haohan Chi", "target": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models", "value": "author_of"}, {"source": "Huan-ang Gao", "target": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models", "value": "author_of"}, {"source": "Ziming Liu", "target": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models", "value": "author_of"}, {"source": "Jianing Liu", "target": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models", "value": "author_of"}, {"source": "Chenyu Liu", "target": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models", "value": "author_of"}, {"source": "Jinwei Li", "target": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models", "value": "author_of"}, {"source": "Kaisen Yang", "target": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models", "value": "author_of"}, {"source": "Yangcheng Yu", "target": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models", "value": "author_of"}, {"source": "Zeda Wang", "target": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models", "value": "author_of"}, {"source": "Wenyi Li", "target": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models", "value": "author_of"}, {"source": "Leichen Wang", "target": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models", "value": "author_of"}, {"source": "Xingtao Hu", "target": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models", "value": "author_of"}, {"source": "Hao Sun", "target": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models", "value": "author_of"}, {"source": "Hang Zhao", "target": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models", "value": "author_of"}, {"source": "Hao Zhao", "target": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models", "value": "author_of"}, {"source": "Kangan Qian", "target": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving", "value": "author_of"}, {"source": "Sicong Jiang", "target": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving", "value": "author_of"}, {"source": "Yang Zhong", "target": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving", "value": "author_of"}, {"source": "Ziang Luo", "target": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving", "value": "author_of"}, {"source": "Zilin Huang", "target": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving", "value": "author_of"}, {"source": "Tianze Zhu", "target": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving", "value": "author_of"}, {"source": "Kun Jiang", "target": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving", "value": "author_of"}, {"source": "Mengmeng Yang", "target": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving", "value": "author_of"}, {"source": "Zheng Fu", "target": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving", "value": "author_of"}, {"source": "Jinyu Miao", "target": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving", "value": "author_of"}, {"source": "Yining Shi", "target": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving", "value": "author_of"}, {"source": "He Zhe Lim", "target": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving", "value": "author_of"}, {"source": "Li Liu", "target": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving", "value": "author_of"}, {"source": "Tianbao Zhou", "target": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving", "value": "author_of"}, {"source": "Huang Yu", "target": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving", "value": "author_of"}, {"source": "Yifei Hu", "target": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving", "value": "author_of"}, {"source": "Guang Li", "target": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving", "value": "author_of"}, {"source": "Guang Chen", "target": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving", "value": "author_of"}, {"source": "Hao Ye", "target": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving", "value": "author_of"}, {"source": "Lijun Sun", "target": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving", "value": "author_of"}, {"source": "Diange Yang", "target": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving", "value": "author_of"}, {"source": "Ronald J. Williams", "target": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning", "value": "author_of"}, {"source": "Yue Li", "target": "Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning", "value": "author_of"}, {"source": "Meng Tian", "target": "Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning", "value": "author_of"}, {"source": "Dechang Zhu", "target": "Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning", "value": "author_of"}, {"source": "Jiangtong Zhu", "target": "Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning", "value": "author_of"}, {"source": "Zhenyu Lin", "target": "Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning", "value": "author_of"}, {"source": "Zhiwei Xiong", "target": "Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning", "value": "author_of"}, {"source": "Xinhai Zhao", "target": "Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning", "value": "author_of"}, {"source": "Anqing Jiang", "target": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model", "value": "author_of"}, {"source": "Yu Gao", "target": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model", "value": "author_of"}, {"source": "Yiru Wang", "target": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model", "value": "author_of"}, {"source": "Zhigang Sun", "target": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model", "value": "author_of"}, {"source": "Shuo Wang", "target": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model", "value": "author_of"}, {"source": "Yuwen Heng", "target": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model", "value": "author_of"}, {"source": "Hao Sun", "target": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model", "value": "author_of"}, {"source": "Shichen Tang", "target": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model", "value": "author_of"}, {"source": "Lijuan Zhu", "target": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model", "value": "author_of"}, {"source": "Jinhao Chai", "target": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model", "value": "author_of"}, {"source": "Jijun Wang", "target": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model", "value": "author_of"}, {"source": "Zichong Gu", "target": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model", "value": "author_of"}, {"source": "Hao Jiang", "target": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model", "value": "author_of"}, {"source": "Li Sun", "target": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model", "value": "author_of"}, {"source": "Yingyan Li", "target": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving", "value": "author_of"}, {"source": "Shuyao Shang", "target": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving", "value": "author_of"}, {"source": "Weisong Liu", "target": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving", "value": "author_of"}, {"source": "Bing Zhan", "target": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving", "value": "author_of"}, {"source": "Haochen Wang", "target": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving", "value": "author_of"}, {"source": "Yuqi Wang", "target": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving", "value": "author_of"}, {"source": "Yuntao Chen", "target": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving", "value": "author_of"}, {"source": "Xiaoman Wang", "target": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving", "value": "author_of"}, {"source": "Yasong An", "target": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving", "value": "author_of"}, {"source": "Chufeng Tang", "target": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving", "value": "author_of"}, {"source": "Lu Hou", "target": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving", "value": "author_of"}, {"source": "Lue Fan", "target": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving", "value": "author_of"}, {"source": "Zhaoxiang Zhang", "target": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving", "value": "author_of"}, {"source": "Bernhard Kerbl", "target": "3D Gaussian Splatting for Real-Time Radiance Field Rendering", "value": "author_of"}, {"source": "Georgios Kopanas", "target": "3D Gaussian Splatting for Real-Time Radiance Field Rendering", "value": "author_of"}, {"source": "Thomas Leimk\u00fchler", "target": "3D Gaussian Splatting for Real-Time Radiance Field Rendering", "value": "author_of"}, {"source": "George Drettakis", "target": "3D Gaussian Splatting for Real-Time Radiance Field Rendering", "value": "author_of"}, {"source": "Johannes L. Sch\u00f6nberger", "target": "Structure-from-Motion Revisited", "value": "author_of"}, {"source": "Jan-Michael Frahm", "target": "Structure-from-Motion Revisited", "value": "author_of"}, {"source": "Zhenxin Li", "target": "Generalized Trajectory Scoring for End-to-end Multimodal Planning", "value": "author_of"}, {"source": "Wenhao Yao", "target": "Generalized Trajectory Scoring for End-to-end Multimodal Planning", "value": "author_of"}, {"source": "Zi Wang", "target": "Generalized Trajectory Scoring for End-to-end Multimodal Planning", "value": "author_of"}, {"source": "Xinglong Sun", "target": "Generalized Trajectory Scoring for End-to-end Multimodal Planning", "value": "author_of"}, {"source": "Joshua Chen", "target": "Generalized Trajectory Scoring for End-to-end Multimodal Planning", "value": "author_of"}, {"source": "Nadine Chang", "target": "Generalized Trajectory Scoring for End-to-end Multimodal Planning", "value": "author_of"}, {"source": "Maying Shen", "target": "Generalized Trajectory Scoring for End-to-end Multimodal Planning", "value": "author_of"}, {"source": "Zuxuan Wu", "target": "Generalized Trajectory Scoring for End-to-end Multimodal Planning", "value": "author_of"}, {"source": "Shiyi Lan", "target": "Generalized Trajectory Scoring for End-to-end Multimodal Planning", "value": "author_of"}, {"source": "Jose M. Alvarez", "target": "Generalized Trajectory Scoring for End-to-end Multimodal Planning", "value": "author_of"}, {"source": "Lan Feng", "target": "RAP: 3D Rasterization Augmented End-to-End Planning", "value": "author_of"}, {"source": "Yang Gao", "target": "RAP: 3D Rasterization Augmented End-to-End Planning", "value": "author_of"}, {"source": "Eloi Zablocki", "target": "RAP: 3D Rasterization Augmented End-to-End Planning", "value": "author_of"}, {"source": "Quanyi Li", "target": "RAP: 3D Rasterization Augmented End-to-End Planning", "value": "author_of"}, {"source": "Wuyang Li", "target": "RAP: 3D Rasterization Augmented End-to-End Planning", "value": "author_of"}, {"source": "Sichao Liu", "target": "RAP: 3D Rasterization Augmented End-to-End Planning", "value": "author_of"}, {"source": "Matthieu Cord", "target": "RAP: 3D Rasterization Augmented End-to-End Planning", "value": "author_of"}, {"source": "Alexandre Alahi", "target": "RAP: 3D Rasterization Augmented End-to-End Planning", "value": "author_of"}, {"source": "Maciej K. Wozniak", "target": "PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Lianhang Liu", "target": "PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Yixi Cai", "target": "PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Patric Jensfelt", "target": "PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Junnan Li", "target": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models", "value": "author_of"}, {"source": "Dongxu Li", "target": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models", "value": "author_of"}, {"source": "Silvio Savarese", "target": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models", "value": "author_of"}, {"source": "Steven Hoi", "target": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models", "value": "author_of"}, {"source": "Yaron Lipman", "target": "Flow Matching for Generative Modeling", "value": "author_of"}, {"source": "Ricky T. Q. Chen", "target": "Flow Matching for Generative Modeling", "value": "author_of"}, {"source": "Heli Ben-Hamu", "target": "Flow Matching for Generative Modeling", "value": "author_of"}, {"source": "Maximilian Nickel", "target": "Flow Matching for Generative Modeling", "value": "author_of"}, {"source": "Matt Le", "target": "Flow Matching for Generative Modeling", "value": "author_of"}, {"source": "Cheng Chi", "target": "Diffusion policy: Visuomotor policy learning via action diffusion", "value": "author_of"}, {"source": "Zhenjia Xu", "target": "Diffusion policy: Visuomotor policy learning via action diffusion", "value": "author_of"}, {"source": "Siyuan Feng", "target": "Diffusion policy: Visuomotor policy learning via action diffusion", "value": "author_of"}, {"source": "Eric Cousineau", "target": "Diffusion policy: Visuomotor policy learning via action diffusion", "value": "author_of"}, {"source": "Yilun Du", "target": "Diffusion policy: Visuomotor policy learning via action diffusion", "value": "author_of"}, {"source": "Benjamin Burchfiel", "target": "Diffusion policy: Visuomotor policy learning via action diffusion", "value": "author_of"}, {"source": "Russ Tedrake", "target": "Diffusion policy: Visuomotor policy learning via action diffusion", "value": "author_of"}, {"source": "Shuran Song", "target": "Diffusion policy: Visuomotor policy learning via action diffusion", "value": "author_of"}, {"source": "Fanlong Zeng", "target": "Large Language Models for Robotics: A Survey", "value": "author_of"}, {"source": "Wensheng Gan", "target": "Large Language Models for Robotics: A Survey", "value": "author_of"}, {"source": "Zezheng Huai", "target": "Large Language Models for Robotics: A Survey", "value": "author_of"}, {"source": "Lichao Sun", "target": "Large Language Models for Robotics: A Survey", "value": "author_of"}, {"source": "Hechang Chen", "target": "Large Language Models for Robotics: A Survey", "value": "author_of"}, {"source": "Yongheng Wang", "target": "Large Language Models for Robotics: A Survey", "value": "author_of"}, {"source": "Ning Liu", "target": "Large Language Models for Robotics: A Survey", "value": "author_of"}, {"source": "Philip S. Yu", "target": "Large Language Models for Robotics: A Survey", "value": "author_of"}, {"source": "Zhenjie Yang", "target": "Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)", "value": "author_of"}, {"source": "Xiaosong Jia", "target": "Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)", "value": "author_of"}, {"source": "Qifeng Li", "target": "Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)", "value": "author_of"}, {"source": "Xue Yang", "target": "Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)", "value": "author_of"}, {"source": "Maoqing Yao", "target": "Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)", "value": "author_of"}, {"source": "Junchi Yan", "target": "Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)", "value": "author_of"}, {"source": "Dapeng Zhang", "target": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey", "value": "author_of"}, {"source": "Jing Sun", "target": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey", "value": "author_of"}, {"source": "Chenghui Hu", "target": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey", "value": "author_of"}, {"source": "Xiaoyan Wu", "target": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey", "value": "author_of"}, {"source": "Zhenlong Yuan", "target": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey", "value": "author_of"}, {"source": "Rui Zhou", "target": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey", "value": "author_of"}, {"source": "Fei Shen", "target": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey", "value": "author_of"}, {"source": "Qingguo Zhou", "target": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey", "value": "author_of"}, {"source": "Jiazhi Yang", "target": "ReSim: Reliable World Simulation for Autonomous Driving", "value": "author_of"}, {"source": "Kashyap Chitta", "target": "ReSim: Reliable World Simulation for Autonomous Driving", "value": "author_of"}, {"source": "Shenyuan Gao", "target": "ReSim: Reliable World Simulation for Autonomous Driving", "value": "author_of"}, {"source": "Long Chen", "target": "ReSim: Reliable World Simulation for Autonomous Driving", "value": "author_of"}, {"source": "Yuqian Shao", "target": "ReSim: Reliable World Simulation for Autonomous Driving", "value": "author_of"}, {"source": "Xiaosong Jia", "target": "ReSim: Reliable World Simulation for Autonomous Driving", "value": "author_of"}, {"source": "Hongyang Li", "target": "ReSim: Reliable World Simulation for Autonomous Driving", "value": "author_of"}, {"source": "Andreas Geiger", "target": "ReSim: Reliable World Simulation for Autonomous Driving", "value": "author_of"}, {"source": "Xiangyu Yue", "target": "ReSim: Reliable World Simulation for Autonomous Driving", "value": "author_of"}, {"source": "Li Chen", "target": "ReSim: Reliable World Simulation for Autonomous Driving", "value": "author_of"}, {"source": "Hugo Touvron", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Louis Martin", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Kevin Stone", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Peter Albert", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Amjad Almahairi", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Yasmine Babaei", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Nikolay Bashlykov", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Soumya Batra", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Prajjwal Bhargava", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Shruti Bhosale", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Dan Bikel", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Lukas Blecher", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Cristian Canton Ferrer", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Moya Chen", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Guillem Cucurull", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "David Esiobu", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Jude Fernandes", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Jeremy Fu", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Wenyin Fu", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Brian Fuller", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Cynthia Gao", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Vedanuj Goswami", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Naman Goyal", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Anthony Hartshorn", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Saghar Hosseini", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Rui Hou", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Hakan Inan", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Marcin Kardas", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Viktor Kerkez", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Madian Khabsa", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Isabel Kloumann", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Artem Korenev", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Punit Singh Koura", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Marie-Anne Lachaux", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Thibaut Lavril", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Jenya Lee", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Diana Liskovich", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Yinghai Lu", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Yuning Mao", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Xavier Martinet", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Todor Mihaylov", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Pushkar Mishra", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Igor Molybog", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Yixin Nie", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Andrew Poulton", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Jeremy Reizenstein", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Rashi Rungta", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Kalyan Saladi", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Alan Schelten", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Ruan Silva", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Eric Michael Smith", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Ranjan Subramanian", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Xiaoqing Ellen Tan", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Binh Tang", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Ross Taylor", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Adina Williams", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Jian Xiang Kuan", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Puxin Xu", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Zheng Yan", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Iliyan Zarov", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Yuchen Zhang", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Angela Fan", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Melanie Kambadur", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Sharan Narang", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Aurelien Rodriguez", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Robert Stojnic", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Sergey Edunov", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Thomas Scialom", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "author_of"}, {"source": "Weifan Guan", "target": "Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey", "value": "author_of"}, {"source": "Qinghao Hu", "target": "Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey", "value": "author_of"}, {"source": "Aosheng Li", "target": "Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey", "value": "author_of"}, {"source": "Jian Cheng", "target": "Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey", "value": "author_of"}, {"source": "Yuechen Luo", "target": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving", "value": "author_of"}, {"source": "Fang Li", "target": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving", "value": "author_of"}, {"source": "Shaoqing Xu", "target": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving", "value": "author_of"}, {"source": "Zhiyi Lai", "target": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving", "value": "author_of"}, {"source": "Lei Yang", "target": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving", "value": "author_of"}, {"source": "Qimao Chen", "target": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving", "value": "author_of"}, {"source": "Ziang Luo", "target": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving", "value": "author_of"}, {"source": "Zixun Xie", "target": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving", "value": "author_of"}, {"source": "Shengyin Jiang", "target": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving", "value": "author_of"}, {"source": "Jiaxin Liu", "target": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving", "value": "author_of"}, {"source": "Long Chen", "target": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving", "value": "author_of"}, {"source": "Bing Wang", "target": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving", "value": "author_of"}, {"source": "Zhi-xin Yang", "target": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving", "value": "author_of"}, {"source": "Ruiyang Hao", "target": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition", "value": "author_of"}, {"source": "Haibao Yu", "target": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition", "value": "author_of"}, {"source": "Jiaru Zhong", "target": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition", "value": "author_of"}, {"source": "Chuanye Wang", "target": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition", "value": "author_of"}, {"source": "Jiahao Wang", "target": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition", "value": "author_of"}, {"source": "Yiming Kan", "target": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition", "value": "author_of"}, {"source": "Wenxian Yang", "target": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition", "value": "author_of"}, {"source": "Siqi Fan", "target": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition", "value": "author_of"}, {"source": "Huilin Yin", "target": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition", "value": "author_of"}, {"source": "Jianing Qiu", "target": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition", "value": "author_of"}, {"source": "Yao Mu", "target": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition", "value": "author_of"}, {"source": "Jiankai Sun", "target": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition", "value": "author_of"}, {"source": "Li Chen", "target": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition", "value": "author_of"}, {"source": "Walter Zimmer", "target": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition", "value": "author_of"}, {"source": "Dandan Zhang", "target": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition", "value": "author_of"}, {"source": "Shanghang Zhang", "target": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition", "value": "author_of"}, {"source": "Mac Schwager", "target": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition", "value": "author_of"}, {"source": "Ping Luo", "target": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition", "value": "author_of"}, {"source": "Zaiqing Nie", "target": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition", "value": "author_of"}, {"source": "Yang Liu", "target": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI", "value": "author_of"}, {"source": "Weixing Chen", "target": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI", "value": "author_of"}, {"source": "Yongjie Bai", "target": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI", "value": "author_of"}, {"source": "Xiaodan Liang", "target": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI", "value": "author_of"}, {"source": "Guanbin Li", "target": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI", "value": "author_of"}, {"source": "Wen Gao", "target": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI", "value": "author_of"}, {"source": "Liang Lin", "target": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI", "value": "author_of"}, {"source": "Yueen Ma", "target": "A Survey on Vision-Language-Action Models for Embodied AI", "value": "author_of"}, {"source": "Zixing Song", "target": "A Survey on Vision-Language-Action Models for Embodied AI", "value": "author_of"}, {"source": "Yuzheng Zhuang", "target": "A Survey on Vision-Language-Action Models for Embodied AI", "value": "author_of"}, {"source": "Jianye Hao", "target": "A Survey on Vision-Language-Action Models for Embodied AI", "value": "author_of"}, {"source": "Irwin King", "target": "A Survey on Vision-Language-Action Models for Embodied AI", "value": "author_of"}, {"source": "Guosheng Zhao", "target": "DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation", "value": "author_of"}, {"source": "Chaojun Ni", "target": "DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation", "value": "author_of"}, {"source": "Xiaofeng Wang", "target": "DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation", "value": "author_of"}, {"source": "Zheng Zhu", "target": "DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation", "value": "author_of"}, {"source": "Xueyang Zhang", "target": "DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation", "value": "author_of"}, {"source": "Yida Wang", "target": "DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation", "value": "author_of"}, {"source": "Guan Huang", "target": "DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation", "value": "author_of"}, {"source": "Xinze Chen", "target": "DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation", "value": "author_of"}, {"source": "Boyuan Wang", "target": "DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation", "value": "author_of"}, {"source": "Youyi Zhang", "target": "DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation", "value": "author_of"}, {"source": "Wenjun Mei", "target": "DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation", "value": "author_of"}, {"source": "Xingang Wang", "target": "DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation", "value": "author_of"}, {"source": "Fanqing Meng", "target": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation", "value": "author_of"}, {"source": "Jiaqi Liao", "target": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation", "value": "author_of"}, {"source": "Xinyu Tan", "target": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation", "value": "author_of"}, {"source": "Wenqi Shao", "target": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation", "value": "author_of"}, {"source": "Quanfeng Lu", "target": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation", "value": "author_of"}, {"source": "Kaipeng Zhang", "target": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation", "value": "author_of"}, {"source": "Yu Cheng", "target": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation", "value": "author_of"}, {"source": "Dianqi Li", "target": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation", "value": "author_of"}, {"source": "Yu Qiao", "target": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation", "value": "author_of"}, {"source": "Ping Luo", "target": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation", "value": "author_of"}, {"source": "Yu Shang", "target": "AgentSquare: Automatic LLM Agent Search in Modular Design Space", "value": "author_of"}, {"source": "Yu Li", "target": "AgentSquare: Automatic LLM Agent Search in Modular Design Space", "value": "author_of"}, {"source": "Keyu Zhao", "target": "AgentSquare: Automatic LLM Agent Search in Modular Design Space", "value": "author_of"}, {"source": "Likai Ma", "target": "AgentSquare: Automatic LLM Agent Search in Modular Design Space", "value": "author_of"}, {"source": "Jiahe Liu", "target": "AgentSquare: Automatic LLM Agent Search in Modular Design Space", "value": "author_of"}, {"source": "Fengli Xu", "target": "AgentSquare: Automatic LLM Agent Search in Modular Design Space", "value": "author_of"}, {"source": "Yong Li", "target": "AgentSquare: Automatic LLM Agent Search in Modular Design Space", "value": "author_of"}, {"source": "Jie Feng", "target": "CityGPT: Empowering Urban Spatial Cognition of Large Language Models", "value": "author_of"}, {"source": "Tianhui Liu", "target": "CityGPT: Empowering Urban Spatial Cognition of Large Language Models", "value": "author_of"}, {"source": "Yuwei Du", "target": "CityGPT: Empowering Urban Spatial Cognition of Large Language Models", "value": "author_of"}, {"source": "Siqi Guo", "target": "CityGPT: Empowering Urban Spatial Cognition of Large Language Models", "value": "author_of"}, {"source": "Yuming Lin", "target": "CityGPT: Empowering Urban Spatial Cognition of Large Language Models", "value": "author_of"}, {"source": "Yong Li", "target": "CityGPT: Empowering Urban Spatial Cognition of Large Language Models", "value": "author_of"}, {"source": "Kaiwen Zhang", "target": "Epona: Autoregressive Diffusion World Model for Autonomous Driving", "value": "author_of"}, {"source": "Zhenyu Tang", "target": "Epona: Autoregressive Diffusion World Model for Autonomous Driving", "value": "author_of"}, {"source": "Xiaotao Hu", "target": "Epona: Autoregressive Diffusion World Model for Autonomous Driving", "value": "author_of"}, {"source": "Xingang Pan", "target": "Epona: Autoregressive Diffusion World Model for Autonomous Driving", "value": "author_of"}, {"source": "Xiaoyang Guo", "target": "Epona: Autoregressive Diffusion World Model for Autonomous Driving", "value": "author_of"}, {"source": "Yuan Liu", "target": "Epona: Autoregressive Diffusion World Model for Autonomous Driving", "value": "author_of"}, {"source": "Jingwei Huang", "target": "Epona: Autoregressive Diffusion World Model for Autonomous Driving", "value": "author_of"}, {"source": "Li Yuan", "target": "Epona: Autoregressive Diffusion World Model for Autonomous Driving", "value": "author_of"}, {"source": "Qian Zhang", "target": "Epona: Autoregressive Diffusion World Model for Autonomous Driving", "value": "author_of"}, {"source": "Xiao-Xiao Long", "target": "Epona: Autoregressive Diffusion World Model for Autonomous Driving", "value": "author_of"}, {"source": "Xun Cao", "target": "Epona: Autoregressive Diffusion World Model for Autonomous Driving", "value": "author_of"}, {"source": "Wei Yin", "target": "Epona: Autoregressive Diffusion World Model for Autonomous Driving", "value": "author_of"}, {"source": "Yinhan Liu", "target": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "value": "author_of"}, {"source": "Myle Ott", "target": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "value": "author_of"}, {"source": "Naman Goyal", "target": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "value": "author_of"}, {"source": "Jingfei Du", "target": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "value": "author_of"}, {"source": "Mandar Joshi", "target": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "value": "author_of"}, {"source": "Danqi Chen", "target": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "value": "author_of"}, {"source": "Omer Levy", "target": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "value": "author_of"}, {"source": "Mike Lewis", "target": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "value": "author_of"}, {"source": "Luke Zettlemoyer", "target": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "value": "author_of"}, {"source": "Veselin Stoyanov", "target": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "value": "author_of"}, {"source": "Yongliang Wu", "target": "Unlearning Concepts in Diffusion Model via Concept Domain Correction and Concept Preserving Gradient", "value": "author_of"}, {"source": "Shiji Zhou", "target": "Unlearning Concepts in Diffusion Model via Concept Domain Correction and Concept Preserving Gradient", "value": "author_of"}, {"source": "Mingzhuo Yang", "target": "Unlearning Concepts in Diffusion Model via Concept Domain Correction and Concept Preserving Gradient", "value": "author_of"}, {"source": "Lianzhe Wang", "target": "Unlearning Concepts in Diffusion Model via Concept Domain Correction and Concept Preserving Gradient", "value": "author_of"}, {"source": "Heng Chang", "target": "Unlearning Concepts in Diffusion Model via Concept Domain Correction and Concept Preserving Gradient", "value": "author_of"}, {"source": "Wenbo Zhu", "target": "Unlearning Concepts in Diffusion Model via Concept Domain Correction and Concept Preserving Gradient", "value": "author_of"}, {"source": "Xinting Hu", "target": "Unlearning Concepts in Diffusion Model via Concept Domain Correction and Concept Preserving Gradient", "value": "author_of"}, {"source": "Xiao Zhou", "target": "Unlearning Concepts in Diffusion Model via Concept Domain Correction and Concept Preserving Gradient", "value": "author_of"}, {"source": "Xu Yang", "target": "Unlearning Concepts in Diffusion Model via Concept Domain Correction and Concept Preserving Gradient", "value": "author_of"}, {"source": "Xingjun Ma", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Yifeng Gao", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Yixu Wang", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Ruofan Wang", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Xin Wang", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Ye Sun", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Yifan Ding", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Hengyuan Xu", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Yunhao Chen", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Yunhan Zhao", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Hanxun Huang", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Yige Li", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Jiaming Zhang", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Xiang Zheng", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Yang Bai", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Henghui Ding", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Zuxuan Wu", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Xipeng Qiu", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Jingfeng Zhang", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Yiming Li", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Jun Sun", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Cong Wang", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Jindong Gu", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Baoyuan Wu", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Siheng Chen", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Tianwei Zhang", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Yang Liu", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Min Gong", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Tongliang Liu", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Shirui Pan", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Cihang Xie", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Tianyu Pang", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Yinpeng Dong", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Ruoxi Jia", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Yang Zhang", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Shi-jie Ma", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Xiangyu Zhang", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Neil Gong", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Chaowei Xiao", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Sarah Erfani", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Bo Li", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Masashi Sugiyama", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Dacheng Tao", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "James Bailey", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Yu-Gang Jiang", "target": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "value": "author_of"}, {"source": "Zicheng Zhang", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Junying Wang", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Farong Wen", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Yijin Guo", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Xiangyu Zhao", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Xinyu Fang", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Shengyuan Ding", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Ziheng Jia", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Jiahao Xiao", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Ye Shen", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Yushuo Zheng", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Xiaorong Zhu", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Yalun Wu", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Ziheng Jiao", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Wei Sun", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Zijian Chen", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Kaiwei Zhang", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Kang Fu", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Yuqin Cao", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Ming Hu", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Yue Zhou", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Xuemei Zhou", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Juntai Cao", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Wei Zhou", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Jinyu Cao", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Ronghui Li", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Donghao Zhou", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Yuan Tian", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Xiangyang Zhu", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Chun-yuan Li", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Haoning Wu", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Xiaohong Liu", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Junjun He", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Yu Zhou", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Hui Liu", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Lin Zhang", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Zesheng Wang", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Huiyu Duan", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Yingjie Zhou", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Xiongkuo Min", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Qi Jia", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Dongzhan Zhou", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Wenlong Zhang", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Jiezhang Cao", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Xue Yang", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Junzhi Yu", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Songyang Zhang", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Haodong Duan", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Guangtao Zhai", "target": "Large multimodal models evaluation: a survey", "value": "author_of"}, {"source": "Yangyang Guo", "target": "The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense", "value": "author_of"}, {"source": "Fangkai Jiao", "target": "The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense", "value": "author_of"}, {"source": "Liqiang Nie", "target": "The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense", "value": "author_of"}, {"source": "Mohan Kankanhalli", "target": "The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense", "value": "author_of"}, {"source": "Xuannan Liu", "target": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs", "value": "author_of"}, {"source": "Zekun Li", "target": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs", "value": "author_of"}, {"source": "Zheqi He", "target": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs", "value": "author_of"}, {"source": "Peipei Li", "target": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs", "value": "author_of"}, {"source": "Shuhan Xia", "target": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs", "value": "author_of"}, {"source": "Xing Cui", "target": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs", "value": "author_of"}, {"source": "Huaibo Huang", "target": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs", "value": "author_of"}, {"source": "Xi Yang", "target": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs", "value": "author_of"}, {"source": "Ran He", "target": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs", "value": "author_of"}, {"source": "Jinmiao Zhao", "target": "Gradient-Guided Learning Network for Infrared Small Target Detection", "value": "author_of"}, {"source": "Chuang Yu", "target": "Gradient-Guided Learning Network for Infrared Small Target Detection", "value": "author_of"}, {"source": "Zelin Shi", "target": "Gradient-Guided Learning Network for Infrared Small Target Detection", "value": "author_of"}, {"source": "Yunpeng Liu", "target": "Gradient-Guided Learning Network for Infrared Small Target Detection", "value": "author_of"}, {"source": "Yingdi Zhang", "target": "Gradient-Guided Learning Network for Infrared Small Target Detection", "value": "author_of"}, {"source": "Waleed Khalid", "target": "A Retrieval-Augmented Generation Approach to Extracting Algorithmic Logic from Neural Networks", "value": "author_of"}, {"source": "Dmitry Ignatov", "target": "A Retrieval-Augmented Generation Approach to Extracting Algorithmic Logic from Neural Networks", "value": "author_of"}, {"source": "Radu Timofte", "target": "A Retrieval-Augmented Generation Approach to Extracting Algorithmic Logic from Neural Networks", "value": "author_of"}, {"source": "Yu Tian", "target": "CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation", "value": "author_of"}, {"source": "Zhongheng Yang", "target": "CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation", "value": "author_of"}, {"source": "Chenshi Liu", "target": "CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation", "value": "author_of"}, {"source": "Yiyun Su", "target": "CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation", "value": "author_of"}, {"source": "Ziwei Hong", "target": "CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation", "value": "author_of"}, {"source": "Zexi Gong", "target": "CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation", "value": "author_of"}, {"source": "Jingyuan Xu", "target": "CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation", "value": "author_of"}, {"source": "Yang Lu", "target": "IMobileTransformer: A fusion-based lightweight model for rice disease identification", "value": "author_of"}, {"source": "Haoyang Zhou", "target": "IMobileTransformer: A fusion-based lightweight model for rice disease identification", "value": "author_of"}, {"source": "Peng Wang", "target": "IMobileTransformer: A fusion-based lightweight model for rice disease identification", "value": "author_of"}, {"source": "Erzhi Wang", "target": "IMobileTransformer: A fusion-based lightweight model for rice disease identification", "value": "author_of"}, {"source": "Gongfa Li", "target": "IMobileTransformer: A fusion-based lightweight model for rice disease identification", "value": "author_of"}, {"source": "Tongjian Yu", "target": "IMobileTransformer: A fusion-based lightweight model for rice disease identification", "value": "author_of"}, {"source": "Jinhui Yi", "target": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images", "value": "author_of"}, {"source": "Gina Lopez", "target": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images", "value": "author_of"}, {"source": "S. Hadir", "target": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images", "value": "author_of"}, {"source": "Jan Weyler", "target": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images", "value": "author_of"}, {"source": "Lasse Klingbeil", "target": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images", "value": "author_of"}, {"source": "Marion Deichmann", "target": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images", "value": "author_of"}, {"source": "Juergen Gall", "target": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images", "value": "author_of"}, {"source": "S. J. Seidel", "target": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images", "value": "author_of"}, {"source": "Isaac Robinson", "target": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers", "value": "author_of"}, {"source": "Peter Robicheaux", "target": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers", "value": "author_of"}, {"source": "Matvei Popov", "target": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers", "value": "author_of"}, {"source": "Deva Ramanan", "target": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers", "value": "author_of"}, {"source": "Neehar Peri", "target": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers", "value": "author_of"}, {"source": "Zhou Wang", "target": "Image quality assessment: from error visibility to structural similarity", "value": "author_of"}, {"source": "A. Bovik", "target": "Image quality assessment: from error visibility to structural similarity", "value": "author_of"}, {"source": "H. Sheikh", "target": "Image quality assessment: from error visibility to structural similarity", "value": "author_of"}, {"source": "Eero P. Simoncelli", "target": "Image quality assessment: from error visibility to structural similarity", "value": "author_of"}, {"source": "Wei Cao", "target": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction", "value": "author_of"}, {"source": "Hao Zhang", "target": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction", "value": "author_of"}, {"source": "Fengrui Tian", "target": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction", "value": "author_of"}, {"source": "Yulun Wu", "target": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction", "value": "author_of"}, {"source": "Yingying Li", "target": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction", "value": "author_of"}, {"source": "Shenlong Wang", "target": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction", "value": "author_of"}, {"source": "Ning Yu", "target": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction", "value": "author_of"}, {"source": "Yaoyao Liu", "target": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction", "value": "author_of"}, {"source": "Yuxue Yang", "target": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "value": "author_of"}, {"source": "Lue Fan", "target": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "value": "author_of"}, {"source": "Ziqi Shi", "target": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "value": "author_of"}, {"source": "Junran Peng", "target": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "value": "author_of"}, {"source": "Feng Wang", "target": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "value": "author_of"}, {"source": "Zhaoxiang Zhang", "target": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "value": "author_of"}, {"source": "William Peebles", "target": "Scalable Diffusion Models with Transformers", "value": "author_of"}, {"source": "Saining Xie", "target": "Scalable Diffusion Models with Transformers", "value": "author_of"}, {"source": "Jianlin Su", "target": "RoFormer: Enhanced Transformer with Rotary Position Embedding", "value": "author_of"}, {"source": "Yu Lu", "target": "RoFormer: Enhanced Transformer with Rotary Position Embedding", "value": "author_of"}, {"source": "Shengfeng Pan", "target": "RoFormer: Enhanced Transformer with Rotary Position Embedding", "value": "author_of"}, {"source": "Ahmed Murtadha", "target": "RoFormer: Enhanced Transformer with Rotary Position Embedding", "value": "author_of"}, {"source": "Bo Wen", "target": "RoFormer: Enhanced Transformer with Rotary Position Embedding", "value": "author_of"}, {"source": "Yunfeng Liu", "target": "RoFormer: Enhanced Transformer with Rotary Position Embedding", "value": "author_of"}, {"source": "Sifan Tu", "target": "The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey", "value": "author_of"}, {"source": "Xin Zhou", "target": "The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey", "value": "author_of"}, {"source": "Dingkang Liang", "target": "The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey", "value": "author_of"}, {"source": "Xingyu Jiang", "target": "The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey", "value": "author_of"}, {"source": "Yumeng Zhang", "target": "The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey", "value": "author_of"}, {"source": "Xiaofan Li", "target": "The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey", "value": "author_of"}, {"source": "Xiang Bai", "target": "The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey", "value": "author_of"}, {"source": "Andreas Geiger", "target": "Vision meets robotics: The KITTI dataset", "value": "author_of"}, {"source": "Philip Lenz", "target": "Vision meets robotics: The KITTI dataset", "value": "author_of"}, {"source": "C. Stiller", "target": "Vision meets robotics: The KITTI dataset", "value": "author_of"}, {"source": "R. Urtasun", "target": "Vision meets robotics: The KITTI dataset", "value": "author_of"}, {"source": "S. Umeyama", "target": "Least-Squares Estimation of Transformation Parameters Between Two Point Patterns", "value": "author_of"}, {"source": "Run Wang", "target": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations", "value": "author_of"}, {"source": "Chaoyi Zhou", "target": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations", "value": "author_of"}, {"source": "Amir Salarpour", "target": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations", "value": "author_of"}, {"source": "Xi Liu", "target": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations", "value": "author_of"}, {"source": "Zhi-Qi Cheng", "target": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations", "value": "author_of"}, {"source": "Feng Luo", "target": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations", "value": "author_of"}, {"source": "Mert D. Pes\u00e9", "target": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations", "value": "author_of"}, {"source": "Siyu Huang", "target": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations", "value": "author_of"}, {"source": "Xingbang Hao", "target": "Deep Learning", "value": "author_of"}, {"source": "Guigang Zhang", "target": "Deep Learning", "value": "author_of"}, {"source": "Shang Ma", "target": "Deep Learning", "value": "author_of"}, {"source": "Nikhil Keetha", "target": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction", "value": "author_of"}, {"source": "Norman M\u00fcller", "target": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction", "value": "author_of"}, {"source": "Johannes Sch\u00f6nberger", "target": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction", "value": "author_of"}, {"source": "Lorenzo Porzi", "target": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction", "value": "author_of"}, {"source": "Yuchen Zhang", "target": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction", "value": "author_of"}, {"source": "Tobias Fischer", "target": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction", "value": "author_of"}, {"source": "Arno Knapitsch", "target": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction", "value": "author_of"}, {"source": "Duncan Zauss", "target": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction", "value": "author_of"}, {"source": "Ethan Weber", "target": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction", "value": "author_of"}, {"source": "Nelson Antunes", "target": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction", "value": "author_of"}, {"source": "Jonathon Luiten", "target": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction", "value": "author_of"}, {"source": "Manuel Lopez-Antequera", "target": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction", "value": "author_of"}, {"source": "Samuel Rota Bul\u00f2", "target": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction", "value": "author_of"}, {"source": "Christian Richardt", "target": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction", "value": "author_of"}, {"source": "Deva Ramanan", "target": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction", "value": "author_of"}, {"source": "Sebastian Scherer", "target": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction", "value": "author_of"}, {"source": "Peter Kontschieder", "target": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction", "value": "author_of"}, {"source": "Team Seedream", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": ":", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Yunpeng Chen", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Yu Gao", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Lixue Gong", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Meng Guo", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Qiushan Guo", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Zhiyao Guo", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Xiaoxia Hou", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Weilin Huang", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Yixuan Huang", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Xiaowen Jian", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Huafeng Kuang", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Zhichao Lai", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Fanshi Li", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Liang Li", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Xiaochen Lian", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Chao Liao", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Liyang Liu", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Wei Liu", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Yanzuo Lu", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Zhengxiong Luo", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Tongtong Ou", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Guang Shi", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Yichun Shi", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Shiqi Sun", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Yu Tian", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Zhi Tian", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Peng Wang", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Rui Wang", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Xun Wang", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Ye Wang", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Guofeng Wu", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Jie Wu", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Wenxu Wu", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Yonghui Wu", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Xin Xia", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Xuefeng Xiao", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Shuang Xu", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Xin Yan", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Ceyuan Yang", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Jianchao Yang", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Zhonghua Zhai", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Chenlin Zhang", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Heng Zhang", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Qi Zhang", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Xinyu Zhang", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Yuwei Zhang", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Shijia Zhao", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Wenliang Zhao", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Wenjia Zhu", "target": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "value": "author_of"}, {"source": "Junyan Ye", "target": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation", "value": "author_of"}, {"source": "Dongzhi Jiang", "target": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation", "value": "author_of"}, {"source": "Zihao Wang", "target": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation", "value": "author_of"}, {"source": "Leqi Zhu", "target": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation", "value": "author_of"}, {"source": "Zhenghao Hu", "target": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation", "value": "author_of"}, {"source": "Zilong Huang", "target": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation", "value": "author_of"}, {"source": "Jun He", "target": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation", "value": "author_of"}, {"source": "Zhiyuan Yan", "target": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation", "value": "author_of"}, {"source": "Jinghua Yu", "target": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation", "value": "author_of"}, {"source": "Hongsheng Li", "target": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation", "value": "author_of"}, {"source": "Conghui He", "target": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation", "value": "author_of"}, {"source": "Weijia Li", "target": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation", "value": "author_of"}, {"source": "Yi Xin", "target": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "value": "author_of"}, {"source": "Qi Qin", "target": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "value": "author_of"}, {"source": "Siqi Luo", "target": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "value": "author_of"}, {"source": "Kaiwen Zhu", "target": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "value": "author_of"}, {"source": "Juncheng Yan", "target": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "value": "author_of"}, {"source": "Yan Tai", "target": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "value": "author_of"}, {"source": "Jiayi Lei", "target": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "value": "author_of"}, {"source": "Yuewen Cao", "target": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "value": "author_of"}, {"source": "Keqi Wang", "target": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "value": "author_of"}, {"source": "Yibin Wang", "target": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "value": "author_of"}, {"source": "Jinbin Bai", "target": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "value": "author_of"}, {"source": "Qian Yu", "target": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "value": "author_of"}, {"source": "Dengyang Jiang", "target": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "value": "author_of"}, {"source": "Yuandong Pu", "target": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "value": "author_of"}, {"source": "Haoxing Chen", "target": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "value": "author_of"}, {"source": "Le Zhuo", "target": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "value": "author_of"}, {"source": "Junjun He", "target": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "value": "author_of"}, {"source": "Gen Luo", "target": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "value": "author_of"}, {"source": "Tianbin Li", "target": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "value": "author_of"}, {"source": "Ming Hu", "target": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "value": "author_of"}, {"source": "Jin Ye", "target": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "value": "author_of"}, {"source": "Shenglong Ye", "target": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "value": "author_of"}, {"source": "Bo Zhang", "target": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "value": "author_of"}, {"source": "Chang Xu", "target": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "value": "author_of"}, {"source": "Wenhai Wang", "target": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "value": "author_of"}, {"source": "Hongsheng Li", "target": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "value": "author_of"}, {"source": "Guangtao Zhai", "target": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "value": "author_of"}, {"source": "Tianfan Xue", "target": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "value": "author_of"}, {"source": "Bin Fu", "target": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "value": "author_of"}, {"source": "Xiaohong Liu", "target": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "value": "author_of"}, {"source": "Yu Qiao", "target": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "value": "author_of"}, {"source": "Yihao Liu", "target": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "value": "author_of"}, {"source": "NextStep Team", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Chunrui Han", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Guopeng Li", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Jingwei Wu", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Quan Sun", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Yan Cai", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Yuang Peng", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Zheng Ge", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Deyu Zhou", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Haomiao Tang", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Hongyu Zhou", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Kenkun Liu", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Ailin Huang", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Bin Wang", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Changxin Miao", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Deshan Sun", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "En Yu", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Fukun Yin", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Gang Yu", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Hao Nie", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Haoran Lv", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Hanpeng Hu", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Jia Wang", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Jian Zhou", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Jianjian Sun", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Kaijun Tan", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Kang An", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Kangheng Lin", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Liang Zhao", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Mei Chen", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Peng Xing", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Rui Wang", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Shiyu Liu", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Shutao Xia", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Tianhao You", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Wei Ji", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Xianfang Zeng", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Xin Han", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Xuelin Zhang", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Yana Wei", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Yanming Xu", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Yimin Jiang", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Yingming Wang", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Yu Zhou", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Yucheng Han", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Ziyang Meng", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Binxing Jiao", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Daxin Jiang", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Xiangyu Zhang", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Yibo Zhu", "target": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "value": "author_of"}, {"source": "Long Ouyang", "target": "Training language models to follow instructions with human feedback", "value": "author_of"}, {"source": "Jeff Wu", "target": "Training language models to follow instructions with human feedback", "value": "author_of"}, {"source": "Xu Jiang", "target": "Training language models to follow instructions with human feedback", "value": "author_of"}, {"source": "Diogo Almeida", "target": "Training language models to follow instructions with human feedback", "value": "author_of"}, {"source": "Carroll L. Wainwright", "target": "Training language models to follow instructions with human feedback", "value": "author_of"}, {"source": "Pamela Mishkin", "target": "Training language models to follow instructions with human feedback", "value": "author_of"}, {"source": "Chong Zhang", "target": "Training language models to follow instructions with human feedback", "value": "author_of"}, {"source": "Sandhini Agarwal", "target": "Training language models to follow instructions with human feedback", "value": "author_of"}, {"source": "Katarina Slama", "target": "Training language models to follow instructions with human feedback", "value": "author_of"}, {"source": "Alex Ray", "target": "Training language models to follow instructions with human feedback", "value": "author_of"}, {"source": "John Schulman", "target": "Training language models to follow instructions with human feedback", "value": "author_of"}, {"source": "Jacob Hilton", "target": "Training language models to follow instructions with human feedback", "value": "author_of"}, {"source": "Fraser Kelton", "target": "Training language models to follow instructions with human feedback", "value": "author_of"}, {"source": "Luke Miller", "target": "Training language models to follow instructions with human feedback", "value": "author_of"}, {"source": "Maddie Simens", "target": "Training language models to follow instructions with human feedback", "value": "author_of"}, {"source": "Amanda Askell", "target": "Training language models to follow instructions with human feedback", "value": "author_of"}, {"source": "Peter Welinder", "target": "Training language models to follow instructions with human feedback", "value": "author_of"}, {"source": "Paul Christiano", "target": "Training language models to follow instructions with human feedback", "value": "author_of"}, {"source": "Jan Leike", "target": "Training language models to follow instructions with human feedback", "value": "author_of"}, {"source": "Ryan Lowe", "target": "Training language models to follow instructions with human feedback", "value": "author_of"}, {"source": "Bowen Jin", "target": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning", "value": "author_of"}, {"source": "Hansi Zeng", "target": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning", "value": "author_of"}, {"source": "Zhenrui Yue", "target": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning", "value": "author_of"}, {"source": "Jinsung Yoon", "target": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning", "value": "author_of"}, {"source": "Sercan Arik", "target": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning", "value": "author_of"}, {"source": "Dong Wang", "target": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning", "value": "author_of"}, {"source": "Hamed Zamani", "target": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning", "value": "author_of"}, {"source": "Jiawei Han", "target": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning", "value": "author_of"}, {"source": "Karan Singhal", "target": "Toward expert-level medical question answering with large language models", "value": "author_of"}, {"source": "Tao Tu", "target": "Toward expert-level medical question answering with large language models", "value": "author_of"}, {"source": "Juraj Gottweis", "target": "Toward expert-level medical question answering with large language models", "value": "author_of"}, {"source": "R. Sayres", "target": "Toward expert-level medical question answering with large language models", "value": "author_of"}, {"source": "Ellery Wulczyn", "target": "Toward expert-level medical question answering with large language models", "value": "author_of"}, {"source": "Mohamed Amin", "target": "Toward expert-level medical question answering with large language models", "value": "author_of"}, {"source": "Le Hou", "target": "Toward expert-level medical question answering with large language models", "value": "author_of"}, {"source": "Kevin Clark", "target": "Toward expert-level medical question answering with large language models", "value": "author_of"}, {"source": "Stephen R. Pfohl", "target": "Toward expert-level medical question answering with large language models", "value": "author_of"}, {"source": "Heather Cole-Lewis", "target": "Toward expert-level medical question answering with large language models", "value": "author_of"}, {"source": "Darlene Neal", "target": "Toward expert-level medical question answering with large language models", "value": "author_of"}, {"source": "Q. Rashid", "target": "Toward expert-level medical question answering with large language models", "value": "author_of"}, {"source": "Mike Schaekermann", "target": "Toward expert-level medical question answering with large language models", "value": "author_of"}, {"source": "Amy Wang", "target": "Toward expert-level medical question answering with large language models", "value": "author_of"}, {"source": "Dev Dash", "target": "Toward expert-level medical question answering with large language models", "value": "author_of"}, {"source": "Jonathan H. Chen", "target": "Toward expert-level medical question answering with large language models", "value": "author_of"}, {"source": "Nigam H. Shah", "target": "Toward expert-level medical question answering with large language models", "value": "author_of"}, {"source": "Sami Lachgar", "target": "Toward expert-level medical question answering with large language models", "value": "author_of"}, {"source": "P. Mansfield", "target": "Toward expert-level medical question answering with large language models", "value": "author_of"}, {"source": "Sushant Prakash", "target": "Toward expert-level medical question answering with large language models", "value": "author_of"}, {"source": "Bradley Green", "target": "Toward expert-level medical question answering with large language models", "value": "author_of"}, {"source": "Ewa Dominowska", "target": "Toward expert-level medical question answering with large language models", "value": "author_of"}, {"source": "Blaise Ag\u00fcera y Arcas", "target": "Toward expert-level medical question answering with large language models", "value": "author_of"}, {"source": "Nenad Toma\u0161ev", "target": "Toward expert-level medical question answering with large language models", "value": "author_of"}, {"source": "Yun Liu", "target": "Toward expert-level medical question answering with large language models", "value": "author_of"}, {"source": "Renee Wong", "target": "Toward expert-level medical question answering with large language models", "value": "author_of"}, {"source": "Christopher Semturs", "target": "Toward expert-level medical question answering with large language models", "value": "author_of"}, {"source": "S. Mahdavi", "target": "Toward expert-level medical question answering with large language models", "value": "author_of"}, {"source": "Joelle K. Barral", "target": "Toward expert-level medical question answering with large language models", "value": "author_of"}, {"source": "Dale R. Webster", "target": "Toward expert-level medical question answering with large language models", "value": "author_of"}, {"source": "G. Corrado", "target": "Toward expert-level medical question answering with large language models", "value": "author_of"}, {"source": "Yossi Matias", "target": "Toward expert-level medical question answering with large language models", "value": "author_of"}, {"source": "Shekoofeh Azizi", "target": "Toward expert-level medical question answering with large language models", "value": "author_of"}, {"source": "A. Karthikesalingam", "target": "Toward expert-level medical question answering with large language models", "value": "author_of"}, {"source": "Vivek Natarajan", "target": "Toward expert-level medical question answering with large language models", "value": "author_of"}, {"source": "Tianzhe Chu", "target": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training", "value": "author_of"}, {"source": "Yuexiang Zhai", "target": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training", "value": "author_of"}, {"source": "Jihan Yang", "target": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training", "value": "author_of"}, {"source": "Shengbang Tong", "target": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training", "value": "author_of"}, {"source": "Saining Xie", "target": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training", "value": "author_of"}, {"source": "Dale Schuurmans", "target": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training", "value": "author_of"}, {"source": "Quoc V. Le", "target": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training", "value": "author_of"}, {"source": "Sergey Levine", "target": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training", "value": "author_of"}, {"source": "Yi Ma", "target": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training", "value": "author_of"}, {"source": "Peijia Lin", "target": "Equivariant Diffusion for Crystal Structure Prediction", "value": "author_of"}, {"source": "Pin Chen", "target": "Equivariant Diffusion for Crystal Structure Prediction", "value": "author_of"}, {"source": "Rui Jiao", "target": "Equivariant Diffusion for Crystal Structure Prediction", "value": "author_of"}, {"source": "Qing Mo", "target": "Equivariant Diffusion for Crystal Structure Prediction", "value": "author_of"}, {"source": "Jianhuan Cen", "target": "Equivariant Diffusion for Crystal Structure Prediction", "value": "author_of"}, {"source": "Wenbing Huang", "target": "Equivariant Diffusion for Crystal Structure Prediction", "value": "author_of"}, {"source": "Yang Liu", "target": "Equivariant Diffusion for Crystal Structure Prediction", "value": "author_of"}, {"source": "Dan Huang", "target": "Equivariant Diffusion for Crystal Structure Prediction", "value": "author_of"}, {"source": "Yutong Lu", "target": "Equivariant Diffusion for Crystal Structure Prediction", "value": "author_of"}, {"source": "Wenqiang Sun", "target": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling", "value": "author_of"}, {"source": "Haiyu Zhang", "target": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling", "value": "author_of"}, {"source": "Haoyuan Wang", "target": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling", "value": "author_of"}, {"source": "Junta Wu", "target": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling", "value": "author_of"}, {"source": "Zehan Wang", "target": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling", "value": "author_of"}, {"source": "Zhenwei Wang", "target": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling", "value": "author_of"}, {"source": "Yunhong Wang", "target": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling", "value": "author_of"}, {"source": "Jun Zhang", "target": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling", "value": "author_of"}, {"source": "Tengfei Wang", "target": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling", "value": "author_of"}, {"source": "Chunchao Guo", "target": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling", "value": "author_of"}, {"source": "Jianfeng Xiang", "target": "Native and Compact Structured Latents for 3D Generation", "value": "author_of"}, {"source": "Xiaoxue Chen", "target": "Native and Compact Structured Latents for 3D Generation", "value": "author_of"}, {"source": "Sicheng Xu", "target": "Native and Compact Structured Latents for 3D Generation", "value": "author_of"}, {"source": "Ruicheng Wang", "target": "Native and Compact Structured Latents for 3D Generation", "value": "author_of"}, {"source": "Zelong Lv", "target": "Native and Compact Structured Latents for 3D Generation", "value": "author_of"}, {"source": "Yu Deng", "target": "Native and Compact Structured Latents for 3D Generation", "value": "author_of"}, {"source": "Hongyuan Zhu", "target": "Native and Compact Structured Latents for 3D Generation", "value": "author_of"}, {"source": "Yue Dong", "target": "Native and Compact Structured Latents for 3D Generation", "value": "author_of"}, {"source": "Hao Zhao", "target": "Native and Compact Structured Latents for 3D Generation", "value": "author_of"}, {"source": "Nicholas Jing Yuan", "target": "Native and Compact Structured Latents for 3D Generation", "value": "author_of"}, {"source": "Jiaolong Yang", "target": "Native and Compact Structured Latents for 3D Generation", "value": "author_of"}, {"source": "Basile Terver", "target": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?", "value": "author_of"}, {"source": "Tsung-Yen Yang", "target": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?", "value": "author_of"}, {"source": "Jean Ponce", "target": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?", "value": "author_of"}, {"source": "Adrien Bardes", "target": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?", "value": "author_of"}, {"source": "Yann LeCun", "target": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?", "value": "author_of"}, {"source": "Guangyi Zhang", "target": "Progressive Learned Image Transmission for Semantic Communication Using Hierarchical VAE", "value": "author_of"}, {"source": "Hanlei Li", "target": "Progressive Learned Image Transmission for Semantic Communication Using Hierarchical VAE", "value": "author_of"}, {"source": "Yunlong Cai", "target": "Progressive Learned Image Transmission for Semantic Communication Using Hierarchical VAE", "value": "author_of"}, {"source": "Qiyu Hu", "target": "Progressive Learned Image Transmission for Semantic Communication Using Hierarchical VAE", "value": "author_of"}, {"source": "Guanding Yu", "target": "Progressive Learned Image Transmission for Semantic Communication Using Hierarchical VAE", "value": "author_of"}, {"source": "Zhijing Qin", "target": "Progressive Learned Image Transmission for Semantic Communication Using Hierarchical VAE", "value": "author_of"}, {"source": "Wenjun Lin", "target": "STORM: Search-Guided Generative World Models for Robotic Manipulation", "value": "author_of"}, {"source": "Jensen Zhang", "target": "STORM: Search-Guided Generative World Models for Robotic Manipulation", "value": "author_of"}, {"source": "Kaitong Cai", "target": "STORM: Search-Guided Generative World Models for Robotic Manipulation", "value": "author_of"}, {"source": "Keze Wang", "target": "STORM: Search-Guided Generative World Models for Robotic Manipulation", "value": "author_of"}, {"source": "Qianqian Wang", "target": "Continuous 3D Perception Model with Persistent State", "value": "author_of"}, {"source": "Yifei Zhang", "target": "Continuous 3D Perception Model with Persistent State", "value": "author_of"}, {"source": "Aleksander Holynski", "target": "Continuous 3D Perception Model with Persistent State", "value": "author_of"}, {"source": "Alexei A. Efros", "target": "Continuous 3D Perception Model with Persistent State", "value": "author_of"}, {"source": "Angjoo Kanazawa", "target": "Continuous 3D Perception Model with Persistent State", "value": "author_of"}, {"source": "Xuanchi Ren", "target": "Gen3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control", "value": "author_of"}, {"source": "Tianchang Shen", "target": "Gen3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control", "value": "author_of"}, {"source": "Jiahui Huang", "target": "Gen3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control", "value": "author_of"}, {"source": "Huan Ling", "target": "Gen3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control", "value": "author_of"}, {"source": "Yifan Lu", "target": "Gen3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control", "value": "author_of"}, {"source": "Merlin Nimier-David", "target": "Gen3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control", "value": "author_of"}, {"source": "Thomas M\u00fcller", "target": "Gen3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control", "value": "author_of"}, {"source": "Alexander Keller", "target": "Gen3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control", "value": "author_of"}, {"source": "Sanja Fidler", "target": "Gen3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control", "value": "author_of"}, {"source": "Jun Gao", "target": "Gen3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control", "value": "author_of"}, {"source": "Shangzhan Zhang", "target": "FLARE: Feed-Forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views", "value": "author_of"}, {"source": "Jianyuan Wang", "target": "FLARE: Feed-Forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views", "value": "author_of"}, {"source": "Yinghao Xu", "target": "FLARE: Feed-Forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views", "value": "author_of"}, {"source": "Nan Xue", "target": "FLARE: Feed-Forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views", "value": "author_of"}, {"source": "Christian Rupprecht", "target": "FLARE: Feed-Forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views", "value": "author_of"}, {"source": "Xiaowei Zhou", "target": "FLARE: Feed-Forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views", "value": "author_of"}, {"source": "Yujun Shen", "target": "FLARE: Feed-Forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views", "value": "author_of"}, {"source": "Gordon Wetzstein", "target": "FLARE: Feed-Forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views", "value": "author_of"}, {"source": "Luigi Piccinelli", "target": "UniDepthV2: Universal Monocular Metric Depth Estimation Made Simpler", "value": "author_of"}, {"source": "Christos Sakaridis", "target": "UniDepthV2: Universal Monocular Metric Depth Estimation Made Simpler", "value": "author_of"}, {"source": "Yung-Hsu Yang", "target": "UniDepthV2: Universal Monocular Metric Depth Estimation Made Simpler", "value": "author_of"}, {"source": "Mattia Segu", "target": "UniDepthV2: Universal Monocular Metric Depth Estimation Made Simpler", "value": "author_of"}, {"source": "Siyuan Li", "target": "UniDepthV2: Universal Monocular Metric Depth Estimation Made Simpler", "value": "author_of"}, {"source": "Wim Abbeloos", "target": "UniDepthV2: Universal Monocular Metric Depth Estimation Made Simpler", "value": "author_of"}, {"source": "Luc Van Gool", "target": "UniDepthV2: Universal Monocular Metric Depth Estimation Made Simpler", "value": "author_of"}, {"source": "Shuo Xing", "target": "OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Chengyuan Qian", "target": "OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Yuping Wang", "target": "OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Hongyuan Hua", "target": "OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Kexin Tian", "target": "OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Yang Zhou", "target": "OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Zhengzhong Tu", "target": "OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving", "value": "author_of"}, {"source": "Ana Davila", "target": "Adaptive transfer learning for surgical tool presence detection in laparoscopic videos through gradual freezing fine-tuning", "value": "author_of"}, {"source": "Jacinto Colan", "target": "Adaptive transfer learning for surgical tool presence detection in laparoscopic videos through gradual freezing fine-tuning", "value": "author_of"}, {"source": "Yasuhisa Hasegawa", "target": "Adaptive transfer learning for surgical tool presence detection in laparoscopic videos through gradual freezing fine-tuning", "value": "author_of"}, {"source": "Ross Girshick", "target": "Rich feature hierarchies for accurate object detection and semantic segmentation", "value": "author_of"}, {"source": "Jeff Donahue", "target": "Rich feature hierarchies for accurate object detection and semantic segmentation", "value": "author_of"}, {"source": "Trevor Darrell", "target": "Rich feature hierarchies for accurate object detection and semantic segmentation", "value": "author_of"}, {"source": "Jitendra Malik", "target": "Rich feature hierarchies for accurate object detection and semantic segmentation", "value": "author_of"}, {"source": "Haotian Lv", "target": "Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism", "value": "author_of"}, {"source": "Yuhui Zhang", "target": "Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism", "value": "author_of"}, {"source": "Jiangbo Dai", "target": "Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism", "value": "author_of"}, {"source": "Hanli Wu", "target": "Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism", "value": "author_of"}, {"source": "Jiaji Wang", "target": "Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism", "value": "author_of"}, {"source": "Dawei Wang", "target": "Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism", "value": "author_of"}, {"source": "Takuya Akiba", "target": "Evolutionary Optimization of Model Merging Recipes", "value": "author_of"}, {"source": "Makoto Shing", "target": "Evolutionary Optimization of Model Merging Recipes", "value": "author_of"}, {"source": "Yujin Tang", "target": "Evolutionary Optimization of Model Merging Recipes", "value": "author_of"}, {"source": "Qi Sun", "target": "Evolutionary Optimization of Model Merging Recipes", "value": "author_of"}, {"source": "David Ha", "target": "Evolutionary Optimization of Model Merging Recipes", "value": "author_of"}, {"source": "Kyunghyun Cho", "target": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "value": "author_of"}, {"source": "Bart van Merrienboer", "target": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "value": "author_of"}, {"source": "Caglar Gulcehre", "target": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "value": "author_of"}, {"source": "Dzmitry Bahdanau", "target": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "value": "author_of"}, {"source": "Fethi Bougares", "target": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "value": "author_of"}, {"source": "Holger Schwenk", "target": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "value": "author_of"}, {"source": "Yoshua Bengio", "target": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "value": "author_of"}, {"source": "Erfei Cui", "target": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving", "value": "author_of"}, {"source": "Wenhai Wang", "target": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving", "value": "author_of"}, {"source": "Zhiqi Li", "target": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving", "value": "author_of"}, {"source": "Jiangwei Xie", "target": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving", "value": "author_of"}, {"source": "Haoming Zou", "target": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving", "value": "author_of"}, {"source": "Hanming Deng", "target": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving", "value": "author_of"}, {"source": "Gen Luo", "target": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving", "value": "author_of"}, {"source": "Lewei Lu", "target": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving", "value": "author_of"}, {"source": "Xizhou Zhu", "target": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving", "value": "author_of"}, {"source": "Jifeng Dai", "target": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving", "value": "author_of"}, {"source": "Haodong Duan", "target": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Xinyu Fang", "target": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Junming Yang", "target": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Xiangyu Zhao", "target": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Yuxuan Qiao", "target": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Mo Li", "target": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Amit Agarwal", "target": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Zhe Chen", "target": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Lin Chen", "target": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Yuan Liu", "target": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Yubo Ma", "target": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Hailong Sun", "target": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Yifan Zhang", "target": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Shiyin Lu", "target": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Tack Hwa Wong", "target": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Weiyun Wang", "target": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Peiheng Zhou", "target": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Xiaozhe Li", "target": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Chaoyou Fu", "target": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Junbo Cui", "target": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Jixuan Chen", "target": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Enxin Song", "target": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Song Mao", "target": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Shengyuan Ding", "target": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Tianhao Liang", "target": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Zicheng Zhang", "target": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Xiaoyi Dong", "target": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Yuhang Zang", "target": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Pan Zhang", "target": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Jiaqi Wang", "target": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Dahua Lin", "target": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Kai Chen", "target": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models", "value": "author_of"}, {"source": "Jason Wei", "target": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "value": "author_of"}, {"source": "Xuezhi Wang", "target": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "value": "author_of"}, {"source": "Dale Schuurmans", "target": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "value": "author_of"}, {"source": "Maarten Bosma", "target": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "value": "author_of"}, {"source": "Brian Ichter", "target": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "value": "author_of"}, {"source": "Fei Xia", "target": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "value": "author_of"}, {"source": "Ed Chi", "target": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "value": "author_of"}, {"source": "Quoc Le", "target": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "value": "author_of"}, {"source": "Denny Zhou", "target": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "value": "author_of"}, {"source": "Anwesha Mukherjee", "target": "Federated Learning Architectures: A Performance Evaluation with Crop Yield Prediction Application", "value": "author_of"}, {"source": "Rajkumar Buyya", "target": "Federated Learning Architectures: A Performance Evaluation with Crop Yield Prediction Application", "value": "author_of"}, {"source": "Zhiyuan You", "target": "Enhancing Descriptive Image Quality Assessment with A Large-scale Multi-modal Dataset", "value": "author_of"}, {"source": "Jinjin Gu", "target": "Enhancing Descriptive Image Quality Assessment with A Large-scale Multi-modal Dataset", "value": "author_of"}, {"source": "Xin Cai", "target": "Enhancing Descriptive Image Quality Assessment with A Large-scale Multi-modal Dataset", "value": "author_of"}, {"source": "Zheyuan Li", "target": "Enhancing Descriptive Image Quality Assessment with A Large-scale Multi-modal Dataset", "value": "author_of"}, {"source": "Kaiwen Zhu", "target": "Enhancing Descriptive Image Quality Assessment with A Large-scale Multi-modal Dataset", "value": "author_of"}, {"source": "Chao Dong", "target": "Enhancing Descriptive Image Quality Assessment with A Large-scale Multi-modal Dataset", "value": "author_of"}, {"source": "Tianfan Xue", "target": "Enhancing Descriptive Image Quality Assessment with A Large-scale Multi-modal Dataset", "value": "author_of"}, {"source": "Matthew E. Peters", "target": "Deep contextualized word representations", "value": "author_of"}, {"source": "Mark Neumann", "target": "Deep contextualized word representations", "value": "author_of"}, {"source": "Mohit Iyyer", "target": "Deep contextualized word representations", "value": "author_of"}, {"source": "Matt Gardner", "target": "Deep contextualized word representations", "value": "author_of"}, {"source": "Christopher Clark", "target": "Deep contextualized word representations", "value": "author_of"}, {"source": "Kenton Lee", "target": "Deep contextualized word representations", "value": "author_of"}, {"source": "Luke Zettlemoyer", "target": "Deep contextualized word representations", "value": "author_of"}, {"source": "DeepSeek-AI", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Daya Guo", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Dejian Yang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Haowei Zhang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Junxiao Song", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Peiyi Wang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Qihao Zhu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Runxin Xu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Ruoyu Zhang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Shirong Ma", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Xiao Bi", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Xiaokang Zhang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Xingkai Yu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Yu Wu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Z. F. Wu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Zhibin Gou", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Zhihong Shao", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Zhuoshu Li", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Ziyi Gao", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Aixin Liu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Bing Xue", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Bingxuan Wang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Bochao Wu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Bei Feng", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Chengda Lu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Chenggang Zhao", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Chengqi Deng", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Chenyu Zhang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Chong Ruan", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Damai Dai", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Deli Chen", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Dongjie Ji", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Erhang Li", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Fangyun Lin", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Fucong Dai", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Fuli Luo", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Guangbo Hao", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Guanting Chen", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Guowei Li", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "H. Zhang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Han Bao", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Hanwei Xu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Haocheng Wang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Honghui Ding", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Huajian Xin", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Huazuo Gao", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Hui Qu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Hui Li", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Jianzhong Guo", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Jiashi Li", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Jiawei Wang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Jingchang Chen", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Jingyang Yuan", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Junjie Qiu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Junlong Li", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "J. L. Cai", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Jiaqi Ni", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Jian Liang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Jin Chen", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Kai Dong", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Kai Hu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Kaige Gao", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Kang Guan", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Kexin Huang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Kuai Yu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Lean Wang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Lecong Zhang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Liang Zhao", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Litong Wang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Liyue Zhang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Lei Xu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Leyi Xia", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Mingchuan Zhang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Minghua Zhang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Minghui Tang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Meng Li", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Miaojun Wang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Mingming Li", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Ning Tian", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Panpan Huang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Peng Zhang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Qiancheng Wang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Qinyu Chen", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Qiushi Du", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Ruiqi Ge", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Ruisong Zhang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Ruizhe Pan", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Runji Wang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "R. J. Chen", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "R. L. Jin", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Ruyi Chen", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Shanghao Lu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Shangyan Zhou", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Shanhuang Chen", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Shengfeng Ye", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Shiyu Wang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Shuiping Yu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Shunfeng Zhou", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Shuting Pan", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "S. S. Li", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Shuang Zhou", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Shaoqing Wu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Shengfeng Ye", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Tao Yun", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Tian Pei", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Tianyu Sun", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "T. Wang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Wangding Zeng", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Wanjia Zhao", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Wen Liu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Wenfeng Liang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Wenjun Gao", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Wenqin Yu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Wentao Zhang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "W. L. Xiao", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Wei An", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Xiaodong Liu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Xiaohan Wang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Xiaokang Chen", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Xiaotao Nie", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Xin Cheng", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Xin Liu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Xin Xie", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Xingchao Liu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Xinyu Yang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Xinyuan Li", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Xuecheng Su", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Xuheng Lin", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "X. Q. Li", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Xiangyue Jin", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Xiaojin Shen", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Xiaosha Chen", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Xiaowen Sun", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Xiaoxiang Wang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Xinnan Song", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Xinyi Zhou", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Xianzu Wang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Xinxia Shan", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Y. K. Li", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Y. Q. Wang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Y. X. Wei", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Yang Zhang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Yanhong Xu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Yao Li", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Yao Zhao", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Yaofeng Sun", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Yaohui Wang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Yi Yu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Yichao Zhang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Yifan Shi", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Yiliang Xiong", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Ying He", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Yishi Piao", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Yisong Wang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Yixuan Tan", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Yiyang Ma", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Yiyuan Liu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Yongqiang Guo", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Yuan Ou", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Yuduan Wang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Yue Gong", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Yuheng Zou", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Yujia He", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Yunfan Xiong", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Yuxiang Luo", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Yuxiang You", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Yuxuan Liu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Yuyang Zhou", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Y. X. Zhu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Yanhong Xu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Yanping Huang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Yaohui Li", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Yi Zheng", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Yuchen Zhu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Yunxian Ma", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Ying Tang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Yukun Zha", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Yuting Yan", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Z. Z. Ren", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Zehui Ren", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Zhangli Sha", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Zhe Fu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Zhean Xu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Zhenda Xie", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Zhengyan Zhang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Zhewen Hao", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Zhicheng Ma", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Zhigang Yan", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Zhiyu Wu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Zihui Gu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Zijia Zhu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Zijun Liu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Zilin Li", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Ziwei Xie", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Ziyang Song", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Zizheng Pan", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Zhen Huang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Zhipeng Xu", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Zhongyu Zhang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Zhen Zhang", "target": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "value": "author_of"}, {"source": "Jonathan Long", "target": "Fully Convolutional Networks for Semantic Segmentation", "value": "author_of"}, {"source": "Evan Shelhamer", "target": "Fully Convolutional Networks for Semantic Segmentation", "value": "author_of"}, {"source": "Trevor Darrell", "target": "Fully Convolutional Networks for Semantic Segmentation", "value": "author_of"}, {"source": "Jiaming Zhang", "target": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with Transformers", "value": "author_of"}, {"source": "Huayao Liu", "target": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with Transformers", "value": "author_of"}, {"source": "Kailun Yang", "target": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with Transformers", "value": "author_of"}, {"source": "Xinxin Hu", "target": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with Transformers", "value": "author_of"}, {"source": "Ruiping Liu", "target": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with Transformers", "value": "author_of"}, {"source": "Rainer Stiefelhagen", "target": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with Transformers", "value": "author_of"}, {"source": "Cheng Chi", "target": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion", "value": "author_of"}, {"source": "Zhenjia Xu", "target": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion", "value": "author_of"}, {"source": "Siyuan Feng", "target": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion", "value": "author_of"}, {"source": "Eric Cousineau", "target": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion", "value": "author_of"}, {"source": "Yilun Du", "target": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion", "value": "author_of"}, {"source": "Benjamin Burchfiel", "target": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion", "value": "author_of"}, {"source": "Russ Tedrake", "target": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion", "value": "author_of"}, {"source": "Shuran Song", "target": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion", "value": "author_of"}, {"source": "Yang Liu", "target": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI", "value": "author_of"}, {"source": "Weixing Chen", "target": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI", "value": "author_of"}, {"source": "Yongjie Bai", "target": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI", "value": "author_of"}, {"source": "Xiaodan Liang", "target": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI", "value": "author_of"}, {"source": "Guanbin Li", "target": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI", "value": "author_of"}, {"source": "Wen Gao", "target": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI", "value": "author_of"}, {"source": "Liang Lin", "target": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI", "value": "author_of"}, {"source": "Xuanchi Ren", "target": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control", "value": "author_of"}, {"source": "Tianchang Shen", "target": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control", "value": "author_of"}, {"source": "Jiahui Huang", "target": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control", "value": "author_of"}, {"source": "Huan Ling", "target": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control", "value": "author_of"}, {"source": "Yifan Lu", "target": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control", "value": "author_of"}, {"source": "Merlin Nimier-David", "target": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control", "value": "author_of"}, {"source": "Thomas M\u00fcller", "target": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control", "value": "author_of"}, {"source": "Alexander Keller", "target": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control", "value": "author_of"}, {"source": "Sanja Fidler", "target": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control", "value": "author_of"}, {"source": "Jun Gao", "target": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control", "value": "author_of"}, {"source": "Shangzhan Zhang", "target": "FLARE: Feed-forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views", "value": "author_of"}, {"source": "Jianyuan Wang", "target": "FLARE: Feed-forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views", "value": "author_of"}, {"source": "Yinghao Xu", "target": "FLARE: Feed-forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views", "value": "author_of"}, {"source": "Nan Xue", "target": "FLARE: Feed-forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views", "value": "author_of"}, {"source": "Christian Rupprecht", "target": "FLARE: Feed-forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views", "value": "author_of"}, {"source": "Xiaowei Zhou", "target": "FLARE: Feed-forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views", "value": "author_of"}, {"source": "Yujun Shen", "target": "FLARE: Feed-forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views", "value": "author_of"}, {"source": "Gordon Wetzstein", "target": "FLARE: Feed-forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views", "value": "author_of"}, {"source": "Attention Is All You Need", "target": "Deep Residual Learning for Image Recognition", "value": "cites"}, {"source": "Attention Is All You Need", "target": "Adam: A Method for Stochastic Optimization", "value": "cites"}, {"source": "Attention Is All You Need", "target": "Long Short-Term Memory", "value": "cites"}, {"source": "Attention Is All You Need", "target": "Dropout: a simple way to prevent neural networks from overfitting", "value": "cites"}, {"source": "Attention Is All You Need", "target": "Rethinking the Inception Architecture for Computer Vision", "value": "cites"}, {"source": "A comprehensive review of recommender systems: Transitioning from theory to practice", "target": "Attention Is All You Need", "value": "cites"}, {"source": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation", "target": "Attention Is All You Need", "value": "cites"}, {"source": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection", "target": "Attention Is All You Need", "value": "cites"}, {"source": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation", "target": "Attention Is All You Need", "value": "cites"}, {"source": "DiffusionEngine: Diffusion model is scalable data engine for object detection", "target": "Attention Is All You Need", "value": "cites"}, {"source": "Deep Residual Learning for Image Recognition", "target": "ImageNet classification with deep convolutional neural networks", "value": "cites"}, {"source": "Deep Residual Learning for Image Recognition", "target": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "value": "cites"}, {"source": "Deep Residual Learning for Image Recognition", "target": "Long Short-Term Memory", "value": "cites"}, {"source": "Deep Residual Learning for Image Recognition", "target": "Et al", "value": "cites"}, {"source": "Deep Residual Learning for Image Recognition", "target": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "value": "cites"}, {"source": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects", "target": "Deep Residual Learning for Image Recognition", "value": "cites"}, {"source": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning", "target": "Deep Residual Learning for Image Recognition", "value": "cites"}, {"source": "DiffusionEngine: Diffusion model is scalable data engine for object detection", "target": "Deep Residual Learning for Image Recognition", "value": "cites"}, {"source": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID", "target": "Deep Residual Learning for Image Recognition", "value": "cites"}, {"source": "Noise Perception Self-Supervised Learning for Unsupervised Person Re-Identification", "target": "Deep Residual Learning for Image Recognition", "value": "cites"}, {"source": "Adam: A Method for Stochastic Optimization", "target": "ImageNet classification with deep convolutional neural networks", "value": "cites"}, {"source": "Adam: A Method for Stochastic Optimization", "target": "Auto-Encoding Variational Bayes", "value": "cites"}, {"source": "Adam: A Method for Stochastic Optimization", "target": "Supporting Online Material for Reducing the Dimensionality of Data with Neural Networks", "value": "cites"}, {"source": "Adam: A Method for Stochastic Optimization", "target": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "value": "cites"}, {"source": "Adam: A Method for Stochastic Optimization", "target": "Speech recognition with deep recurrent neural networks", "value": "cites"}, {"source": "Weakly Supervised Image Dehazing via Physics-Based Decomposition", "target": "Adam: A Method for Stochastic Optimization", "value": "cites"}, {"source": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer", "target": "Adam: A Method for Stochastic Optimization", "value": "cites"}, {"source": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models", "target": "Adam: A Method for Stochastic Optimization", "value": "cites"}, {"source": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction", "target": "Adam: A Method for Stochastic Optimization", "value": "cites"}, {"source": "Generating an annual 30\u00a0m rice cover product for monsoon Asia (2018\u20132023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model", "target": "Adam: A Method for Stochastic Optimization", "value": "cites"}, {"source": "Rethinking the Inception Architecture for Computer Vision", "target": "ImageNet classification with deep convolutional neural networks", "value": "cites"}, {"source": "Rethinking the Inception Architecture for Computer Vision", "target": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "value": "cites"}, {"source": "Rethinking the Inception Architecture for Computer Vision", "target": "Going deeper with convolutions", "value": "cites"}, {"source": "Rethinking the Inception Architecture for Computer Vision", "target": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "value": "cites"}, {"source": "Rethinking the Inception Architecture for Computer Vision", "target": "ImageNet Large Scale Visual Recognition Challenge", "value": "cites"}, {"source": "Diffusion Transformers with Representation Autoencoders", "target": "Rethinking the Inception Architecture for Computer Vision", "value": "cites"}, {"source": "Scaling Large Language Models for Next-Generation Single-Cell Analysis", "target": "Rethinking the Inception Architecture for Computer Vision", "value": "cites"}, {"source": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights", "target": "Rethinking the Inception Architecture for Computer Vision", "value": "cites"}, {"source": "HybridVisionNet: An advanced hybrid deep learning framework for automated multi-class ocular disease diagnosis using fundus imaging", "target": "Rethinking the Inception Architecture for Computer Vision", "value": "cites"}, {"source": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot", "target": "Rethinking the Inception Architecture for Computer Vision", "value": "cites"}, {"source": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation", "target": "Attention is All you Need", "value": "cites"}, {"source": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation", "target": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "value": "cites"}, {"source": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation", "target": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium", "value": "cites"}, {"source": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation", "target": "nuScenes: A Multimodal Dataset for Autonomous Driving", "value": "cites"}, {"source": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation", "target": "CARLA: An Open Urban Driving Simulator", "value": "cites"}, {"source": "Sora as a World Model? A Complete Survey on Text-to-Video Generation", "target": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation", "value": "cites"}, {"source": "OmniNWM: Omniscient Driving Navigation World Models", "target": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation", "value": "cites"}, {"source": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask", "target": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation", "value": "cites"}, {"source": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving", "target": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation", "value": "cites"}, {"source": "MAD: Motion Appearance Decoupling for efficient Driving World Models", "target": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation", "value": "cites"}, {"source": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "target": "I and J", "value": "cites"}, {"source": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "target": "ImageNet classification with deep convolutional neural networks", "value": "cites"}, {"source": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "target": "ImageNet: A large-scale hierarchical image database", "value": "cites"}, {"source": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "target": "A and V", "value": "cites"}, {"source": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "target": "Going deeper with convolutions", "value": "cites"}, {"source": "What matters for Representation Alignment: Global Information or Spatial Structure?", "target": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "value": "cites"}, {"source": "SCB-DETR: Multiscale Deformable Transformers for Occlusion-Resilient Student Learning Behavior Detection in Smart Classroom", "target": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "value": "cites"}, {"source": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination", "target": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "value": "cites"}, {"source": "Bio-Inspired Fine-Tuning for Selective Transfer Learning in Image Classification", "target": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "value": "cites"}, {"source": "Hand Sign Language Detection Using Deep Learning", "target": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "value": "cites"}, {"source": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "target": "Deep Residual Learning for Image Recognition", "value": "cites"}, {"source": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "target": "ImageNet classification with deep convolutional neural networks", "value": "cites"}, {"source": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "target": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "value": "cites"}, {"source": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "target": "and as an in", "value": "cites"}, {"source": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "target": "Microsoft COCO: Common Objects in Context", "value": "cites"}, {"source": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects", "target": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "value": "cites"}, {"source": "FANet: Frequency-Aware Attention-Based Tiny-Object Detection in Remote Sensing Images", "target": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "value": "cites"}, {"source": "UAV-based multimodal object detection via feature enhancement and dynamic gated fusion", "target": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "value": "cites"}, {"source": "An empirical analysis of deep learning methods for small object detection from satellite imagery", "target": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "value": "cites"}, {"source": "DiffusionEngine: Diffusion model is scalable data engine for object detection", "target": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "value": "cites"}, {"source": "Auto-Encoding Variational Bayes", "target": "Representation Learning: A Review and New Perspectives", "value": "cites"}, {"source": "Auto-Encoding Variational Bayes", "target": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "value": "cites"}, {"source": "Auto-Encoding Variational Bayes", "target": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion", "value": "cites"}, {"source": "Auto-Encoding Variational Bayes", "target": "In Advances in Neural Information Processing Systems", "value": "cites"}, {"source": "Auto-Encoding Variational Bayes", "target": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models", "value": "cites"}, {"source": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation", "target": "Auto-Encoding Variational Bayes", "value": "cites"}, {"source": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions", "target": "Auto-Encoding Variational Bayes", "value": "cites"}, {"source": "GenAD: Generative End-to-End Autonomous Driving", "target": "Auto-Encoding Variational Bayes", "value": "cites"}, {"source": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model", "target": "Auto-Encoding Variational Bayes", "value": "cites"}, {"source": "Improving Video Generation with Human Feedback", "target": "Auto-Encoding Variational Bayes", "value": "cites"}, {"source": "Speech Recognition with Deep Recurrent Neural Networks", "target": "Long Short-Term Memory", "value": "cites"}, {"source": "Speech Recognition with Deep Recurrent Neural Networks", "target": "Learning representations by back-propagating errors", "value": "cites"}, {"source": "Speech Recognition with Deep Recurrent Neural Networks", "target": "Bidirectional recurrent neural networks", "value": "cites"}, {"source": "Speech Recognition with Deep Recurrent Neural Networks", "target": "Connectionist Temporal Classi\ufb01cation: Labelling Unsegmented Sequence Data with Recurrent Neural Networks", "value": "cites"}, {"source": "Speech Recognition with Deep Recurrent Neural Networks", "target": "2005 Special Issue: Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "value": "cites"}, {"source": "A high-performance neuroprosthesis for speech decoding and avatar control", "target": "Speech Recognition with Deep Recurrent Neural Networks", "value": "cites"}, {"source": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses", "target": "Speech Recognition with Deep Recurrent Neural Networks", "value": "cites"}, {"source": "A high-performance speech neuroprosthesis", "target": "Speech Recognition with Deep Recurrent Neural Networks", "value": "cites"}, {"source": "Loss of plasticity in deep continual learning", "target": "Speech Recognition with Deep Recurrent Neural Networks", "value": "cites"}, {"source": "An analog-AI chip for energy-efficient speech recognition and transcription", "target": "Speech Recognition with Deep Recurrent Neural Networks", "value": "cites"}, {"source": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models", "target": "Deep Residual Learning for Image Recognition", "value": "cites"}, {"source": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models", "target": "Attention is All you Need", "value": "cites"}, {"source": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models", "target": "Adam: A Method for Stochastic Optimization", "value": "cites"}, {"source": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models", "target": "A Mathematical Theory of Communication", "value": "cites"}, {"source": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models", "target": "Going deeper with convolutions", "value": "cites"}, {"source": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers", "target": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models", "value": "cites"}, {"source": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling", "target": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models", "value": "cites"}, {"source": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram", "target": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models", "value": "cites"}, {"source": "L$^3$: Large Lookup Layers", "target": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models", "value": "cites"}, {"source": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "target": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models", "value": "cites"}, {"source": "Going Deeper with Convolutions", "target": "ImageNet classification with deep convolutional neural networks", "value": "cites"}, {"source": "Going Deeper with Convolutions", "target": "ImageNet: A large-scale hierarchical image database", "value": "cites"}, {"source": "Going Deeper with Convolutions", "target": "Gradient-based learning applied to document recognition", "value": "cites"}, {"source": "Going Deeper with Convolutions", "target": "Regression Shrinkage and Selection via the Lasso", "value": "cites"}, {"source": "Going Deeper with Convolutions", "target": "Microsoft COCO: Common Objects in Context", "value": "cites"}, {"source": "LifeCLEF Plant Identification Task 2015", "target": "Going Deeper with Convolutions", "value": "cites"}, {"source": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics", "target": "Going Deeper with Convolutions", "value": "cites"}, {"source": "Enhancing pine wilt disease detection with synthetic data and external attention-based transformers", "target": "Going Deeper with Convolutions", "value": "cites"}, {"source": "A comprehensive review on YOLO versions for object detection", "target": "Going Deeper with Convolutions", "value": "cites"}, {"source": "Harnessing large vision and language models in agriculture: a review", "target": "Going Deeper with Convolutions", "value": "cites"}, {"source": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "target": "Deep Residual Learning for Image Recognition", "value": "cites"}, {"source": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "target": "ImageNet classification with deep convolutional neural networks", "value": "cites"}, {"source": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "target": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "value": "cites"}, {"source": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "target": "Gradient-based learning applied to document recognition", "value": "cites"}, {"source": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "target": "Going deeper with convolutions", "value": "cites"}, {"source": "A comprehensive review on YOLO versions for object detection", "target": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "value": "cites"}, {"source": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer", "target": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "value": "cites"}, {"source": "Multi-axis vision transformer for medical image segmentation", "target": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "value": "cites"}, {"source": "A comprehensive review of facial beauty prediction using deep learning techniques", "target": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "value": "cites"}, {"source": "A systematic comparison of predictive models on the retina", "target": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "value": "cites"}, {"source": "ImageNet Large Scale Visual Recognition Challenge", "target": "ImageNet classification with deep convolutional neural networks", "value": "cites"}, {"source": "ImageNet Large Scale Visual Recognition Challenge", "target": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "value": "cites"}, {"source": "ImageNet Large Scale Visual Recognition Challenge", "target": "ImageNet: A large-scale hierarchical image database", "value": "cites"}, {"source": "ImageNet Large Scale Visual Recognition Challenge", "target": "Microsoft COCO: Common Objects in Context", "value": "cites"}, {"source": "ImageNet Large Scale Visual Recognition Challenge", "target": "Distinctive Image Features from Scale-Invariant Keypoints", "value": "cites"}, {"source": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training", "target": "ImageNet Large Scale Visual Recognition Challenge", "value": "cites"}, {"source": "Diffusion Transformers with Representation Autoencoders", "target": "ImageNet Large Scale Visual Recognition Challenge", "value": "cites"}, {"source": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications", "target": "ImageNet Large Scale Visual Recognition Challenge", "value": "cites"}, {"source": "A comprehensive review on YOLO versions for object detection", "target": "ImageNet Large Scale Visual Recognition Challenge", "value": "cites"}, {"source": "Aligning machine and human visual representations across abstraction levels", "target": "ImageNet Large Scale Visual Recognition Challenge", "value": "cites"}, {"source": "Diffusion Transformers with Representation Autoencoders", "target": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "value": "cites"}, {"source": "Diffusion Transformers with Representation Autoencoders", "target": "Learning Transferable Visual Models From Natural Language Supervision", "value": "cites"}, {"source": "Diffusion Transformers with Representation Autoencoders", "target": "ImageNet Large Scale Visual Recognition Challenge", "value": "cites"}, {"source": "Diffusion Transformers with Representation Autoencoders", "target": "GENERATIVE ADVERSARIAL NETS", "value": "cites"}, {"source": "Diffusion Transformers with Representation Autoencoders", "target": "Rethinking the Inception Architecture for Computer Vision", "value": "cites"}, {"source": "Improved Mean Flows: On the Challenges of Fastforward Generative Models", "target": "Diffusion Transformers with Representation Autoencoders", "value": "cites"}, {"source": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training", "target": "Diffusion Transformers with Representation Autoencoders", "value": "cites"}, {"source": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder", "target": "Diffusion Transformers with Representation Autoencoders", "value": "cites"}, {"source": "PixelDiT: Pixel Diffusion Transformers for Image Generation", "target": "Diffusion Transformers with Representation Autoencoders", "value": "cites"}, {"source": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "target": "Diffusion Transformers with Representation Autoencoders", "value": "cites"}, {"source": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "target": "Deep Residual Learning for Image Recognition", "value": "cites"}, {"source": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "target": "Attention is All you Need", "value": "cites"}, {"source": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "target": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "value": "cites"}, {"source": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "target": "ImageNet: A large-scale hierarchical image database", "value": "cites"}, {"source": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "target": "Dropout: a simple way to prevent neural networks from overfitting", "value": "cites"}, {"source": "A comprehensive review of object detection with traditional and deep learning methods", "target": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "value": "cites"}, {"source": "Diffusion Language Models are Super Data Learners", "target": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "value": "cites"}, {"source": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas", "target": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "value": "cites"}, {"source": "FLEX: Continuous Agent Evolution via Forward Learning from Experience", "target": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "value": "cites"}, {"source": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning", "target": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "value": "cites"}, {"source": "nuScenes: A multimodal dataset for autonomous driving", "target": "Deep Residual Learning for Image Recognition", "value": "cites"}, {"source": "nuScenes: A multimodal dataset for autonomous driving", "target": "ImageNet classification with deep convolutional neural networks", "value": "cites"}, {"source": "nuScenes: A multimodal dataset for autonomous driving", "target": "ImageNet: A large-scale hierarchical image database", "value": "cites"}, {"source": "nuScenes: A multimodal dataset for autonomous driving", "target": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "value": "cites"}, {"source": "nuScenes: A multimodal dataset for autonomous driving", "target": "Histograms of oriented gradients for human detection", "value": "cites"}, {"source": "Controllable Video Generation: A Survey", "target": "nuScenes: A multimodal dataset for autonomous driving", "value": "cites"}, {"source": "Adversarial Attacks on Autonomous Driving Systems in the Physical World: A Survey", "target": "nuScenes: A multimodal dataset for autonomous driving", "value": "cites"}, {"source": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "target": "nuScenes: A multimodal dataset for autonomous driving", "value": "cites"}, {"source": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes", "target": "nuScenes: A multimodal dataset for autonomous driving", "value": "cites"}, {"source": "Detect Anything via Next Point Prediction", "target": "nuScenes: A multimodal dataset for autonomous driving", "value": "cites"}, {"source": "CARLA: An Open Urban Driving Simulator", "target": "Adam: A Method for Stochastic Optimization", "value": "cites"}, {"source": "CARLA: An Open Urban Driving Simulator", "target": "ImageNet classification with deep convolutional neural networks", "value": "cites"}, {"source": "CARLA: An Open Urban Driving Simulator", "target": "Human-level control through deep reinforcement learning", "value": "cites"}, {"source": "CARLA: An Open Urban Driving Simulator", "target": "Asynchronous Methods for Deep Reinforcement Learning", "value": "cites"}, {"source": "CARLA: An Open Urban Driving Simulator", "target": "RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation", "value": "cites"}, {"source": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving", "target": "CARLA: An Open Urban Driving Simulator", "value": "cites"}, {"source": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "target": "CARLA: An Open Urban Driving Simulator", "value": "cites"}, {"source": "Pseudo-Simulation for Autonomous Driving", "target": "CARLA: An Open Urban Driving Simulator", "value": "cites"}, {"source": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving", "target": "CARLA: An Open Urban Driving Simulator", "value": "cites"}, {"source": "A Survey on Vision-Language-Action Models for Autonomous Driving", "target": "CARLA: An Open Urban Driving Simulator", "value": "cites"}, {"source": "Sora as a World Model? A Complete Survey on Text-to-Video Generation", "target": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "value": "cites"}, {"source": "Sora as a World Model? A Complete Survey on Text-to-Video Generation", "target": "Long Short-Term Memory", "value": "cites"}, {"source": "Sora as a World Model? A Complete Survey on Text-to-Video Generation", "target": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis", "value": "cites"}, {"source": "Sora as a World Model? A Complete Survey on Text-to-Video Generation", "target": "Going deeper with convolutions", "value": "cites"}, {"source": "Sora as a World Model? A Complete Survey on Text-to-Video Generation", "target": "Learning Transferable Visual Models From Natural Language Supervision", "value": "cites"}, {"source": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond", "target": "Sora as a World Model? A Complete Survey on Text-to-Video Generation", "value": "cites"}, {"source": "Understanding World or Predicting Future? A Comprehensive Survey of World Models", "target": "Sora as a World Model? A Complete Survey on Text-to-Video Generation", "value": "cites"}, {"source": "A Survey of Multimodal Learning: Methods, Applications, and Future", "target": "Sora as a World Model? A Complete Survey on Text-to-Video Generation", "value": "cites"}, {"source": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey", "target": "Sora as a World Model? A Complete Survey on Text-to-Video Generation", "value": "cites"}, {"source": "Effects of Generative AI in Tourism Industry", "target": "Sora as a World Model? A Complete Survey on Text-to-Video Generation", "value": "cites"}, {"source": "OmniNWM: Omniscient Driving Navigation World Models", "target": "Learning Transferable Visual Models From Natural Language Supervision", "value": "cites"}, {"source": "OmniNWM: Omniscient Driving Navigation World Models", "target": "Squeeze-and-Excitation Networks", "value": "cites"}, {"source": "OmniNWM: Omniscient Driving Navigation World Models", "target": "Decoupled Weight Decay Regularization", "value": "cites"}, {"source": "OmniNWM: Omniscient Driving Navigation World Models", "target": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks", "value": "cites"}, {"source": "OmniNWM: Omniscient Driving Navigation World Models", "target": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium", "value": "cites"}, {"source": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control", "target": "OmniNWM: Omniscient Driving Navigation World Models", "value": "cites"}, {"source": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World", "target": "OmniNWM: Omniscient Driving Navigation World Models", "value": "cites"}, {"source": "DVGT: Driving Visual Geometry Transformer", "target": "OmniNWM: Omniscient Driving Navigation World Models", "value": "cites"}, {"source": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask", "target": "Learning Transferable Visual Models From Natural Language Supervision", "value": "cites"}, {"source": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask", "target": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium", "value": "cites"}, {"source": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask", "target": "nuScenes: A Multimodal Dataset for Autonomous Driving", "value": "cites"}, {"source": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask", "target": "Adding Conditional Control to Text-to-Image Diffusion Models", "value": "cites"}, {"source": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask", "target": "Scaling Instruction-Finetuned Language Models", "value": "cites"}, {"source": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving", "target": "High-Resolution Image Synthesis with Latent Diffusion Models", "value": "cites"}, {"source": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving", "target": "AUTO-ENCODING VARIATIONAL BAYES", "value": "cites"}, {"source": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving", "target": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium", "value": "cites"}, {"source": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving", "target": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric", "value": "cites"}, {"source": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving", "target": "nuScenes: A Multimodal Dataset for Autonomous Driving", "value": "cites"}, {"source": "MAD: Motion Appearance Decoupling for efficient Driving World Models", "target": "Decoupled Weight Decay Regularization", "value": "cites"}, {"source": "MAD: Motion Appearance Decoupling for efficient Driving World Models", "target": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "value": "cites"}, {"source": "MAD: Motion Appearance Decoupling for efficient Driving World Models", "target": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium", "value": "cites"}, {"source": "MAD: Motion Appearance Decoupling for efficient Driving World Models", "target": "CARLA: An Open Urban Driving Simulator", "value": "cites"}, {"source": "MAD: Motion Appearance Decoupling for efficient Driving World Models", "target": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "value": "cites"}, {"source": "What matters for Representation Alignment: Global Information or Spatial Structure?", "target": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "value": "cites"}, {"source": "What matters for Representation Alignment: Global Information or Spatial Structure?", "target": "ImageNet: A large-scale hierarchical image database", "value": "cites"}, {"source": "What matters for Representation Alignment: Global Information or Spatial Structure?", "target": "Learning Transferable Visual Models From Natural Language Supervision", "value": "cites"}, {"source": "What matters for Representation Alignment: Global Information or Spatial Structure?", "target": "Histograms of oriented gradients for human detection", "value": "cites"}, {"source": "What matters for Representation Alignment: Global Information or Spatial Structure?", "target": "Generative Adversarial Networks", "value": "cites"}, {"source": "Generalization of Diffusion Models Arises with a Balanced Representation Space", "target": "What matters for Representation Alignment: Global Information or Spatial Structure?", "value": "cites"}, {"source": "Stable Velocity: A Variance Perspective on Flow Matching", "target": "What matters for Representation Alignment: Global Information or Spatial Structure?", "value": "cites"}, {"source": "Laminating Representation Autoencoders for Efficient Diffusion", "target": "What matters for Representation Alignment: Global Information or Spatial Structure?", "value": "cites"}, {"source": "Adaptive 1D Video Diffusion Autoencoder", "target": "What matters for Representation Alignment: Global Information or Spatial Structure?", "value": "cites"}, {"source": "Test-Time Conditioning with Representation-Aligned Visual Features", "target": "What matters for Representation Alignment: Global Information or Spatial Structure?", "value": "cites"}, {"source": "Bio-inspired fine-tuning for selective transfer learning in image classification", "target": "Deep Residual Learning for Image Recognition", "value": "cites"}, {"source": "Bio-inspired fine-tuning for selective transfer learning in image classification", "target": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "value": "cites"}, {"source": "Bio-inspired fine-tuning for selective transfer learning in image classification", "target": "Gradient-based learning applied to document recognition", "value": "cites"}, {"source": "Bio-inspired fine-tuning for selective transfer learning in image classification", "target": "Going deeper with convolutions", "value": "cites"}, {"source": "Bio-inspired fine-tuning for selective transfer learning in image classification", "target": "Densely Connected Convolutional Networks", "value": "cites"}, {"source": "An extratropical cyclone center location method on satellite images based on transfer learning", "target": "Bio-inspired fine-tuning for selective transfer learning in image classification", "value": "cites"}, {"source": "An Advanced Convolutional Neural Network Architecture Utilizing Transfer Learning for Melanoma Detection", "target": "Bio-inspired fine-tuning for selective transfer learning in image classification", "value": "cites"}, {"source": "Adaptive Transfer Learning for Surgical Tool Presence Detection in Laparoscopic Videos Through Gradual Freezing Fine\u2010Tuning", "target": "Bio-inspired fine-tuning for selective transfer learning in image classification", "value": "cites"}, {"source": "VGG Induced Deep Hand Sign Language Detection", "target": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "value": "cites"}, {"source": "VGG Induced Deep Hand Sign Language Detection", "target": "ImageNet Large Scale Visual Recognition Challenge", "value": "cites"}, {"source": "VGG Induced Deep Hand Sign Language Detection", "target": "MediaPipe: A Framework for Building Perception Pipelines", "value": "cites"}, {"source": "VGG Induced Deep Hand Sign Language Detection", "target": "Hand Pose Estimation and Hand Shape Classification Using Multi-layered Randomized Decision Forests", "value": "cites"}, {"source": "VGG Induced Deep Hand Sign Language Detection", "target": "A robust static hand gesture recognition system using geometry based normalizations and Krawtchouk moments", "value": "cites"}, {"source": "Hand signal classification system for sign language communication in Virtual Reality", "target": "VGG Induced Deep Hand Sign Language Detection", "value": "cites"}, {"source": "Real-Time Static Hand Sign Recognition System using MediaPipe and Fully Connected Neural Network", "target": "VGG Induced Deep Hand Sign Language Detection", "value": "cites"}, {"source": "Real-Time Gesture Recognition to Aid Communication in Children with Motor Impairments", "target": "VGG Induced Deep Hand Sign Language Detection", "value": "cites"}, {"source": "Microsoft COCO: Common Objects in Context", "target": "ImageNet classification with deep convolutional neural networks", "value": "cites"}, {"source": "Microsoft COCO: Common Objects in Context", "target": "ImageNet: A large-scale hierarchical image database", "value": "cites"}, {"source": "Microsoft COCO: Common Objects in Context", "target": "Learning Multiple Layers of Features from Tiny Images", "value": "cites"}, {"source": "Microsoft COCO: Common Objects in Context", "target": "Histograms of oriented gradients for human detection", "value": "cites"}, {"source": "Microsoft COCO: Common Objects in Context", "target": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation", "value": "cites"}, {"source": "MuMu-LLaMA: Multi-modal music understanding and generation via large language models", "target": "Microsoft COCO: Common Objects in Context", "value": "cites"}, {"source": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy", "target": "Microsoft COCO: Common Objects in Context", "value": "cites"}, {"source": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism", "target": "Microsoft COCO: Common Objects in Context", "value": "cites"}, {"source": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking", "target": "Microsoft COCO: Common Objects in Context", "value": "cites"}, {"source": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness", "target": "Microsoft COCO: Common Objects in Context", "value": "cites"}, {"source": "Representation Learning: A Review and New Perspectives", "target": "ImageNet classification with deep convolutional neural networks", "value": "cites"}, {"source": "Representation Learning: A Review and New Perspectives", "target": "Gradient-based learning applied to document recognition", "value": "cites"}, {"source": "Representation Learning: A Review and New Perspectives", "target": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper", "value": "cites"}, {"source": "Representation Learning: A Review and New Perspectives", "target": "Visualizing Data using t-SNE", "value": "cites"}, {"source": "Representation Learning: A Review and New Perspectives", "target": "Learning Multiple Layers of Features from Tiny Images", "value": "cites"}, {"source": "LLM Social Simulations Are a Promising Research Method", "target": "Representation Learning: A Review and New Perspectives", "value": "cites"}, {"source": "Federated Contrastive Learning With Feature-Based Distillation for Human Activity Recognition", "target": "Representation Learning: A Review and New Perspectives", "value": "cites"}, {"source": "Diffuse and Disperse: Image Generation with Representation Regularization", "target": "Representation Learning: A Review and New Perspectives", "value": "cites"}, {"source": "Deep Autoencoder Neural Networks: A Comprehensive Review and New Perspectives", "target": "Representation Learning: A Review and New Perspectives", "value": "cites"}, {"source": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems", "target": "Representation Learning: A Review and New Perspectives", "value": "cites"}, {"source": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation", "target": "U-Net: Convolutional Networks for Biomedical Image Segmentation", "value": "cites"}, {"source": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation", "target": "Microsoft COCO: Common Objects in Context", "value": "cites"}, {"source": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation", "target": "Learning Transferable Visual Models From Natural Language Supervision", "value": "cites"}, {"source": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation", "target": "Generative Adversarial Networks", "value": "cites"}, {"source": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation", "target": "Decoupled Weight Decay Regularization", "value": "cites"}, {"source": "Improved Distribution Matching Distillation for Fast Image Synthesis", "target": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation", "value": "cites"}, {"source": "Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation", "target": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation", "value": "cites"}, {"source": "Evolutionary optimization of model merging recipes", "target": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation", "value": "cites"}, {"source": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models", "target": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation", "value": "cites"}, {"source": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment", "target": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation", "value": "cites"}, {"source": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions", "target": "U-Net: Convolutional Networks for Biomedical Image Segmentation", "value": "cites"}, {"source": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions", "target": "Learning Transferable Visual Models From Natural Language Supervision", "value": "cites"}, {"source": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions", "target": "Denoising Diffusion Probabilistic Models", "value": "cites"}, {"source": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions", "target": "High-Resolution Image Synthesis with Latent Diffusion Models", "value": "cites"}, {"source": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions", "target": "Auto-Encoding Variational Bayes", "value": "cites"}, {"source": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "target": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions", "value": "cites"}, {"source": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency", "target": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions", "value": "cites"}, {"source": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models", "target": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions", "value": "cites"}, {"source": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation", "target": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions", "value": "cites"}, {"source": "EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation", "target": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions", "value": "cites"}, {"source": "GenAD: Generative End-to-End Autonomous Driving", "target": "Deep Residual Learning for Image Recognition", "value": "cites"}, {"source": "GenAD: Generative End-to-End Autonomous Driving", "target": "Attention is All you Need", "value": "cites"}, {"source": "GenAD: Generative End-to-End Autonomous Driving", "target": "Decoupled Weight Decay Regularization", "value": "cites"}, {"source": "GenAD: Generative End-to-End Autonomous Driving", "target": "Feature Pyramid Networks for Object Detection", "value": "cites"}, {"source": "GenAD: Generative End-to-End Autonomous Driving", "target": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation", "value": "cites"}, {"source": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving", "target": "GenAD: Generative End-to-End Autonomous Driving", "value": "cites"}, {"source": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability", "target": "GenAD: Generative End-to-End Autonomous Driving", "value": "cites"}, {"source": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving", "target": "GenAD: Generative End-to-End Autonomous Driving", "value": "cites"}, {"source": "How Far is Video Generation from World Model: A Physical Law Perspective", "target": "GenAD: Generative End-to-End Autonomous Driving", "value": "cites"}, {"source": "EMMA: End-to-End Multimodal Model for Autonomous Driving", "target": "GenAD: Generative End-to-End Autonomous Driving", "value": "cites"}, {"source": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model", "target": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "value": "cites"}, {"source": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model", "target": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "value": "cites"}, {"source": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model", "target": "Language Models are Few-Shot Learners", "value": "cites"}, {"source": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model", "target": "Learning Transferable Visual Models From Natural Language Supervision", "value": "cites"}, {"source": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model", "target": "ImageNet Large Scale Visual Recognition Challenge", "value": "cites"}, {"source": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "target": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model", "value": "cites"}, {"source": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "target": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model", "value": "cites"}, {"source": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models", "target": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model", "value": "cites"}, {"source": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step", "target": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model", "value": "cites"}, {"source": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency", "target": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model", "value": "cites"}, {"source": "Improving Video Generation with Human Feedback", "target": "Adam: A Method for Stochastic Optimization", "value": "cites"}, {"source": "Improving Video Generation with Human Feedback", "target": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper", "value": "cites"}, {"source": "Improving Video Generation with Human Feedback", "target": "Learning Transferable Visual Models From Natural Language Supervision", "value": "cites"}, {"source": "Improving Video Generation with Human Feedback", "target": "Denoising Diffusion Probabilistic Models", "value": "cites"}, {"source": "Improving Video Generation with Human Feedback", "target": "Proximal Policy Optimization Algorithms", "value": "cites"}, {"source": "Flow-GRPO: Training Flow Matching Models via Online RL", "target": "Improving Video Generation with Human Feedback", "value": "cites"}, {"source": "DanceGRPO: Unleashing GRPO on Visual Generation", "target": "Improving Video Generation with Human Feedback", "value": "cites"}, {"source": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "target": "Improving Video Generation with Human Feedback", "value": "cites"}, {"source": "SkyReels-V2: Infinite-length Film Generative Model", "target": "Improving Video Generation with Human Feedback", "value": "cites"}, {"source": "Unified Reward Model for Multimodal Understanding and Generation", "target": "Improving Video Generation with Human Feedback", "value": "cites"}, {"source": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling", "target": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "value": "cites"}, {"source": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling", "target": "GPT-4 Technical Report", "value": "cites"}, {"source": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling", "target": "LLaMA: Open and Efficient Foundation Language Models", "value": "cites"}, {"source": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling", "target": "Chain of Thought Prompting Elicits Reasoning in Large Language Models", "value": "cites"}, {"source": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling", "target": "Training Verifiers to Solve Math Word Problems", "value": "cites"}, {"source": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram", "target": "Attention is All you Need", "value": "cites"}, {"source": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram", "target": "Crystal structure of the nucleosome core particle at 2.8\u2009\u00c5 resolution", "value": "cites"}, {"source": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram", "target": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "value": "cites"}, {"source": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram", "target": "A catalogue of splice junction sequences.", "value": "cites"}, {"source": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram", "target": "Origin of the Genetic Code", "value": "cites"}, {"source": "L$^3$: Large Lookup Layers", "target": "Decoupled Weight Decay Regularization", "value": "cites"}, {"source": "L$^3$: Large Lookup Layers", "target": "SGDR: Stochastic Gradient Descent with Warm Restarts", "value": "cites"}, {"source": "L$^3$: Large Lookup Layers", "target": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "value": "cites"}, {"source": "L$^3$: Large Lookup Layers", "target": "Compression of individual sequences via variable-rate coding", "value": "cites"}, {"source": "L$^3$: Large Lookup Layers", "target": "Pointer Sentinel Mixture Models", "value": "cites"}, {"source": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "target": "Training Verifiers to Solve Math Word Problems", "value": "cites"}, {"source": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "target": "Measuring Massive Multitask Language Understanding", "value": "cites"}, {"source": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "target": "Let's Verify Step by Step", "value": "cites"}, {"source": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "target": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark", "value": "cites"}, {"source": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "target": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding", "value": "cites"}, {"source": "LifeCLEF Plant Identification Task 2015", "target": "ImageNet classification with deep convolutional neural networks", "value": "cites"}, {"source": "LifeCLEF Plant Identification Task 2015", "target": "ImageNet: A large-scale hierarchical image database", "value": "cites"}, {"source": "LifeCLEF Plant Identification Task 2015", "target": "Going deeper with convolutions", "value": "cites"}, {"source": "LifeCLEF Plant Identification Task 2015", "target": "Pattern Recognition and Machine Learning", "value": "cites"}, {"source": "LifeCLEF Plant Identification Task 2015", "target": "Bagging Predictors", "value": "cites"}, {"source": "Automatic Image-Based Plant Disease Severity Estimation Using Deep Learning", "target": "LifeCLEF Plant Identification Task 2015", "value": "cites"}, {"source": "Plant identification using deep neural networks via optimization of transfer learning parameters", "target": "LifeCLEF Plant Identification Task 2015", "value": "cites"}, {"source": "New perspectives on plant disease characterization based on deep learning", "target": "LifeCLEF Plant Identification Task 2015", "value": "cites"}, {"source": "Deep Learning for Plant Identification in Natural Environment", "target": "LifeCLEF Plant Identification Task 2015", "value": "cites"}, {"source": "Going deeper in the automated identification of Herbarium specimens", "target": "LifeCLEF Plant Identification Task 2015", "value": "cites"}, {"source": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics", "target": "Going deeper with convolutions", "value": "cites"}, {"source": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics", "target": "Densely Connected Convolutional Networks", "value": "cites"}, {"source": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics", "target": "MobileNetV2: Inverted Residuals and Linear Bottlenecks", "value": "cites"}, {"source": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics", "target": "Communication-Efficient Learning of Deep Networks from Decentralized Data", "value": "cites"}, {"source": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics", "target": "A Comprehensive Survey on Transfer Learning", "value": "cites"}, {"source": "Privacy Preserved and Decentralized Smartphone Recommendation System", "target": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics", "value": "cites"}, {"source": "A Survey on Heterogeneity Taxonomy, Security and Privacy Preservation in the Integration of IoT, Wireless Sensor Networks and Federated Learning", "target": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics", "value": "cites"}, {"source": "A privacy-preserving federated learning with a secure collaborative for malware detection models using Internet of Things resources", "target": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics", "value": "cites"}, {"source": "Graph-based deep learning techniques for remote sensing applications: Techniques, taxonomy, and applications - A comprehensive review", "target": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics", "value": "cites"}, {"source": "Federated Learning Architectures: A Performance Evaluation With Crop Yield Prediction Application", "target": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics", "value": "cites"}, {"source": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training", "target": "Learning Transferable Visual Models From Natural Language Supervision", "value": "cites"}, {"source": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training", "target": "ImageNet Large Scale Visual Recognition Challenge", "value": "cites"}, {"source": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training", "target": "Segment Anything", "value": "cites"}, {"source": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training", "target": "Visual Instruction Tuning", "value": "cites"}, {"source": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training", "target": "Improved Baselines with Visual Instruction Tuning", "value": "cites"}, {"source": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset", "target": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training", "value": "cites"}, {"source": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs", "target": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training", "value": "cites"}, {"source": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views", "target": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training", "value": "cites"}, {"source": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning", "target": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training", "value": "cites"}, {"source": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe", "target": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training", "value": "cites"}, {"source": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications", "target": "Deep Residual Learning for Image Recognition", "value": "cites"}, {"source": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications", "target": "Attention is All you Need", "value": "cites"}, {"source": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications", "target": "Long Short-Term Memory", "value": "cites"}, {"source": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications", "target": "ImageNet: A large-scale hierarchical image database", "value": "cites"}, {"source": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications", "target": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "value": "cites"}, {"source": "Vision-Language-Action (VLA) Models: Concepts, Progress, Applications and Challenges", "target": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications", "value": "cites"}, {"source": "Leave No Observation Behind: Real-time Correction for VLA Action Chunks", "target": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications", "value": "cites"}, {"source": "Latent Chain-of-Thought World Modeling for End-to-End Driving", "target": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications", "value": "cites"}, {"source": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks", "target": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications", "value": "cites"}, {"source": "Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models", "target": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications", "value": "cites"}, {"source": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "target": "Deep Residual Learning for Image Recognition", "value": "cites"}, {"source": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "target": "Attention is All you Need", "value": "cites"}, {"source": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "target": "Adam: A Method for Stochastic Optimization", "value": "cites"}, {"source": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "target": "ImageNet classification with deep convolutional neural networks", "value": "cites"}, {"source": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "target": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "value": "cites"}, {"source": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation", "target": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "value": "cites"}, {"source": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects", "target": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "value": "cites"}, {"source": "TransSIL: A Silhouette Cue-Aware Image Classification Framework for Bird Ecological Monitoring Systems", "target": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "value": "cites"}, {"source": "CBRFormer: rendering technology-based transformer for refinement segmentation of bridge crack images", "target": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "value": "cites"}, {"source": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation", "target": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "value": "cites"}, {"source": "Learning Transferable Visual Models From Natural Language Supervision", "target": "Deep Residual Learning for Image Recognition", "value": "cites"}, {"source": "Learning Transferable Visual Models From Natural Language Supervision", "target": "Attention is All you Need", "value": "cites"}, {"source": "Learning Transferable Visual Models From Natural Language Supervision", "target": "Adam: A Method for Stochastic Optimization", "value": "cites"}, {"source": "Learning Transferable Visual Models From Natural Language Supervision", "target": "ImageNet classification with deep convolutional neural networks", "value": "cites"}, {"source": "Learning Transferable Visual Models From Natural Language Supervision", "target": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "value": "cites"}, {"source": "Deep contrastive learning enables genome-wide virtual screening.", "target": "Learning Transferable Visual Models From Natural Language Supervision", "value": "cites"}, {"source": "LLaVA-based semantic feature modulation diffusion model for underwater image enhancement", "target": "Learning Transferable Visual Models From Natural Language Supervision", "value": "cites"}, {"source": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking", "target": "Learning Transferable Visual Models From Natural Language Supervision", "value": "cites"}, {"source": "3DGS-Drag: Dragging Gaussians for Intuitive Point-Based 3D Editing", "target": "Learning Transferable Visual Models From Natural Language Supervision", "value": "cites"}, {"source": "Attention mechanisms in neural networks", "target": "Learning Transferable Visual Models From Natural Language Supervision", "value": "cites"}, {"source": "Improved Mean Flows: On the Challenges of Fastforward Generative Models", "target": "Deep Residual Learning for Image Recognition", "value": "cites"}, {"source": "Improved Mean Flows: On the Challenges of Fastforward Generative Models", "target": "Attention is All you Need", "value": "cites"}, {"source": "Improved Mean Flows: On the Challenges of Fastforward Generative Models", "target": "ImageNet: A large-scale hierarchical image database", "value": "cites"}, {"source": "Improved Mean Flows: On the Challenges of Fastforward Generative Models", "target": "Denoising Diffusion Probabilistic Models", "value": "cites"}, {"source": "Improved Mean Flows: On the Challenges of Fastforward Generative Models", "target": "High-Resolution Image Synthesis with Latent Diffusion Models", "value": "cites"}, {"source": "Bidirectional Normalizing Flow: From Data to Noise and Back", "target": "Improved Mean Flows: On the Challenges of Fastforward Generative Models", "value": "cites"}, {"source": "One-step Latent-free Image Generation with Pixel Mean Flows", "target": "Improved Mean Flows: On the Challenges of Fastforward Generative Models", "value": "cites"}, {"source": "Meta Flow Maps enable scalable reward alignment", "target": "Improved Mean Flows: On the Challenges of Fastforward Generative Models", "value": "cites"}, {"source": "Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective", "target": "Improved Mean Flows: On the Challenges of Fastforward Generative Models", "value": "cites"}, {"source": "Generative Modeling via Drifting", "target": "Improved Mean Flows: On the Challenges of Fastforward Generative Models", "value": "cites"}, {"source": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training", "target": "Attention is All you Need", "value": "cites"}, {"source": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training", "target": "ImageNet: A large-scale hierarchical image database", "value": "cites"}, {"source": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training", "target": "Denoising Diffusion Probabilistic Models", "value": "cites"}, {"source": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training", "target": "A Simple Framework for Contrastive Learning of Visual Representations", "value": "cites"}, {"source": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training", "target": "High-Resolution Image Synthesis with Latent Diffusion Models", "value": "cites"}, {"source": "PixelDiT: Pixel Diffusion Transformers for Image Generation", "target": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training", "value": "cites"}, {"source": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning", "target": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training", "value": "cites"}, {"source": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints", "target": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training", "value": "cites"}, {"source": "One-step Latent-free Image Generation with Pixel Mean Flows", "target": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training", "value": "cites"}, {"source": "Generative Modeling via Drifting", "target": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training", "value": "cites"}, {"source": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder", "target": "ImageNet: A large-scale hierarchical image database", "value": "cites"}, {"source": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder", "target": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "value": "cites"}, {"source": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder", "target": "GENERATIVE ADVERSARIAL NETS", "value": "cites"}, {"source": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder", "target": "Denoising Diffusion Probabilistic Models", "value": "cites"}, {"source": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder", "target": "High-Resolution Image Synthesis with Latent Diffusion Models", "value": "cites"}, {"source": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders", "target": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder", "value": "cites"}, {"source": "Simulating the Visual World with Artificial Intelligence: A Roadmap", "target": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder", "value": "cites"}, {"source": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks", "target": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder", "value": "cites"}, {"source": "RecTok: Reconstruction Distillation along Rectified Flow", "target": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder", "value": "cites"}, {"source": "RePack then Refine: Efficient Diffusion Transformer with Vision Foundation Model", "target": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder", "value": "cites"}, {"source": "PixelDiT: Pixel Diffusion Transformers for Image Generation", "target": "ImageNet Large Scale Visual Recognition Challenge", "value": "cites"}, {"source": "PixelDiT: Pixel Diffusion Transformers for Image Generation", "target": "Decoupled Weight Decay Regularization", "value": "cites"}, {"source": "PixelDiT: Pixel Diffusion Transformers for Image Generation", "target": "High-Resolution Image Synthesis with Latent Diffusion Models", "value": "cites"}, {"source": "PixelDiT: Pixel Diffusion Transformers for Image Generation", "target": "Diffusion Models Beat GANs on Image Synthesis", "value": "cites"}, {"source": "PixelDiT: Pixel Diffusion Transformers for Image Generation", "target": "DINOv2: Learning Robust Visual Features without Supervision", "value": "cites"}, {"source": "One-step Latent-free Image Generation with Pixel Mean Flows", "target": "PixelDiT: Pixel Diffusion Transformers for Image Generation", "value": "cites"}, {"source": "Generative Modeling via Drifting", "target": "PixelDiT: Pixel Diffusion Transformers for Image Generation", "value": "cites"}, {"source": "Laminating Representation Autoencoders for Efficient Diffusion", "target": "PixelDiT: Pixel Diffusion Transformers for Image Generation", "value": "cites"}, {"source": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss", "target": "PixelDiT: Pixel Diffusion Transformers for Image Generation", "value": "cites"}, {"source": "RePack then Refine: Efficient Diffusion Transformer with Vision Foundation Model", "target": "PixelDiT: Pixel Diffusion Transformers for Image Generation", "value": "cites"}, {"source": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "target": "U-Net: Convolutional Networks for Biomedical Image Segmentation", "value": "cites"}, {"source": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "target": "Learning Transferable Visual Models From Natural Language Supervision", "value": "cites"}, {"source": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "target": "Decoupled Weight Decay Regularization", "value": "cites"}, {"source": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "target": "Denoising Diffusion Probabilistic Models", "value": "cites"}, {"source": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "target": "High-Resolution Image Synthesis with Latent Diffusion Models", "value": "cites"}, {"source": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities", "target": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "value": "cites"}, {"source": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery", "target": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "value": "cites"}, {"source": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation", "target": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "value": "cites"}, {"source": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "target": "Attention is All you Need", "value": "cites"}, {"source": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "target": "ImageNet: A large-scale hierarchical image database", "value": "cites"}, {"source": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "target": "Distributed Representations of Words and Phrases and their Compositionality", "value": "cites"}, {"source": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "target": "GloVe: Global Vectors for Word Representation", "value": "cites"}, {"source": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "target": "Deep Contextualized Word Representations", "value": "cites"}, {"source": "Protein Language Models: Is Scaling Necessary?", "target": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "value": "cites"}, {"source": "Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis", "target": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "value": "cites"}, {"source": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation", "target": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "value": "cites"}, {"source": "Generative Classifiers Avoid Shortcut Solutions", "target": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "value": "cites"}, {"source": "Edge Large AI Model Agent-Empowered Cognitive Multimodal Semantic Communication", "target": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "value": "cites"}, {"source": "Diffusion Language Models are Super Data Learners", "target": "Language Models are Few-Shot Learners", "value": "cites"}, {"source": "Diffusion Language Models are Super Data Learners", "target": "Language Models are Unsupervised Multitask Learners", "value": "cites"}, {"source": "Diffusion Language Models are Super Data Learners", "target": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "value": "cites"}, {"source": "Diffusion Language Models are Super Data Learners", "target": "The Llama 3 Herd of Models", "value": "cites"}, {"source": "Diffusion Language Models are Super Data Learners", "target": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "value": "cites"}, {"source": "A Survey on Diffusion Language Models", "target": "Diffusion Language Models are Super Data Learners", "value": "cites"}, {"source": "Training Optimal Large Diffusion Language Models", "target": "Diffusion Language Models are Super Data Learners", "value": "cites"}, {"source": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models", "target": "Diffusion Language Models are Super Data Learners", "value": "cites"}, {"source": "Encoder-Decoder Diffusion Language Models for Efficient Training and Inference", "target": "Diffusion Language Models are Super Data Learners", "value": "cites"}, {"source": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model", "target": "Diffusion Language Models are Super Data Learners", "value": "cites"}, {"source": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas", "target": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "value": "cites"}, {"source": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas", "target": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "value": "cites"}, {"source": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas", "target": "Paper", "value": "cites"}, {"source": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas", "target": "Qwen Technical Report", "value": "cites"}, {"source": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas", "target": "Code Llama: Open Foundation Models for Code", "value": "cites"}, {"source": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way", "target": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas", "value": "cites"}, {"source": "Set Block Decoding is a Language Model Inference Accelerator", "target": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas", "value": "cites"}, {"source": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows", "target": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas", "value": "cites"}, {"source": "Constrained Decoding of Diffusion LLMs with Context-Free Grammars", "target": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas", "value": "cites"}, {"source": "Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models", "target": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas", "value": "cites"}, {"source": "FLEX: Continuous Agent Evolution via Forward Learning from Experience", "target": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "value": "cites"}, {"source": "FLEX: Continuous Agent Evolution via Forward Learning from Experience", "target": "Language Models are Few-Shot Learners", "value": "cites"}, {"source": "FLEX: Continuous Agent Evolution via Forward Learning from Experience", "target": "Learning representations by back-propagating errors", "value": "cites"}, {"source": "FLEX: Continuous Agent Evolution via Forward Learning from Experience", "target": "Language Models are Unsupervised Multitask Learners", "value": "cites"}, {"source": "FLEX: Continuous Agent Evolution via Forward Learning from Experience", "target": "Proximal Policy Optimization Algorithms", "value": "cites"}, {"source": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications", "target": "FLEX: Continuous Agent Evolution via Forward Learning from Experience", "value": "cites"}, {"source": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search", "target": "FLEX: Continuous Agent Evolution via Forward Learning from Experience", "value": "cites"}, {"source": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking", "target": "FLEX: Continuous Agent Evolution via Forward Learning from Experience", "value": "cites"}, {"source": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents", "target": "FLEX: Continuous Agent Evolution via Forward Learning from Experience", "value": "cites"}, {"source": "Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting", "target": "FLEX: Continuous Agent Evolution via Forward Learning from Experience", "value": "cites"}, {"source": "Controllable Video Generation: A Survey", "target": "Learning Transferable Visual Models From Natural Language Supervision", "value": "cites"}, {"source": "Controllable Video Generation: A Survey", "target": "GENERATIVE ADVERSARIAL NETS", "value": "cites"}, {"source": "Controllable Video Generation: A Survey", "target": "Denoising Diffusion Probabilistic Models", "value": "cites"}, {"source": "Controllable Video Generation: A Survey", "target": "AUTO-ENCODING VARIATIONAL BAYES", "value": "cites"}, {"source": "Controllable Video Generation: A Survey", "target": "nuScenes: A Multimodal Dataset for Autonomous Driving", "value": "cites"}, {"source": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation", "target": "Controllable Video Generation: A Survey", "value": "cites"}, {"source": "Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis", "target": "Controllable Video Generation: A Survey", "value": "cites"}, {"source": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing", "target": "Controllable Video Generation: A Survey", "value": "cites"}, {"source": "Paper2Video: Automatic Video Generation from Scientific Papers", "target": "Controllable Video Generation: A Survey", "value": "cites"}, {"source": "ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment", "target": "Controllable Video Generation: A Survey", "value": "cites"}, {"source": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes", "target": "nuScenes: A Multimodal Dataset for Autonomous Driving", "value": "cites"}, {"source": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes", "target": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "value": "cites"}, {"source": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes", "target": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "value": "cites"}, {"source": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes", "target": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "value": "cites"}, {"source": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes", "target": "LLaVA-OneVision: Easy Visual Task Transfer", "value": "cites"}, {"source": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views", "target": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes", "value": "cites"}, {"source": "Spatial Reasoning in Multimodal Large Language Models: A Survey of Tasks, Benchmarks and Methods", "target": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes", "value": "cites"}, {"source": "SpatialTree: How Spatial Abilities Branch Out in MLLMs", "target": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes", "value": "cites"}, {"source": "From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs", "target": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes", "value": "cites"}, {"source": "SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery", "target": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes", "value": "cites"}, {"source": "Detect Anything via Next Point Prediction", "target": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "value": "cites"}, {"source": "Detect Anything via Next Point Prediction", "target": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "value": "cites"}, {"source": "Detect Anything via Next Point Prediction", "target": "Microsoft COCO: Common Objects in Context", "value": "cites"}, {"source": "Detect Anything via Next Point Prediction", "target": "You Only Look Once: Unified, Real-Time Object Detection", "value": "cites"}, {"source": "Detect Anything via Next Point Prediction", "target": "Learning Transferable Visual Models From Natural Language Supervision", "value": "cites"}, {"source": "RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes", "target": "Detect Anything via Next Point Prediction", "value": "cites"}, {"source": "WeDetect: Fast Open-Vocabulary Object Detection as Retrieval", "target": "Detect Anything via Next Point Prediction", "value": "cites"}, {"source": "v1: Learning to Point Visual Tokens for Multimodal Grounded Reasoning", "target": "Detect Anything via Next Point Prediction", "value": "cites"}, {"source": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design", "target": "Detect Anything via Next Point Prediction", "value": "cites"}, {"source": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving", "target": "Detect Anything via Next Point Prediction", "value": "cites"}, {"source": "Asynchronous Methods for Deep Reinforcement Learning", "target": "Adam: A Method for Stochastic Optimization", "value": "cites"}, {"source": "Asynchronous Methods for Deep Reinforcement Learning", "target": "Long Short-Term Memory", "value": "cites"}, {"source": "Asynchronous Methods for Deep Reinforcement Learning", "target": "Reinforcement Learning: An Introduction", "value": "cites"}, {"source": "Asynchronous Methods for Deep Reinforcement Learning", "target": "Human-level control through deep reinforcement learning", "value": "cites"}, {"source": "Asynchronous Methods for Deep Reinforcement Learning", "target": "Continuous control with deep reinforcement learning", "value": "cites"}, {"source": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning", "target": "Asynchronous Methods for Deep Reinforcement Learning", "value": "cites"}, {"source": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models", "target": "Asynchronous Methods for Deep Reinforcement Learning", "value": "cites"}, {"source": "Learning to Reason under Off-Policy Guidance", "target": "Asynchronous Methods for Deep Reinforcement Learning", "value": "cites"}, {"source": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models", "target": "Asynchronous Methods for Deep Reinforcement Learning", "value": "cites"}, {"source": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement learning for LLMs", "target": "Asynchronous Methods for Deep Reinforcement Learning", "value": "cites"}, {"source": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation", "target": "Deep Residual Learning for Image Recognition", "value": "cites"}, {"source": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation", "target": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "value": "cites"}, {"source": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation", "target": "U-Net: Convolutional Networks for Biomedical Image Segmentation", "value": "cites"}, {"source": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation", "target": "Microsoft COCO: Common Objects in Context", "value": "cites"}, {"source": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation", "target": "Fully convolutional networks for semantic segmentation", "value": "cites"}, {"source": "Diffusion Models in Vision: A Survey", "target": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation", "value": "cites"}, {"source": "SegNeXt: Rethinking Convolutional Attention Design for Semantic Segmentation", "target": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation", "value": "cites"}, {"source": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation With Transformers", "target": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation", "value": "cites"}, {"source": "Segment Anything in High Quality", "target": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation", "value": "cites"}, {"source": "PIDNet: A Real-time Semantic Segmentation Network Inspired by PID Controllers", "target": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation", "value": "cites"}, {"source": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving", "target": "Learning Transferable Visual Models From Natural Language Supervision", "value": "cites"}, {"source": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving", "target": "GPT-4 Technical Report", "value": "cites"}, {"source": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving", "target": "nuScenes: A Multimodal Dataset for Autonomous Driving", "value": "cites"}, {"source": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving", "target": "CARLA: An Open Urban Driving Simulator", "value": "cites"}, {"source": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving", "target": "Qwen2 Technical Report", "value": "cites"}, {"source": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving", "target": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving", "value": "cites"}, {"source": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning", "target": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving", "value": "cites"}, {"source": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "target": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving", "value": "cites"}, {"source": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models", "target": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving", "value": "cites"}, {"source": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving", "target": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving", "value": "cites"}, {"source": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "target": "Denoising Diffusion Probabilistic Models", "value": "cites"}, {"source": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "target": "Chain of Thought Prompting Elicits Reasoning in Large Language Models", "value": "cites"}, {"source": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "target": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning", "value": "cites"}, {"source": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "target": "Visual Instruction Tuning", "value": "cites"}, {"source": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "target": "nuScenes: A Multimodal Dataset for Autonomous Driving", "value": "cites"}, {"source": "A Survey on Vision-Language-Action Models for Autonomous Driving", "target": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "value": "cites"}, {"source": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "target": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "value": "cites"}, {"source": "Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning", "target": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "value": "cites"}, {"source": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model", "target": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "value": "cites"}, {"source": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving", "target": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "value": "cites"}, {"source": "Pseudo-Simulation for Autonomous Driving", "target": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric", "value": "cites"}, {"source": "Pseudo-Simulation for Autonomous Driving", "target": "nuScenes: A Multimodal Dataset for Autonomous Driving", "value": "cites"}, {"source": "Pseudo-Simulation for Autonomous Driving", "target": "3D Gaussian Splatting for Real-Time Radiance Field Rendering", "value": "cites"}, {"source": "Pseudo-Simulation for Autonomous Driving", "target": "Structure-from-Motion Revisited", "value": "cites"}, {"source": "Pseudo-Simulation for Autonomous Driving", "target": "CARLA: An Open Urban Driving Simulator", "value": "cites"}, {"source": "Generalized Trajectory Scoring for End-to-end Multimodal Planning", "target": "Pseudo-Simulation for Autonomous Driving", "value": "cites"}, {"source": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model", "target": "Pseudo-Simulation for Autonomous Driving", "value": "cites"}, {"source": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving", "target": "Pseudo-Simulation for Autonomous Driving", "value": "cites"}, {"source": "RAP: 3D Rasterization Augmented End-to-End Planning", "target": "Pseudo-Simulation for Autonomous Driving", "value": "cites"}, {"source": "PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving", "target": "Pseudo-Simulation for Autonomous Driving", "value": "cites"}, {"source": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving", "target": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models", "value": "cites"}, {"source": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving", "target": "CARLA: An Open Urban Driving Simulator", "value": "cites"}, {"source": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving", "target": "Qwen Technical Report", "value": "cites"}, {"source": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving", "target": "Flow Matching for Generative Modeling", "value": "cites"}, {"source": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving", "target": "Diffusion policy: Visuomotor policy learning via action diffusion", "value": "cites"}, {"source": "Large Language Models for Robotics: A Survey", "target": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving", "value": "cites"}, {"source": "A Survey on Vision-Language-Action Models for Autonomous Driving", "target": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving", "value": "cites"}, {"source": "Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)", "target": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving", "value": "cites"}, {"source": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey", "target": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving", "value": "cites"}, {"source": "ReSim: Reliable World Simulation for Autonomous Driving", "target": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving", "value": "cites"}, {"source": "A Survey on Vision-Language-Action Models for Autonomous Driving", "target": "Language Models are Few-Shot Learners", "value": "cites"}, {"source": "A Survey on Vision-Language-Action Models for Autonomous Driving", "target": "Learning Transferable Visual Models From Natural Language Supervision", "value": "cites"}, {"source": "A Survey on Vision-Language-Action Models for Autonomous Driving", "target": "LLaMA: Open and Efficient Foundation Language Models", "value": "cites"}, {"source": "A Survey on Vision-Language-Action Models for Autonomous Driving", "target": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "value": "cites"}, {"source": "A Survey on Vision-Language-Action Models for Autonomous Driving", "target": "Visual Instruction Tuning", "value": "cites"}, {"source": "Vision-Language-Action (VLA) Models: Concepts, Progress, Applications and Challenges", "target": "A Survey on Vision-Language-Action Models for Autonomous Driving", "value": "cites"}, {"source": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving", "target": "A Survey on Vision-Language-Action Models for Autonomous Driving", "value": "cites"}, {"source": "Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey", "target": "A Survey on Vision-Language-Action Models for Autonomous Driving", "value": "cites"}, {"source": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving", "target": "A Survey on Vision-Language-Action Models for Autonomous Driving", "value": "cites"}, {"source": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition", "target": "A Survey on Vision-Language-Action Models for Autonomous Driving", "value": "cites"}, {"source": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond", "target": "Deep Residual Learning for Image Recognition", "value": "cites"}, {"source": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond", "target": "Attention is All you Need", "value": "cites"}, {"source": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond", "target": "ImageNet classification with deep convolutional neural networks", "value": "cites"}, {"source": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond", "target": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "value": "cites"}, {"source": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond", "target": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "value": "cites"}, {"source": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI", "target": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond", "value": "cites"}, {"source": "A Survey on Vision-Language-Action Models for Embodied AI", "target": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond", "value": "cites"}, {"source": "Understanding World or Predicting Future? A Comprehensive Survey of World Models", "target": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond", "value": "cites"}, {"source": "DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation", "target": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond", "value": "cites"}, {"source": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation", "target": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond", "value": "cites"}, {"source": "Understanding World or Predicting Future? A Comprehensive Survey of World Models", "target": "Attention is All you Need", "value": "cites"}, {"source": "Understanding World or Predicting Future? A Comprehensive Survey of World Models", "target": "ImageNet classification with deep convolutional neural networks", "value": "cites"}, {"source": "Understanding World or Predicting Future? A Comprehensive Survey of World Models", "target": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "value": "cites"}, {"source": "Understanding World or Predicting Future? A Comprehensive Survey of World Models", "target": "Gradient-based learning applied to document recognition", "value": "cites"}, {"source": "Understanding World or Predicting Future? A Comprehensive Survey of World Models", "target": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "value": "cites"}, {"source": "Sora as a World Model? A Complete Survey on Text-to-Video Generation", "target": "Understanding World or Predicting Future? A Comprehensive Survey of World Models", "value": "cites"}, {"source": "AgentSquare: Automatic LLM Agent Search in Modular Design Space", "target": "Understanding World or Predicting Future? A Comprehensive Survey of World Models", "value": "cites"}, {"source": "Vision-Language-Action (VLA) Models: Concepts, Progress, Applications and Challenges", "target": "Understanding World or Predicting Future? A Comprehensive Survey of World Models", "value": "cites"}, {"source": "CityGPT: Empowering Urban Spatial Cognition of Large Language Models", "target": "Understanding World or Predicting Future? A Comprehensive Survey of World Models", "value": "cites"}, {"source": "Epona: Autoregressive Diffusion World Model for Autonomous Driving", "target": "Understanding World or Predicting Future? A Comprehensive Survey of World Models", "value": "cites"}, {"source": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey", "target": "Learning Transferable Visual Models From Natural Language Supervision", "value": "cites"}, {"source": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey", "target": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "value": "cites"}, {"source": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey", "target": "Denoising Diffusion Probabilistic Models", "value": "cites"}, {"source": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey", "target": "High-Resolution Image Synthesis with Latent Diffusion Models", "value": "cites"}, {"source": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey", "target": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium", "value": "cites"}, {"source": "Unlearning Concepts in Diffusion Model via Concept Domain Correction and Concept Preserving Gradient", "target": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey", "value": "cites"}, {"source": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "target": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey", "value": "cites"}, {"source": "Large multimodal models evaluation: a survey", "target": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey", "value": "cites"}, {"source": "The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense", "target": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey", "value": "cites"}, {"source": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs", "target": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey", "value": "cites"}, {"source": "Squeeze-and-Excitation Networks", "target": "Deep Residual Learning for Image Recognition", "value": "cites"}, {"source": "Squeeze-and-Excitation Networks", "target": "Attention is All you Need", "value": "cites"}, {"source": "Squeeze-and-Excitation Networks", "target": "ImageNet classification with deep convolutional neural networks", "value": "cites"}, {"source": "Squeeze-and-Excitation Networks", "target": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "value": "cites"}, {"source": "Squeeze-and-Excitation Networks", "target": "Long Short-Term Memory", "value": "cites"}, {"source": "Gradient-Guided Learning Network for Infrared Small Target Detection", "target": "Squeeze-and-Excitation Networks", "value": "cites"}, {"source": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism", "target": "Squeeze-and-Excitation Networks", "value": "cites"}, {"source": "A Retrieval-Augmented Generation Approach to Extracting Algorithmic Logic from Neural Networks", "target": "Squeeze-and-Excitation Networks", "value": "cites"}, {"source": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness", "target": "Squeeze-and-Excitation Networks", "value": "cites"}, {"source": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation", "target": "Squeeze-and-Excitation Networks", "value": "cites"}, {"source": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks", "target": "Deep Residual Learning for Image Recognition", "value": "cites"}, {"source": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks", "target": "ImageNet classification with deep convolutional neural networks", "value": "cites"}, {"source": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks", "target": "Going deeper with convolutions", "value": "cites"}, {"source": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks", "target": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "value": "cites"}, {"source": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks", "target": "Dropout: a simple way to prevent neural networks from overfitting", "value": "cites"}, {"source": "Enhancing pine wilt disease detection with synthetic data and external attention-based transformers", "target": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks", "value": "cites"}, {"source": "CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation", "target": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks", "value": "cites"}, {"source": "IMobileTransformer: A fusion-based lightweight model for rice disease identification", "target": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks", "value": "cites"}, {"source": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images", "target": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks", "value": "cites"}, {"source": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers", "target": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks", "value": "cites"}, {"source": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control", "target": "Image quality assessment: from error visibility to structural similarity", "value": "cites"}, {"source": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control", "target": "Microsoft COCO: Common Objects in Context", "value": "cites"}, {"source": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control", "target": "Learning Transferable Visual Models From Natural Language Supervision", "value": "cites"}, {"source": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control", "target": "Generative Adversarial Networks", "value": "cites"}, {"source": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control", "target": "Denoising Diffusion Probabilistic Models", "value": "cites"}, {"source": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction", "target": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control", "value": "cites"}, {"source": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "target": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control", "value": "cites"}, {"source": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World", "target": "Attention is All you Need", "value": "cites"}, {"source": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World", "target": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium", "value": "cites"}, {"source": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World", "target": "nuScenes: A Multimodal Dataset for Autonomous Driving", "value": "cites"}, {"source": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World", "target": "Scalable Diffusion Models with Transformers", "value": "cites"}, {"source": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World", "target": "RoFormer: Enhanced Transformer with Rotary Position Embedding", "value": "cites"}, {"source": "The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey", "target": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World", "value": "cites"}, {"source": "DVGT: Driving Visual Geometry Transformer", "target": "Decoupled Weight Decay Regularization", "value": "cites"}, {"source": "DVGT: Driving Visual Geometry Transformer", "target": "Vision meets robotics: The KITTI dataset", "value": "cites"}, {"source": "DVGT: Driving Visual Geometry Transformer", "target": "nuScenes: A Multimodal Dataset for Autonomous Driving", "value": "cites"}, {"source": "DVGT: Driving Visual Geometry Transformer", "target": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "value": "cites"}, {"source": "DVGT: Driving Visual Geometry Transformer", "target": "Least-Squares Estimation of Transformation Parameters Between Two Point Patterns", "value": "cites"}, {"source": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations", "target": "DVGT: Driving Visual Geometry Transformer", "value": "cites"}, {"source": "Adding Conditional Control to Text-to-Image Diffusion Models", "target": "U-Net: Convolutional Networks for Biomedical Image Segmentation", "value": "cites"}, {"source": "Adding Conditional Control to Text-to-Image Diffusion Models", "target": "Et al", "value": "cites"}, {"source": "Adding Conditional Control to Text-to-Image Diffusion Models", "target": "Deep Learning", "value": "cites"}, {"source": "Adding Conditional Control to Text-to-Image Diffusion Models", "target": "Gradient-based learning applied to document recognition", "value": "cites"}, {"source": "Adding Conditional Control to Text-to-Image Diffusion Models", "target": "Learning Transferable Visual Models From Natural Language Supervision", "value": "cites"}, {"source": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction", "target": "Adding Conditional Control to Text-to-Image Diffusion Models", "value": "cites"}, {"source": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "target": "Adding Conditional Control to Text-to-Image Diffusion Models", "value": "cites"}, {"source": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation", "target": "Adding Conditional Control to Text-to-Image Diffusion Models", "value": "cites"}, {"source": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "target": "Adding Conditional Control to Text-to-Image Diffusion Models", "value": "cites"}, {"source": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "target": "Adding Conditional Control to Text-to-Image Diffusion Models", "value": "cites"}, {"source": "Scaling Instruction-Finetuned Language Models", "target": "Language Models are Few-Shot Learners", "value": "cites"}, {"source": "Scaling Instruction-Finetuned Language Models", "target": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "value": "cites"}, {"source": "Scaling Instruction-Finetuned Language Models", "target": "Language Models are Unsupervised Multitask Learners", "value": "cites"}, {"source": "Scaling Instruction-Finetuned Language Models", "target": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "value": "cites"}, {"source": "Scaling Instruction-Finetuned Language Models", "target": "Training language models to follow instructions with human feedback", "value": "cites"}, {"source": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "target": "Scaling Instruction-Finetuned Language Models", "value": "cites"}, {"source": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "target": "Scaling Instruction-Finetuned Language Models", "value": "cites"}, {"source": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning", "target": "Scaling Instruction-Finetuned Language Models", "value": "cites"}, {"source": "Toward expert-level medical question answering with large language models", "target": "Scaling Instruction-Finetuned Language Models", "value": "cites"}, {"source": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training", "target": "Scaling Instruction-Finetuned Language Models", "value": "cites"}, {"source": "High-Resolution Image Synthesis with Latent Diffusion Models", "target": "Attention is All you Need", "value": "cites"}, {"source": "High-Resolution Image Synthesis with Latent Diffusion Models", "target": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "value": "cites"}, {"source": "High-Resolution Image Synthesis with Latent Diffusion Models", "target": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "value": "cites"}, {"source": "High-Resolution Image Synthesis with Latent Diffusion Models", "target": "U-Net: Convolutional Networks for Biomedical Image Segmentation", "value": "cites"}, {"source": "High-Resolution Image Synthesis with Latent Diffusion Models", "target": "ImageNet: A large-scale hierarchical image database", "value": "cites"}, {"source": "Equivariant Diffusion for Crystal Structure Prediction", "target": "High-Resolution Image Synthesis with Latent Diffusion Models", "value": "cites"}, {"source": "Generative Classifiers Avoid Shortcut Solutions", "target": "High-Resolution Image Synthesis with Latent Diffusion Models", "value": "cites"}, {"source": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling", "target": "High-Resolution Image Synthesis with Latent Diffusion Models", "value": "cites"}, {"source": "Native and Compact Structured Latents for 3D Generation", "target": "High-Resolution Image Synthesis with Latent Diffusion Models", "value": "cites"}, {"source": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder", "target": "High-Resolution Image Synthesis with Latent Diffusion Models", "value": "cites"}, {"source": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric", "target": "ImageNet classification with deep convolutional neural networks", "value": "cites"}, {"source": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric", "target": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "value": "cites"}, {"source": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric", "target": "and as an in", "value": "cites"}, {"source": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric", "target": "Et al", "value": "cites"}, {"source": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric", "target": "Image quality assessment: from error visibility to structural similarity", "value": "cites"}, {"source": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?", "target": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric", "value": "cites"}, {"source": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness", "target": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric", "value": "cites"}, {"source": "Progressive Learned Image Transmission for Semantic Communication Using Hierarchical VAE", "target": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric", "value": "cites"}, {"source": "3DGS-Drag: Dragging Gaussians for Intuitive Point-Based 3D Editing", "target": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric", "value": "cites"}, {"source": "STORM: Search-Guided Generative World Models for Robotic Manipulation", "target": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric", "value": "cites"}, {"source": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "target": "Deep Residual Learning for Image Recognition", "value": "cites"}, {"source": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "target": "ImageNet: A large-scale hierarchical image database", "value": "cites"}, {"source": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "target": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "value": "cites"}, {"source": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "target": "Microsoft COCO: Common Objects in Context", "value": "cites"}, {"source": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "target": "A New Approach to Linear Filtering and Prediction Problems", "value": "cites"}, {"source": "Continuous 3D Perception Model with Persistent State", "target": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "value": "cites"}, {"source": "Gen3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control", "target": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "value": "cites"}, {"source": "FLARE: Feed-Forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views", "target": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "value": "cites"}, {"source": "UniDepthV2: Universal Monocular Metric Depth Estimation Made Simpler", "target": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "value": "cites"}, {"source": "OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving", "target": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "value": "cites"}, {"source": "Attention Is All You Need", "target": "Transformer", "value": "proposed_model"}, {"source": "Transformer", "target": "WMT 2014 English-to-German translation task", "value": "evaluated_on"}, {"source": "Transformer", "target": "WMT 2014 English-to-French translation task", "value": "evaluated_on"}, {"source": "Transformer", "target": "English constituency parsing", "value": "evaluated_on"}, {"source": "Transformer", "target": "BLEU", "value": "uses_metric"}, {"source": "Deep Residual Learning for Image Recognition", "target": "residual learning framework", "value": "proposed_model"}, {"source": "Deep Residual Learning for Image Recognition", "target": "VGG nets", "value": "baseline_model"}, {"source": "Deep Residual Learning for Image Recognition", "target": "ImageNet", "value": "evaluated_on"}, {"source": "Deep Residual Learning for Image Recognition", "target": "CIFAR-10", "value": "evaluated_on"}, {"source": "Deep Residual Learning for Image Recognition", "target": "COCO object detection dataset", "value": "evaluated_on"}, {"source": "Deep Residual Learning for Image Recognition", "target": "error", "value": "uses_metric"}, {"source": "Deep Residual Learning for Image Recognition", "target": "relative improvement", "value": "uses_metric"}, {"source": "Adam: A Method for Stochastic Optimization", "target": "Adam", "value": "proposed_model"}, {"source": "Adam: A Method for Stochastic Optimization", "target": "AdaMax", "value": "proposed_model"}, {"source": "Long Short-Term Memory", "target": "Long Short-Term Memory", "value": "proposed_model"}, {"source": "Dropout: a simple way to prevent neural networks from overfitting", "target": "Dropout", "value": "proposed_model"}, {"source": "Rethinking the Inception Architecture for Computer Vision", "target": "Inception Architecture", "value": "proposed_model"}, {"source": "Rethinking the Inception Architecture for Computer Vision", "target": "ILSVRC 2012 classification challenge validation set", "value": "evaluated_on"}, {"source": "Rethinking the Inception Architecture for Computer Vision", "target": "top-1 error", "value": "uses_metric"}, {"source": "Rethinking the Inception Architecture for Computer Vision", "target": "top-5 error", "value": "uses_metric"}, {"source": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation", "target": "InstaDrive", "value": "proposed_model"}, {"source": "InstaDrive", "target": "nuScenes", "value": "evaluated_on"}, {"source": "InstaDrive", "target": "CARLA", "value": "evaluated_on"}, {"source": "InstaDrive", "target": "video generation quality", "value": "uses_metric"}, {"source": "InstaDrive", "target": "safety evaluation", "value": "uses_metric"}, {"source": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection", "target": "CNC-VLM", "value": "proposed_model"}, {"source": "CNC-VLM", "target": "CNC fault detection dataset", "value": "evaluated_on"}, {"source": "CNC-VLM", "target": "accuracy", "value": "uses_metric"}, {"source": "CNC-VLM", "target": "F1-score", "value": "uses_metric"}, {"source": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation", "target": "mamba segmentation", "value": "proposed_model"}, {"source": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation", "target": "four-point laser metric calibration", "value": "uses_metric"}, {"source": "DiffusionEngine: Diffusion model is scalable data engine for object detection", "target": "DiffusionEngine", "value": "proposed_model"}, {"source": "ImageNet classification with deep convolutional neural networks", "target": "deep convolutional neural network", "value": "proposed_model"}, {"source": "ImageNet classification with deep convolutional neural networks", "target": "ImageNet", "value": "evaluated_on"}, {"source": "ImageNet classification with deep convolutional neural networks", "target": "top-1 error", "value": "uses_metric"}, {"source": "ImageNet classification with deep convolutional neural networks", "target": "top-5 error", "value": "uses_metric"}, {"source": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "target": "ConvNet models", "value": "proposed_model"}, {"source": "ConvNet models", "target": "ImageNet Challenge 2014", "value": "evaluated_on"}, {"source": "ConvNet models", "target": "other datasets", "value": "evaluated_on"}, {"source": "ConvNet models", "target": "accuracy", "value": "uses_metric"}, {"source": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "target": "Region Proposal Network (RPN)", "value": "proposed_model"}, {"source": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "target": "Fast R-CNN", "value": "baseline_model"}, {"source": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "target": "SPPnet", "value": "baseline_model"}, {"source": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "target": "PASCAL VOC 2007", "value": "evaluated_on"}, {"source": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "target": "PASCAL VOC 2012", "value": "evaluated_on"}, {"source": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "target": "MS COCO", "value": "evaluated_on"}, {"source": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "target": "ILSVRC", "value": "evaluated_on"}, {"source": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "target": "COCO 2015", "value": "evaluated_on"}, {"source": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "target": "frame rate", "value": "uses_metric"}, {"source": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "target": "object detection accuracy", "value": "uses_metric"}, {"source": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning", "target": "Federated Learning Optimal Transport (FLOT)", "value": "proposed_model"}, {"source": "Federated Learning Optimal Transport (FLOT)", "target": "GTSRB", "value": "evaluated_on"}, {"source": "Federated Learning Optimal Transport (FLOT)", "target": "KBTS", "value": "evaluated_on"}, {"source": "Federated Learning Optimal Transport (FLOT)", "target": "CIFAR10", "value": "evaluated_on"}, {"source": "Federated Learning Optimal Transport (FLOT)", "target": "EMNIST", "value": "evaluated_on"}, {"source": "Federated Learning Optimal Transport (FLOT)", "target": "accuracy", "value": "uses_metric"}, {"source": "Federated Learning Optimal Transport (FLOT)", "target": "scalability", "value": "uses_metric"}, {"source": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID", "target": "WarmGait", "value": "proposed_model"}, {"source": "WarmGait", "target": "thermal array sensors", "value": "evaluated_on"}, {"source": "WarmGait", "target": "average recognition accuracy", "value": "uses_metric"}, {"source": "WarmGait", "target": "Taylor Finite Difference (TFD)", "value": "baseline_model"}, {"source": "Noise Perception Self-Supervised Learning for Unsupervised Person Re-Identification", "target": "NPSSL", "value": "proposed_model"}, {"source": "Noise Perception Self-Supervised Learning for Unsupervised Person Re-Identification", "target": "Noise Perception Self-Paced Learning", "value": "proposed_model"}, {"source": "NPSSL", "target": "Duke dataset", "value": "evaluated_on"}, {"source": "Noise Perception Self-Supervised Learning for Unsupervised Person Re-Identification", "target": "Unsupervised Domain Adaptation", "value": "baseline_model"}, {"source": "Auto-Encoding Variational Bayes", "target": "stochastic variational inference and learning algorithm", "value": "proposed_model"}, {"source": "Auto-Encoding Variational Bayes", "target": "reparameterization of the variational lower bound", "value": "proposed_model"}, {"source": "Auto-Encoding Variational Bayes", "target": "approximate inference model", "value": "proposed_model"}, {"source": "stochastic variational inference and learning algorithm", "target": "i.i.d. datasets", "value": "evaluated_on"}, {"source": "reparameterization of the variational lower bound", "target": "variational lower bound", "value": "uses_metric"}, {"source": "approximate inference model", "target": "i.i.d. datasets", "value": "evaluated_on"}, {"source": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "target": "AdaGrad", "value": "proposed_model"}, {"source": "AdaGrad", "target": "Online Learning", "value": "evaluated_on"}, {"source": "AdaGrad", "target": "Stochastic Optimization", "value": "evaluated_on"}, {"source": "AdaGrad", "target": "Convergence Rate", "value": "uses_metric"}, {"source": "Speech recognition with deep recurrent neural networks", "target": "deep recurrent neural networks", "value": "proposed_model"}, {"source": "Speech recognition with deep recurrent neural networks", "target": "deep Long Short-term Memory RNNs", "value": "proposed_model"}, {"source": "Speech recognition with deep recurrent neural networks", "target": "deep feedforward networks", "value": "baseline_model"}, {"source": "deep Long Short-term Memory RNNs", "target": "TIMIT phoneme recognition benchmark", "value": "evaluated_on"}, {"source": "deep Long Short-term Memory RNNs", "target": "test set error of 17.7%", "value": "uses_metric"}, {"source": "Weakly Supervised Image Dehazing via Physics-Based Decomposition", "target": "PBD", "value": "proposed_model"}, {"source": "Weakly Supervised Image Dehazing via Physics-Based Decomposition", "target": "GAN", "value": "baseline_model"}, {"source": "PBD", "target": "seven benchmarks", "value": "evaluated_on"}, {"source": "PBD", "target": "reconstruction loss", "value": "uses_metric"}, {"source": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer", "target": "AdamW", "value": "proposed_model"}, {"source": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer", "target": "Adam", "value": "baseline_model"}, {"source": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer", "target": "face mask detection model", "value": "evaluated_on"}, {"source": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer", "target": "accuracy", "value": "uses_metric"}, {"source": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models", "target": "Engram", "value": "proposed_model"}, {"source": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models", "target": "Mixture-of-Experts (MoE)", "value": "baseline_model"}, {"source": "Engram", "target": "MMLU", "value": "evaluated_on"}, {"source": "Engram", "target": "CMMLU", "value": "evaluated_on"}, {"source": "Engram", "target": "BBH", "value": "evaluated_on"}, {"source": "Engram", "target": "ARC-Challenge", "value": "evaluated_on"}, {"source": "Engram", "target": "HumanEval", "value": "evaluated_on"}, {"source": "Engram", "target": "MATH", "value": "evaluated_on"}, {"source": "Engram", "target": "Multi-Query NIAH", "value": "evaluated_on"}, {"source": "Engram", "target": "MMLU", "value": "uses_metric"}, {"source": "Engram", "target": "CMMLU", "value": "uses_metric"}, {"source": "Engram", "target": "BBH", "value": "uses_metric"}, {"source": "Engram", "target": "ARC-Challenge", "value": "uses_metric"}, {"source": "Engram", "target": "HumanEval", "value": "uses_metric"}, {"source": "Engram", "target": "MATH", "value": "uses_metric"}, {"source": "Engram", "target": "Multi-Query NIAH", "value": "uses_metric"}, {"source": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction", "target": "Neural Hierarchical Interpolation for Time Series Forecasting (N-HiTS)", "value": "proposed_model"}, {"source": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction", "target": "Network-Based Expansion Analysis for Interpretable Time Series Forecasting (N-BEATS)", "value": "proposed_model"}, {"source": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction", "target": "long short-term memory (LSTM)", "value": "baseline_model"}, {"source": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction", "target": "two headwater streams in Georgia and North Carolina, USA", "value": "evaluated_on"}, {"source": "Neural Hierarchical Interpolation for Time Series Forecasting (N-HiTS)", "target": "Multi-Quantile Loss", "value": "uses_metric"}, {"source": "Network-Based Expansion Analysis for Interpretable Time Series Forecasting (N-BEATS)", "target": "Multi-Quantile Loss", "value": "uses_metric"}, {"source": "Neural Hierarchical Interpolation for Time Series Forecasting (N-HiTS)", "target": "95th percentile prediction uncertainty (95 PPU)", "value": "uses_metric"}, {"source": "Network-Based Expansion Analysis for Interpretable Time Series Forecasting (N-BEATS)", "target": "95th percentile prediction uncertainty (95 PPU)", "value": "uses_metric"}, {"source": "Generating an annual 30 m rice cover product for monsoon Asia (2018\u20132023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model", "target": "NASA-IBM geospatial foundation model", "value": "proposed_model"}, {"source": "Generating an annual 30 m rice cover product for monsoon Asia (2018\u20132023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model", "target": "harmonized Landsat and Sentinel-2 data", "value": "evaluated_on"}, {"source": "Going deeper with convolutions", "target": "Inception", "value": "proposed_model"}, {"source": "Going deeper with convolutions", "target": "GoogLeNet", "value": "proposed_model"}, {"source": "Inception", "target": "ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014)", "value": "evaluated_on"}, {"source": "GoogLeNet", "target": "ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014)", "value": "evaluated_on"}, {"source": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "target": "Batch Normalization", "value": "proposed_model"}, {"source": "Batch Normalization", "target": "ImageNet", "value": "evaluated_on"}, {"source": "Batch Normalization", "target": "top-5 validation error", "value": "uses_metric"}, {"source": "Batch Normalization", "target": "test error", "value": "uses_metric"}, {"source": "ImageNet Large Scale Visual Recognition Challenge", "target": "ImageNet", "value": "evaluated_on"}, {"source": "Diffusion Transformers with Representation Autoencoders", "target": "Representation Autoencoders (RAEs)", "value": "proposed_model"}, {"source": "Diffusion Transformers with Representation Autoencoders", "target": "VAE", "value": "baseline_model"}, {"source": "Diffusion Transformers with Representation Autoencoders", "target": "ImageNet", "value": "evaluated_on"}, {"source": "Diffusion Transformers with Representation Autoencoders", "target": "FID", "value": "uses_metric"}, {"source": "Representation Autoencoders (RAEs)", "target": "VAE", "value": "baseline_model"}, {"source": "Representation Autoencoders (RAEs)", "target": "DINO", "value": "proposed_model"}, {"source": "Representation Autoencoders (RAEs)", "target": "SigLIP", "value": "proposed_model"}, {"source": "Representation Autoencoders (RAEs)", "target": "MAE", "value": "proposed_model"}, {"source": "Diffusion Transformers (DiT)", "target": "DDT head", "value": "proposed_model"}, {"source": "Scaling Large Language Models for Next-Generation Single-Cell Analysis", "target": "C2S-Scale", "value": "proposed_model"}, {"source": "C2S-Scale", "target": "Cell2Sentence (C2S) framework", "value": "baseline_model"}, {"source": "C2S-Scale", "target": "single-cell foundation models (scFMs)", "value": "baseline_model"}, {"source": "C2S-Scale", "target": "Large Language Models (LLMs)", "value": "baseline_model"}, {"source": "C2S-Scale", "target": "corpus comprising over one billion tokens of transcriptomic data, biological text, and metadata", "value": "evaluated_on"}, {"source": "C2S-Scale", "target": "human cell models", "value": "evaluated_on"}, {"source": "C2S-Scale", "target": "predictive and generative capabilities", "value": "uses_metric"}, {"source": "C2S-Scale", "target": "performance in perturbation response prediction, natural language interpretation, and complex biological reasoning", "value": "uses_metric"}, {"source": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights", "target": "DI", "value": "proposed_model"}, {"source": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights", "target": "DiffPure", "value": "baseline_model"}, {"source": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights", "target": "Google Cloud Vision", "value": "evaluated_on"}, {"source": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights", "target": "Lp constraint", "value": "uses_metric"}, {"source": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights", "target": "imperceptibility metrics", "value": "uses_metric"}, {"source": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights", "target": "finer-grained measures", "value": "uses_metric"}, {"source": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights", "target": "user study", "value": "uses_metric"}, {"source": "HybridVisionNet: An advanced hybrid deep learning framework for automated multi-class ocular disease diagnosis using fundus imaging", "target": "HybridVisionNet", "value": "proposed_model"}, {"source": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot", "target": "YOLO-OG", "value": "proposed_model"}, {"source": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot", "target": "OGNet", "value": "proposed_model"}, {"source": "YOLO-OG", "target": "Dish-10", "value": "evaluated_on"}, {"source": "YOLO-OG", "target": "Dish-20", "value": "evaluated_on"}, {"source": "YOLO-OG", "target": "mean Average Precision (mAP)", "value": "uses_metric"}, {"source": "OGNet", "target": "mean Average Precision (mAP)", "value": "uses_metric"}, {"source": "Attention is All you Need", "target": "Transformer", "value": "proposed_model"}, {"source": "Transformer", "target": "WMT 2014 English-to-German translation task", "value": "evaluated_on"}, {"source": "Transformer", "target": "WMT 2014 English-to-French translation task", "value": "evaluated_on"}, {"source": "Transformer", "target": "English constituency parsing", "value": "evaluated_on"}, {"source": "Transformer", "target": "BLEU", "value": "uses_metric"}, {"source": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "target": "Unified Text-to-Text Transformer", "value": "proposed_model"}, {"source": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "target": "dozens of language understanding tasks", "value": "evaluated_on"}, {"source": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "target": "Colossal Clean Crawled Corpus", "value": "evaluated_on"}, {"source": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "target": "summarization", "value": "uses_metric"}, {"source": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "target": "question answering", "value": "uses_metric"}, {"source": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "target": "text classification", "value": "uses_metric"}, {"source": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium", "target": "Two Time-Scale Update Rule", "value": "proposed_model"}, {"source": "Two Time-Scale Update Rule", "target": "GANs", "value": "baseline_model"}, {"source": "nuScenes: A Multimodal Dataset for Autonomous Driving", "target": "nuScenes", "value": "proposed_model"}, {"source": "nuScenes: A Multimodal Dataset for Autonomous Driving", "target": "lidar based detection and tracking", "value": "baseline_model"}, {"source": "nuScenes: A Multimodal Dataset for Autonomous Driving", "target": "image based detection and tracking", "value": "baseline_model"}, {"source": "nuScenes: A Multimodal Dataset for Autonomous Driving", "target": "nuScenes", "value": "evaluated_on"}, {"source": "nuScenes: A Multimodal Dataset for Autonomous Driving", "target": "KITTI", "value": "evaluated_on"}, {"source": "nuScenes: A Multimodal Dataset for Autonomous Driving", "target": "novel 3D detection and tracking metrics", "value": "uses_metric"}, {"source": "CARLA: An Open Urban Driving Simulator", "target": "CARLA", "value": "proposed_model"}, {"source": "CARLA: An Open Urban Driving Simulator", "target": "classic modular pipeline", "value": "baseline_model"}, {"source": "CARLA: An Open Urban Driving Simulator", "target": "end-to-end model trained via imitation learning", "value": "baseline_model"}, {"source": "CARLA: An Open Urban Driving Simulator", "target": "end-to-end model trained via reinforcement learning", "value": "baseline_model"}, {"source": "CARLA: An Open Urban Driving Simulator", "target": "controlled scenarios of increasing difficulty", "value": "evaluated_on"}, {"source": "CARLA: An Open Urban Driving Simulator", "target": "metrics provided by CARLA", "value": "uses_metric"}, {"source": "Sora as a World Model? A Complete Survey on Text-to-Video Generation", "target": "text-to-video generation", "value": "proposed_model"}, {"source": "Sora as a World Model? A Complete Survey on Text-to-Video Generation", "target": "Sora", "value": "baseline_model"}, {"source": "Sora as a World Model? A Complete Survey on Text-to-Video Generation", "target": "MNIST", "value": "evaluated_on"}, {"source": "Sora as a World Model? A Complete Survey on Text-to-Video Generation", "target": "world modeling", "value": "uses_metric"}, {"source": "OmniNWM: Omniscient Driving Navigation World Models", "target": "OmniNWM", "value": "proposed_model"}, {"source": "OmniNWM: Omniscient Driving Navigation World Models", "target": "existing models", "value": "baseline_model"}, {"source": "OmniNWM", "target": "video generation", "value": "evaluated_on"}, {"source": "OmniNWM", "target": "control accuracy", "value": "evaluated_on"}, {"source": "OmniNWM", "target": "long-horizon stability", "value": "evaluated_on"}, {"source": "OmniNWM", "target": "video generation", "value": "uses_metric"}, {"source": "OmniNWM", "target": "control accuracy", "value": "uses_metric"}, {"source": "OmniNWM", "target": "long-horizon stability", "value": "uses_metric"}, {"source": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask", "target": "ConsisDrive", "value": "proposed_model"}, {"source": "ConsisDrive", "target": "nuScenes", "value": "evaluated_on"}, {"source": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving", "target": "UniDriveDreamer", "value": "proposed_model"}, {"source": "UniDriveDreamer", "target": "multi-camera video", "value": "evaluated_on"}, {"source": "UniDriveDreamer", "target": "LiDAR sequence", "value": "evaluated_on"}, {"source": "UniDriveDreamer", "target": "video generation", "value": "uses_metric"}, {"source": "UniDriveDreamer", "target": "LiDAR generation", "value": "uses_metric"}, {"source": "UniDriveDreamer", "target": "LiDAR-specific variational autoencoder (VAE)", "value": "proposed_model"}, {"source": "UniDriveDreamer", "target": "video VAE", "value": "proposed_model"}, {"source": "UniDriveDreamer", "target": "Unified Latent Anchoring (ULA)", "value": "proposed_model"}, {"source": "UniDriveDreamer", "target": "diffusion transformer", "value": "proposed_model"}, {"source": "MAD: Motion Appearance Decoupling for efficient Driving World Models", "target": "MAD-LTX", "value": "proposed_model"}, {"source": "MAD: Motion Appearance Decoupling for efficient Driving World Models", "target": "SVD", "value": "baseline_model"}, {"source": "MAD: Motion Appearance Decoupling for efficient Driving World Models", "target": "LTX", "value": "baseline_model"}, {"source": "MAD-LTX", "target": "autonomous driving", "value": "evaluated_on"}, {"source": "MAD-LTX", "target": "driving domains", "value": "evaluated_on"}, {"source": "MAD-LTX", "target": "structured motion", "value": "uses_metric"}, {"source": "MAD-LTX", "target": "physically consistent interactions", "value": "uses_metric"}, {"source": "MAD-LTX", "target": "photorealistic, temporally coherent videos", "value": "uses_metric"}, {"source": "MAD-LTX", "target": "text, ego, and object controls", "value": "uses_metric"}, {"source": "What matters for Representation Alignment: Global Information or Spatial Structure?", "target": "iREPA", "value": "proposed_model"}, {"source": "iREPA", "target": "REPA", "value": "baseline_model"}, {"source": "iREPA", "target": "REPA-E", "value": "baseline_model"}, {"source": "iREPA", "target": "Meanflow", "value": "baseline_model"}, {"source": "iREPA", "target": "JiT", "value": "baseline_model"}, {"source": "What matters for Representation Alignment: Global Information or Spatial Structure?", "target": "ImageNet-1K", "value": "evaluated_on"}, {"source": "What matters for Representation Alignment: Global Information or Spatial Structure?", "target": "ImageNet-1K accuracy", "value": "uses_metric"}, {"source": "SCB-DETR: Multiscale Deformable Transformers for Occlusion-Resilient Student Learning Behavior Detection in Smart Classroom", "target": "SCB-DETR", "value": "proposed_model"}, {"source": "SCB-DETR: Multiscale Deformable Transformers for Occlusion-Resilient Student Learning Behavior Detection in Smart Classroom", "target": "baseline model", "value": "baseline_model"}, {"source": "SCB-DETR", "target": "SCBehavior", "value": "evaluated_on"}, {"source": "SCB-DETR", "target": "mean Average Precision (mAP)", "value": "uses_metric"}, {"source": "SCB-DETR", "target": "AP50", "value": "uses_metric"}, {"source": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination", "target": "ultrasound-cardiac-feature-net (UCF-Net)", "value": "proposed_model"}, {"source": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination", "target": "filtered integral quasi-super-twisting algorithm (FIQSTA)", "value": "proposed_model"}, {"source": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination", "target": "proportional (P) controller", "value": "baseline_model"}, {"source": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination", "target": "sliding mode controller", "value": "baseline_model"}, {"source": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination", "target": "super-twisting algorithm (STA)", "value": "baseline_model"}, {"source": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination", "target": "integral quasi-STA", "value": "baseline_model"}, {"source": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination", "target": "cardiac phantom", "value": "evaluated_on"}, {"source": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination", "target": "parasternal short axis", "value": "evaluated_on"}, {"source": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination", "target": "parasternal long axis", "value": "evaluated_on"}, {"source": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination", "target": "subcostal", "value": "evaluated_on"}, {"source": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination", "target": "apical four chambers views", "value": "evaluated_on"}, {"source": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination", "target": "trajectory passing through the main views", "value": "evaluated_on"}, {"source": "Bio-Inspired Fine-Tuning for Selective Transfer Learning in Image Classification", "target": "BioTune", "value": "proposed_model"}, {"source": "Bio-Inspired Fine-Tuning for Selective Transfer Learning in Image Classification", "target": "AutoRGN", "value": "baseline_model"}, {"source": "Bio-Inspired Fine-Tuning for Selective Transfer Learning in Image Classification", "target": "LoRA", "value": "baseline_model"}, {"source": "BioTune", "target": "nine image classification datasets", "value": "evaluated_on"}, {"source": "BioTune", "target": "medical imaging", "value": "evaluated_on"}, {"source": "BioTune", "target": "accuracy", "value": "uses_metric"}, {"source": "BioTune", "target": "efficiency", "value": "uses_metric"}, {"source": "Hand Sign Language Detection Using Deep Learning", "target": "VGG-16 net", "value": "proposed_model"}, {"source": "Hand Sign Language Detection Using Deep Learning", "target": "NUS dataset", "value": "evaluated_on"}, {"source": "Hand Sign Language Detection Using Deep Learning", "target": "accuracy", "value": "uses_metric"}, {"source": "Microsoft COCO: Common Objects in Context", "target": "Deformable Parts Model", "value": "baseline_model"}, {"source": "Microsoft COCO: Common Objects in Context", "target": "PASCAL", "value": "evaluated_on"}, {"source": "Microsoft COCO: Common Objects in Context", "target": "ImageNet", "value": "evaluated_on"}, {"source": "Microsoft COCO: Common Objects in Context", "target": "SUN", "value": "evaluated_on"}, {"source": "Deformable Parts Model", "target": "bounding box detection", "value": "uses_metric"}, {"source": "Deformable Parts Model", "target": "segmentation detection", "value": "uses_metric"}, {"source": "FANet: Frequency-Aware Attention-Based Tiny-Object Detection in Remote Sensing Images", "target": "FANet", "value": "proposed_model"}, {"source": "FANet: Frequency-Aware Attention-Based Tiny-Object Detection in Remote Sensing Images", "target": "Multi-Scale Frequency Feature Enhancement Module (MSFFEM)", "value": "proposed_model"}, {"source": "FANet: Frequency-Aware Attention-Based Tiny-Object Detection in Remote Sensing Images", "target": "Channel Attention-based RoI Enhancement Module (CAREM)", "value": "proposed_model"}, {"source": "FANet", "target": "AI-TOD", "value": "evaluated_on"}, {"source": "FANet", "target": "VisDrone2019", "value": "evaluated_on"}, {"source": "FANet", "target": "DOTA-v1.5", "value": "evaluated_on"}, {"source": "FANet", "target": "detection performance", "value": "uses_metric"}, {"source": "Representation Learning: A Review and New Perspectives", "target": "probabilistic models", "value": "proposed_model"}, {"source": "Representation Learning: A Review and New Perspectives", "target": "auto-encoders", "value": "proposed_model"}, {"source": "Representation Learning: A Review and New Perspectives", "target": "manifold learning", "value": "proposed_model"}, {"source": "Representation Learning: A Review and New Perspectives", "target": "deep networks", "value": "proposed_model"}, {"source": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion", "target": "Stacked Denoising Autoencoders", "value": "proposed_model"}, {"source": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation", "target": "SDXL-Lightning", "value": "proposed_model"}, {"source": "SDXL-Lightning", "target": "SDXL", "value": "baseline_model"}, {"source": "SDXL-Lightning", "target": "quality", "value": "uses_metric"}, {"source": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions", "target": "EchoMimic", "value": "proposed_model"}, {"source": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions", "target": "various public datasets", "value": "evaluated_on"}, {"source": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions", "target": "our collected dataset", "value": "evaluated_on"}, {"source": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions", "target": "quantitative evaluations", "value": "uses_metric"}, {"source": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions", "target": "qualitative evaluations", "value": "uses_metric"}, {"source": "GenAD: Generative End-to-End Autonomous Driving", "target": "GenAD", "value": "proposed_model"}, {"source": "GenAD", "target": "nuScenes", "value": "evaluated_on"}, {"source": "GenAD", "target": "state-of-the-art performance", "value": "uses_metric"}, {"source": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model", "target": "Ovis", "value": "proposed_model"}, {"source": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model", "target": "Multimodal Large Language Models (MLLMs)", "value": "baseline_model"}, {"source": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model", "target": "Qwen-VL-Plus", "value": "baseline_model"}, {"source": "Ovis", "target": "various multimodal benchmarks", "value": "evaluated_on"}, {"source": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model", "target": "empirical evaluations", "value": "uses_metric"}, {"source": "Improving Video Generation with Human Feedback", "target": "VideoReward", "value": "proposed_model"}, {"source": "Improving Video Generation with Human Feedback", "target": "Flow-DPO", "value": "proposed_model"}, {"source": "Improving Video Generation with Human Feedback", "target": "Flow-RWR", "value": "proposed_model"}, {"source": "Improving Video Generation with Human Feedback", "target": "Flow-NRG", "value": "proposed_model"}, {"source": "VideoReward", "target": "large-scale human preference dataset", "value": "evaluated_on"}, {"source": "Flow-DPO", "target": "Flow-RWR", "value": "baseline_model"}, {"source": "Flow-DPO", "target": "supervised fine-tuning methods", "value": "baseline_model"}, {"source": "Speech Recognition with Deep Recurrent Neural Networks", "target": "deep recurrent neural networks", "value": "proposed_model"}, {"source": "Speech Recognition with Deep Recurrent Neural Networks", "target": "deep Long Short-term Memory RNNs", "value": "proposed_model"}, {"source": "Speech Recognition with Deep Recurrent Neural Networks", "target": "deep feedforward networks", "value": "baseline_model"}, {"source": "deep Long Short-term Memory RNNs", "target": "TIMIT phoneme recognition benchmark", "value": "evaluated_on"}, {"source": "deep Long Short-term Memory RNNs", "target": "test set error", "value": "uses_metric"}, {"source": "Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks", "target": "Connectionist Temporal Classification", "value": "proposed_model"}, {"source": "Connectionist Temporal Classification", "target": "Recurrent Neural Networks", "value": "baseline_model"}, {"source": "2005 Special Issue: Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "target": "bidirectional LSTM", "value": "proposed_model"}, {"source": "2005 Special Issue: Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "target": "other neural network architectures", "value": "proposed_model"}, {"source": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses", "target": "machine learning", "value": "proposed_model"}, {"source": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses", "target": "artificial synapses", "value": "proposed_model"}, {"source": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses", "target": "flexible sensors", "value": "evaluated_on"}, {"source": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses", "target": "human activities", "value": "evaluated_on"}, {"source": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses", "target": "artificial sensory organs", "value": "evaluated_on"}, {"source": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses", "target": "soft robotics", "value": "evaluated_on"}, {"source": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses", "target": "data analysis", "value": "uses_metric"}, {"source": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses", "target": "intelligent decision-making", "value": "uses_metric"}, {"source": "A high-performance speech neuroprosthesis", "target": "speech-to-text BCI", "value": "proposed_model"}, {"source": "speech-to-text BCI", "target": "50-word vocabulary", "value": "evaluated_on"}, {"source": "speech-to-text BCI", "target": "125,000-word vocabulary", "value": "evaluated_on"}, {"source": "speech-to-text BCI", "target": "word error rate", "value": "uses_metric"}, {"source": "speech-to-text BCI", "target": "words per minute", "value": "uses_metric"}, {"source": "Loss of plasticity in deep continual learning", "target": "continual backpropagation algorithm", "value": "proposed_model"}, {"source": "Loss of plasticity in deep continual learning", "target": "deep-learning methods", "value": "baseline_model"}, {"source": "Loss of plasticity in deep continual learning", "target": "backpropagation algorithm", "value": "baseline_model"}, {"source": "Loss of plasticity in deep continual learning", "target": "ImageNet", "value": "evaluated_on"}, {"source": "Loss of plasticity in deep continual learning", "target": "plasticity", "value": "uses_metric"}, {"source": "An analog-AI chip for energy-efficient speech recognition and transcription", "target": "keyword-spotting network", "value": "proposed_model"}, {"source": "An analog-AI chip for energy-efficient speech recognition and transcription", "target": "MLPerf recurrent neural-network transducer (RNNT)", "value": "proposed_model"}, {"source": "keyword-spotting network", "target": "speech-recognition tasks", "value": "evaluated_on"}, {"source": "MLPerf recurrent neural-network transducer (RNNT)", "target": "speech-recognition tasks", "value": "evaluated_on"}, {"source": "keyword-spotting network", "target": "accuracy", "value": "uses_metric"}, {"source": "MLPerf recurrent neural-network transducer (RNNT)", "target": "accuracy", "value": "uses_metric"}, {"source": "An analog-AI chip for energy-efficient speech recognition and transcription", "target": "energy efficiency", "value": "uses_metric"}, {"source": "An analog-AI chip for energy-efficient speech recognition and transcription", "target": "TOPS/W chip-sustained performance", "value": "uses_metric"}, {"source": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers", "target": "LiteToken", "value": "proposed_model"}, {"source": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers", "target": "BPE tokenizers", "value": "baseline_model"}, {"source": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers", "target": "commonly used tokenizers", "value": "evaluated_on"}, {"source": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers", "target": "token fragmentation", "value": "uses_metric"}, {"source": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers", "target": "parameters", "value": "uses_metric"}, {"source": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers", "target": "robustness to noisy or misspelled inputs", "value": "uses_metric"}, {"source": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers", "target": "overall performance", "value": "uses_metric"}, {"source": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling", "target": "MeKi", "value": "proposed_model"}, {"source": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling", "target": "dense LLM baselines", "value": "baseline_model"}, {"source": "MeKi", "target": "edge devices", "value": "evaluated_on"}, {"source": "MeKi", "target": "inference speed", "value": "uses_metric"}, {"source": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram", "target": "Gengram", "value": "proposed_model"}, {"source": "Gengram", "target": "genomic foundation models (GFMs)", "value": "baseline_model"}, {"source": "Gengram", "target": "functional genomics tasks", "value": "evaluated_on"}, {"source": "L$^3$: Large Lookup Layers", "target": "L$^3$", "value": "proposed_model"}, {"source": "L$^3$: Large Lookup Layers", "target": "Mixture-of-Experts (MoE)", "value": "baseline_model"}, {"source": "L$^3$: Large Lookup Layers", "target": "dense models", "value": "baseline_model"}, {"source": "L$^3$: Large Lookup Layers", "target": "iso-sparse MoEs", "value": "baseline_model"}, {"source": "L$^3$", "target": "language modeling", "value": "evaluated_on"}, {"source": "L$^3$", "target": "downstream tasks", "value": "evaluated_on"}, {"source": "L$^3$", "target": "speed", "value": "uses_metric"}, {"source": "L$^3$", "target": "quality", "value": "uses_metric"}, {"source": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "target": "LongCat-Flash-Lite", "value": "proposed_model"}, {"source": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "target": "Mixture-of-Experts (MoE)", "value": "baseline_model"}, {"source": "LongCat-Flash-Lite", "target": "agentic and coding domains", "value": "evaluated_on"}, {"source": "Going Deeper with Convolutions", "target": "Inception", "value": "proposed_model"}, {"source": "Going Deeper with Convolutions", "target": "GoogLeNet", "value": "proposed_model"}, {"source": "Inception", "target": "ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014)", "value": "evaluated_on"}, {"source": "GoogLeNet", "target": "ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014)", "value": "evaluated_on"}, {"source": "Gradient-based learning applied to document recognition", "target": "Convolutional Neural Network (CNN)", "value": "proposed_model"}, {"source": "Gradient-based learning applied to document recognition", "target": "MNIST", "value": "evaluated_on"}, {"source": "Regression Shrinkage and Selection via the Lasso", "target": "Lasso", "value": "proposed_model"}, {"source": "LifeCLEF Plant Identification Task 2015", "target": "LifeCLEF plant identification challenge", "value": "evaluated_on"}, {"source": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics", "target": "FedMicro-IDA", "value": "proposed_model"}, {"source": "FedMicro-IDA", "target": "MaleVis", "value": "evaluated_on"}, {"source": "FedMicro-IDA", "target": "detection and classification performance", "value": "uses_metric"}, {"source": "Enhancing pine wilt disease detection with synthetic data and external attention-based transformers", "target": "external attention-based transformers", "value": "proposed_model"}, {"source": "Harnessing large vision and language models in agriculture: a review", "target": "tailored models for agricultural question-answering", "value": "proposed_model"}, {"source": "Harnessing large vision and language models in agriculture: a review", "target": "robotic automation", "value": "proposed_model"}, {"source": "Harnessing large vision and language models in agriculture: a review", "target": "advanced image analysis from remote sensing and spectral data", "value": "proposed_model"}, {"source": "Harnessing large vision and language models in agriculture: a review", "target": "traditional models", "value": "baseline_model"}, {"source": "Harnessing large vision and language models in agriculture: a review", "target": "Web of Science", "value": "evaluated_on"}, {"source": "Harnessing large vision and language models in agriculture: a review", "target": "arXiv", "value": "evaluated_on"}, {"source": "Harnessing large vision and language models in agriculture: a review", "target": "bibliometric analysis", "value": "uses_metric"}, {"source": "A systematic comparison of predictive models on the retina", "target": "linear-nonlinear (LN) models", "value": "proposed_model"}, {"source": "A systematic comparison of predictive models on the retina", "target": "convolutional neural networks (CNNs)", "value": "proposed_model"}, {"source": "linear-nonlinear (LN) models", "target": "convolutional neural networks (CNNs)", "value": "baseline_model"}, {"source": "linear-nonlinear (LN) models", "target": "marmoset and salamander retinas datasets", "value": "evaluated_on"}, {"source": "convolutional neural networks (CNNs)", "target": "marmoset and salamander retinas datasets", "value": "evaluated_on"}, {"source": "linear-nonlinear (LN) models", "target": "predictive performance", "value": "uses_metric"}, {"source": "convolutional neural networks (CNNs)", "target": "predictive performance", "value": "uses_metric"}, {"source": "linear-nonlinear (LN) models", "target": "cross-stimulus generalization", "value": "uses_metric"}, {"source": "convolutional neural networks (CNNs)", "target": "cross-stimulus generalization", "value": "uses_metric"}, {"source": "Distinctive Image Features from Scale-Invariant Keypoints", "target": "SIFT", "value": "proposed_model"}, {"source": "SIFT", "target": "Image matching dataset", "value": "evaluated_on"}, {"source": "SIFT", "target": "Accuracy", "value": "uses_metric"}, {"source": "LLaVA-OneVision-1.5", "target": "LLaVA-OneVision-1.5-8B", "value": "proposed_model"}, {"source": "LLaVA-OneVision-1.5", "target": "LLaVA-OneVision-1.5-4B", "value": "proposed_model"}, {"source": "LLaVA-OneVision-1.5", "target": "Qwen2.5-VL-7B", "value": "baseline_model"}, {"source": "LLaVA-OneVision-1.5", "target": "Qwen2.5-VL-3B", "value": "baseline_model"}, {"source": "LLaVA-OneVision-1.5-8B", "target": "27 benchmarks", "value": "evaluated_on"}, {"source": "LLaVA-OneVision-1.5-4B", "target": "27 benchmarks", "value": "evaluated_on"}, {"source": "LLaVA-OneVision-1.5", "target": "27 benchmarks", "value": "evaluated_on"}, {"source": "LLaVA-OneVision-1.5", "target": "27 benchmarks", "value": "uses_metric"}, {"source": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications", "target": "Vision-Language-Action (VLA) models", "value": "proposed_model"}, {"source": "Vision-Language-Action (VLA) models", "target": "large language models (LLMs)", "value": "baseline_model"}, {"source": "Vision-Language-Action (VLA) models", "target": "vision-language models (VLMs)", "value": "baseline_model"}, {"source": "Vision-Language-Action (VLA) models", "target": "publicly available datasets", "value": "evaluated_on"}, {"source": "Vision-Language-Action (VLA) models", "target": "evaluation benchmarks", "value": "uses_metric"}, {"source": "Aligning machine and human visual representations across abstraction levels", "target": "teacher model", "value": "proposed_model"}, {"source": "Aligning machine and human visual representations across abstraction levels", "target": "human-aligned models", "value": "proposed_model"}, {"source": "Aligning machine and human visual representations across abstraction levels", "target": "pretrained state-of-the-art vision foundation models", "value": "baseline_model"}, {"source": "human-aligned models", "target": "dataset of human judgements spanning multiple levels of semantic abstractions", "value": "evaluated_on"}, {"source": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "target": "Vision Transformer (ViT)", "value": "proposed_model"}, {"source": "Vision Transformer (ViT)", "target": "ImageNet", "value": "evaluated_on"}, {"source": "Vision Transformer (ViT)", "target": "CIFAR-100", "value": "evaluated_on"}, {"source": "Vision Transformer (ViT)", "target": "VTAB", "value": "evaluated_on"}, {"source": "Vision Transformer (ViT)", "target": "state-of-the-art convolutional networks", "value": "baseline_model"}, {"source": "Learning Transferable Visual Models From Natural Language Supervision", "target": "CLIP", "value": "proposed_model"}, {"source": "Learning Transferable Visual Models From Natural Language Supervision", "target": "ResNet-50", "value": "baseline_model"}, {"source": "CLIP", "target": "over 30 different existing computer vision datasets", "value": "evaluated_on"}, {"source": "CLIP", "target": "ImageNet", "value": "evaluated_on"}, {"source": "CLIP", "target": "400 million (image, text) pairs", "value": "evaluated_on"}, {"source": "Learning Transferable Visual Models From Natural Language Supervision", "target": "accuracy", "value": "uses_metric"}, {"source": "GENERATIVE ADVERSARIAL NETS", "target": "Generative Adversarial Network (GAN)", "value": "proposed_model"}, {"source": "Improved Mean Flows: On the Challenges of Fastforward Generative Models", "target": "improved MeanFlow (iMF)", "value": "proposed_model"}, {"source": "Improved Mean Flows: On the Challenges of Fastforward Generative Models", "target": "MeanFlow (MF)", "value": "baseline_model"}, {"source": "improved MeanFlow (iMF)", "target": "ImageNet 256\u00d7256", "value": "evaluated_on"}, {"source": "improved MeanFlow (iMF)", "target": "FID", "value": "uses_metric"}, {"source": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training", "target": "pixel-space diffusion and consistency models", "value": "proposed_model"}, {"source": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training", "target": "ImageNet", "value": "evaluated_on"}, {"source": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training", "target": "ImageNet-256", "value": "evaluated_on"}, {"source": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training", "target": "ImageNet-512", "value": "evaluated_on"}, {"source": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training", "target": "FID", "value": "uses_metric"}, {"source": "pixel-space diffusion and consistency models", "target": "DiT", "value": "baseline_model"}, {"source": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder", "target": "SVG-T2I", "value": "proposed_model"}, {"source": "SVG-T2I", "target": "SVG (Self-supervised representations for Visual Generation)", "value": "baseline_model"}, {"source": "SVG-T2I", "target": "GenEval", "value": "evaluated_on"}, {"source": "SVG-T2I", "target": "DPG-Bench", "value": "evaluated_on"}, {"source": "SVG-T2I", "target": "0.75", "value": "uses_metric"}, {"source": "SVG-T2I", "target": "85.78", "value": "uses_metric"}, {"source": "PixelDiT: Pixel Diffusion Transformers for Image Generation", "target": "PixelDiT", "value": "proposed_model"}, {"source": "PixelDiT", "target": "Diffusion Transformers (DiTs)", "value": "baseline_model"}, {"source": "PixelDiT", "target": "ImageNet 256x256", "value": "evaluated_on"}, {"source": "PixelDiT", "target": "GenEval", "value": "evaluated_on"}, {"source": "PixelDiT", "target": "DPG-bench", "value": "evaluated_on"}, {"source": "PixelDiT", "target": "FID", "value": "uses_metric"}, {"source": "PixelDiT", "target": "pixel generative models", "value": "baseline_model"}, {"source": "PixelDiT", "target": "latent diffusion models", "value": "baseline_model"}, {"source": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "target": "TUNA", "value": "proposed_model"}, {"source": "TUNA", "target": "multimodal understanding and generation benchmarks", "value": "evaluated_on"}, {"source": "TUNA", "target": "performance", "value": "uses_metric"}, {"source": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "target": "BERT", "value": "proposed_model"}, {"source": "BERT", "target": "GLUE", "value": "evaluated_on"}, {"source": "BERT", "target": "MultiNLI", "value": "evaluated_on"}, {"source": "BERT", "target": "SQuAD v1.1", "value": "evaluated_on"}, {"source": "BERT", "target": "SQuAD v2.0", "value": "evaluated_on"}, {"source": "BERT", "target": "GLUE score", "value": "uses_metric"}, {"source": "BERT", "target": "MultiNLI accuracy", "value": "uses_metric"}, {"source": "BERT", "target": "SQuAD v1.1 question answering Test F1", "value": "uses_metric"}, {"source": "BERT", "target": "SQuAD v2.0 Test F1", "value": "uses_metric"}, {"source": "Diffusion Language Models are Super Data Learners", "target": "Diffusion language models (DLMs)", "value": "proposed_model"}, {"source": "Diffusion Language Models are Super Data Learners", "target": "autoregressive (AR) models", "value": "baseline_model"}, {"source": "1.7B DLM", "target": "10B unique Python tokens", "value": "evaluated_on"}, {"source": "1B-parameter DLM", "target": "HellaSwag", "value": "evaluated_on"}, {"source": "1B-parameter DLM", "target": "MMLU", "value": "evaluated_on"}, {"source": "1B-parameter DLM", "target": "accuracy", "value": "uses_metric"}, {"source": "Diffusion Language Models are Super Data Learners", "target": "validation cross-entropy", "value": "uses_metric"}, {"source": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas", "target": "DreamOn", "value": "proposed_model"}, {"source": "DreamOn", "target": "Dream-Coder-7B", "value": "baseline_model"}, {"source": "DreamOn", "target": "DiffuCoder-7B", "value": "baseline_model"}, {"source": "DreamOn", "target": "HumanEval-Infilling", "value": "evaluated_on"}, {"source": "DreamOn", "target": "SantaCoder-FIM", "value": "evaluated_on"}, {"source": "FLEX: Continuous Agent Evolution via Forward Learning from Experience", "target": "FLEX", "value": "proposed_model"}, {"source": "FLEX", "target": "AIME25", "value": "evaluated_on"}, {"source": "FLEX", "target": "USPTO50k", "value": "evaluated_on"}, {"source": "FLEX", "target": "ProteinGym", "value": "evaluated_on"}, {"source": "FLEX", "target": "mathematical reasoning", "value": "uses_metric"}, {"source": "FLEX", "target": "chemical retrosynthesis", "value": "uses_metric"}, {"source": "FLEX", "target": "protein fitness prediction", "value": "uses_metric"}, {"source": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning", "target": "Agent-R1", "value": "proposed_model"}, {"source": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning", "target": "Multihop QA benchmark tasks", "value": "evaluated_on"}, {"source": "nuScenes: A multimodal dataset for autonomous driving", "target": "nuScenes", "value": "proposed_model"}, {"source": "nuScenes: A multimodal dataset for autonomous driving", "target": "KITTI dataset", "value": "baseline_model"}, {"source": "nuScenes: A multimodal dataset for autonomous driving", "target": "nuScenes", "value": "evaluated_on"}, {"source": "nuScenes: A multimodal dataset for autonomous driving", "target": "novel 3D detection and tracking metrics", "value": "uses_metric"}, {"source": "Controllable Video Generation: A Survey", "target": "single-condition generation", "value": "proposed_model"}, {"source": "Controllable Video Generation: A Survey", "target": "multi-condition generation", "value": "proposed_model"}, {"source": "Controllable Video Generation: A Survey", "target": "universal controllable generation", "value": "proposed_model"}, {"source": "Controllable Video Generation: A Survey", "target": "video generation foundation models", "value": "baseline_model"}, {"source": "Controllable Video Generation: A Survey", "target": "text-to-video generation", "value": "baseline_model"}, {"source": "Controllable Video Generation: A Survey", "target": "Awesome-Controllable-Video-Generation", "value": "evaluated_on"}, {"source": "Adversarial Attacks on Autonomous Driving Systems in the Physical World: A Survey", "target": "multi-label classification method", "value": "proposed_model"}, {"source": "Adversarial Attacks on Autonomous Driving Systems in the Physical World: A Survey", "target": "Autonomous Driving Systems (ADS)", "value": "baseline_model"}, {"source": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "target": "Alpamayo-R1 (AR1)", "value": "proposed_model"}, {"source": "Alpamayo-R1 (AR1)", "target": "trajectory-only baseline", "value": "baseline_model"}, {"source": "Alpamayo-R1 (AR1)", "target": "Chain of Causation (CoC) dataset", "value": "evaluated_on"}, {"source": "Alpamayo-R1 (AR1)", "target": "planning accuracy", "value": "uses_metric"}, {"source": "Alpamayo-R1 (AR1)", "target": "close encounter rate", "value": "uses_metric"}, {"source": "Alpamayo-R1 (AR1)", "target": "reasoning quality", "value": "uses_metric"}, {"source": "Alpamayo-R1 (AR1)", "target": "reasoning-action consistency", "value": "uses_metric"}, {"source": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes", "target": "Ego3D-VLM", "value": "proposed_model"}, {"source": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes", "target": "GPT-4o", "value": "baseline_model"}, {"source": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes", "target": "Gemini1.5-Pro", "value": "baseline_model"}, {"source": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes", "target": "InternVL3", "value": "baseline_model"}, {"source": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes", "target": "Qwen2.5-VL", "value": "baseline_model"}, {"source": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes", "target": "Ego3D-Bench", "value": "evaluated_on"}, {"source": "Ego3D-VLM", "target": "Ego3D-Bench", "value": "evaluated_on"}, {"source": "Ego3D-VLM", "target": "multi-choice QA", "value": "uses_metric"}, {"source": "Ego3D-VLM", "target": "absolute distance estimation", "value": "uses_metric"}, {"source": "Detect Anything via Next Point Prediction", "target": "Rex-Omni", "value": "proposed_model"}, {"source": "Detect Anything via Next Point Prediction", "target": "YOLO", "value": "baseline_model"}, {"source": "Detect Anything via Next Point Prediction", "target": "DETR", "value": "baseline_model"}, {"source": "Detect Anything via Next Point Prediction", "target": "Grounding DINO", "value": "baseline_model"}, {"source": "Rex-Omni", "target": "COCO", "value": "evaluated_on"}, {"source": "Rex-Omni", "target": "LVIS", "value": "evaluated_on"}, {"source": "Human-level control through deep reinforcement learning", "target": "Deep Q-Network (DQN)", "value": "proposed_model"}, {"source": "Human-level control through deep reinforcement learning", "target": "Atari 2600 games", "value": "evaluated_on"}, {"source": "Human-level control through deep reinforcement learning", "target": "human-level performance", "value": "uses_metric"}, {"source": "Asynchronous Methods for Deep Reinforcement Learning", "target": "asynchronous gradient descent", "value": "proposed_model"}, {"source": "Asynchronous Methods for Deep Reinforcement Learning", "target": "asynchronous actor-critic", "value": "proposed_model"}, {"source": "asynchronous actor-critic", "target": "Atari domain", "value": "evaluated_on"}, {"source": "asynchronous actor-critic", "target": "continuous motor control problems", "value": "evaluated_on"}, {"source": "asynchronous actor-critic", "target": "random 3D mazes", "value": "evaluated_on"}, {"source": "asynchronous actor-critic", "target": "training time", "value": "uses_metric"}, {"source": "RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation", "target": "RefineNet", "value": "proposed_model"}, {"source": "RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation", "target": "PASCAL VOC 2012", "value": "evaluated_on"}, {"source": "RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation", "target": "intersection-over-union", "value": "uses_metric"}, {"source": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving", "target": "LightEMMA", "value": "proposed_model"}, {"source": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving", "target": "Vision-Language Models (VLMs)", "value": "baseline_model"}, {"source": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving", "target": "nuScenes", "value": "evaluated_on"}, {"source": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving", "target": "computational metrics", "value": "uses_metric"}, {"source": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "target": "ReCogDrive", "value": "proposed_model"}, {"source": "ReCogDrive", "target": "Vision-Language Models (VLMs)", "value": "baseline_model"}, {"source": "ReCogDrive", "target": "NAVSIM", "value": "evaluated_on"}, {"source": "ReCogDrive", "target": "Bench2Drive", "value": "evaluated_on"}, {"source": "ReCogDrive", "target": "DriveBench", "value": "uses_metric"}, {"source": "Pseudo-Simulation for Autonomous Driving", "target": "pseudo-simulation", "value": "proposed_model"}, {"source": "pseudo-simulation", "target": "open-loop evaluation", "value": "baseline_model"}, {"source": "pseudo-simulation", "target": "real datasets", "value": "evaluated_on"}, {"source": "pseudo-simulation", "target": "closed-loop simulation", "value": "uses_metric"}, {"source": "pseudo-simulation", "target": "R^2", "value": "uses_metric"}, {"source": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving", "target": "DriveMoE", "value": "proposed_model"}, {"source": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving", "target": "Drive-\u03c0\u2080", "value": "baseline_model"}, {"source": "DriveMoE", "target": "Bench2Drive", "value": "evaluated_on"}, {"source": "DriveMoE", "target": "state-of-the-art (SOTA) performance", "value": "uses_metric"}, {"source": "A Survey on Vision-Language-Action Models for Autonomous Driving", "target": "VLA for Autonomous Driving (VLA4AD)", "value": "proposed_model"}, {"source": "Vision-Language-Action (VLA) paradigms", "target": "multimodal large language models (MLLM)", "value": "baseline_model"}, {"source": "A Survey on Vision-Language-Action Models for Autonomous Driving", "target": "existing datasets and benchmarks", "value": "evaluated_on"}, {"source": "existing datasets and benchmarks", "target": "driving safety", "value": "uses_metric"}, {"source": "existing datasets and benchmarks", "target": "accuracy", "value": "uses_metric"}, {"source": "existing datasets and benchmarks", "target": "explanation quality", "value": "uses_metric"}, {"source": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond", "target": "General world models", "value": "proposed_model"}, {"source": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond", "target": "Sora model", "value": "baseline_model"}, {"source": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond", "target": "autonomous-driving world models", "value": "baseline_model"}, {"source": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond", "target": "world models deployed within autonomous agents", "value": "baseline_model"}, {"source": "Understanding World or Predicting Future? A Comprehensive Survey of World Models", "target": "GPT-4", "value": "proposed_model"}, {"source": "Understanding World or Predicting Future? A Comprehensive Survey of World Models", "target": "Sora", "value": "proposed_model"}, {"source": "A Survey of Multimodal Learning: Methods, Applications, and Future", "target": "multimodal machine learning", "value": "proposed_model"}, {"source": "A Survey of Multimodal Learning: Methods, Applications, and Future", "target": "state-of-the-art methods", "value": "baseline_model"}, {"source": "A Survey of Multimodal Learning: Methods, Applications, and Future", "target": "datasets covered in multimodal learning research", "value": "evaluated_on"}, {"source": "multimodal machine learning", "target": "multimodal learning", "value": "uses_metric"}, {"source": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey", "target": "multimodal generative models", "value": "proposed_model"}, {"source": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey", "target": "multimodal foundation models", "value": "baseline_model"}, {"source": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey", "target": "Any-to-Text", "value": "evaluated_on"}, {"source": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey", "target": "Any-to-Vision", "value": "evaluated_on"}, {"source": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey", "target": "Any-to-Any", "value": "evaluated_on"}, {"source": "multimodal generative models", "target": "Any-to-Text", "value": "uses_metric"}, {"source": "multimodal generative models", "target": "Any-to-Vision", "value": "uses_metric"}, {"source": "multimodal generative models", "target": "Any-to-Any", "value": "uses_metric"}, {"source": "Effects of Generative AI in Tourism Industry", "target": "new theoretical framework for decision making in the tourism industry", "value": "proposed_model"}, {"source": "Effects of Generative AI in Tourism Industry", "target": "existing responsive AI instruments", "value": "baseline_model"}, {"source": "Effects of Generative AI in Tourism Industry", "target": "tourism and hospitality scenarios", "value": "evaluated_on"}, {"source": "Squeeze-and-Excitation Networks", "target": "Squeeze-and-Excitation (SE) block", "value": "proposed_model"}, {"source": "Squeeze-and-Excitation Networks", "target": "SENet", "value": "proposed_model"}, {"source": "Squeeze-and-Excitation Networks", "target": "ILSVRC 2017", "value": "evaluated_on"}, {"source": "Squeeze-and-Excitation Networks", "target": "top-5 error", "value": "uses_metric"}, {"source": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks", "target": "EfficientNets", "value": "proposed_model"}, {"source": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks", "target": "MobileNets", "value": "baseline_model"}, {"source": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks", "target": "ResNet", "value": "baseline_model"}, {"source": "EfficientNets", "target": "ImageNet", "value": "evaluated_on"}, {"source": "EfficientNets", "target": "CIFAR-100", "value": "evaluated_on"}, {"source": "EfficientNets", "target": "Flowers", "value": "evaluated_on"}, {"source": "EfficientNet-B7", "target": "top-1 accuracy", "value": "uses_metric"}, {"source": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control", "target": "Light-X", "value": "proposed_model"}, {"source": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control", "target": "Light-Syn", "value": "proposed_model"}, {"source": "Light-X", "target": "Light-Syn dataset", "value": "evaluated_on"}, {"source": "Light-X", "target": "baseline methods", "value": "baseline_model"}, {"source": "Light-X", "target": "prior video relighting methods", "value": "baseline_model"}, {"source": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World", "target": "DriveLaW", "value": "proposed_model"}, {"source": "DriveLaW", "target": "NAVSIM", "value": "evaluated_on"}, {"source": "DriveLaW", "target": "FID", "value": "uses_metric"}, {"source": "DriveLaW", "target": "FVD", "value": "uses_metric"}, {"source": "DVGT: Driving Visual Geometry Transformer", "target": "DVGT", "value": "proposed_model"}, {"source": "DVGT", "target": "DINO", "value": "baseline_model"}, {"source": "DVGT", "target": "nuScenes", "value": "evaluated_on"}, {"source": "DVGT", "target": "OpenScene", "value": "evaluated_on"}, {"source": "DVGT", "target": "Waymo", "value": "evaluated_on"}, {"source": "DVGT", "target": "KITTI", "value": "evaluated_on"}, {"source": "DVGT", "target": "DDAD", "value": "evaluated_on"}, {"source": "Adding Conditional Control to Text-to-Image Diffusion Models", "target": "ControlNet", "value": "proposed_model"}, {"source": "ControlNet", "target": "Stable Diffusion", "value": "baseline_model"}, {"source": "ControlNet", "target": "small (<50k) and large (>1m) datasets", "value": "evaluated_on"}, {"source": "Scaling Instruction-Finetuned Language Models", "target": "Flan-PaLM 540B", "value": "proposed_model"}, {"source": "Scaling Instruction-Finetuned Language Models", "target": "Flan-T5", "value": "proposed_model"}, {"source": "Scaling Instruction-Finetuned Language Models", "target": "PaLM 540B", "value": "baseline_model"}, {"source": "Scaling Instruction-Finetuned Language Models", "target": "PaLM 62B", "value": "baseline_model"}, {"source": "Flan-PaLM 540B", "target": "MMLU", "value": "evaluated_on"}, {"source": "Flan-PaLM 540B", "target": "BBH", "value": "evaluated_on"}, {"source": "Flan-PaLM 540B", "target": "TyDiQA", "value": "evaluated_on"}, {"source": "Flan-PaLM 540B", "target": "MGSM", "value": "evaluated_on"}, {"source": "Flan-PaLM 540B", "target": "five-shot MMLU", "value": "uses_metric"}, {"source": "High-Resolution Image Synthesis with Latent Diffusion Models", "target": "latent diffusion models (LDMs)", "value": "proposed_model"}, {"source": "latent diffusion models (LDMs)", "target": "diffusion models (DMs)", "value": "baseline_model"}, {"source": "latent diffusion models (LDMs)", "target": "image inpainting", "value": "evaluated_on"}, {"source": "latent diffusion models (LDMs)", "target": "unconditional image generation", "value": "evaluated_on"}, {"source": "latent diffusion models (LDMs)", "target": "semantic scene synthesis", "value": "evaluated_on"}, {"source": "latent diffusion models (LDMs)", "target": "super-resolution", "value": "evaluated_on"}, {"source": "latent diffusion models (LDMs)", "target": "visual fidelity", "value": "uses_metric"}, {"source": "latent diffusion models (LDMs)", "target": "computational requirements", "value": "uses_metric"}, {"source": "AUTO-ENCODING VARIATIONAL BAYES", "target": "Variational Autoencoder (VAE)", "value": "proposed_model"}, {"source": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric", "target": "VGG network", "value": "proposed_model"}, {"source": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric", "target": "PSNR", "value": "baseline_model"}, {"source": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric", "target": "SSIM", "value": "baseline_model"}, {"source": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric", "target": "new dataset of human perceptual similarity judgments", "value": "evaluated_on"}, {"source": "VGG network", "target": "ImageNet", "value": "evaluated_on"}, {"source": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric", "target": "PSNR", "value": "uses_metric"}, {"source": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric", "target": "SSIM", "value": "uses_metric"}, {"source": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "target": "Waymo Open Dataset", "value": "proposed_model"}, {"source": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "target": "Waymo Open Dataset", "value": "evaluated_on"}, {"source": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "target": "diversity metric", "value": "uses_metric"}, {"source": "Generative Adversarial Networks", "target": "Generative Adversarial Networks", "value": "proposed_model"}, {"source": "Generative Adversarial Networks", "target": "Data augmentation", "value": "evaluated_on"}, {"source": "Generative Adversarial Networks", "target": "face images generation", "value": "evaluated_on"}, {"source": "Generalization of Diffusion Models Arises with a Balanced Representation Space", "target": "representation-based method for detecting memorization", "value": "proposed_model"}, {"source": "Generalization of Diffusion Models Arises with a Balanced Representation Space", "target": "training-free editing technique", "value": "proposed_model"}, {"source": "Generalization of Diffusion Models Arises with a Balanced Representation Space", "target": "unconditional and text-to-image diffusion models", "value": "evaluated_on"}, {"source": "two-layer ReLU denoising autoencoder (DAE)", "target": "unconditional and text-to-image diffusion models", "value": "baseline_model"}, {"source": "Stable Velocity: A Variance Perspective on Flow Matching", "target": "Stable Velocity", "value": "proposed_model"}, {"source": "Stable Velocity: A Variance Perspective on Flow Matching", "target": "Stable Velocity Matching (StableVM)", "value": "proposed_model"}, {"source": "Stable Velocity: A Variance Perspective on Flow Matching", "target": "Variance-Aware Representation Alignment (VA-REPA)", "value": "proposed_model"}, {"source": "Stable Velocity: A Variance Perspective on Flow Matching", "target": "Stable Velocity Sampling (StableVS)", "value": "proposed_model"}, {"source": "Stable Velocity: A Variance Perspective on Flow Matching", "target": "SD3.5", "value": "baseline_model"}, {"source": "Stable Velocity: A Variance Perspective on Flow Matching", "target": "Flux", "value": "baseline_model"}, {"source": "Stable Velocity: A Variance Perspective on Flow Matching", "target": "Qwen-Image", "value": "baseline_model"}, {"source": "Stable Velocity: A Variance Perspective on Flow Matching", "target": "Wan2.2", "value": "baseline_model"}, {"source": "Stable Velocity: A Variance Perspective on Flow Matching", "target": "ImageNet 256\u00d7256", "value": "evaluated_on"}, {"source": "Stable Velocity: A Variance Perspective on Flow Matching", "target": "training efficiency", "value": "uses_metric"}, {"source": "Stable Velocity: A Variance Perspective on Flow Matching", "target": "sample quality", "value": "uses_metric"}, {"source": "Laminating Representation Autoencoders for Efficient Diffusion", "target": "FlatDINO", "value": "proposed_model"}, {"source": "Laminating Representation Autoencoders for Efficient Diffusion", "target": "DINOv2", "value": "baseline_model"}, {"source": "FlatDINO", "target": "ImageNet 256x256", "value": "evaluated_on"}, {"source": "DiT-XL", "target": "ImageNet 256x256", "value": "evaluated_on"}, {"source": "Laminating Representation Autoencoders for Efficient Diffusion", "target": "gFID", "value": "uses_metric"}, {"source": "Adaptive 1D Video Diffusion Autoencoder", "target": "One-Dimensional Diffusion Video Autoencoder (One-DVA)", "value": "proposed_model"}, {"source": "Adaptive 1D Video Diffusion Autoencoder", "target": "3D-CNN VAEs", "value": "baseline_model"}, {"source": "Test-Time Conditioning with Representation-Aligned Visual Features", "target": "REPA-G", "value": "proposed_model"}, {"source": "REPA-G", "target": "ImageNet", "value": "evaluated_on"}, {"source": "REPA-G", "target": "COCO", "value": "evaluated_on"}, {"source": "Bio-inspired fine-tuning for selective transfer learning in image classification", "target": "BioTune", "value": "proposed_model"}, {"source": "Bio-inspired fine-tuning for selective transfer learning in image classification", "target": "AutoRGN", "value": "baseline_model"}, {"source": "Bio-inspired fine-tuning for selective transfer learning in image classification", "target": "LoRA", "value": "baseline_model"}, {"source": "BioTune", "target": "nine image classification datasets", "value": "evaluated_on"}, {"source": "BioTune", "target": "medical imaging", "value": "evaluated_on"}, {"source": "BioTune", "target": "accuracy", "value": "uses_metric"}, {"source": "BioTune", "target": "efficiency", "value": "uses_metric"}, {"source": "Densely Connected Convolutional Networks", "target": "Dense Convolutional Network (DenseNet)", "value": "proposed_model"}, {"source": "Dense Convolutional Network (DenseNet)", "target": "CIFAR-10", "value": "evaluated_on"}, {"source": "Dense Convolutional Network (DenseNet)", "target": "CIFAR-100", "value": "evaluated_on"}, {"source": "Dense Convolutional Network (DenseNet)", "target": "SVHN", "value": "evaluated_on"}, {"source": "Dense Convolutional Network (DenseNet)", "target": "ImageNet", "value": "evaluated_on"}, {"source": "Dense Convolutional Network (DenseNet)", "target": "accuracy", "value": "uses_metric"}, {"source": "Dense Convolutional Network (DenseNet)", "target": "computation", "value": "uses_metric"}, {"source": "An Advanced Convolutional Neural Network Architecture Utilizing Transfer Learning for Melanoma Detection", "target": "Convolutional Neural Network", "value": "proposed_model"}, {"source": "An Advanced Convolutional Neural Network Architecture Utilizing Transfer Learning for Melanoma Detection", "target": "Transfer Learning", "value": "proposed_model"}, {"source": "Adaptive Transfer Learning for Surgical Tool Presence Detection in Laparoscopic Videos Through Gradual Freezing Fine\u2010Tuning", "target": "staged adaptive fine-tuning approach", "value": "proposed_model"}, {"source": "staged adaptive fine-tuning approach", "target": "ResNet-50", "value": "baseline_model"}, {"source": "staged adaptive fine-tuning approach", "target": "DenseNet-121", "value": "baseline_model"}, {"source": "Adaptive Transfer Learning for Surgical Tool Presence Detection in Laparoscopic Videos Through Gradual Freezing Fine\u2010Tuning", "target": "Cholec80", "value": "evaluated_on"}, {"source": "Adaptive Transfer Learning for Surgical Tool Presence Detection in Laparoscopic Videos Through Gradual Freezing Fine\u2010Tuning", "target": "CATARACTS", "value": "evaluated_on"}, {"source": "Adaptive Transfer Learning for Surgical Tool Presence Detection in Laparoscopic Videos Through Gradual Freezing Fine\u2010Tuning", "target": "mean average precision (mAP)", "value": "uses_metric"}, {"source": "VGG Induced Deep Hand Sign Language Detection", "target": "VGG-16 net", "value": "proposed_model"}, {"source": "VGG-16 net", "target": "NUS dataset", "value": "evaluated_on"}, {"source": "VGG-16 net", "target": "accuracy", "value": "uses_metric"}, {"source": "MediaPipe: A Framework for Building Perception Pipelines", "target": "MediaPipe", "value": "proposed_model"}, {"source": "Hand Pose Estimation and Hand Shape Classification Using Multi-layered Randomized Decision Forests", "target": "Multi-layered Randomized Decision Forests", "value": "proposed_model"}, {"source": "A robust static hand gesture recognition system using geometry based normalizations and Krawtchouk moments", "target": "geometry based normalizations", "value": "proposed_model"}, {"source": "A robust static hand gesture recognition system using geometry based normalizations and Krawtchouk moments", "target": "Krawtchouk moments", "value": "proposed_model"}, {"source": "Hand signal classification system for sign language communication in Virtual Reality", "target": "machine learning model", "value": "proposed_model"}, {"source": "machine learning model", "target": "user's hand signals", "value": "evaluated_on"}, {"source": "Real-Time Static Hand Sign Recognition System using MediaPipe and Fully Connected Neural Network", "target": "MediaPipe and a fully connected neural network (FCNN)", "value": "proposed_model"}, {"source": "MediaPipe and a fully connected neural network (FCNN)", "target": "American Sign Language (ASL) dataset", "value": "evaluated_on"}, {"source": "Real-Time Static Hand Sign Recognition System using MediaPipe and Fully Connected Neural Network", "target": "accuracy", "value": "uses_metric"}, {"source": "Real-Time Static Hand Sign Recognition System using MediaPipe and Fully Connected Neural Network", "target": "fast recognition", "value": "uses_metric"}, {"source": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation", "target": "R-CNN", "value": "proposed_model"}, {"source": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation", "target": "OverFeat", "value": "baseline_model"}, {"source": "R-CNN", "target": "PASCAL VOC", "value": "evaluated_on"}, {"source": "R-CNN", "target": "VOC 2012", "value": "evaluated_on"}, {"source": "R-CNN", "target": "ILSVRC2013", "value": "evaluated_on"}, {"source": "R-CNN", "target": "mean average precision (mAP)", "value": "uses_metric"}, {"source": "MuMu-LLaMA: Multi-modal music understanding and generation via large language models", "target": "MuMu-LLaMA", "value": "proposed_model"}, {"source": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy", "target": "SuPLoRA", "value": "proposed_model"}, {"source": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy", "target": "supertype-subtype concept hierarchy", "value": "proposed_model"}, {"source": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy", "target": "group-wise suppression method", "value": "proposed_model"}, {"source": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy", "target": "benchmark", "value": "evaluated_on"}, {"source": "SuPLoRA", "target": "standard diffusion regularization", "value": "uses_metric"}, {"source": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism", "target": "Multi-modal Chain and Global Attention Network (MCGA-Net)", "value": "proposed_model"}, {"source": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism", "target": "DCGAN-based data augmentation strategy", "value": "proposed_model"}, {"source": "Multi-modal Chain and Global Attention Network (MCGA-Net)", "target": "GPR images", "value": "evaluated_on"}, {"source": "Multi-modal Chain and Global Attention Network (MCGA-Net)", "target": "Precision", "value": "uses_metric"}, {"source": "Multi-modal Chain and Global Attention Network (MCGA-Net)", "target": "Recall", "value": "uses_metric"}, {"source": "Multi-modal Chain and Global Attention Network (MCGA-Net)", "target": "mAP@50", "value": "uses_metric"}, {"source": "Multi-modal Chain and Global Attention Network (MCGA-Net)", "target": "MS COCO", "value": "evaluated_on"}, {"source": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking", "target": "Qwen3-VL-Embedding", "value": "proposed_model"}, {"source": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking", "target": "Qwen3-VL-Reranker", "value": "proposed_model"}, {"source": "Qwen3-VL-Embedding", "target": "Qwen3-VL", "value": "baseline_model"}, {"source": "Qwen3-VL-Reranker", "target": "Qwen3-VL", "value": "baseline_model"}, {"source": "Qwen3-VL-Embedding", "target": "MMEB-V2", "value": "evaluated_on"}, {"source": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness", "target": "FPSMark", "value": "proposed_model"}, {"source": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness", "target": "intrinsic signal localization network", "value": "proposed_model"}, {"source": "FPSMark", "target": "partial screen-shooting scenarios", "value": "evaluated_on"}, {"source": "FPSMark", "target": "extraction accuracy", "value": "uses_metric"}, {"source": "existing methods", "target": "FPSMark", "value": "baseline_model"}, {"source": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper", "target": "EM algorithm", "value": "proposed_model"}, {"source": "Visualizing Data using t-SNE", "target": "t-SNE", "value": "proposed_model"}, {"source": "LLM Social Simulations Are a Promising Research Method", "target": "LLM social simulations", "value": "proposed_model"}, {"source": "LLM social simulations", "target": "social science datasets", "value": "evaluated_on"}, {"source": "LLM social simulations", "target": "empirical comparisons", "value": "uses_metric"}, {"source": "Federated Contrastive Learning With Feature-Based Distillation for Human Activity Recognition", "target": "FCLFD", "value": "proposed_model"}, {"source": "FCLFD", "target": "WISDM", "value": "evaluated_on"}, {"source": "FCLFD", "target": "PAMAP2", "value": "evaluated_on"}, {"source": "FCLFD", "target": "F1", "value": "uses_metric"}, {"source": "Diffuse and Disperse: Image Generation with Representation Regularization", "target": "Dispersive Loss", "value": "proposed_model"}, {"source": "Diffuse and Disperse: Image Generation with Representation Regularization", "target": "REPA", "value": "baseline_model"}, {"source": "Dispersive Loss", "target": "ImageNet", "value": "evaluated_on"}, {"source": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems", "target": "Knowledge-Aware Bayesian Bandits (KABB)", "value": "proposed_model"}, {"source": "Knowledge-Aware Bayesian Bandits (KABB)", "target": "multi-agent systems", "value": "evaluated_on"}, {"source": "Knowledge-Aware Bayesian Bandits (KABB)", "target": "cost-performance balance", "value": "uses_metric"}, {"source": "U-Net: Convolutional Networks for Biomedical Image Segmentation", "target": "U-Net", "value": "proposed_model"}, {"source": "U-Net: Convolutional Networks for Biomedical Image Segmentation", "target": "sliding-window convolutional network", "value": "baseline_model"}, {"source": "U-Net", "target": "ISBI challenge for segmentation of neuronal structures in electron microscopic stacks", "value": "evaluated_on"}, {"source": "U-Net", "target": "ISBI cell tracking challenge 2015", "value": "evaluated_on"}, {"source": "Improved Distribution Matching Distillation for Fast Image Synthesis", "target": "DMD2", "value": "proposed_model"}, {"source": "Improved Distribution Matching Distillation for Fast Image Synthesis", "target": "Distribution Matching Distillation (DMD)", "value": "baseline_model"}, {"source": "DMD2", "target": "ImageNet-64x64", "value": "evaluated_on"}, {"source": "DMD2", "target": "COCO 2014", "value": "evaluated_on"}, {"source": "DMD2", "target": "FID", "value": "uses_metric"}, {"source": "Distribution Matching Distillation (DMD)", "target": "FID", "value": "uses_metric"}, {"source": "Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation", "target": "Latent Adversarial Diffusion Distillation (LADD)", "value": "proposed_model"}, {"source": "Latent Adversarial Diffusion Distillation (LADD)", "target": "adversarial diffusion distillation (ADD)", "value": "baseline_model"}, {"source": "Latent Adversarial Diffusion Distillation (LADD)", "target": "Stable Diffusion 3 (8B)", "value": "evaluated_on"}, {"source": "Latent Adversarial Diffusion Distillation (LADD)", "target": "SD3-Turbo", "value": "evaluated_on"}, {"source": "adversarial diffusion distillation (ADD)", "target": "DINOv2", "value": "uses_metric"}, {"source": "Latent Adversarial Diffusion Distillation (LADD)", "target": "latent diffusion models", "value": "uses_metric"}, {"source": "SD3-Turbo", "target": "state-of-the-art text-to-image generators", "value": "baseline_model"}, {"source": "Evolutionary optimization of model merging recipes", "target": "evolutionary approach", "value": "proposed_model"}, {"source": "evolutionary approach", "target": "Japanese LLM with Math reasoning capabilities", "value": "proposed_model"}, {"source": "evolutionary approach", "target": "culturally-aware Japanese VLM", "value": "proposed_model"}, {"source": "Japanese LLM with Math reasoning capabilities", "target": "Japanese LLM benchmarks", "value": "evaluated_on"}, {"source": "Japanese LLM with Math reasoning capabilities", "target": "state-of-the-art performance", "value": "uses_metric"}, {"source": "culturally-aware Japanese VLM", "target": "state-of-the-art performance", "value": "uses_metric"}, {"source": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models", "target": "autoregressive transformer", "value": "proposed_model"}, {"source": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models", "target": "bidirectional diffusion transformer", "value": "baseline_model"}, {"source": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models", "target": "VBench-Long", "value": "evaluated_on"}, {"source": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models", "target": "total score", "value": "uses_metric"}, {"source": "distribution matching distillation (DMD)", "target": "4-step generator", "value": "proposed_model"}, {"source": "distribution matching distillation (DMD)", "target": "50-step diffusion model", "value": "baseline_model"}, {"source": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment", "target": "PuLID", "value": "proposed_model"}, {"source": "PuLID", "target": "Lightning T2I branch", "value": "baseline_model"}, {"source": "PuLID", "target": "standard diffusion branch", "value": "baseline_model"}, {"source": "Denoising Diffusion Probabilistic Models", "target": "diffusion probabilistic models", "value": "proposed_model"}, {"source": "Denoising Diffusion Probabilistic Models", "target": "CIFAR10", "value": "evaluated_on"}, {"source": "Denoising Diffusion Probabilistic Models", "target": "LSUN", "value": "evaluated_on"}, {"source": "Denoising Diffusion Probabilistic Models", "target": "Inception score", "value": "uses_metric"}, {"source": "Denoising Diffusion Probabilistic Models", "target": "FID score", "value": "uses_metric"}, {"source": "Denoising Diffusion Probabilistic Models", "target": "ProgressiveGAN", "value": "baseline_model"}, {"source": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "target": "HunyuanVideo", "value": "proposed_model"}, {"source": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "target": "Runway Gen-3", "value": "baseline_model"}, {"source": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "target": "Luma 1.6", "value": "baseline_model"}, {"source": "HunyuanVideo: A Systematic Framework For Large Video Generative Models", "target": "three top-performing Chinese video generative models", "value": "baseline_model"}, {"source": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency", "target": "Loopy", "value": "proposed_model"}, {"source": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency", "target": "audio-driven portrait diffusion models", "value": "baseline_model"}, {"source": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models", "target": "OmniHuman", "value": "proposed_model"}, {"source": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models", "target": "existing end-to-end audio-driven methods", "value": "baseline_model"}, {"source": "OmniHuman", "target": "existing end-to-end audio-driven methods", "value": "evaluated_on"}, {"source": "OmniHuman", "target": "highly realistic human video generation", "value": "uses_metric"}, {"source": "OmniHuman", "target": "more realistic videos", "value": "uses_metric"}, {"source": "OmniHuman", "target": "greater flexibility in inputs", "value": "uses_metric"}, {"source": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation", "target": "Hallo2", "value": "proposed_model"}, {"source": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation", "target": "Hallo", "value": "baseline_model"}, {"source": "Hallo2", "target": "HDTF", "value": "evaluated_on"}, {"source": "Hallo2", "target": "CelebV", "value": "evaluated_on"}, {"source": "Hallo2", "target": "Wild", "value": "evaluated_on"}, {"source": "EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation", "target": "EchoMimicV2", "value": "proposed_model"}, {"source": "EchoMimicV2", "target": "existing methods", "value": "baseline_model"}, {"source": "EchoMimicV2", "target": "novel benchmark for evaluating the effectiveness of half-body human animation", "value": "evaluated_on"}, {"source": "EchoMimicV2", "target": "quantitative evaluations", "value": "uses_metric"}, {"source": "EchoMimicV2", "target": "qualitative evaluations", "value": "uses_metric"}, {"source": "EchoMimicV2", "target": "half-body data", "value": "evaluated_on"}, {"source": "EchoMimicV2", "target": "headshot data", "value": "evaluated_on"}, {"source": "Feature Pyramid Networks for Object Detection", "target": "Feature Pyramid Network (FPN)", "value": "proposed_model"}, {"source": "Feature Pyramid Networks for Object Detection", "target": "Faster R-CNN", "value": "baseline_model"}, {"source": "Feature Pyramid Networks for Object Detection", "target": "COCO detection benchmark", "value": "evaluated_on"}, {"source": "Feature Pyramid Networks for Object Detection", "target": "state-of-the-art single-model results", "value": "uses_metric"}, {"source": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation", "target": "RNN Encoder-Decoder", "value": "proposed_model"}, {"source": "RNN Encoder-Decoder", "target": "statistical machine translation system", "value": "baseline_model"}, {"source": "RNN Encoder-Decoder", "target": "log-linear model", "value": "baseline_model"}, {"source": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving", "target": "DriveMLM", "value": "proposed_model"}, {"source": "DriveMLM", "target": "Autopilot", "value": "baseline_model"}, {"source": "DriveMLM", "target": "Apollo", "value": "baseline_model"}, {"source": "DriveMLM", "target": "CARLA Town05 Long", "value": "evaluated_on"}, {"source": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability", "target": "Vista", "value": "proposed_model"}, {"source": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability", "target": "most advanced general-purpose video generator", "value": "baseline_model"}, {"source": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability", "target": "best-performing driving world model", "value": "baseline_model"}, {"source": "Vista", "target": "multiple datasets", "value": "evaluated_on"}, {"source": "Vista", "target": "FID", "value": "uses_metric"}, {"source": "Vista", "target": "FVD", "value": "uses_metric"}, {"source": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving", "target": "DiffusionDrive", "value": "proposed_model"}, {"source": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving", "target": "vanilla diffusion policy", "value": "baseline_model"}, {"source": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving", "target": "NAVSIM", "value": "evaluated_on"}, {"source": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving", "target": "PDMS", "value": "uses_metric"}, {"source": "DiffusionDrive", "target": "NAVSIM", "value": "evaluated_on"}, {"source": "DiffusionDrive", "target": "PDMS", "value": "uses_metric"}, {"source": "How Far is Video Generation from World Model: A Physical Law Perspective", "target": "diffusion-based video generation models", "value": "proposed_model"}, {"source": "How Far is Video Generation from World Model: A Physical Law Perspective", "target": "2D simulation testbed for object movement and collisions", "value": "evaluated_on"}, {"source": "How Far is Video Generation from World Model: A Physical Law Perspective", "target": "physical laws adherence", "value": "uses_metric"}, {"source": "How Far is Video Generation from World Model: A Physical Law Perspective", "target": "Sora", "value": "baseline_model"}, {"source": "EMMA: End-to-End Multimodal Model for Autonomous Driving", "target": "EMMA", "value": "proposed_model"}, {"source": "EMMA", "target": "Gemini", "value": "baseline_model"}, {"source": "EMMA", "target": "nuScenes", "value": "evaluated_on"}, {"source": "EMMA", "target": "Waymo Open Motion Dataset (WOMD)", "value": "evaluated_on"}, {"source": "EMMA", "target": "Waymo Open Dataset (WOD)", "value": "evaluated_on"}, {"source": "EMMA", "target": "state-of-the-art performance in motion planning", "value": "uses_metric"}, {"source": "EMMA", "target": "competitive results", "value": "uses_metric"}, {"source": "Language Models are Few-Shot Learners", "target": "GPT-3", "value": "proposed_model"}, {"source": "GPT-3", "target": "translation, question-answering, and cloze tasks", "value": "evaluated_on"}, {"source": "GPT-3", "target": "few-shot performance", "value": "uses_metric"}, {"source": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling", "target": "InternVL 2.5", "value": "proposed_model"}, {"source": "InternVL 2.5", "target": "InternVL 2.0", "value": "baseline_model"}, {"source": "InternVL 2.5", "target": "MMMU benchmark", "value": "evaluated_on"}, {"source": "InternVL 2.5", "target": "Chain-of-Thought (CoT) reasoning", "value": "uses_metric"}, {"source": "InternVL3", "target": "InternVL3-78B", "value": "proposed_model"}, {"source": "InternVL3-78B", "target": "MMMU", "value": "evaluated_on"}, {"source": "InternVL3-78B", "target": "ChatGPT-4o", "value": "baseline_model"}, {"source": "InternVL3-78B", "target": "Claude 3.5 Sonnet", "value": "baseline_model"}, {"source": "InternVL3-78B", "target": "Gemini 2.5 Pro", "value": "baseline_model"}, {"source": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models", "target": "VLMEvalKit", "value": "proposed_model"}, {"source": "VLMEvalKit", "target": "OpenVLM Leaderboard", "value": "uses_metric"}, {"source": "LLaVA-CoT", "target": "LLaVA-CoT", "value": "proposed_model"}, {"source": "LLaVA-CoT", "target": "Gemini-1.5-pro", "value": "baseline_model"}, {"source": "LLaVA-CoT", "target": "GPT-4o-mini", "value": "baseline_model"}, {"source": "LLaVA-CoT", "target": "Llama-3.2-90B-Vision-Instruct", "value": "baseline_model"}, {"source": "LLaVA-CoT", "target": "multimodal reasoning benchmarks", "value": "evaluated_on"}, {"source": "LLaVA-CoT", "target": "multimodal reasoning benchmarks", "value": "uses_metric"}, {"source": "InternVL 3.5", "target": "Cascade Reinforcement Learning (Cascade RL) framework", "value": "proposed_model"}, {"source": "InternVL 3.5", "target": "Visual Resolution Router (ViR)", "value": "proposed_model"}, {"source": "InternVL 3.5", "target": "Decoupled Vision-Language Deployment (DvD) strategy", "value": "proposed_model"}, {"source": "InternVL 3.5", "target": "InternVL3", "value": "baseline_model"}, {"source": "InternVL 3.5", "target": "MMMU", "value": "evaluated_on"}, {"source": "InternVL 3.5", "target": "MathVista", "value": "evaluated_on"}, {"source": "InternVL 3.5", "target": "overall reasoning performance", "value": "uses_metric"}, {"source": "InternVL 3.5", "target": "inference speedup", "value": "uses_metric"}, {"source": "InternVL3.5-241B-A28B", "target": "MMMU", "value": "evaluated_on"}, {"source": "InternVL3.5-241B-A28B", "target": "MathVista", "value": "evaluated_on"}, {"source": "Proximal Policy Optimization Algorithms", "target": "Proximal Policy Optimization (PPO)", "value": "proposed_model"}, {"source": "Proximal Policy Optimization (PPO)", "target": "Trust Region Policy Optimization (TRPO)", "value": "baseline_model"}, {"source": "Proximal Policy Optimization (PPO)", "target": "simulated robotic locomotion", "value": "evaluated_on"}, {"source": "Proximal Policy Optimization (PPO)", "target": "Atari game playing", "value": "evaluated_on"}, {"source": "Proximal Policy Optimization (PPO)", "target": "sample complexity", "value": "uses_metric"}, {"source": "Flow-GRPO", "target": "Flow-GRPO", "value": "proposed_model"}, {"source": "Flow-GRPO", "target": "SD3.5-M", "value": "baseline_model"}, {"source": "Flow-GRPO", "target": "GenEval", "value": "evaluated_on"}, {"source": "Flow-GRPO", "target": "accuracy", "value": "uses_metric"}, {"source": "DanceGRPO", "target": "Group Relative Policy Optimization (GRPO)", "value": "proposed_model"}, {"source": "DanceGRPO", "target": "DDPO", "value": "baseline_model"}, {"source": "DanceGRPO", "target": "DPOK", "value": "baseline_model"}, {"source": "DanceGRPO", "target": "HPS-v2.1", "value": "evaluated_on"}, {"source": "DanceGRPO", "target": "VideoAlign", "value": "evaluated_on"}, {"source": "DanceGRPO", "target": "GenEval", "value": "evaluated_on"}, {"source": "DanceGRPO", "target": "CLIP Score", "value": "uses_metric"}, {"source": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "target": "Seedance 1.0", "value": "proposed_model"}, {"source": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "target": "state-of-the-art video generation models", "value": "baseline_model"}, {"source": "Seedance 1.0", "target": "multi-source data curation", "value": "evaluated_on"}, {"source": "Seedance 1.0", "target": "prompt following", "value": "uses_metric"}, {"source": "Seedance 1.0", "target": "motion plausibility", "value": "uses_metric"}, {"source": "Seedance 1.0", "target": "visual quality", "value": "uses_metric"}, {"source": "Seedance 1.0", "target": "spatiotemporal fluidity", "value": "uses_metric"}, {"source": "Seedance 1.0", "target": "structural stability", "value": "uses_metric"}, {"source": "Seedance 1.0", "target": "instruction adherence", "value": "uses_metric"}, {"source": "Seedance 1.0", "target": "narrative coherence", "value": "uses_metric"}, {"source": "Seedance 1.0", "target": "subject representation", "value": "uses_metric"}, {"source": "SkyReels-V2: Infinite-length Film Generative Model", "target": "SkyReels-V2", "value": "proposed_model"}, {"source": "SkyReels-V2", "target": "Multi-modal Large Language Model (MLLM)", "value": "baseline_model"}, {"source": "SkyReels-V2", "target": "video data", "value": "evaluated_on"}, {"source": "SkyReels-V2", "target": "human-annotated and synthetic distortion data", "value": "evaluated_on"}, {"source": "SkyReels-V2", "target": "prompt adherence", "value": "uses_metric"}, {"source": "SkyReels-V2", "target": "visual quality", "value": "uses_metric"}, {"source": "SkyReels-V2", "target": "motion dynamics", "value": "uses_metric"}, {"source": "SkyReels-V2", "target": "duration", "value": "uses_metric"}, {"source": "SkyReels-V2", "target": "temporal visual quality", "value": "uses_metric"}, {"source": "SkyReels-V2", "target": "resolution", "value": "uses_metric"}, {"source": "SkyReels-V2", "target": "shot-aware generation", "value": "uses_metric"}, {"source": "SkyReels-V2", "target": "realistic long-form synthesis", "value": "uses_metric"}, {"source": "SkyReels-V2", "target": "professional film-style generation", "value": "uses_metric"}, {"source": "SkyReels-V2", "target": "dynamic artifacts", "value": "uses_metric"}, {"source": "SkyReels-V2", "target": "visual fidelity", "value": "uses_metric"}, {"source": "Unified Reward Model for Multimodal Understanding and Generation", "target": "UnifiedReward", "value": "proposed_model"}, {"source": "UnifiedReward", "target": "large-scale human preference dataset", "value": "evaluated_on"}, {"source": "UnifiedReward", "target": "Direct Preference Optimization (DPO)", "value": "baseline_model"}, {"source": "UnifiedReward", "target": "performance", "value": "uses_metric"}, {"source": "GPT-4 Technical Report", "target": "GPT-4", "value": "proposed_model"}, {"source": "GPT-4", "target": "Transformer-based model", "value": "baseline_model"}, {"source": "GPT-4", "target": "simulated bar exam", "value": "evaluated_on"}, {"source": "GPT-4", "target": "factuality", "value": "uses_metric"}, {"source": "GPT-4", "target": "adherence to desired behavior", "value": "uses_metric"}, {"source": "LLaMA", "target": "LLaMA-13B", "value": "proposed_model"}, {"source": "LLaMA", "target": "LLaMA-65B", "value": "proposed_model"}, {"source": "LLaMA-13B", "target": "GPT-3 (175B)", "value": "baseline_model"}, {"source": "LLaMA-65B", "target": "Chinchilla-70B", "value": "baseline_model"}, {"source": "LLaMA-65B", "target": "PaLM-540B", "value": "baseline_model"}, {"source": "LLaMA", "target": "benchmarks", "value": "evaluated_on"}, {"source": "LLaMA", "target": "publicly available datasets", "value": "evaluated_on"}, {"source": "LLaMA", "target": "benchmarks", "value": "uses_metric"}, {"source": "Chain of Thought Prompting Elicits Reasoning in Large Language Models", "target": "chain of thought prompting", "value": "proposed_model"}, {"source": "chain of thought prompting", "target": "GSM8K", "value": "evaluated_on"}, {"source": "chain of thought prompting", "target": "accuracy", "value": "uses_metric"}, {"source": "chain of thought prompting", "target": "GPT-3", "value": "baseline_model"}, {"source": "Training Verifiers to Solve Math Word Problems", "target": "verifiers", "value": "proposed_model"}, {"source": "Training Verifiers to Solve Math Word Problems", "target": "finetuning baseline", "value": "baseline_model"}, {"source": "Training Verifiers to Solve Math Word Problems", "target": "GSM8K", "value": "evaluated_on"}, {"source": "Training Verifiers to Solve Math Word Problems", "target": "test performance", "value": "uses_metric"}, {"source": "verifiers", "target": "GSM8K", "value": "evaluated_on"}, {"source": "transformer models", "target": "GSM8K", "value": "evaluated_on"}, {"source": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "target": "Mamba", "value": "proposed_model"}, {"source": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "target": "Mamba-3B", "value": "proposed_model"}, {"source": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "target": "Transformer", "value": "baseline_model"}, {"source": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "target": "linear attention", "value": "baseline_model"}, {"source": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "target": "gated convolution", "value": "baseline_model"}, {"source": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "target": "recurrent models", "value": "baseline_model"}, {"source": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "target": "structured state space models (SSMs)", "value": "baseline_model"}, {"source": "Mamba", "target": "language", "value": "evaluated_on"}, {"source": "Mamba", "target": "audio", "value": "evaluated_on"}, {"source": "Mamba", "target": "genomics", "value": "evaluated_on"}, {"source": "Mamba-3B", "target": "language", "value": "evaluated_on"}, {"source": "Mamba", "target": "state-of-the-art performance", "value": "uses_metric"}, {"source": "Mamba-3B", "target": "state-of-the-art performance", "value": "uses_metric"}, {"source": "SGDR: Stochastic Gradient Descent with Warm Restarts", "target": "SGDR", "value": "proposed_model"}, {"source": "SGDR", "target": "CIFAR-10", "value": "evaluated_on"}, {"source": "SGDR", "target": "CIFAR-100", "value": "evaluated_on"}, {"source": "SGDR", "target": "EEG recordings dataset", "value": "evaluated_on"}, {"source": "SGDR", "target": "downsampled ImageNet", "value": "evaluated_on"}, {"source": "SGDR", "target": "error rate", "value": "uses_metric"}, {"source": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "target": "Sparsely-Gated Mixture-of-Experts layer (MoE)", "value": "proposed_model"}, {"source": "Sparsely-Gated Mixture-of-Experts layer (MoE)", "target": "language modeling", "value": "evaluated_on"}, {"source": "Sparsely-Gated Mixture-of-Experts layer (MoE)", "target": "machine translation", "value": "evaluated_on"}, {"source": "Sparsely-Gated Mixture-of-Experts layer (MoE)", "target": "large language modeling benchmarks", "value": "evaluated_on"}, {"source": "Sparsely-Gated Mixture-of-Experts layer (MoE)", "target": "machine translation benchmarks", "value": "evaluated_on"}, {"source": "Sparsely-Gated Mixture-of-Experts layer (MoE)", "target": "computational efficiency", "value": "uses_metric"}, {"source": "Pointer Sentinel Mixture Models", "target": "pointer sentinel mixture architecture", "value": "proposed_model"}, {"source": "Pointer Sentinel Mixture Models", "target": "pointer sentinel-LSTM model", "value": "proposed_model"}, {"source": "pointer sentinel-LSTM model", "target": "standard softmax LSTM", "value": "baseline_model"}, {"source": "pointer sentinel-LSTM model", "target": "Penn Treebank", "value": "evaluated_on"}, {"source": "Pointer Sentinel Mixture Models", "target": "Penn Treebank", "value": "evaluated_on"}, {"source": "Pointer Sentinel Mixture Models", "target": "WikiText corpus", "value": "proposed_model"}, {"source": "pointer sentinel-LSTM model", "target": "perplexity", "value": "uses_metric"}, {"source": "Measuring Massive Multitask Language Understanding", "target": "massive multitask language understanding test", "value": "proposed_model"}, {"source": "Measuring Massive Multitask Language Understanding", "target": "massive multitask language understanding test", "value": "evaluated_on"}, {"source": "Measuring Massive Multitask Language Understanding", "target": "multitask accuracy", "value": "uses_metric"}, {"source": "GPT-3", "target": "massive multitask language understanding test", "value": "baseline_model"}, {"source": "Let's Verify Step by Step", "target": "process-supervised model", "value": "proposed_model"}, {"source": "Let's Verify Step by Step", "target": "MATH", "value": "evaluated_on"}, {"source": "process-supervised model", "target": "MATH", "value": "evaluated_on"}, {"source": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark", "target": "GPQA", "value": "proposed_model"}, {"source": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark", "target": "GPT-4", "value": "baseline_model"}, {"source": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark", "target": "GPQA", "value": "evaluated_on"}, {"source": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark", "target": "accuracy", "value": "uses_metric"}, {"source": "GPT-4", "target": "GPQA", "value": "evaluated_on"}, {"source": "GPT-4", "target": "accuracy", "value": "uses_metric"}, {"source": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding", "target": "GShard", "value": "proposed_model"}, {"source": "GShard", "target": "multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts", "value": "proposed_model"}, {"source": "GShard", "target": "prior art", "value": "baseline_model"}, {"source": "multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts", "target": "translation from 100 languages to English", "value": "evaluated_on"}, {"source": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding", "target": "model quality", "value": "uses_metric"}, {"source": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding", "target": "computation cost", "value": "uses_metric"}, {"source": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding", "target": "ease of programming", "value": "uses_metric"}, {"source": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding", "target": "efficient implementation on parallel devices", "value": "uses_metric"}, {"source": "Pattern Recognition and Machine Learning", "target": "Probability Distributions", "value": "proposed_model"}, {"source": "Pattern Recognition and Machine Learning", "target": "Linear Models for Regression", "value": "proposed_model"}, {"source": "Pattern Recognition and Machine Learning", "target": "Linear Models for Classification", "value": "proposed_model"}, {"source": "Pattern Recognition and Machine Learning", "target": "Neural Networks", "value": "proposed_model"}, {"source": "Pattern Recognition and Machine Learning", "target": "Kernel Methods", "value": "proposed_model"}, {"source": "Pattern Recognition and Machine Learning", "target": "Sparse Kernel Machines", "value": "proposed_model"}, {"source": "Pattern Recognition and Machine Learning", "target": "Graphical Models", "value": "proposed_model"}, {"source": "Pattern Recognition and Machine Learning", "target": "Mixture Models and EM", "value": "proposed_model"}, {"source": "Pattern Recognition and Machine Learning", "target": "Approximate Inference", "value": "proposed_model"}, {"source": "Pattern Recognition and Machine Learning", "target": "Sampling Methods", "value": "proposed_model"}, {"source": "Pattern Recognition and Machine Learning", "target": "Continuous Latent Variables", "value": "proposed_model"}, {"source": "Pattern Recognition and Machine Learning", "target": "Sequential Data", "value": "proposed_model"}, {"source": "Pattern Recognition and Machine Learning", "target": "Combining Models", "value": "proposed_model"}, {"source": "Automatic Image-Based Plant Disease Severity Estimation Using Deep Learning", "target": "deep convolutional neural networks", "value": "proposed_model"}, {"source": "Automatic Image-Based Plant Disease Severity Estimation Using Deep Learning", "target": "shallow networks", "value": "baseline_model"}, {"source": "Automatic Image-Based Plant Disease Severity Estimation Using Deep Learning", "target": "deep models", "value": "baseline_model"}, {"source": "deep convolutional neural networks", "target": "PlantVillage", "value": "evaluated_on"}, {"source": "VGG16", "target": "PlantVillage", "value": "evaluated_on"}, {"source": "VGG16", "target": "accuracy", "value": "uses_metric"}, {"source": "Plant identification using deep neural networks via optimization of transfer learning parameters", "target": "deep neural networks", "value": "proposed_model"}, {"source": "Deep Learning for Plant Identification in Natural Environment", "target": "26-layer deep learning model consisting of 8 residual building blocks", "value": "proposed_model"}, {"source": "Deep Learning for Plant Identification in Natural Environment", "target": "BJFU100 dataset", "value": "evaluated_on"}, {"source": "26-layer deep learning model consisting of 8 residual building blocks", "target": "recognition rate", "value": "uses_metric"}, {"source": "Going deeper in the automated identification of Herbarium specimens", "target": "Deep Learning", "value": "proposed_model"}, {"source": "Going deeper in the automated identification of Herbarium specimens", "target": "herbarium images", "value": "evaluated_on"}, {"source": "Going deeper in the automated identification of Herbarium specimens", "target": "big dataset with thousands of species from herbaria", "value": "evaluated_on"}, {"source": "Going deeper in the automated identification of Herbarium specimens", "target": "different datasets from different herbaria", "value": "evaluated_on"}, {"source": "Going deeper in the automated identification of Herbarium specimens", "target": "accuracy", "value": "uses_metric"}, {"source": "MobileNetV2: Inverted Residuals and Linear Bottlenecks", "target": "MobileNetV2", "value": "proposed_model"}, {"source": "MobileNetV2: Inverted Residuals and Linear Bottlenecks", "target": "SSDLite", "value": "proposed_model"}, {"source": "MobileNetV2: Inverted Residuals and Linear Bottlenecks", "target": "Mobile DeepLabv3", "value": "proposed_model"}, {"source": "MobileNetV2", "target": "Imagenet", "value": "evaluated_on"}, {"source": "MobileNetV2", "target": "COCO", "value": "evaluated_on"}, {"source": "MobileNetV2", "target": "VOC", "value": "evaluated_on"}, {"source": "MobileNetV2", "target": "accuracy", "value": "uses_metric"}, {"source": "MobileNetV2", "target": "multiply-adds (MAdd)", "value": "uses_metric"}, {"source": "MobileNetV2", "target": "number of parameters", "value": "uses_metric"}, {"source": "Communication-Efficient Learning of Deep Networks from Decentralized Data", "target": "Federated Learning", "value": "proposed_model"}, {"source": "Federated Learning", "target": "four datasets", "value": "evaluated_on"}, {"source": "Federated Learning", "target": "synchronized stochastic gradient descent", "value": "baseline_model"}, {"source": "A Comprehensive Survey on Transfer Learning", "target": "Transfer Learning", "value": "proposed_model"}, {"source": "A Comprehensive Survey on Transfer Learning", "target": "Amazon Reviews", "value": "evaluated_on"}, {"source": "A Comprehensive Survey on Transfer Learning", "target": "Reuters-21578", "value": "evaluated_on"}, {"source": "A Comprehensive Survey on Transfer Learning", "target": "Office-31", "value": "evaluated_on"}, {"source": "Privacy Preserved and Decentralized Smartphone Recommendation System", "target": "federated deep neural network (FDNN)", "value": "proposed_model"}, {"source": "Privacy Preserved and Decentralized Smartphone Recommendation System", "target": "Flipkart dataset", "value": "evaluated_on"}, {"source": "Privacy Preserved and Decentralized Smartphone Recommendation System", "target": "accuracy", "value": "uses_metric"}, {"source": "A Survey on Heterogeneity Taxonomy, Security and Privacy Preservation in the Integration of IoT, Wireless Sensor Networks and Federated Learning", "target": "Federated learning (FL)", "value": "proposed_model"}, {"source": "Federated learning (FL)", "target": "Internet of Things (IoT)", "value": "evaluated_on"}, {"source": "Federated learning (FL)", "target": "Wireless Sensor Networks (WSNs)", "value": "evaluated_on"}, {"source": "A privacy-preserving federated learning with a secure collaborative for malware detection models using Internet of Things resources", "target": "privacy-preserving federated learning", "value": "proposed_model"}, {"source": "A privacy-preserving federated learning with a secure collaborative for malware detection models using Internet of Things resources", "target": "malware detection models", "value": "proposed_model"}, {"source": "Federated Learning Architectures: A Performance Evaluation With Crop Yield Prediction Application", "target": "centralized federated learning framework", "value": "proposed_model"}, {"source": "Federated Learning Architectures: A Performance Evaluation With Crop Yield Prediction Application", "target": "decentralized federated learning framework", "value": "proposed_model"}, {"source": "centralized federated learning framework", "target": "Long Short-Term Memory Network", "value": "baseline_model"}, {"source": "decentralized federated learning framework", "target": "Long Short-Term Memory Network", "value": "baseline_model"}, {"source": "centralized federated learning framework", "target": "crop yield prediction", "value": "evaluated_on"}, {"source": "decentralized federated learning framework", "target": "crop yield prediction", "value": "evaluated_on"}, {"source": "centralized federated learning framework", "target": "prediction accuracy", "value": "uses_metric"}, {"source": "centralized federated learning framework", "target": "precision", "value": "uses_metric"}, {"source": "centralized federated learning framework", "target": "recall", "value": "uses_metric"}, {"source": "centralized federated learning framework", "target": "F1-Score", "value": "uses_metric"}, {"source": "centralized federated learning framework", "target": "training time", "value": "uses_metric"}, {"source": "decentralized federated learning framework", "target": "prediction accuracy", "value": "uses_metric"}, {"source": "decentralized federated learning framework", "target": "precision", "value": "uses_metric"}, {"source": "decentralized federated learning framework", "target": "recall", "value": "uses_metric"}, {"source": "decentralized federated learning framework", "target": "F1-Score", "value": "uses_metric"}, {"source": "decentralized federated learning framework", "target": "training time", "value": "uses_metric"}, {"source": "Segment Anything (SA) project", "target": "Segment Anything Model (SAM)", "value": "proposed_model"}, {"source": "Segment Anything (SA) project", "target": "SA-1B", "value": "evaluated_on"}, {"source": "Segment Anything (SA) project", "target": "zero-shot performance", "value": "uses_metric"}, {"source": "Visual Instruction Tuning", "target": "LLaVA", "value": "proposed_model"}, {"source": "Visual Instruction Tuning", "target": "GPT-4", "value": "baseline_model"}, {"source": "LLaVA", "target": "Science QA", "value": "evaluated_on"}, {"source": "LLaVA", "target": "synthetic multimodal instruction-following dataset", "value": "evaluated_on"}, {"source": "LLaVA", "target": "accuracy", "value": "uses_metric"}, {"source": "LLaVA", "target": "relative score", "value": "uses_metric"}, {"source": "Improved Baselines with Visual Instruction Tuning", "target": "LLaVA", "value": "proposed_model"}, {"source": "Improved Baselines with Visual Instruction Tuning", "target": "CLIP-ViT-L-336px", "value": "baseline_model"}, {"source": "Improved Baselines with Visual Instruction Tuning", "target": "11 benchmarks", "value": "evaluated_on"}, {"source": "Improved Baselines with Visual Instruction Tuning", "target": "state-of-the-art", "value": "uses_metric"}, {"source": "LLaVA", "target": "MLP projection", "value": "baseline_model"}, {"source": "LLaVA", "target": "academic-task-oriented VQA data", "value": "evaluated_on"}, {"source": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset", "target": "DepictQA-Wild", "value": "proposed_model"}, {"source": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset", "target": "Vision Language Models (VLMs)", "value": "baseline_model"}, {"source": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset", "target": "VLM-based Image Quality Assessment (IQA)", "value": "baseline_model"}, {"source": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset", "target": "traditional score-based methods", "value": "baseline_model"}, {"source": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset", "target": "GPT-4V", "value": "baseline_model"}, {"source": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset", "target": "DQ-495K", "value": "evaluated_on"}, {"source": "DepictQA-Wild", "target": "DQ-495K", "value": "evaluated_on"}, {"source": "DepictQA-Wild", "target": "distortion identification", "value": "uses_metric"}, {"source": "DepictQA-Wild", "target": "instant rating", "value": "uses_metric"}, {"source": "DepictQA-Wild", "target": "reasoning tasks", "value": "uses_metric"}, {"source": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs", "target": "Chain-of-Focus (CoF)", "value": "proposed_model"}, {"source": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs", "target": "Qwen2.5-VL", "value": "baseline_model"}, {"source": "Chain-of-Focus (CoF)", "target": "V* benchmark", "value": "evaluated_on"}, {"source": "Chain-of-Focus (CoF)", "target": "outcome accuracies", "value": "uses_metric"}, {"source": "Chain-of-Focus (CoF)", "target": "formats", "value": "uses_metric"}, {"source": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views", "target": "3DThinker", "value": "proposed_model"}, {"source": "3DThinker", "target": "multiple benchmarks", "value": "evaluated_on"}, {"source": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning", "target": "UniME-V2", "value": "proposed_model"}, {"source": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning", "target": "MLLM-as-a-Judge mechanism", "value": "proposed_model"}, {"source": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning", "target": "UniME-V2-Reranker", "value": "proposed_model"}, {"source": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning", "target": "MMEB benchmark", "value": "evaluated_on"}, {"source": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning", "target": "retrieval tasks", "value": "evaluated_on"}, {"source": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning", "target": "state-of-the-art performance", "value": "uses_metric"}, {"source": "OpenMMReasoner", "target": "OpenMMReasoner", "value": "proposed_model"}, {"source": "OpenMMReasoner", "target": "Qwen2.5-VL-7B-Instruct", "value": "baseline_model"}, {"source": "OpenMMReasoner", "target": "nine multimodal reasoning benchmarks", "value": "evaluated_on"}, {"source": "OpenMMReasoner", "target": "nine multimodal reasoning benchmarks", "value": "uses_metric"}, {"source": "Vision-Language-Action (VLA) Models: Concepts, Progress, Applications and Challenges", "target": "Vision-Language-Action (VLA) models", "value": "proposed_model"}, {"source": "Vision-Language-Action (VLA) models", "target": "vision-language models (VLMs)", "value": "baseline_model"}, {"source": "Vision-Language-Action (VLA) models", "target": "autonomous vehicles", "value": "evaluated_on"}, {"source": "Vision-Language-Action (VLA) models", "target": "medical and industrial robotics", "value": "evaluated_on"}, {"source": "Vision-Language-Action (VLA) models", "target": "precision agriculture", "value": "evaluated_on"}, {"source": "Vision-Language-Action (VLA) models", "target": "humanoid robotics", "value": "evaluated_on"}, {"source": "Vision-Language-Action (VLA) models", "target": "augmented reality", "value": "evaluated_on"}, {"source": "Vision-Language-Action (VLA) models", "target": "agentic adaptation", "value": "uses_metric"}, {"source": "Vision-Language-Action (VLA) models", "target": "cross-embodiment planning", "value": "uses_metric"}, {"source": "Leave No Observation Behind: Real-time Correction for VLA Action Chunks", "target": "Asynchronous Action Chunk Correction (A2C2)", "value": "proposed_model"}, {"source": "Asynchronous Action Chunk Correction (A2C2)", "target": "Real Time Chunking (RTC)", "value": "baseline_model"}, {"source": "Asynchronous Action Chunk Correction (A2C2)", "target": "dynamic Kinetix task suite (12 tasks)", "value": "evaluated_on"}, {"source": "Asynchronous Action Chunk Correction (A2C2)", "target": "LIBERO Spatial", "value": "evaluated_on"}, {"source": "Asynchronous Action Chunk Correction (A2C2)", "target": "success rate", "value": "uses_metric"}, {"source": "Latent Chain-of-Thought World Modeling for End-to-End Driving", "target": "Latent-CoT-Drive (LCDrive)", "value": "proposed_model"}, {"source": "Latent-CoT-Drive (LCDrive)", "target": "Vision-Language-Action (VLA) models", "value": "baseline_model"}, {"source": "Latent-CoT-Drive (LCDrive)", "target": "non-reasoning baselines", "value": "baseline_model"}, {"source": "Latent-CoT-Drive (LCDrive)", "target": "text-reasoning baselines", "value": "baseline_model"}, {"source": "Latent-CoT-Drive (LCDrive)", "target": "large-scale end-to-end driving benchmark", "value": "evaluated_on"}, {"source": "Latent-CoT-Drive (LCDrive)", "target": "trajectory quality", "value": "uses_metric"}, {"source": "HyperVLA", "target": "HyperVLA", "value": "proposed_model"}, {"source": "HyperVLA", "target": "OpenVLA", "value": "baseline_model"}, {"source": "HyperVLA", "target": "Vision-Language-Action (VLA) models", "value": "baseline_model"}, {"source": "HyperVLA", "target": "large-scale robotic data", "value": "evaluated_on"}, {"source": "HyperVLA", "target": "success rate", "value": "uses_metric"}, {"source": "HyperVLA", "target": "inference costs", "value": "uses_metric"}, {"source": "HyperVLA", "target": "number of activated parameters", "value": "uses_metric"}, {"source": "HyperVLA", "target": "inference speed", "value": "uses_metric"}, {"source": "Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models", "target": "TEAM-VLA", "value": "proposed_model"}, {"source": "TEAM-VLA", "target": "LIBERO", "value": "evaluated_on"}, {"source": "TEAM-VLA", "target": "inference speed", "value": "uses_metric"}, {"source": "TEAM-VLA", "target": "task success rate", "value": "uses_metric"}, {"source": "SegMamba-V2", "target": "SegMamba-V2", "value": "proposed_model"}, {"source": "SegMamba-V2", "target": "Transformer", "value": "baseline_model"}, {"source": "SegMamba-V2", "target": "Mamba", "value": "baseline_model"}, {"source": "SegMamba-V2", "target": "CRC-2000", "value": "evaluated_on"}, {"source": "SegMamba-V2", "target": "three other large-scale 3D medical image segmentation datasets", "value": "evaluated_on"}, {"source": "TransSIL: A Silhouette Cue-Aware Image Classification Framework for Bird Ecological Monitoring Systems", "target": "TransSIL", "value": "proposed_model"}, {"source": "TransSIL", "target": "CUB200-2011", "value": "evaluated_on"}, {"source": "TransSIL", "target": "NABirds", "value": "evaluated_on"}, {"source": "CBRFormer: rendering technology-based transformer for refinement segmentation of bridge crack images", "target": "CBRFormer", "value": "proposed_model"}, {"source": "Deep contrastive learning enables genome-wide virtual screening", "target": "DrugCLIP", "value": "proposed_model"}, {"source": "DrugCLIP", "target": "docking", "value": "baseline_model"}, {"source": "DrugCLIP", "target": "GenomeScreenDB", "value": "evaluated_on"}, {"source": "DrugCLIP", "target": "hit rate", "value": "uses_metric"}, {"source": "DrugCLIP", "target": "AlphaFold2", "value": "baseline_model"}, {"source": "LLaVA-based semantic feature modulation diffusion model for underwater image enhancement", "target": "LLaVA-based semantic feature modulation diffusion model", "value": "proposed_model"}, {"source": "LLaVA-based semantic feature modulation diffusion model", "target": "underwater image enhancement", "value": "evaluated_on"}, {"source": "3DGS-Drag: Dragging Gaussians for Intuitive Point-Based 3D Editing", "target": "3DGS-Drag", "value": "proposed_model"}, {"source": "3DGS-Drag", "target": "3D Gaussian Splatting", "value": "baseline_model"}, {"source": "3DGS-Drag", "target": "diffusion guidance", "value": "baseline_model"}, {"source": "3DGS-Drag", "target": "state-of-the-art performance", "value": "uses_metric"}, {"source": "Attention mechanisms in neural networks", "target": "autoregressive transformers", "value": "proposed_model"}, {"source": "Attention mechanisms in neural networks", "target": "bidirectional encoders for representation learning", "value": "proposed_model"}, {"source": "Attention mechanisms in neural networks", "target": "Vision Transformers", "value": "proposed_model"}, {"source": "Attention mechanisms in neural networks", "target": "cross-modal attention", "value": "proposed_model"}, {"source": "Attention mechanisms in neural networks", "target": "standard datasets", "value": "evaluated_on"}, {"source": "Attention mechanisms in neural networks", "target": "performance benchmarks", "value": "uses_metric"}, {"source": "Bidirectional Normalizing Flow: From Data to Noise and Back", "target": "BiFlow", "value": "proposed_model"}, {"source": "Bidirectional Normalizing Flow: From Data to Noise and Back", "target": "TARFlow", "value": "baseline_model"}, {"source": "BiFlow", "target": "ImageNet", "value": "evaluated_on"}, {"source": "BiFlow", "target": "generation quality", "value": "uses_metric"}, {"source": "BiFlow", "target": "sampling speed", "value": "uses_metric"}, {"source": "One-step Latent-free Image Generation with Pixel Mean Flows", "target": "pixel MeanFlow", "value": "proposed_model"}, {"source": "pixel MeanFlow", "target": "ImageNet", "value": "evaluated_on"}, {"source": "pixel MeanFlow", "target": "FID", "value": "uses_metric"}, {"source": "Meta Flow Maps enable scalable reward alignment", "target": "Meta Flow Maps (MFMs)", "value": "proposed_model"}, {"source": "Meta Flow Maps (MFMs)", "target": "Best-of-1000", "value": "baseline_model"}, {"source": "Meta Flow Maps (MFMs)", "target": "ImageNet", "value": "evaluated_on"}, {"source": "Best-of-1000", "target": "ImageNet", "value": "evaluated_on"}, {"source": "Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective", "target": "Sequential Flow Matching", "value": "proposed_model"}, {"source": "Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective", "target": "diffusion models", "value": "baseline_model"}, {"source": "Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective", "target": "flow-matching models", "value": "baseline_model"}, {"source": "Sequential Flow Matching", "target": "forecasting tasks", "value": "evaluated_on"}, {"source": "Sequential Flow Matching", "target": "decision-making tasks", "value": "evaluated_on"}, {"source": "Sequential Flow Matching", "target": "state estimation tasks", "value": "evaluated_on"}, {"source": "Sequential Flow Matching", "target": "performance", "value": "uses_metric"}, {"source": "Sequential Flow Matching", "target": "sampling speed", "value": "uses_metric"}, {"source": "Generative Modeling via Drifting", "target": "Drifting Models", "value": "proposed_model"}, {"source": "Drifting Models", "target": "ImageNet", "value": "evaluated_on"}, {"source": "Drifting Models", "target": "FID", "value": "uses_metric"}, {"source": "A Simple Framework for Contrastive Learning of Visual Representations", "target": "SimCLR", "value": "proposed_model"}, {"source": "SimCLR", "target": "ImageNet", "value": "evaluated_on"}, {"source": "SimCLR", "target": "top-1 accuracy", "value": "uses_metric"}, {"source": "SimCLR", "target": "top-5 accuracy", "value": "uses_metric"}, {"source": "SimCLR", "target": "ResNet-50", "value": "baseline_model"}, {"source": "SimCLR", "target": "AlexNet", "value": "baseline_model"}, {"source": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning", "target": "Directional Decoupling Alignment (D\u00b2-Align)", "value": "proposed_model"}, {"source": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning", "target": "DivGenBench", "value": "proposed_model"}, {"source": "Directional Decoupling Alignment (D\u00b2-Align)", "target": "DivGenBench", "value": "evaluated_on"}, {"source": "Directional Decoupling Alignment (D\u00b2-Align)", "target": "quantitative metrics for both quality and diversity", "value": "uses_metric"}, {"source": "existing methods", "target": "automated reward metrics", "value": "uses_metric"}, {"source": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints", "target": "ImagerySearch", "value": "proposed_model"}, {"source": "ImagerySearch", "target": "LDT-Bench", "value": "evaluated_on"}, {"source": "ImagerySearch", "target": "VBench", "value": "evaluated_on"}, {"source": "ImagerySearch", "target": "creative generation capabilities", "value": "uses_metric"}, {"source": "LDT-Bench", "target": "creative generation capabilities", "value": "uses_metric"}, {"source": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders", "target": "Representation Autoencoders (RAEs)", "value": "proposed_model"}, {"source": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders", "target": "FLUX VAE", "value": "baseline_model"}, {"source": "Representation Autoencoders (RAEs)", "target": "ImageNet", "value": "evaluated_on"}, {"source": "Representation Autoencoders (RAEs)", "target": "web, synthetic, and text-rendering data", "value": "evaluated_on"}, {"source": "Representation Autoencoders (RAEs)", "target": "high-quality datasets", "value": "evaluated_on"}, {"source": "FLUX VAE", "target": "high-quality datasets", "value": "evaluated_on"}, {"source": "Representation Autoencoders (RAEs)", "target": "general fidelity", "value": "uses_metric"}, {"source": "Representation Autoencoders (RAEs)", "target": "generation quality", "value": "uses_metric"}, {"source": "Representation Autoencoders (RAEs)", "target": "performance", "value": "uses_metric"}, {"source": "FLUX VAE", "target": "performance", "value": "uses_metric"}, {"source": "Simulating the Visual World with Artificial Intelligence: A Roadmap", "target": "video foundation models", "value": "proposed_model"}, {"source": "video foundation models", "target": "implicit world model", "value": "baseline_model"}, {"source": "video foundation models", "target": "video renderer", "value": "baseline_model"}, {"source": "world model", "target": "video generation model", "value": "baseline_model"}, {"source": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks", "target": "World models", "value": "proposed_model"}, {"source": "World models", "target": "visual prediction", "value": "evaluated_on"}, {"source": "World models", "target": "3D estimation", "value": "evaluated_on"}, {"source": "World models", "target": "symbol grounding", "value": "evaluated_on"}, {"source": "RecTok: Reconstruction Distillation along Rectified Flow", "target": "RecTok", "value": "proposed_model"}, {"source": "RecTok", "target": "visual tokenizers", "value": "baseline_model"}, {"source": "RecTok", "target": "gFID-50K", "value": "evaluated_on"}, {"source": "RecTok", "target": "gFID-50K", "value": "uses_metric"}, {"source": "RePack then Refine", "target": "Repack then Refine", "value": "proposed_model"}, {"source": "Repack then Refine", "target": "Latent Diffusion Models", "value": "baseline_model"}, {"source": "Repack then Refine", "target": "ImageNet-1K", "value": "evaluated_on"}, {"source": "Repack then Refine", "target": "FID", "value": "uses_metric"}, {"source": "RePack-DiT-XL/1", "target": "ImageNet-1K", "value": "evaluated_on"}, {"source": "RePack-DiT-XL/1", "target": "FID", "value": "uses_metric"}, {"source": "Diffusion Models Beat GANs on Image Synthesis", "target": "diffusion models", "value": "proposed_model"}, {"source": "Diffusion Models Beat GANs on Image Synthesis", "target": "BigGAN-deep", "value": "baseline_model"}, {"source": "diffusion models", "target": "ImageNet 128\u00d7128", "value": "evaluated_on"}, {"source": "diffusion models", "target": "ImageNet 256\u00d7256", "value": "evaluated_on"}, {"source": "diffusion models", "target": "ImageNet 512\u00d7512", "value": "evaluated_on"}, {"source": "diffusion models", "target": "FID", "value": "uses_metric"}, {"source": "DINOv2: Learning Robust Visual Features without Supervision", "target": "ViT model (Dosovitskiy et al., 2020)", "value": "proposed_model"}, {"source": "DINOv2: Learning Robust Visual Features without Supervision", "target": "OpenCLIP (Ilharco et al., 2021)", "value": "baseline_model"}, {"source": "DINOv2: Learning Robust Visual Features without Supervision", "target": "automatic pipeline to build a dedicated, diverse, and curated image dataset", "value": "evaluated_on"}, {"source": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss", "target": "PixelGen", "value": "proposed_model"}, {"source": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss", "target": "latent diffusion models", "value": "baseline_model"}, {"source": "PixelGen", "target": "ImageNet-256", "value": "evaluated_on"}, {"source": "PixelGen", "target": "FID", "value": "uses_metric"}, {"source": "PixelGen", "target": "GenEval", "value": "uses_metric"}, {"source": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities", "target": "GPT-4o", "value": "proposed_model"}, {"source": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities", "target": "diffusion-based models", "value": "baseline_model"}, {"source": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities", "target": "autoregressive-based architectures", "value": "baseline_model"}, {"source": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities", "target": "hybrid approaches", "value": "proposed_model"}, {"source": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities", "target": "datasets and benchmarks", "value": "evaluated_on"}, {"source": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery", "target": "MentisOculi", "value": "proposed_model"}, {"source": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery", "target": "MentisOculi", "value": "evaluated_on"}, {"source": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery", "target": "multimodal large language models (MLLMs)", "value": "baseline_model"}, {"source": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery", "target": "unified multimodal models (UMMs)", "value": "baseline_model"}, {"source": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery", "target": "visual strategies", "value": "baseline_model"}, {"source": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery", "target": "performance", "value": "uses_metric"}, {"source": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation", "target": "OpenVision 3", "value": "proposed_model"}, {"source": "OpenVision 3", "target": "CLIP vision encoder", "value": "baseline_model"}, {"source": "OpenVision 3", "target": "SeedBench", "value": "evaluated_on"}, {"source": "OpenVision 3", "target": "POPE", "value": "evaluated_on"}, {"source": "OpenVision 3", "target": "ImageNet", "value": "evaluated_on"}, {"source": "OpenVision 3", "target": "gFID", "value": "uses_metric"}, {"source": "Distributed Representations of Words and Phrases and their Compositionality", "target": "continuous Skip-gram model", "value": "proposed_model"}, {"source": "Distributed Representations of Words and Phrases and their Compositionality", "target": "negative sampling", "value": "proposed_model"}, {"source": "Distributed Representations of Words and Phrases and their Compositionality", "target": "hierarchical softmax", "value": "baseline_model"}, {"source": "GloVe: Global Vectors for Word Representation", "target": "global logbilinear regression model", "value": "proposed_model"}, {"source": "global logbilinear regression model", "target": "global matrix factorization", "value": "baseline_model"}, {"source": "global logbilinear regression model", "target": "local context window methods", "value": "baseline_model"}, {"source": "global logbilinear regression model", "target": "word analogy task", "value": "evaluated_on"}, {"source": "global logbilinear regression model", "target": "similarity tasks", "value": "evaluated_on"}, {"source": "global logbilinear regression model", "target": "named entity recognition", "value": "evaluated_on"}, {"source": "global logbilinear regression model", "target": "75%", "value": "uses_metric"}, {"source": "Deep Contextualized Word Representations", "target": "deep bidirectional language model (biLM)", "value": "proposed_model"}, {"source": "deep bidirectional language model (biLM)", "target": "large text corpus", "value": "evaluated_on"}, {"source": "Deep Contextualized Word Representations", "target": "question answering", "value": "uses_metric"}, {"source": "Deep Contextualized Word Representations", "target": "textual entailment", "value": "uses_metric"}, {"source": "Deep Contextualized Word Representations", "target": "sentiment analysis", "value": "uses_metric"}, {"source": "Protein Language Models: Is Scaling Necessary?", "target": "Protein Language Models", "value": "proposed_model"}, {"source": "Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis", "target": "Modern models for common NLP tasks", "value": "proposed_model"}, {"source": "Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis", "target": "2018 Twitter data spanning 51 U.S. regions and 99 countries", "value": "evaluated_on"}, {"source": "Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis", "target": "gender bias in word embeddings", "value": "uses_metric"}, {"source": "Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis", "target": "statistical gender gaps in education, politics, economics, and health", "value": "uses_metric"}, {"source": "Generative Classifiers Avoid Shortcut Solutions", "target": "generative classifiers", "value": "proposed_model"}, {"source": "Generative Classifiers Avoid Shortcut Solutions", "target": "five standard image and text distribution shift benchmarks", "value": "evaluated_on"}, {"source": "Generative Classifiers Avoid Shortcut Solutions", "target": "medical datasets", "value": "evaluated_on"}, {"source": "Generative Classifiers Avoid Shortcut Solutions", "target": "satellite datasets", "value": "evaluated_on"}, {"source": "Generative Classifiers Avoid Shortcut Solutions", "target": "Gaussian toy setting", "value": "evaluated_on"}, {"source": "Generative Classifiers Avoid Shortcut Solutions", "target": "state-of-the-art performance", "value": "uses_metric"}, {"source": "Generative Classifiers Avoid Shortcut Solutions", "target": "impact of spurious correlations", "value": "uses_metric"}, {"source": "generative classifiers", "target": "diffusion-based generative classifiers", "value": "baseline_model"}, {"source": "generative classifiers", "target": "autoregressive generative classifiers", "value": "baseline_model"}, {"source": "Language Models are Unsupervised Multitask Learners", "target": "GPT-2", "value": "proposed_model"}, {"source": "GPT-2", "target": "WebText", "value": "evaluated_on"}, {"source": "The Llama 3 Herd of Models", "target": "Llama 3", "value": "proposed_model"}, {"source": "The Llama 3 Herd of Models", "target": "GPT-4", "value": "baseline_model"}, {"source": "A Survey on Diffusion Language Models", "target": "Diffusion Language Models (DLMs)", "value": "proposed_model"}, {"source": "Diffusion Language Models (DLMs)", "target": "autoregressive (AR) paradigm", "value": "baseline_model"}, {"source": "Diffusion Language Models (DLMs)", "target": "masked language models", "value": "baseline_model"}, {"source": "Training Optimal Large Diffusion Language Models", "target": "Quokka", "value": "proposed_model"}, {"source": "Quokka", "target": "Chinchilla", "value": "baseline_model"}, {"source": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models", "target": "IGPO", "value": "proposed_model"}, {"source": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models", "target": "GRPO", "value": "baseline_model"}, {"source": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models", "target": "GSM8K", "value": "evaluated_on"}, {"source": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models", "target": "Math500", "value": "evaluated_on"}, {"source": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models", "target": "AMC", "value": "evaluated_on"}, {"source": "IGPO", "target": "GSM8K", "value": "uses_metric"}, {"source": "IGPO", "target": "Math500", "value": "uses_metric"}, {"source": "IGPO", "target": "AMC", "value": "uses_metric"}, {"source": "Encoder-Decoder Diffusion Language Models for Efficient Training and Inference", "target": "E2D2", "value": "proposed_model"}, {"source": "E2D2", "target": "summarization", "value": "evaluated_on"}, {"source": "E2D2", "target": "translation", "value": "evaluated_on"}, {"source": "E2D2", "target": "mathematical reasoning", "value": "evaluated_on"}, {"source": "E2D2", "target": "generation quality", "value": "uses_metric"}, {"source": "E2D2", "target": "inference throughput", "value": "uses_metric"}, {"source": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model", "target": "Stable-DiffCoder", "value": "proposed_model"}, {"source": "Stable-DiffCoder", "target": "autoregressive (AR) models", "value": "baseline_model"}, {"source": "Stable-DiffCoder", "target": "Diffusion-based language models (DLLMs)", "value": "baseline_model"}, {"source": "Stable-DiffCoder", "target": "code benchmarks", "value": "evaluated_on"}, {"source": "Stable-DiffCoder", "target": "low-resource coding languages", "value": "evaluated_on"}, {"source": "Stable-DiffCoder", "target": "performance", "value": "uses_metric"}, {"source": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "target": "XLNet", "value": "proposed_model"}, {"source": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "target": "BERT", "value": "baseline_model"}, {"source": "XLNet", "target": "question answering", "value": "evaluated_on"}, {"source": "XLNet", "target": "sentiment analysis", "value": "evaluated_on"}, {"source": "Qwen Technical Report", "target": "Qwen", "value": "proposed_model"}, {"source": "Qwen Technical Report", "target": "Qwen-Chat", "value": "proposed_model"}, {"source": "Qwen Technical Report", "target": "Code-Qwen", "value": "proposed_model"}, {"source": "Qwen Technical Report", "target": "Code-Qwen-Chat", "value": "proposed_model"}, {"source": "Qwen Technical Report", "target": "Math-Qwen-Chat", "value": "proposed_model"}, {"source": "Qwen-Chat", "target": "Reinforcement Learning from Human Feedback (RLHF)", "value": "baseline_model"}, {"source": "Qwen", "target": "downstream tasks", "value": "evaluated_on"}, {"source": "Qwen-Chat", "target": "code interpreter", "value": "evaluated_on"}, {"source": "Qwen", "target": "performance", "value": "uses_metric"}, {"source": "Qwen-Chat", "target": "performance", "value": "uses_metric"}, {"source": "Code-Qwen", "target": "performance", "value": "uses_metric"}, {"source": "Code-Qwen-Chat", "target": "performance", "value": "uses_metric"}, {"source": "Math-Qwen-Chat", "target": "performance", "value": "uses_metric"}, {"source": "Code Llama", "target": "Code Llama", "value": "proposed_model"}, {"source": "Code Llama", "target": "Code Llama - Python", "value": "proposed_model"}, {"source": "Code Llama", "target": "Code Llama - Instruct", "value": "proposed_model"}, {"source": "Code Llama", "target": "Llama 2", "value": "baseline_model"}, {"source": "Code Llama", "target": "HumanEval", "value": "evaluated_on"}, {"source": "Code Llama", "target": "MBPP", "value": "evaluated_on"}, {"source": "Code Llama", "target": "MultiPL-E", "value": "evaluated_on"}, {"source": "Code Llama - Python", "target": "HumanEval", "value": "evaluated_on"}, {"source": "Code Llama - Python", "target": "MBPP", "value": "evaluated_on"}, {"source": "Code Llama", "target": "HumanEval", "value": "uses_metric"}, {"source": "Code Llama", "target": "MBPP", "value": "uses_metric"}, {"source": "Code Llama", "target": "MultiPL-E", "value": "uses_metric"}, {"source": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way", "target": "dLLM-Var", "value": "proposed_model"}, {"source": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way", "target": "dLLMs", "value": "baseline_model"}, {"source": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way", "target": "autoregressive models", "value": "baseline_model"}, {"source": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way", "target": "Qwen", "value": "baseline_model"}, {"source": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way", "target": "Llama", "value": "baseline_model"}, {"source": "dLLM-Var", "target": "standard benchmarks", "value": "evaluated_on"}, {"source": "dLLM-Var", "target": "speedup", "value": "uses_metric"}, {"source": "dLLM-Var", "target": "accuracy", "value": "uses_metric"}, {"source": "Set Block Decoding is a Language Model Inference Accelerator", "target": "Set Block Decoding (SBD)", "value": "proposed_model"}, {"source": "Set Block Decoding (SBD)", "target": "next token prediction (NTP)", "value": "baseline_model"}, {"source": "Set Block Decoding (SBD)", "target": "discrete diffusion literature", "value": "evaluated_on"}, {"source": "Set Block Decoding (SBD)", "target": "performance", "value": "uses_metric"}, {"source": "Set Block Decoding (SBD)", "target": "accuracy", "value": "uses_metric"}, {"source": "Set Block Decoding (SBD)", "target": "speedups", "value": "uses_metric"}, {"source": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows", "target": "OneFlow", "value": "proposed_model"}, {"source": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows", "target": "autoregressive baselines", "value": "baseline_model"}, {"source": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows", "target": "autoregressive models", "value": "baseline_model"}, {"source": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows", "target": "diffusion-based approaches", "value": "baseline_model"}, {"source": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows", "target": "generation and understanding tasks", "value": "evaluated_on"}, {"source": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows", "target": "training FLOPs", "value": "uses_metric"}, {"source": "Constrained Decoding of Diffusion LLMs with Context-Free Grammars", "target": "constrained decoding method for diffusion models", "value": "proposed_model"}, {"source": "Constrained Decoding of Diffusion LLMs with Context-Free Grammars", "target": "Large language models (LLMs)", "value": "baseline_model"}, {"source": "Constrained Decoding of Diffusion LLMs with Context-Free Grammars", "target": "C++ code infilling", "value": "evaluated_on"}, {"source": "Constrained Decoding of Diffusion LLMs with Context-Free Grammars", "target": "structured data extraction in JSON", "value": "evaluated_on"}, {"source": "Constrained Decoding of Diffusion LLMs with Context-Free Grammars", "target": "syntactic correctness", "value": "uses_metric"}, {"source": "Constrained Decoding of Diffusion LLMs with Context-Free Grammars", "target": "functional correctness", "value": "uses_metric"}, {"source": "Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models", "target": "mask-agnostic loss function", "value": "proposed_model"}, {"source": "Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models", "target": "Masked Diffusion Language Models (MDLMs)", "value": "baseline_model"}, {"source": "Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models", "target": "Autoregressive Language Models (ARLMs)", "value": "baseline_model"}, {"source": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications", "target": "Large Language Model (LLM)-based agents", "value": "proposed_model"}, {"source": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications", "target": "open-sourced libraries and benchmarks", "value": "evaluated_on"}, {"source": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications", "target": "self-evolving agent memory", "value": "uses_metric"}, {"source": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search", "target": "Empirical-MCTS", "value": "proposed_model"}, {"source": "Empirical-MCTS", "target": "Monte Carlo Tree Search (MCTS)", "value": "baseline_model"}, {"source": "Empirical-MCTS", "target": "AIME25", "value": "evaluated_on"}, {"source": "Empirical-MCTS", "target": "ARC-AGI-2", "value": "evaluated_on"}, {"source": "Empirical-MCTS", "target": "MathArena Apex", "value": "evaluated_on"}, {"source": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking", "target": "TAME", "value": "proposed_model"}, {"source": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking", "target": "Trust-Memevo", "value": "evaluated_on"}, {"source": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking", "target": "trustworthiness", "value": "uses_metric"}, {"source": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking", "target": "task performance", "value": "uses_metric"}, {"source": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents", "target": "ProcMEM", "value": "proposed_model"}, {"source": "ProcMEM", "target": "Non-Parametric PPO", "value": "baseline_model"}, {"source": "ProcMEM", "target": "in-domain, cross-task, and cross-agent scenarios", "value": "evaluated_on"}, {"source": "ProcMEM", "target": "reuse rates", "value": "uses_metric"}, {"source": "ProcMEM", "target": "performance gains", "value": "uses_metric"}, {"source": "ProcMEM", "target": "memory compression", "value": "uses_metric"}, {"source": "Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting", "target": "agentic time series forecasting (ATSF)", "value": "proposed_model"}, {"source": "agentic time series forecasting (ATSF)", "target": "workflow-based design", "value": "baseline_model"}, {"source": "agentic time series forecasting (ATSF)", "target": "agentic reinforcement learning", "value": "baseline_model"}, {"source": "agentic time series forecasting (ATSF)", "target": "hybrid agentic workflow paradigm", "value": "baseline_model"}, {"source": "Follow-Your-Emoji-Faster", "target": "Stable Diffusion", "value": "proposed_model"}, {"source": "Follow-Your-Emoji-Faster", "target": "EmojiBench++", "value": "evaluated_on"}, {"source": "Follow-Your-Emoji-Faster", "target": "animation quality", "value": "uses_metric"}, {"source": "Follow-Your-Emoji-Faster", "target": "controllability", "value": "uses_metric"}, {"source": "Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis", "target": "Follow-Your-Instruction", "value": "proposed_model"}, {"source": "Follow-Your-Instruction", "target": "existing baseline models", "value": "baseline_model"}, {"source": "Follow-Your-Instruction", "target": "2D, 3D, and 4D generative tasks", "value": "evaluated_on"}, {"source": "Follow-Your-Instruction", "target": "performance", "value": "uses_metric"}, {"source": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing", "target": "HunyuanVideoT2V", "value": "proposed_model"}, {"source": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing", "target": "approximately 1M real video clips", "value": "evaluated_on"}, {"source": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing", "target": "fewer than 150k curated editing pairs", "value": "evaluated_on"}, {"source": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing", "target": "editing instruction following", "value": "uses_metric"}, {"source": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing", "target": "editing quality", "value": "uses_metric"}, {"source": "Paper2Video: Automatic Video Generation from Scientific Papers", "target": "PaperTalker", "value": "proposed_model"}, {"source": "Paper2Video: Automatic Video Generation from Scientific Papers", "target": "Paper2Video", "value": "evaluated_on"}, {"source": "Paper2Video: Automatic Video Generation from Scientific Papers", "target": "Meta Similarity", "value": "uses_metric"}, {"source": "Paper2Video: Automatic Video Generation from Scientific Papers", "target": "PresentArena", "value": "uses_metric"}, {"source": "Paper2Video: Automatic Video Generation from Scientific Papers", "target": "PresentQuiz", "value": "uses_metric"}, {"source": "Paper2Video: Automatic Video Generation from Scientific Papers", "target": "IP Memory", "value": "uses_metric"}, {"source": "ContextFlow", "target": "ContextFlow", "value": "proposed_model"}, {"source": "ContextFlow", "target": "U-Net", "value": "baseline_model"}, {"source": "ContextFlow", "target": "Diffusion Transformers (DiTs)", "value": "baseline_model"}, {"source": "ContextFlow", "target": "Guidance Responsiveness Metric", "value": "uses_metric"}, {"source": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "target": "DeepSeek-R1", "value": "proposed_model"}, {"source": "DeepSeek-R1", "target": "conventional supervised learning on human demonstrations", "value": "baseline_model"}, {"source": "DeepSeek-R1", "target": "mathematics", "value": "evaluated_on"}, {"source": "DeepSeek-R1", "target": "coding competitions", "value": "evaluated_on"}, {"source": "DeepSeek-R1", "target": "STEM fields", "value": "evaluated_on"}, {"source": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "target": "phi-3-mini", "value": "proposed_model"}, {"source": "phi-3-mini", "target": "Mixtral 8x7B", "value": "baseline_model"}, {"source": "phi-3-mini", "target": "GPT-3.5", "value": "baseline_model"}, {"source": "phi-3-mini", "target": "MMLU", "value": "evaluated_on"}, {"source": "phi-3-mini", "target": "MT-bench", "value": "evaluated_on"}, {"source": "phi-3-mini", "target": "MMLU", "value": "uses_metric"}, {"source": "phi-3-mini", "target": "MT-bench", "value": "uses_metric"}, {"source": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "target": "phi-3-small", "value": "proposed_model"}, {"source": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "target": "phi-3-medium", "value": "proposed_model"}, {"source": "phi-3-small", "target": "MMLU", "value": "evaluated_on"}, {"source": "phi-3-small", "target": "MT-bench", "value": "evaluated_on"}, {"source": "phi-3-small", "target": "MMLU", "value": "uses_metric"}, {"source": "phi-3-small", "target": "MT-bench", "value": "uses_metric"}, {"source": "phi-3-medium", "target": "MMLU", "value": "evaluated_on"}, {"source": "phi-3-medium", "target": "MT-bench", "value": "evaluated_on"}, {"source": "phi-3-medium", "target": "MMLU", "value": "uses_metric"}, {"source": "phi-3-medium", "target": "MT-bench", "value": "uses_metric"}, {"source": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "target": "phi-3.5-mini", "value": "proposed_model"}, {"source": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "target": "phi-3.5-MoE", "value": "proposed_model"}, {"source": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "target": "phi-3.5-Vision", "value": "proposed_model"}, {"source": "phi-3.5-MoE", "target": "Llama 3.1", "value": "baseline_model"}, {"source": "phi-3.5-MoE", "target": "Mixtral series", "value": "baseline_model"}, {"source": "phi-3.5-MoE", "target": "Gemini-1.5-Flash", "value": "baseline_model"}, {"source": "phi-3.5-MoE", "target": "GPT-4o-mini", "value": "baseline_model"}, {"source": "LLaVA-OneVision", "target": "LLaVA-NeXT blog series", "value": "proposed_model"}, {"source": "LLaVA-OneVision", "target": "single-image scenario", "value": "evaluated_on"}, {"source": "LLaVA-OneVision", "target": "multi-image scenario", "value": "evaluated_on"}, {"source": "LLaVA-OneVision", "target": "video scenario", "value": "evaluated_on"}, {"source": "Spatial Reasoning in Multimodal Large Language Models: A Survey of Tasks, Benchmarks and Methods", "target": "Multimodal large language models (MLLMs)", "value": "proposed_model"}, {"source": "Spatial Reasoning in Multimodal Large Language Models: A Survey of Tasks, Benchmarks and Methods", "target": "existing benchmarks across text only, vision language, and embodied settings", "value": "evaluated_on"}, {"source": "Spatial Reasoning in Multimodal Large Language Models: A Survey of Tasks, Benchmarks and Methods", "target": "evaluation metrics and methodologies for assessing spatial reasoning ability", "value": "uses_metric"}, {"source": "SpatialTree: How Spatial Abilities Branch Out in MLLMs", "target": "SpatialTree", "value": "proposed_model"}, {"source": "SpatialTree: How Spatial Abilities Branch Out in MLLMs", "target": "mainstream MLLMs", "value": "baseline_model"}, {"source": "SpatialTree: How Spatial Abilities Branch Out in MLLMs", "target": "capability-centric hierarchical benchmark", "value": "evaluated_on"}, {"source": "SpatialTree: How Spatial Abilities Branch Out in MLLMs", "target": "27 sub-abilities", "value": "uses_metric"}, {"source": "From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs", "target": "Multimodal Large Language Models (MLLMs)", "value": "proposed_model"}, {"source": "From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs", "target": "large-scale benchmark built from pedestrian-perspective videos captured with synchronized stereo cameras, LiDAR, and IMU/GPS sensors", "value": "evaluated_on"}, {"source": "From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs", "target": "spatial reasoning questions", "value": "uses_metric"}, {"source": "SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery", "target": "SpatialDreamer", "value": "proposed_model"}, {"source": "SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery", "target": "Geometric Policy Optimization (GeoPO)", "value": "proposed_model"}, {"source": "SpatialDreamer", "target": "Multi-modal Large Language Models (MLLMs)", "value": "baseline_model"}, {"source": "SpatialDreamer", "target": "multiple challenging benchmarks", "value": "evaluated_on"}, {"source": "SpatialDreamer", "target": "highly competitive results", "value": "uses_metric"}, {"source": "You Only Look Once: Unified, Real-Time Object Detection", "target": "YOLO", "value": "proposed_model"}, {"source": "You Only Look Once: Unified, Real-Time Object Detection", "target": "Fast YOLO", "value": "proposed_model"}, {"source": "YOLO", "target": "DPM", "value": "baseline_model"}, {"source": "YOLO", "target": "R-CNN", "value": "baseline_model"}, {"source": "YOLO", "target": "Picasso Dataset", "value": "evaluated_on"}, {"source": "YOLO", "target": "People-Art Dataset", "value": "evaluated_on"}, {"source": "YOLO", "target": "mAP", "value": "uses_metric"}, {"source": "Fast YOLO", "target": "mAP", "value": "uses_metric"}, {"source": "RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes", "target": "Number GroundingDINO (NGDINO)", "value": "proposed_model"}, {"source": "RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes", "target": "RDAgent (referring drone annotation framework with multi-agent system)", "value": "proposed_model"}, {"source": "RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes", "target": "RefDrone", "value": "evaluated_on"}, {"source": "RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes", "target": "gRefCOCO", "value": "evaluated_on"}, {"source": "Number GroundingDINO (NGDINO)", "target": "state-of-the-art REC methods", "value": "baseline_model"}, {"source": "WeDetect: Fast Open-Vocabulary Object Detection as Retrieval", "target": "WeDetect", "value": "proposed_model"}, {"source": "WeDetect: Fast Open-Vocabulary Object Detection as Retrieval", "target": "WeDetect-Uni", "value": "proposed_model"}, {"source": "WeDetect: Fast Open-Vocabulary Object Detection as Retrieval", "target": "WeDetect-Ref", "value": "proposed_model"}, {"source": "WeDetect", "target": "15 benchmarks", "value": "evaluated_on"}, {"source": "WeDetect-Uni", "target": "15 benchmarks", "value": "evaluated_on"}, {"source": "WeDetect-Ref", "target": "15 benchmarks", "value": "evaluated_on"}, {"source": "v1: Learning to Point Visual Tokens for Multimodal Grounded Reasoning", "target": "v1", "value": "proposed_model"}, {"source": "v1", "target": "multimodal mathematical reasoning benchmarks", "value": "evaluated_on"}, {"source": "v1", "target": "multimodal mathematical reasoning benchmarks", "value": "uses_metric"}, {"source": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design", "target": "PosterCopilot", "value": "proposed_model"}, {"source": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design", "target": "existing methods", "value": "baseline_model"}, {"source": "PosterCopilot", "target": "controllability", "value": "uses_metric"}, {"source": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving", "target": "Percept-WAM", "value": "proposed_model"}, {"source": "Percept-WAM", "target": "COCO", "value": "evaluated_on"}, {"source": "Percept-WAM", "target": "nuScenes", "value": "evaluated_on"}, {"source": "Percept-WAM", "target": "NAVSIM", "value": "evaluated_on"}, {"source": "Percept-WAM", "target": "mAP", "value": "uses_metric"}, {"source": "Percept-WAM", "target": "PMDS", "value": "uses_metric"}, {"source": "Percept-WAM", "target": "classical detectors and segmenters", "value": "baseline_model"}, {"source": "Percept-WAM", "target": "DiffusionDrive", "value": "baseline_model"}, {"source": "Continuous control with deep reinforcement learning", "target": "actor-critic, model-free algorithm based on the deterministic policy gradient", "value": "proposed_model"}, {"source": "Continuous control with deep reinforcement learning", "target": "Deep Q-Learning", "value": "baseline_model"}, {"source": "Continuous control with deep reinforcement learning", "target": "simulated physics tasks", "value": "evaluated_on"}, {"source": "Continuous control with deep reinforcement learning", "target": "cartpole swing-up", "value": "evaluated_on"}, {"source": "Continuous control with deep reinforcement learning", "target": "dexterous manipulation", "value": "evaluated_on"}, {"source": "Continuous control with deep reinforcement learning", "target": "legged locomotion", "value": "evaluated_on"}, {"source": "Continuous control with deep reinforcement learning", "target": "car driving", "value": "evaluated_on"}, {"source": "actor-critic, model-free algorithm based on the deterministic policy gradient", "target": "performance", "value": "uses_metric"}, {"source": "planning algorithm with full access to the dynamics of the domain and its derivatives", "target": "performance", "value": "uses_metric"}, {"source": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning", "target": "Reinforcement Learning with Verifiable Rewards (RLVR)", "value": "proposed_model"}, {"source": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning", "target": "Qwen3-8B", "value": "baseline_model"}, {"source": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning", "target": "Qwen3-32B", "value": "baseline_model"}, {"source": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning", "target": "Qwen3-14B", "value": "baseline_model"}, {"source": "Reinforcement Learning with Verifiable Rewards (RLVR)", "target": "AIME'25", "value": "evaluated_on"}, {"source": "Reinforcement Learning with Verifiable Rewards (RLVR)", "target": "AIME'24", "value": "evaluated_on"}, {"source": "Reinforcement Learning with Verifiable Rewards (RLVR)", "target": "performance", "value": "uses_metric"}, {"source": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models", "target": "Clip-Cov", "value": "proposed_model"}, {"source": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models", "target": "KL-Cov", "value": "proposed_model"}, {"source": "Clip-Cov", "target": "covariance", "value": "uses_metric"}, {"source": "KL-Cov", "target": "covariance", "value": "uses_metric"}, {"source": "Policy Gradient-like algorithms", "target": "entropy", "value": "uses_metric"}, {"source": "Learning to Reason under Off-Policy Guidance", "target": "LUFFY", "value": "proposed_model"}, {"source": "LUFFY", "target": "RLVR", "value": "baseline_model"}, {"source": "LUFFY", "target": "six math benchmarks", "value": "evaluated_on"}, {"source": "LUFFY", "target": "out-of-distribution tasks", "value": "evaluated_on"}, {"source": "LUFFY", "target": "average gain", "value": "uses_metric"}, {"source": "LUFFY", "target": "advantage", "value": "uses_metric"}, {"source": "LUFFY", "target": "Mixed-Policy GRPO", "value": "baseline_model"}, {"source": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models", "target": "Large Language Models (LLMs)", "value": "proposed_model"}, {"source": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models", "target": "Fine-tuning", "value": "baseline_model"}, {"source": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models", "target": "Reinforcement learning", "value": "baseline_model"}, {"source": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models", "target": "Test-time scaling", "value": "baseline_model"}, {"source": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models", "target": "Pretraining", "value": "evaluated_on"}, {"source": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models", "target": "Post-training", "value": "evaluated_on"}, {"source": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models", "target": "Catastrophic forgetting", "value": "uses_metric"}, {"source": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models", "target": "Reward hacking", "value": "uses_metric"}, {"source": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models", "target": "Inference-time trade-offs", "value": "uses_metric"}, {"source": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement learning for LLMs", "target": "Tapered Off-Policy REINFORCE (TOPR)", "value": "proposed_model"}, {"source": "Tapered Off-Policy REINFORCE (TOPR)", "target": "GSM8K", "value": "evaluated_on"}, {"source": "Tapered Off-Policy REINFORCE (TOPR)", "target": "MATH", "value": "evaluated_on"}, {"source": "Tapered Off-Policy REINFORCE (TOPR)", "target": "test-time accuracy", "value": "uses_metric"}, {"source": "Tapered Off-Policy REINFORCE (TOPR)", "target": "training data efficiency", "value": "uses_metric"}, {"source": "Tapered Off-Policy REINFORCE (TOPR)", "target": "REINFORCE", "value": "baseline_model"}, {"source": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation", "target": "RefineNet", "value": "proposed_model"}, {"source": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation", "target": "PASCAL VOC 2012", "value": "evaluated_on"}, {"source": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation", "target": "intersection-over-union", "value": "uses_metric"}, {"source": "Fully convolutional networks for semantic segmentation", "target": "Fully convolutional network", "value": "proposed_model"}, {"source": "Fully convolutional network", "target": "AlexNet", "value": "baseline_model"}, {"source": "Fully convolutional network", "target": "VGG net", "value": "baseline_model"}, {"source": "Fully convolutional network", "target": "GoogLeNet", "value": "baseline_model"}, {"source": "Fully convolutional network", "target": "PASCAL VOC", "value": "evaluated_on"}, {"source": "Fully convolutional network", "target": "NYUDv2", "value": "evaluated_on"}, {"source": "Fully convolutional network", "target": "SIFT Flow", "value": "evaluated_on"}, {"source": "Fully convolutional network", "target": "mean IU", "value": "uses_metric"}, {"source": "Diffusion Models in Vision: A Survey", "target": "denoising diffusion probabilistic models", "value": "proposed_model"}, {"source": "Diffusion Models in Vision: A Survey", "target": "noise conditioned score networks", "value": "proposed_model"}, {"source": "Diffusion Models in Vision: A Survey", "target": "stochastic differential equations", "value": "proposed_model"}, {"source": "Diffusion Models in Vision: A Survey", "target": "variational auto-encoders", "value": "baseline_model"}, {"source": "Diffusion Models in Vision: A Survey", "target": "generative adversarial networks", "value": "baseline_model"}, {"source": "Diffusion Models in Vision: A Survey", "target": "energy-based models", "value": "baseline_model"}, {"source": "Diffusion Models in Vision: A Survey", "target": "autoregressive models", "value": "baseline_model"}, {"source": "Diffusion Models in Vision: A Survey", "target": "normalizing flows", "value": "baseline_model"}, {"source": "SegNeXt", "target": "SegNeXt", "value": "proposed_model"}, {"source": "SegNeXt", "target": "EfficientNet-L2 w/ NAS-FPN", "value": "baseline_model"}, {"source": "SegNeXt", "target": "ADE20K", "value": "evaluated_on"}, {"source": "SegNeXt", "target": "Cityscapes", "value": "evaluated_on"}, {"source": "SegNeXt", "target": "COCO-Stuff", "value": "evaluated_on"}, {"source": "SegNeXt", "target": "Pascal VOC", "value": "evaluated_on"}, {"source": "SegNeXt", "target": "Pascal Context", "value": "evaluated_on"}, {"source": "SegNeXt", "target": "iSAID", "value": "evaluated_on"}, {"source": "SegNeXt", "target": "mIoU", "value": "uses_metric"}, {"source": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation With Transformers", "target": "CMX", "value": "proposed_model"}, {"source": "CMX", "target": "Cross-Modal Feature Rectification Module (CM-FRM)", "value": "proposed_model"}, {"source": "CMX", "target": "Feature Fusion Module (FFM)", "value": "proposed_model"}, {"source": "CMX", "target": "depth", "value": "evaluated_on"}, {"source": "CMX", "target": "thermal", "value": "evaluated_on"}, {"source": "CMX", "target": "polarization", "value": "evaluated_on"}, {"source": "CMX", "target": "event", "value": "evaluated_on"}, {"source": "CMX", "target": "LiDAR", "value": "evaluated_on"}, {"source": "CMX", "target": "RGB-Depth benchmarks", "value": "evaluated_on"}, {"source": "CMX", "target": "RGB-Thermal", "value": "evaluated_on"}, {"source": "CMX", "target": "RGB-Polarization", "value": "evaluated_on"}, {"source": "CMX", "target": "RGB-LiDAR", "value": "evaluated_on"}, {"source": "CMX", "target": "EventScape dataset", "value": "evaluated_on"}, {"source": "CMX", "target": "RGB-Event semantic segmentation benchmark", "value": "evaluated_on"}, {"source": "CMX", "target": "state-of-the-art performances", "value": "uses_metric"}, {"source": "Segment Anything in High Quality", "target": "HQ-SAM", "value": "proposed_model"}, {"source": "HQ-SAM", "target": "Segment Anything Model (SAM)", "value": "baseline_model"}, {"source": "HQ-SAM", "target": "suite of 10 diverse segmentation datasets", "value": "evaluated_on"}, {"source": "HQ-SAM", "target": "dataset of 44K fine-grained masks", "value": "evaluated_on"}, {"source": "PIDNet: A Real-time Semantic Segmentation Network Inspired by PID Controllers", "target": "PIDNet", "value": "proposed_model"}, {"source": "PIDNet: A Real-time Semantic Segmentation Network Inspired by PID Controllers", "target": "PIDNet-S", "value": "proposed_model"}, {"source": "PIDNet", "target": "Cityscapes", "value": "evaluated_on"}, {"source": "PIDNet", "target": "CamVid", "value": "evaluated_on"}, {"source": "PIDNet-S", "target": "Cityscapes", "value": "evaluated_on"}, {"source": "PIDNet-S", "target": "CamVid", "value": "evaluated_on"}, {"source": "PIDNet-S", "target": "mIOU", "value": "uses_metric"}, {"source": "Qwen2 Technical Report", "target": "Qwen2 series", "value": "proposed_model"}, {"source": "Qwen2 series", "target": "Qwen1.5", "value": "baseline_model"}, {"source": "Qwen2-72B", "target": "MMLU", "value": "evaluated_on"}, {"source": "Qwen2-72B", "target": "GPQA", "value": "evaluated_on"}, {"source": "Qwen2-72B", "target": "HumanEval", "value": "evaluated_on"}, {"source": "Qwen2-72B", "target": "GSM8K", "value": "evaluated_on"}, {"source": "Qwen2-72B", "target": "BBH", "value": "evaluated_on"}, {"source": "Qwen2-72B-Instruct", "target": "MT-Bench", "value": "evaluated_on"}, {"source": "Qwen2-72B-Instruct", "target": "Arena-Hard", "value": "evaluated_on"}, {"source": "Qwen2-72B-Instruct", "target": "LiveCodeBench", "value": "evaluated_on"}, {"source": "Qwen2-72B", "target": "MMLU", "value": "uses_metric"}, {"source": "Qwen2-72B", "target": "GPQA", "value": "uses_metric"}, {"source": "Qwen2-72B", "target": "HumanEval", "value": "uses_metric"}, {"source": "Qwen2-72B", "target": "GSM8K", "value": "uses_metric"}, {"source": "Qwen2-72B", "target": "BBH", "value": "uses_metric"}, {"source": "Qwen2-72B-Instruct", "target": "MT-Bench", "value": "uses_metric"}, {"source": "Qwen2-72B-Instruct", "target": "Arena-Hard", "value": "uses_metric"}, {"source": "Qwen2-72B-Instruct", "target": "LiveCodeBench", "value": "uses_metric"}, {"source": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving", "target": "FSDrive", "value": "proposed_model"}, {"source": "FSDrive", "target": "nuScenes", "value": "evaluated_on"}, {"source": "FSDrive", "target": "NAVSIM", "value": "evaluated_on"}, {"source": "FSDrive", "target": "DriveLM", "value": "evaluated_on"}, {"source": "FSDrive", "target": "FID", "value": "uses_metric"}, {"source": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning", "target": "AutoVLA", "value": "proposed_model"}, {"source": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning", "target": "Vision-Language-Action (VLA) models", "value": "baseline_model"}, {"source": "AutoVLA", "target": "nuPlan", "value": "evaluated_on"}, {"source": "AutoVLA", "target": "nuScenes", "value": "evaluated_on"}, {"source": "AutoVLA", "target": "Waymo", "value": "evaluated_on"}, {"source": "AutoVLA", "target": "CARLA", "value": "evaluated_on"}, {"source": "AutoVLA", "target": "Group Relative Policy Optimization (GRPO)", "value": "uses_metric"}, {"source": "Impromptu VLA", "target": "Impromptu VLA Dataset", "value": "proposed_model"}, {"source": "Impromptu VLA", "target": "NeuroNCAP", "value": "evaluated_on"}, {"source": "Impromptu VLA", "target": "collision rates", "value": "evaluated_on"}, {"source": "Impromptu VLA", "target": "nuScenes", "value": "evaluated_on"}, {"source": "Impromptu VLA", "target": "L2 accuracy", "value": "uses_metric"}, {"source": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving", "target": "AgentThink", "value": "proposed_model"}, {"source": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving", "target": "Vision-Language Models (VLMs)", "value": "baseline_model"}, {"source": "AgentThink", "target": "DriveLMM-o1", "value": "evaluated_on"}, {"source": "AgentThink", "target": "overall reasoning scores", "value": "uses_metric"}, {"source": "AgentThink", "target": "answer accuracy", "value": "uses_metric"}, {"source": "Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning", "target": "Drive-R1", "value": "proposed_model"}, {"source": "Drive-R1", "target": "nuScenes", "value": "evaluated_on"}, {"source": "Drive-R1", "target": "DriveLM-nuScenes", "value": "evaluated_on"}, {"source": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model", "target": "IRL-VLA", "value": "proposed_model"}, {"source": "IRL-VLA", "target": "VLA architecture", "value": "proposed_model"}, {"source": "IRL-VLA", "target": "VLA policy", "value": "proposed_model"}, {"source": "IRL-VLA", "target": "reward world model", "value": "proposed_model"}, {"source": "IRL-VLA", "target": "PPO (Proximal Policy Optimization)", "value": "baseline_model"}, {"source": "IRL-VLA", "target": "NAVSIM v2 end-to-end driving benchmark", "value": "evaluated_on"}, {"source": "IRL-VLA", "target": "CVPR2025 Autonomous Grand Challenge", "value": "evaluated_on"}, {"source": "IRL-VLA", "target": "state-of-the-art performance", "value": "uses_metric"}, {"source": "IRL-VLA", "target": "1st runner up", "value": "uses_metric"}, {"source": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving", "target": "DriveVLA-W0", "value": "proposed_model"}, {"source": "DriveVLA-W0", "target": "BEV", "value": "baseline_model"}, {"source": "DriveVLA-W0", "target": "VLA", "value": "baseline_model"}, {"source": "DriveVLA-W0", "target": "NAVSIM v1/v2 benchmark", "value": "evaluated_on"}, {"source": "DriveVLA-W0", "target": "in-house dataset", "value": "evaluated_on"}, {"source": "3D Gaussian Splatting for Real-Time Radiance Field Rendering", "target": "3D Gaussian Splatting", "value": "proposed_model"}, {"source": "3D Gaussian Splatting", "target": "several established datasets", "value": "evaluated_on"}, {"source": "Generalized Trajectory Scoring for End-to-end Multimodal Planning", "target": "GTRS (Generalized Trajectory Scoring)", "value": "proposed_model"}, {"source": "GTRS (Generalized Trajectory Scoring)", "target": "Navsim v2 Challenge", "value": "evaluated_on"}, {"source": "RAP: 3D Rasterization Augmented End-to-End Planning", "target": "3D Rasterization", "value": "proposed_model"}, {"source": "RAP: 3D Rasterization Augmented End-to-End Planning", "target": "Rasterization Augmented Planning (RAP)", "value": "proposed_model"}, {"source": "RAP: 3D Rasterization Augmented End-to-End Planning", "target": "Raster-to-Real feature-space alignment", "value": "proposed_model"}, {"source": "Rasterization Augmented Planning (RAP)", "target": "NAVSIM v1", "value": "evaluated_on"}, {"source": "Rasterization Augmented Planning (RAP)", "target": "NAVSIM v2", "value": "evaluated_on"}, {"source": "Rasterization Augmented Planning (RAP)", "target": "Waymo Open Dataset Vision-based E2E Driving", "value": "evaluated_on"}, {"source": "Rasterization Augmented Planning (RAP)", "target": "Bench2Drive", "value": "evaluated_on"}, {"source": "Rasterization Augmented Planning (RAP)", "target": "closed-loop robustness", "value": "uses_metric"}, {"source": "Rasterization Augmented Planning (RAP)", "target": "long-tail generalization", "value": "uses_metric"}, {"source": "PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving", "target": "PRIX", "value": "proposed_model"}, {"source": "PRIX", "target": "multimodal diffusion planners", "value": "baseline_model"}, {"source": "PRIX", "target": "NavSim", "value": "evaluated_on"}, {"source": "PRIX", "target": "nuScenes", "value": "evaluated_on"}, {"source": "PRIX", "target": "inference speed", "value": "uses_metric"}, {"source": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models", "target": "BLIP-2", "value": "proposed_model"}, {"source": "BLIP-2", "target": "Flamingo80B", "value": "baseline_model"}, {"source": "BLIP-2", "target": "VQAv2", "value": "evaluated_on"}, {"source": "BLIP-2", "target": "zero-shot VQAv2", "value": "uses_metric"}, {"source": "Flow Matching for Generative Modeling", "target": "Flow Matching (FM)", "value": "proposed_model"}, {"source": "Flow Matching (FM)", "target": "diffusion models", "value": "baseline_model"}, {"source": "Flow Matching (FM)", "target": "ImageNet", "value": "evaluated_on"}, {"source": "Flow Matching (FM)", "target": "likelihood", "value": "uses_metric"}, {"source": "Flow Matching (FM)", "target": "sample quality", "value": "uses_metric"}, {"source": "Diffusion policy: Visuomotor policy learning via action diffusion", "target": "Diffusion Policy", "value": "proposed_model"}, {"source": "Diffusion policy: Visuomotor policy learning via action diffusion", "target": "existing state-of-the-art robot learning methods", "value": "baseline_model"}, {"source": "Diffusion Policy", "target": "4 different robot manipulation benchmarks", "value": "evaluated_on"}, {"source": "Diffusion Policy", "target": "average improvement of 46.9%", "value": "uses_metric"}, {"source": "Large Language Models for Robotics: A Survey", "target": "LLM-based robotic models", "value": "proposed_model"}, {"source": "Large Language Models for Robotics: A Survey", "target": "robot control", "value": "evaluated_on"}, {"source": "Large Language Models for Robotics: A Survey", "target": "perception", "value": "evaluated_on"}, {"source": "Large Language Models for Robotics: A Survey", "target": "decision-making", "value": "evaluated_on"}, {"source": "Large Language Models for Robotics: A Survey", "target": "planning", "value": "evaluated_on"}, {"source": "Large Language Models for Robotics: A Survey", "target": "dexterity intelligence", "value": "uses_metric"}, {"source": "Large Language Models for Robotics: A Survey", "target": "human-robot interaction", "value": "uses_metric"}, {"source": "Large Language Models for Robotics: A Survey", "target": "autonomy", "value": "uses_metric"}, {"source": "LLM-based robotic models", "target": "Large Language Models (LLMs)", "value": "baseline_model"}, {"source": "Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)", "target": "Raw2Drive", "value": "proposed_model"}, {"source": "Raw2Drive", "target": "CARLA Leaderboard 2.0", "value": "evaluated_on"}, {"source": "Raw2Drive", "target": "Bench2Drive", "value": "evaluated_on"}, {"source": "Raw2Drive", "target": "state-of-the-art performance", "value": "uses_metric"}, {"source": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey", "target": "Vision Language Action (VLA) models", "value": "proposed_model"}, {"source": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey", "target": "Vision Language Models (VLMs)", "value": "baseline_model"}, {"source": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey", "target": "foundational datasets", "value": "evaluated_on"}, {"source": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey", "target": "simulation platforms", "value": "evaluated_on"}, {"source": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey", "target": "benchmarks", "value": "uses_metric"}, {"source": "ReSim: Reliable World Simulation for Autonomous Driving", "target": "ReSim", "value": "proposed_model"}, {"source": "ReSim: Reliable World Simulation for Autonomous Driving", "target": "Video2Reward", "value": "proposed_model"}, {"source": "ReSim", "target": "diffusion transformer architecture", "value": "baseline_model"}, {"source": "ReSim", "target": "NAVSIM", "value": "evaluated_on"}, {"source": "ReSim", "target": "visual fidelity", "value": "uses_metric"}, {"source": "ReSim", "target": "controllability", "value": "uses_metric"}, {"source": "ReSim", "target": "planning performance", "value": "uses_metric"}, {"source": "ReSim", "target": "policy selection performance", "value": "uses_metric"}, {"source": "ReSim", "target": "CARLA", "value": "evaluated_on"}, {"source": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "target": "Llama 2", "value": "proposed_model"}, {"source": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "target": "Llama 2-Chat", "value": "proposed_model"}, {"source": "Llama 2-Chat", "target": "open-source chat models", "value": "baseline_model"}, {"source": "Llama 2-Chat", "target": "closed-source models", "value": "baseline_model"}, {"source": "Llama 2-Chat", "target": "benchmarks", "value": "evaluated_on"}, {"source": "Llama 2-Chat", "target": "human evaluations for helpfulness and safety", "value": "uses_metric"}, {"source": "Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey", "target": "Vision-Language-Action (VLA) models", "value": "proposed_model"}, {"source": "Vision-Language-Action (VLA) models", "target": "vision-language models", "value": "baseline_model"}, {"source": "AdaThinkDrive", "target": "AdaThinkDrive", "value": "proposed_model"}, {"source": "AdaThinkDrive", "target": "never Think baseline", "value": "baseline_model"}, {"source": "AdaThinkDrive", "target": "always Think baseline", "value": "baseline_model"}, {"source": "AdaThinkDrive", "target": "vision only baseline", "value": "baseline_model"}, {"source": "AdaThinkDrive", "target": "Navsim", "value": "evaluated_on"}, {"source": "AdaThinkDrive", "target": "PDMS", "value": "uses_metric"}, {"source": "AdaThinkDrive", "target": "Group Relative Policy Optimization (GRPO)", "value": "proposed_model"}, {"source": "AdaThinkDrive", "target": "Chain of Thought (CoT)", "value": "baseline_model"}, {"source": "AdaThinkDrive", "target": "Vision Language Action (VLA) models", "value": "baseline_model"}, {"source": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition", "target": "UniV2X framework", "value": "proposed_model"}, {"source": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition", "target": "V2X-Seq-SPD dataset", "value": "evaluated_on"}, {"source": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition", "target": "End-to-End Autonomous Driving through V2X Cooperation Challenge", "value": "evaluated_on"}, {"source": "End-to-End Autonomous Driving through V2X Cooperation Challenge", "target": "cooperative temporal perception", "value": "uses_metric"}, {"source": "End-to-End Autonomous Driving through V2X Cooperation Challenge", "target": "cooperative end-to-end planning", "value": "uses_metric"}, {"source": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI", "target": "Multi-modal Large Models (MLMs)", "value": "proposed_model"}, {"source": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI", "target": "World Models (WMs)", "value": "proposed_model"}, {"source": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI", "target": "embodied robots", "value": "baseline_model"}, {"source": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI", "target": "simulators", "value": "baseline_model"}, {"source": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI", "target": "embodied perception", "value": "evaluated_on"}, {"source": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI", "target": "embodied interaction", "value": "evaluated_on"}, {"source": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI", "target": "embodied agent", "value": "evaluated_on"}, {"source": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI", "target": "sim-to-real adaptation", "value": "evaluated_on"}, {"source": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI", "target": "comprehensive datasets", "value": "uses_metric"}, {"source": "A Survey on Vision-Language-Action Models for Embodied AI", "target": "vision-language-action models (VLAs)", "value": "proposed_model"}, {"source": "vision-language-action models (VLAs)", "target": "large language models", "value": "baseline_model"}, {"source": "vision-language-action models (VLAs)", "target": "vision-language models", "value": "baseline_model"}, {"source": "vision-language-action models (VLAs)", "target": "datasets", "value": "evaluated_on"}, {"source": "vision-language-action models (VLAs)", "target": "simulators", "value": "evaluated_on"}, {"source": "vision-language-action models (VLAs)", "target": "benchmarks", "value": "uses_metric"}, {"source": "VLA-based control policies", "target": "vision-language-action models (VLAs)", "value": "proposed_model"}, {"source": "high-level task planners", "target": "vision-language-action models (VLAs)", "value": "proposed_model"}, {"source": "DriveDreamer4D", "target": "DriveDreamer4D", "value": "proposed_model"}, {"source": "DriveDreamer4D", "target": "PVG", "value": "baseline_model"}, {"source": "DriveDreamer4D", "target": "S3Gaussian", "value": "baseline_model"}, {"source": "DriveDreamer4D", "target": "Deformable-GS", "value": "baseline_model"}, {"source": "DriveDreamer4D", "target": "DriveDreamer4D", "value": "evaluated_on"}, {"source": "DriveDreamer4D", "target": "FID", "value": "uses_metric"}, {"source": "DriveDreamer4D", "target": "NTA-IoU", "value": "uses_metric"}, {"source": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation", "target": "PhyGenBench", "value": "proposed_model"}, {"source": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation", "target": "PhyGenEval", "value": "proposed_model"}, {"source": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation", "target": "Sora", "value": "baseline_model"}, {"source": "PhyGenBench", "target": "Sora", "value": "evaluated_on"}, {"source": "PhyGenEval", "target": "PhyGenBench", "value": "uses_metric"}, {"source": "AgentSquare: Automatic LLM Agent Search in Modular Design Space", "target": "AgentSquare", "value": "proposed_model"}, {"source": "AgentSquare: Automatic LLM Agent Search in Modular Design Space", "target": "Modularized LLM Agent Search (MoLAS)", "value": "proposed_model"}, {"source": "AgentSquare", "target": "six benchmarks", "value": "evaluated_on"}, {"source": "AgentSquare", "target": "performance gain", "value": "uses_metric"}, {"source": "CityGPT", "target": "SWFT", "value": "proposed_model"}, {"source": "CityGPT", "target": "CityEval", "value": "evaluated_on"}, {"source": "SWFT", "target": "ChatGLM3-6B", "value": "baseline_model"}, {"source": "SWFT", "target": "Llama3-8B", "value": "baseline_model"}, {"source": "SWFT", "target": "Qwen2.5-7B", "value": "baseline_model"}, {"source": "SWFT", "target": "CityEval", "value": "uses_metric"}, {"source": "SWFT", "target": "CityInstruction", "value": "evaluated_on"}, {"source": "Epona: Autoregressive Diffusion World Model for Autonomous Driving", "target": "Epona", "value": "proposed_model"}, {"source": "Epona", "target": "NAVSIM", "value": "evaluated_on"}, {"source": "Epona", "target": "FVD", "value": "uses_metric"}, {"source": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "target": "RoBERTa", "value": "proposed_model"}, {"source": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "target": "BERT", "value": "baseline_model"}, {"source": "RoBERTa", "target": "GLUE", "value": "evaluated_on"}, {"source": "RoBERTa", "target": "RACE", "value": "evaluated_on"}, {"source": "RoBERTa", "target": "SQuAD", "value": "evaluated_on"}, {"source": "Unlearning Concepts in Diffusion Model via Concept Domain Correction and Concept Preserving Gradient", "target": "DoCo", "value": "proposed_model"}, {"source": "Unlearning Concepts in Diffusion Model via Concept Domain Correction and Concept Preserving Gradient", "target": "Machine Unlearning (MU)", "value": "baseline_model"}, {"source": "DoCo", "target": "various instances, styles, and offensive concepts", "value": "evaluated_on"}, {"source": "DoCo", "target": "generalization", "value": "uses_metric"}, {"source": "DoCo", "target": "utility", "value": "uses_metric"}, {"source": "Machine Unlearning (MU)", "target": "generalization", "value": "uses_metric"}, {"source": "Machine Unlearning (MU)", "target": "utility", "value": "uses_metric"}, {"source": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "target": "Vision Foundation Models (VFMs)", "value": "proposed_model"}, {"source": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "target": "Large Language Models (LLMs)", "value": "proposed_model"}, {"source": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "target": "Vision-Language Pre-training (VLP) models", "value": "proposed_model"}, {"source": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "target": "Vision-Language Models (VLMs)", "value": "proposed_model"}, {"source": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "target": "Diffusion Models (DMs)", "value": "proposed_model"}, {"source": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "target": "large-model-based Agents", "value": "proposed_model"}, {"source": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "target": "commonly used datasets and benchmarks for safety research", "value": "evaluated_on"}, {"source": "The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense", "target": "LLM-Pipeline", "value": "proposed_model"}, {"source": "The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense", "target": "benchmark datasets", "value": "evaluated_on"}, {"source": "The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense", "target": "benchmark evaluations", "value": "uses_metric"}, {"source": "The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense", "target": "evaluation methods for jailbreak", "value": "uses_metric"}, {"source": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs", "target": "Video-SafetyBench", "value": "proposed_model"}, {"source": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs", "target": "RJScore", "value": "proposed_model"}, {"source": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs", "target": "Video-SafetyBench", "value": "evaluated_on"}, {"source": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs", "target": "RJScore", "value": "uses_metric"}, {"source": "Video-SafetyBench", "target": "Large Vision-Language Models (LVLMs)", "value": "baseline_model"}, {"source": "Gradient-Guided Learning Network for Infrared Small Target Detection", "target": "GGL-Net", "value": "proposed_model"}, {"source": "GGL-Net", "target": "NUAA-SIRST dataset", "value": "evaluated_on"}, {"source": "GGL-Net", "target": "NUDT-SIRST dataset", "value": "evaluated_on"}, {"source": "A Retrieval-Augmented Generation Approach to Extracting Algorithmic Logic from Neural Networks", "target": "NN-RAG", "value": "proposed_model"}, {"source": "NN-RAG", "target": "LEMUR", "value": "evaluated_on"}, {"source": "NN-RAG", "target": "structural uniqueness", "value": "uses_metric"}, {"source": "NN-RAG", "target": "novel network structures", "value": "uses_metric"}, {"source": "CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation", "target": "CenterMamba-SAM", "value": "proposed_model"}, {"source": "CenterMamba-SAM", "target": "CenterMamba encoder", "value": "proposed_model"}, {"source": "CenterMamba-SAM", "target": "memory-driven structural prompt generator", "value": "proposed_model"}, {"source": "CenterMamba-SAM", "target": "memory-augmented multi-scale decoder", "value": "proposed_model"}, {"source": "CenterMamba-SAM", "target": "public benchmarks", "value": "evaluated_on"}, {"source": "IMobileTransformer: A fusion-based lightweight model for rice disease identification", "target": "IMobileTransformer", "value": "proposed_model"}, {"source": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images", "target": "UAV-based RGB images", "value": "evaluated_on"}, {"source": "RF-DETR", "target": "RF-DETR (nano)", "value": "proposed_model"}, {"source": "RF-DETR", "target": "RF-DETR (2x-large)", "value": "proposed_model"}, {"source": "RF-DETR", "target": "D-FINE (nano)", "value": "baseline_model"}, {"source": "RF-DETR", "target": "GroundingDINO (tiny)", "value": "baseline_model"}, {"source": "RF-DETR (nano)", "target": "COCO", "value": "evaluated_on"}, {"source": "RF-DETR (2x-large)", "target": "COCO", "value": "evaluated_on"}, {"source": "RF-DETR (2x-large)", "target": "Roboflow100-VL", "value": "evaluated_on"}, {"source": "RF-DETR (nano)", "target": "AP", "value": "uses_metric"}, {"source": "RF-DETR (2x-large)", "target": "AP", "value": "uses_metric"}, {"source": "FreeOrbit4D", "target": "FreeOrbit4D", "value": "proposed_model"}, {"source": "FreeOrbit4D", "target": "diffusion-based methods", "value": "baseline_model"}, {"source": "FreeOrbit4D", "target": "monocular video", "value": "evaluated_on"}, {"source": "FreeOrbit4D", "target": "redirected videos", "value": "evaluated_on"}, {"source": "FreeOrbit4D", "target": "faithful redirected videos", "value": "uses_metric"}, {"source": "FreeOrbit4D", "target": "object-centric multi-view diffusion model", "value": "proposed_model"}, {"source": "FreeOrbit4D", "target": "conditional video diffusion model", "value": "proposed_model"}, {"source": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "target": "NeoVerse", "value": "proposed_model"}, {"source": "NeoVerse", "target": "standard reconstruction and generation benchmarks", "value": "evaluated_on"}, {"source": "Scalable Diffusion Models with Transformers", "target": "Diffusion Transformers (DiTs)", "value": "proposed_model"}, {"source": "Scalable Diffusion Models with Transformers", "target": "U-Net", "value": "baseline_model"}, {"source": "Diffusion Transformers (DiTs)", "target": "ImageNet 512x512", "value": "evaluated_on"}, {"source": "Diffusion Transformers (DiTs)", "target": "ImageNet 256x256", "value": "evaluated_on"}, {"source": "DiT-XL/2", "target": "ImageNet 512x512", "value": "evaluated_on"}, {"source": "DiT-XL/2", "target": "ImageNet 256x256", "value": "evaluated_on"}, {"source": "Diffusion Transformers (DiTs)", "target": "FID", "value": "uses_metric"}, {"source": "Diffusion Transformers (DiTs)", "target": "Gflops", "value": "uses_metric"}, {"source": "DiT-XL/2", "target": "FID", "value": "uses_metric"}, {"source": "RoFormer: Enhanced Transformer with Rotary Position Embedding", "target": "RoPE", "value": "proposed_model"}, {"source": "RoFormer: Enhanced Transformer with Rotary Position Embedding", "target": "RoFormer", "value": "proposed_model"}, {"source": "RoFormer: Enhanced Transformer with Rotary Position Embedding", "target": "long text classification benchmark datasets", "value": "evaluated_on"}, {"source": "The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey", "target": "Driving World Model (DWM)", "value": "proposed_model"}, {"source": "Driving World Model (DWM)", "target": "mainstream simulators", "value": "evaluated_on"}, {"source": "Driving World Model (DWM)", "target": "high-impact datasets", "value": "evaluated_on"}, {"source": "Driving World Model (DWM)", "target": "various metrics", "value": "uses_metric"}, {"source": "Vision meets robotics: The KITTI dataset", "target": "KITTI", "value": "evaluated_on"}, {"source": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations", "target": "FlexMap", "value": "proposed_model"}, {"source": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations", "target": "existing methods", "value": "baseline_model"}, {"source": "FlexMap", "target": "multiple configurations", "value": "evaluated_on"}, {"source": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction", "target": "MapAnything", "value": "proposed_model"}, {"source": "MapAnything", "target": "diverse datasets", "value": "evaluated_on"}, {"source": "MapAnything", "target": "specialist feed-forward models", "value": "baseline_model"}, {"source": "Seedream 4.0", "target": "Seedream 4.0", "value": "proposed_model"}, {"source": "Seedream 4.0", "target": "diffusion transformer", "value": "proposed_model"}, {"source": "Seedream 4.0", "target": "VLM model", "value": "proposed_model"}, {"source": "Seedream 4.0", "target": "billions of text-image pairs", "value": "evaluated_on"}, {"source": "Seedream 4.0", "target": "T2I", "value": "uses_metric"}, {"source": "Seedream 4.0", "target": "multimodal image editing", "value": "uses_metric"}, {"source": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation", "target": "Echo-4o", "value": "proposed_model"}, {"source": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation", "target": "Echo-4o-Image", "value": "proposed_model"}, {"source": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation", "target": "GenEval++", "value": "proposed_model"}, {"source": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation", "target": "Imagine-Bench", "value": "proposed_model"}, {"source": "Echo-4o", "target": "Bagel", "value": "baseline_model"}, {"source": "Echo-4o", "target": "GenEval++", "value": "evaluated_on"}, {"source": "Echo-4o", "target": "Imagine-Bench", "value": "evaluated_on"}, {"source": "Echo-4o", "target": "GenEval++", "value": "uses_metric"}, {"source": "Echo-4o", "target": "Imagine-Bench", "value": "uses_metric"}, {"source": "Echo-4o-Image", "target": "OmniGen2", "value": "evaluated_on"}, {"source": "Echo-4o-Image", "target": "BLIP3-o", "value": "evaluated_on"}, {"source": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "target": "Lumina-DiMOO", "value": "proposed_model"}, {"source": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "target": "autoregressive (AR) or hybrid AR-Diffusion paradigms", "value": "baseline_model"}, {"source": "Lumina-DiMOO", "target": "multiple benchmarks", "value": "evaluated_on"}, {"source": "Lumina-DiMOO", "target": "state-of-the-art performance", "value": "uses_metric"}, {"source": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "target": "NextStep-1", "value": "proposed_model"}, {"source": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "target": "autoregressive models", "value": "baseline_model"}, {"source": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "target": "diffusion models", "value": "baseline_model"}, {"source": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "target": "vector quantization (VQ)", "value": "baseline_model"}, {"source": "NextStep-1", "target": "text-to-image generation tasks", "value": "evaluated_on"}, {"source": "NextStep-1", "target": "high-fidelity image synthesis", "value": "uses_metric"}, {"source": "NextStep-1", "target": "image editing", "value": "uses_metric"}, {"source": "Training language models to follow instructions with human feedback", "target": "InstructGPT", "value": "proposed_model"}, {"source": "Training language models to follow instructions with human feedback", "target": "GPT-3", "value": "baseline_model"}, {"source": "InstructGPT", "target": "prompt distribution", "value": "evaluated_on"}, {"source": "InstructGPT", "target": "public NLP datasets", "value": "evaluated_on"}, {"source": "InstructGPT", "target": "human evaluations", "value": "uses_metric"}, {"source": "InstructGPT", "target": "truthfulness", "value": "uses_metric"}, {"source": "InstructGPT", "target": "toxic output generation", "value": "uses_metric"}, {"source": "Search-R1", "target": "Search-R1", "value": "proposed_model"}, {"source": "Search-R1", "target": "seven question-answering datasets", "value": "evaluated_on"}, {"source": "Search-R1", "target": "performance", "value": "uses_metric"}, {"source": "Toward expert-level medical question answering with large language models", "target": "Med-PaLM 2", "value": "proposed_model"}, {"source": "Toward expert-level medical question answering with large language models", "target": "Med-PaLM", "value": "baseline_model"}, {"source": "Med-PaLM 2", "target": "MedQA", "value": "evaluated_on"}, {"source": "Med-PaLM 2", "target": "MedMCQA", "value": "evaluated_on"}, {"source": "Med-PaLM 2", "target": "PubMedQA", "value": "evaluated_on"}, {"source": "Med-PaLM 2", "target": "MMLU clinical topics", "value": "evaluated_on"}, {"source": "Med-PaLM 2", "target": "score", "value": "uses_metric"}, {"source": "Med-PaLM 2", "target": "performance", "value": "uses_metric"}, {"source": "Med-PaLM 2", "target": "evaluation metrics", "value": "uses_metric"}, {"source": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training", "target": "Supervised fine-tuning (SFT)", "value": "proposed_model"}, {"source": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training", "target": "reinforcement learning (RL)", "value": "proposed_model"}, {"source": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training", "target": "GeneralPoints", "value": "evaluated_on"}, {"source": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training", "target": "V-IRL", "value": "evaluated_on"}, {"source": "reinforcement learning (RL)", "target": "Supervised fine-tuning (SFT)", "value": "baseline_model"}, {"source": "reinforcement learning (RL)", "target": "outcome-based reward", "value": "uses_metric"}, {"source": "Equivariant Diffusion for Crystal Structure Prediction", "target": "EquiCSP", "value": "proposed_model"}, {"source": "Equivariant Diffusion for Crystal Structure Prediction", "target": "existing models", "value": "baseline_model"}, {"source": "EquiCSP", "target": "accurate structures", "value": "uses_metric"}, {"source": "EquiCSP", "target": "faster convergence", "value": "uses_metric"}, {"source": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling", "target": "WorldPlay", "value": "proposed_model"}, {"source": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling", "target": "existing techniques", "value": "baseline_model"}, {"source": "WorldPlay", "target": "diverse scenes", "value": "evaluated_on"}, {"source": "WorldPlay", "target": "long-term geometric consistency", "value": "uses_metric"}, {"source": "WorldPlay", "target": "real-time speeds", "value": "uses_metric"}, {"source": "WorldPlay", "target": "24 FPS", "value": "uses_metric"}, {"source": "WorldPlay", "target": "720p video", "value": "uses_metric"}, {"source": "Native and Compact Structured Latents for 3D Generation", "target": "O-Voxel", "value": "proposed_model"}, {"source": "Native and Compact Structured Latents for 3D Generation", "target": "Sparse Compression VAE", "value": "proposed_model"}, {"source": "Native and Compact Structured Latents for 3D Generation", "target": "flow-matching models", "value": "proposed_model"}, {"source": "Native and Compact Structured Latents for 3D Generation", "target": "existing models", "value": "baseline_model"}, {"source": "Native and Compact Structured Latents for 3D Generation", "target": "diverse public 3D asset datasets", "value": "evaluated_on"}, {"source": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?", "target": "JEPA-WMs", "value": "proposed_model"}, {"source": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?", "target": "DINO-WM", "value": "baseline_model"}, {"source": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?", "target": "V-JEPA-2-AC", "value": "baseline_model"}, {"source": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?", "target": "simulated environments", "value": "evaluated_on"}, {"source": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?", "target": "real-world robotic data", "value": "evaluated_on"}, {"source": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?", "target": "planning success", "value": "uses_metric"}, {"source": "Progressive Learned Image Transmission for Semantic Communication Using Hierarchical VAE", "target": "PLIT", "value": "proposed_model"}, {"source": "PLIT", "target": "rate-distortion performance", "value": "uses_metric"}, {"source": "STORM: Search-Guided Generative World Models for Robotic Manipulation", "target": "STORM", "value": "proposed_model"}, {"source": "STORM", "target": "Vision-Language-Action (VLA) models", "value": "baseline_model"}, {"source": "STORM", "target": "CogACT", "value": "baseline_model"}, {"source": "STORM", "target": "SimplerEnv manipulation benchmark", "value": "evaluated_on"}, {"source": "STORM", "target": "average success rate", "value": "uses_metric"}, {"source": "STORM", "target": "Frechet Video Distance", "value": "uses_metric"}, {"source": "Continuous 3D Perception Model with Persistent State", "target": "CUT3R (Continuous Updating Transformer for 3D Reconstruction)", "value": "proposed_model"}, {"source": "CUT3R (Continuous Updating Transformer for 3D Reconstruction)", "target": "various 3D/4D tasks", "value": "evaluated_on"}, {"source": "Gen3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control", "target": "GEN3C", "value": "proposed_model"}, {"source": "GEN3C", "target": "sparse-view novel view synthesis", "value": "evaluated_on"}, {"source": "GEN3C", "target": "driving scenes", "value": "evaluated_on"}, {"source": "GEN3C", "target": "monocular dynamic video", "value": "evaluated_on"}, {"source": "GEN3C", "target": "camera control", "value": "uses_metric"}, {"source": "GEN3C", "target": "temporal 3D consistency", "value": "uses_metric"}, {"source": "FLARE: Feed-Forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views", "target": "FLARE", "value": "proposed_model"}, {"source": "FLARE", "target": "large-scale public datasets", "value": "evaluated_on"}, {"source": "FLARE", "target": "pose estimation", "value": "uses_metric"}, {"source": "FLARE", "target": "geometry reconstruction", "value": "uses_metric"}, {"source": "FLARE", "target": "novel view synthesis", "value": "uses_metric"}, {"source": "UniDepthV2", "target": "UniDepth", "value": "proposed_model"}, {"source": "UniDepthV2", "target": "ten depth datasets", "value": "evaluated_on"}, {"source": "OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving", "target": "OpenEMMA", "value": "proposed_model"}, {"source": "OpenEMMA", "target": "Multimodal Large Language Models (MLLMs)", "value": "baseline_model"}, {"source": "OpenEMMA", "target": "Chain-of-Thought reasoning process", "value": "uses_metric"}, {"source": "Adaptive transfer learning for surgical tool presence detection in laparoscopic videos through gradual freezing fine-tuning", "target": "staged adaptive fine-tuning approach", "value": "proposed_model"}, {"source": "staged adaptive fine-tuning approach", "target": "ResNet-50", "value": "baseline_model"}, {"source": "staged adaptive fine-tuning approach", "target": "DenseNet-121", "value": "baseline_model"}, {"source": "staged adaptive fine-tuning approach", "target": "Cholec80", "value": "evaluated_on"}, {"source": "staged adaptive fine-tuning approach", "target": "CATARACTS", "value": "evaluated_on"}, {"source": "staged adaptive fine-tuning approach", "target": "mean average precision (mAP)", "value": "uses_metric"}, {"source": "Rich feature hierarchies for accurate object detection and semantic segmentation", "target": "R-CNN", "value": "proposed_model"}, {"source": "Rich feature hierarchies for accurate object detection and semantic segmentation", "target": "OverFeat", "value": "baseline_model"}, {"source": "R-CNN", "target": "PASCAL VOC dataset", "value": "evaluated_on"}, {"source": "R-CNN", "target": "VOC 2012", "value": "evaluated_on"}, {"source": "R-CNN", "target": "ILSVRC2013 detection dataset", "value": "evaluated_on"}, {"source": "R-CNN", "target": "mean average precision (mAP)", "value": "uses_metric"}, {"source": "Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism", "target": "Multi-modal Chain and Global Attention Network (MCGA-Net)", "value": "proposed_model"}, {"source": "Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism", "target": "DCGAN-based data augmentation strategy", "value": "proposed_model"}, {"source": "Multi-modal Chain and Global Attention Network (MCGA-Net)", "target": "MS COCO", "value": "evaluated_on"}, {"source": "Multi-modal Chain and Global Attention Network (MCGA-Net)", "target": "Precision", "value": "uses_metric"}, {"source": "Multi-modal Chain and Global Attention Network (MCGA-Net)", "target": "Recall", "value": "uses_metric"}, {"source": "Multi-modal Chain and Global Attention Network (MCGA-Net)", "target": "mAP@50", "value": "uses_metric"}, {"source": "Evolutionary Optimization of Model Merging Recipes", "target": "evolutionary approach", "value": "proposed_model"}, {"source": "evolutionary approach", "target": "Japanese LLM with Math reasoning capabilities", "value": "proposed_model"}, {"source": "evolutionary approach", "target": "culturally-aware Japanese VLM", "value": "proposed_model"}, {"source": "Japanese LLM with Math reasoning capabilities", "target": "Japanese LLM benchmarks", "value": "evaluated_on"}, {"source": "culturally-aware Japanese VLM", "target": "Japanese LLM benchmarks", "value": "evaluated_on"}, {"source": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "target": "RNN Encoder-Decoder", "value": "proposed_model"}, {"source": "RNN Encoder-Decoder", "target": "statistical machine translation system", "value": "baseline_model"}, {"source": "RNN Encoder-Decoder", "target": "log-linear model", "value": "baseline_model"}, {"source": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving", "target": "DriveMLM", "value": "proposed_model"}, {"source": "DriveMLM", "target": "Autopilot", "value": "baseline_model"}, {"source": "DriveMLM", "target": "Apollo", "value": "baseline_model"}, {"source": "DriveMLM", "target": "CARLA Town05 Long", "value": "evaluated_on"}, {"source": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models", "target": "VLMEvalKit", "value": "proposed_model"}, {"source": "VLMEvalKit", "target": "OpenVLM Leaderboard", "value": "uses_metric"}, {"source": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "target": "chain of thought prompting", "value": "proposed_model"}, {"source": "chain of thought prompting", "target": "GPT-3", "value": "baseline_model"}, {"source": "chain of thought prompting", "target": "GSM8K", "value": "evaluated_on"}, {"source": "chain of thought prompting", "target": "accuracy", "value": "uses_metric"}, {"source": "Federated Learning Architectures: A Performance Evaluation with Crop Yield Prediction Application", "target": "centralized federated learning framework", "value": "proposed_model"}, {"source": "Federated Learning Architectures: A Performance Evaluation with Crop Yield Prediction Application", "target": "decentralized federated learning framework", "value": "proposed_model"}, {"source": "centralized federated learning framework", "target": "cloud-only framework", "value": "baseline_model"}, {"source": "centralized federated learning framework", "target": "crop yield prediction dataset", "value": "evaluated_on"}, {"source": "decentralized federated learning framework", "target": "crop yield prediction dataset", "value": "evaluated_on"}, {"source": "centralized federated learning framework", "target": "prediction accuracy", "value": "uses_metric"}, {"source": "centralized federated learning framework", "target": "precision", "value": "uses_metric"}, {"source": "centralized federated learning framework", "target": "recall", "value": "uses_metric"}, {"source": "centralized federated learning framework", "target": "F1-Score", "value": "uses_metric"}, {"source": "centralized federated learning framework", "target": "training time", "value": "uses_metric"}, {"source": "centralized federated learning framework", "target": "response time", "value": "uses_metric"}, {"source": "decentralized federated learning framework", "target": "prediction accuracy", "value": "uses_metric"}, {"source": "decentralized federated learning framework", "target": "precision", "value": "uses_metric"}, {"source": "decentralized federated learning framework", "target": "recall", "value": "uses_metric"}, {"source": "decentralized federated learning framework", "target": "F1-Score", "value": "uses_metric"}, {"source": "decentralized federated learning framework", "target": "training time", "value": "uses_metric"}, {"source": "Enhancing Descriptive Image Quality Assessment with A Large-scale Multi-modal Dataset", "target": "DepictQA-Wild", "value": "proposed_model"}, {"source": "DepictQA-Wild", "target": "traditional score-based methods", "value": "baseline_model"}, {"source": "DepictQA-Wild", "target": "prior VLM-based IQA models", "value": "baseline_model"}, {"source": "DepictQA-Wild", "target": "GPT-4V", "value": "baseline_model"}, {"source": "DepictQA-Wild", "target": "DQ-495K", "value": "evaluated_on"}, {"source": "DepictQA-Wild", "target": "distortion identification", "value": "uses_metric"}, {"source": "DepictQA-Wild", "target": "instant rating", "value": "uses_metric"}, {"source": "DepictQA-Wild", "target": "reasoning tasks", "value": "uses_metric"}, {"source": "Deep contextualized word representations", "target": "deep bidirectional language model (biLM)", "value": "proposed_model"}, {"source": "deep bidirectional language model (biLM)", "target": "large text corpus", "value": "evaluated_on"}, {"source": "Deep contextualized word representations", "target": "question answering", "value": "uses_metric"}, {"source": "Deep contextualized word representations", "target": "textual entailment", "value": "uses_metric"}, {"source": "Deep contextualized word representations", "target": "sentiment analysis", "value": "uses_metric"}, {"source": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "target": "DeepSeek-R1", "value": "proposed_model"}, {"source": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "target": "conventional supervised learning on human demonstrations", "value": "baseline_model"}, {"source": "DeepSeek-R1", "target": "mathematics", "value": "evaluated_on"}, {"source": "DeepSeek-R1", "target": "coding competitions", "value": "evaluated_on"}, {"source": "DeepSeek-R1", "target": "STEM fields", "value": "evaluated_on"}, {"source": "Fully Convolutional Networks for Semantic Segmentation", "target": "Fully Convolutional Networks", "value": "proposed_model"}, {"source": "Fully Convolutional Networks", "target": "AlexNet", "value": "baseline_model"}, {"source": "Fully Convolutional Networks", "target": "VGG net", "value": "baseline_model"}, {"source": "Fully Convolutional Networks", "target": "GoogLeNet", "value": "baseline_model"}, {"source": "Fully Convolutional Networks", "target": "PASCAL VOC", "value": "evaluated_on"}, {"source": "Fully Convolutional Networks", "target": "NYUDv2", "value": "evaluated_on"}, {"source": "Fully Convolutional Networks", "target": "SIFT Flow", "value": "evaluated_on"}, {"source": "Fully Convolutional Networks", "target": "mean IU", "value": "uses_metric"}, {"source": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with Transformers", "target": "CMX", "value": "proposed_model"}, {"source": "CMX", "target": "Cross-Modal Feature Rectification Module (CM-FRM)", "value": "baseline_model"}, {"source": "CMX", "target": "Feature Fusion Module (FFM)", "value": "baseline_model"}, {"source": "CMX", "target": "RGB-Depth benchmarks", "value": "evaluated_on"}, {"source": "CMX", "target": "RGB-Thermal datasets", "value": "evaluated_on"}, {"source": "CMX", "target": "RGB-Polarization datasets", "value": "evaluated_on"}, {"source": "CMX", "target": "RGB-LiDAR datasets", "value": "evaluated_on"}, {"source": "CMX", "target": "EventScape dataset", "value": "evaluated_on"}, {"source": "CMX", "target": "state-of-the-art performances", "value": "uses_metric"}, {"source": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion", "target": "Diffusion Policy", "value": "proposed_model"}, {"source": "Diffusion Policy", "target": "robot manipulation benchmarks", "value": "evaluated_on"}, {"source": "Diffusion Policy", "target": "average improvement of 46.9%", "value": "uses_metric"}, {"source": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI", "target": "Multi-modal Large Models (MLMs)", "value": "proposed_model"}, {"source": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI", "target": "World Models (WMs)", "value": "proposed_model"}, {"source": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI", "target": "embodied robots", "value": "baseline_model"}, {"source": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI", "target": "simulators", "value": "baseline_model"}, {"source": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI", "target": "embodied perception", "value": "evaluated_on"}, {"source": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI", "target": "embodied interaction", "value": "evaluated_on"}, {"source": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI", "target": "embodied agent", "value": "evaluated_on"}, {"source": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI", "target": "sim-to-real adaptation", "value": "evaluated_on"}, {"source": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control", "target": "GEN3C", "value": "proposed_model"}, {"source": "GEN3C", "target": "sparse-view novel view synthesis", "value": "evaluated_on"}, {"source": "GEN3C", "target": "driving scenes", "value": "evaluated_on"}, {"source": "GEN3C", "target": "monocular dynamic video", "value": "evaluated_on"}, {"source": "GEN3C", "target": "camera control", "value": "uses_metric"}, {"source": "GEN3C", "target": "temporal 3D consistency", "value": "uses_metric"}, {"source": "FLARE: Feed-forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views", "target": "FLARE", "value": "proposed_model"}, {"source": "FLARE", "target": "large-scale public datasets", "value": "evaluated_on"}, {"source": "FLARE", "target": "pose estimation", "value": "uses_metric"}, {"source": "FLARE", "target": "geometry reconstruction", "value": "uses_metric"}, {"source": "FLARE", "target": "novel view synthesis", "value": "uses_metric"}],
                    categories: [{"name": "Dataset"}, {"name": "Researcher"}, {"name": "Metric"}, {"name": "AIPaper"}, {"name": "AIModel"}],
                    roam: true,
                    label: { show: true, position: 'right', formatter: '{b}' },
                    edgeLabel: { fontSize: 11, formatter: '{c}' },
                    edgeSymbol: ['none', 'arrow'], edgeSymbolSize: 10,
                    lineStyle: { color: 'source', curveness: 0.3 },
                    force: { repulsion: 1500, edgeLength: 250 },
                    emphasis: { focus: 'adjacency', lineStyle: { width: 4 } }
                }]
            };
            myChart.setOption(option);
            setTimeout(function() { myChart.resize(); }, 100);
            window.addEventListener('resize', function() { myChart.resize(); });
        }
        if (document.readyState === 'loading') {
            document.addEventListener('DOMContentLoaded', initChart);
        } else {
            initChart();
        }
    </script>
</body>
</html>