{
  "paper_metadata": {
    "id": "1706.03762",
    "title": "Attention Is All You Need",
    "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
    "published_date": "2017-06-12",
    "pdf_url": "https://arxiv.org/pdf/1706.03762v7",
    "authors": [
      "Ashish Vaswani",
      "Noam Shazeer",
      "Niki Parmar",
      "Jakob Uszkoreit",
      "Llion Jones",
      "Aidan N. Gomez",
      "Lukasz Kaiser",
      "Illia Polosukhin"
    ]
  },
  "related_papers_count": {
    "references": 5,
    "citations": 5
  },
  "related_papers": [
    {
      "title": "Deep Residual Learning for Image Recognition",
      "arxiv_id": "1512.03385",
      "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.\n  The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "published_date": "2015-12-10",
      "pdf_url": "https://arxiv.org/pdf/1512.03385v1",
      "citation_count": 219453,
      "year": 2015
    },
    {
      "title": "Adam: A Method for Stochastic Optimization",
      "arxiv_id": "1412.6980",
      "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
      "authors": [
        "Diederik P. Kingma",
        "Jimmy Ba"
      ],
      "published_date": "2014-12-22",
      "pdf_url": "https://arxiv.org/pdf/1412.6980v9",
      "citation_count": 162431,
      "year": 2014
    },
    {
      "title": "Long Short-Term Memory",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Sepp Hochreiter",
        "J. Schmidhuber"
      ],
      "published_date": "1997",
      "pdf_url": "",
      "citation_count": 100313,
      "year": 1997
    },
    {
      "title": "Dropout: a simple way to prevent neural networks from overfitting",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Nitish Srivastava",
        "Geoffrey E. Hinton",
        "A. Krizhevsky",
        "I. Sutskever",
        "R. Salakhutdinov"
      ],
      "published_date": "2014",
      "pdf_url": "",
      "citation_count": 42325,
      "year": 2014
    },
    {
      "title": "Rethinking the Inception Architecture for Computer Vision",
      "arxiv_id": "1512.00567",
      "abstract": "Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the validation set.",
      "authors": [
        "Christian Szegedy",
        "Vincent Vanhoucke",
        "Sergey Ioffe",
        "Jonathon Shlens",
        "Zbigniew Wojna"
      ],
      "published_date": "2015-12-02",
      "pdf_url": "https://arxiv.org/pdf/1512.00567v3",
      "citation_count": 30072,
      "year": 2015
    },
    {
      "title": "A comprehensive review of recommender systems: Transitioning from theory to practice",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Shaina Raza",
        "Mizanur Rahman",
        "Safiullah Kamawal",
        "Armin Toroghi",
        "Ananya Raval",
        "F. Navah",
        "Amirmohammad Kazemeini"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 7,
      "year": 2026
    },
    {
      "title": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
      "arxiv_id": "",
      "abstract": "Autonomous driving relies on robust models trained on high-quality, large-scale multi-view driving videos. While world models offer a cost-effective solution for generating realistic driving videos, they struggle to maintain instance-level temporal consistency and spatial geometric fidelity. To address these challenges, we propose InstaDrive, a novel framework that enhances driving video realism through two key advancements: (1) Instance Flow Guider, which extracts and propagates instance features across frames to enforce temporal consistency, preserving instance identity over time. (2) Spatial Geometric Aligner, which improves spatial reasoning, ensures precise instance positioning, and explicitly models occlusion hierarchies. By incorporating these instance-aware mechanisms, InstaDrive achieves state-of-the-art video generation quality and enhances downstream autonomous driving tasks on the nuScenes dataset. Additionally, we utilize CARLA's autopilot to procedurally and stochastically simulate rare but safety-critical driving scenarios across diverse maps and regions, enabling rigorous safety evaluation for autonomous systems. Our project page is https://shanpoyang654.github.io/InstaDrive/page.html.",
      "authors": [
        "Zhuoran Yang",
        "Xi Guo",
        "Chenjing Ding",
        "Chiyu Wang",
        "Wei Wu",
        "Yanyong Zhang"
      ],
      "published_date": "2026-02-03",
      "pdf_url": "https://arxiv.org/pdf/2602.03242v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Zisheng Wang",
        "Junjie Chen",
        "Chisen Wang",
        "Cong Peng",
        "Jianping Xuan",
        "Tielin Shi",
        "Ming J. Zuo"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 4,
      "year": 2026
    },
    {
      "title": "Enhanced Transformer-BiLSTM Deep Learning Framework for Day-Ahead Energy Price Forecasting",
      "arxiv_id": null,
      "abstract": "Accurate forecasting of energy prices is critical to effectively mitigate operational risks and make strategic bidding decisions in day-ahead (DA) electricity markets. However, it is highly challenging due to volatile characteristics, seasonality, rapid spikes, and other nonlinear factors of price signals. In the given context, deep learning (DL) has gained attention in recent years due to its high potential in nonlinear approximation, but each model has its strengths and limitations. Therefore, this paper proposes a hybrid DL approach for time-series DA energy price forecasting based on the Transformer and Bidirectional Long Short-Term Memory (BiLSTM) model that facilitates strategically combining various components to extract patterns and further improve the sequence processing task. The proposed model uses a transformer architecture to capture patterns, temporal dynamics, and BiLSTM networks to forecast energy price fluctuations. The proposed approach is validated with simulations based on price data from the New York Independent System Operator, and the results show that the proposed approach consistently outperforms state-of-the-art DL models, achieving the lowest MAE (2.7818 $/MWh), RMSE (6.4937 $/MWh), sMAPE (6.6060%), MAPE (6.3741%) and highest R$^{2}$ (0.9393). The effectiveness of the proposed approach was justified through various case studies from different perspectives.",
      "authors": [
        "Abdullah Al Ahad Khan",
        "Md Habib Ullah",
        "Ruchira Tabassum",
        "Md Faisal Kabir"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 3,
      "year": 2026
    },
    {
      "title": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Jinghuan Zhang",
        "Wang Chen",
        "Jian Zhang"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 3,
      "year": 2026
    },
    {
      "title": "ImageNet classification with deep convolutional neural networks",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "A. Krizhevsky",
        "I. Sutskever",
        "Geoffrey E. Hinton"
      ],
      "published_date": "2012",
      "pdf_url": "",
      "citation_count": 126555,
      "year": 2012
    },
    {
      "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "arxiv_id": "1409.1556",
      "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.",
      "authors": [
        "Karen Simonyan",
        "Andrew Zisserman"
      ],
      "published_date": "2014-09-04",
      "pdf_url": "https://arxiv.org/pdf/1409.1556v6",
      "citation_count": 108937,
      "year": 2014
    },
    {
      "title": "Et al",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "P. Cochat",
        "L. Vaucoret",
        "J. Sarles"
      ],
      "published_date": "2008",
      "pdf_url": "",
      "citation_count": 74038,
      "year": 2008
    },
    {
      "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
      "arxiv_id": "",
      "abstract": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.",
      "authors": [
        "Shaoqing Ren",
        "Kaiming He",
        "Ross Girshick",
        "Jian Sun"
      ],
      "published_date": "2015-06-04",
      "pdf_url": "https://arxiv.org/pdf/1506.01497v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Yuqi Cheng",
        "Yunkang Cao",
        "Haiming Yao",
        "Wei Luo",
        "Cheng Jiang",
        "Hui Zhang",
        "Weiming Shen"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 6,
      "year": 2026
    },
    {
      "title": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning",
      "arxiv_id": null,
      "abstract": "Federated learning (FL) has emerged as a promising solution to enable distributed learning without sharing sensitive data. However, FL is vulnerable to data poisoning attacks, where malicious clients inject malicious data during training to compromise the global model. Existing FL defenses suffer from the assumptions of independent and identically distributed (IID) model updates, asymptotic optimal error rate bounds, and strong convexity in the optimization problem. Hence, we propose a novel framework called Federated Learning Optimal Transport (FLOT) that leverages the Wasserstein barycentric technique to obtain a global model from a set of locally trained non-IID models on client devices. In addition, we introduce a loss function-based rejection (LFR) mechanism to suppress malicious updates and a dynamic weighting scheme to optimize the Wasserstein barycentric aggregation function. We provide the theoretical proof of the Byzantine resilience and convergence of FLOT to highlight its efficacy. We evaluate FLOT on four benchmark datasets: GTSRB, KBTS, CIFAR10, and EMNIST. The experimental results underscore the practical significance of FLOT as an effective defense mechanism against data poisoning attacks in FL while maintaining high accuracy and scalability. Also, we observe that FLOT serves as a robust client selection technique under no attack, which demonstrates its effectiveness.",
      "authors": [
        "Naveen Kumar Srinivasa",
        "Ajeet Rao Chalamala",
        "Kumar Singh",
        "Ieee Krishna Mohan Senior Member",
        "K. Naveen",
        "Srinivasa Rao",
        "Ajeet Kumar Singh"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 5,
      "year": 2026
    },
    {
      "title": "DiffusionEngine: Diffusion model is scalable data engine for object detection",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Manlin Zhang",
        "Jie Wu",
        "Yuxi Ren",
        "Jiahong Yang",
        "Ming Li",
        "Andy J. Ma"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 2,
      "year": 2026
    },
    {
      "title": "Enhancing hyperspectral image prediction with contrastive learning in low-label regimes",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Salma Haidar",
        "José Oramas"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 2,
      "year": 2026
    },
    {
      "title": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID",
      "arxiv_id": null,
      "abstract": "Person re-identification (Re-ID) can recognize users based on their clothing, body shape, and other information without the need for clear facial images, and is widely applied in the field of intelligent security. Traditional Re-ID systems mainly rely on high-definition RGB cameras, but the deployment of large-scale high-definition RGB cameras indoors has caused serious privacy and ethical concerns. Recently, wireless-based Re-ID systems (Wi-Fi, RFID, millimeter-wave radar, etc.) have shown promising prospects, but the limited sensing resolution hinders their practical deployment. In this paper, we propose WarmGait, a Re-ID system based on thermal array sensors, which can achieve high-precision Re-ID at low cost and minimize the invasion of user privacy. However, using thermal arrays for Re-ID still faces two major challenges. The first is the low and unclear texture resolution of images caused by low-cost infrared devices. The second is that existing gait recognition methods require maintaining the sequential constraint of gait images, which reduces the flexibility of gait recognition or Re-ID. To address these two challenges, we first designed an edge module inspired by Taylor Finite Difference (TFD) to aggregate image edge information to help improve the resolution of infrared devices. Then, we considered gait as a collection of gait profiles and extracted features from the frame level and collection level for recognition, breaking through the limitations of the number and order of input images. After extensive experimental evaluation, our model can achieve an average recognition accuracy of 87.3% in various scenarios, demonstrating the potential of WarmGait in Re-ID.",
      "authors": [
        "Hongbo Jiang",
        "Lei Ye",
        "Jingyang Hu",
        "Xiaotian Chen",
        "Siyu Chen",
        "Wei Zhang",
        "Kehua Yang"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 2,
      "year": 2026
    },
    {
      "title": "Auto-Encoding Variational Bayes",
      "arxiv_id": "1312.6114",
      "abstract": "How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.",
      "authors": [
        "Diederik P Kingma",
        "Max Welling"
      ],
      "published_date": "2013-12-20",
      "pdf_url": "https://arxiv.org/pdf/1312.6114v11",
      "citation_count": 17200,
      "year": 2013
    },
    {
      "title": "Supporting Online Material for Reducing the Dimensionality of Data with Neural Networks",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Geoffrey E. Hinton",
        "R. Salakhutdinov"
      ],
      "published_date": "2006",
      "pdf_url": "",
      "citation_count": 11479,
      "year": 2006
    },
    {
      "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "John C. Duchi",
        "Elad Hazan",
        "Y. Singer"
      ],
      "published_date": "2011",
      "pdf_url": "",
      "citation_count": 11103,
      "year": 2011
    },
    {
      "title": "Speech recognition with deep recurrent neural networks",
      "arxiv_id": "1303.5778",
      "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \\emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.",
      "authors": [
        "Alex Graves",
        "Abdel-rahman Mohamed",
        "Geoffrey Hinton"
      ],
      "published_date": "2013-03-22",
      "pdf_url": "https://arxiv.org/pdf/1303.5778v1",
      "citation_count": 8848,
      "year": 2013
    },
    {
      "title": "Weakly Supervised Image Dehazing via Physics-Based Decomposition",
      "arxiv_id": null,
      "abstract": "Recent weakly supervised image dehazing (WSID) works have succeeded to improve models’ generalization ability to real scene dehazing by using generative adversarial network (GAN) for unpaired image training. However, it is still difficult for current WSID methods to train one effective dehazing model for various scenes since 1) they always result in residual haze due to insufficient generalization to the feature distribution of real scenes, and 2) they are prone to cause distortions like color shifts, artifacts or halos etc, owing to embedding manual prior or threshold hypothesis for image reconstruction. To solve above problems, in this paper, we propose a novel WSID model via physics-based decomposition (PBD), which estimates atmospheric light, scattering coefficient and scene depth of real haze input to effectively capture the illumination information and haze distribution to recover a preliminary dehazed image by minimizing reconstruction loss. With this constraint, we subtly design a discrete wavelet discriminator (DWD) to effectively improve the generalization to real scene from both spatial and frequency aspect under the supervision of unpaired real clear image. Our PBD is a purely data-driven model freeing from any manual setting or partially correct prior, thus simultaneously ensuring the realness and visibility of dehazed images. Experiments on seven benchmarks verified the strong generalization ability of our PBD, which achieves SOTA dehazing performance with realistic details. Code will be published at https://github.com/NianWang-HJJGCDX/PBD",
      "authors": [
        "Nian Wang",
        "Zhigao Cui",
        "Yanzhao Su",
        "Yunwei Lan",
        "Yuanliang Xue",
        "Cong Zhang",
        "Aihua Li"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 10,
      "year": 2026
    },
    {
      "title": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer",
      "arxiv_id": null,
      "abstract": "As Adam optimizer’s learning rate decay hyperparameter has recently been deprecated, this journal article focuses not only on providing an alternate optimizer but also on comparing the performance of the said optimizer, AdamW, with the Adam optimizer using a face mask detection model. This study experiments with different weight decay values and finds that a weight decay of 0.00009 with the AdamW optimizer consistently achieves a 98% accuracy rate. Aside from that, this study also discusses the differences between Adam with L2-regularization and AdamW on how the weight decay is decoupled from the Adam optimizer’s gradient-based update that impacts the performance of AdamW. Overall, the study provides insights to those new to AdamW and looking for a starting point in optimizing deep learning models.",
      "authors": [
        "Leong Kah Meng",
        "Ho Hooi Yi",
        "Ng Bo Wei",
        "Lim Jia Xin",
        "Zailan Arabee Abdul Salam"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 9,
      "year": 2026
    },
    {
      "title": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
      "arxiv_id": "",
      "abstract": "While Mixture-of-Experts (MoE) scales capacity via conditional computation, Transformers lack a native primitive for knowledge lookup, forcing them to inefficiently simulate retrieval through computation. To address this, we introduce conditional memory as a complementary sparsity axis, instantiated via Engram, a module that modernizes classic $N$-gram embedding for O(1) lookup. By formulating the Sparsity Allocation problem, we uncover a U-shaped scaling law that optimizes the trade-off between neural computation (MoE) and static memory (Engram). Guided by this law, we scale Engram to 27B parameters, achieving superior performance over a strictly iso-parameter and iso-FLOPs MoE baseline. Most notably, while the memory module is expected to aid knowledge retrieval (e.g., MMLU +3.4; CMMLU +4.0), we observe even larger gains in general reasoning (e.g., BBH +5.0; ARC-Challenge +3.7) and code/math domains~(HumanEval +3.0; MATH +2.4). Mechanistic analyses reveal that Engram relieves the backbone's early layers from static reconstruction, effectively deepening the network for complex reasoning. Furthermore, by delegating local dependencies to lookups, it frees up attention capacity for global context, substantially boosting long-context retrieval (e.g., Multi-Query NIAH: 84.2 to 97.0). Finally, Engram establishes infrastructure-aware efficiency: its deterministic addressing enables runtime prefetching from host memory, incurring negligible overhead. We envision conditional memory as an indispensable modeling primitive for next-generation sparse models.",
      "authors": [
        "Xin Cheng",
        "Wangding Zeng",
        "Damai Dai",
        "Qinyu Chen",
        "Bingxuan Wang",
        "Zhenda Xie",
        "Kezhao Huang",
        "Xingkai Yu",
        "Zhewen Hao",
        "Yukun Li",
        "Han Zhang",
        "Huishuai Zhang",
        "Dongyan Zhao",
        "Wenfeng Liang"
      ],
      "published_date": "2026-01-12",
      "pdf_url": "https://arxiv.org/pdf/2601.07372v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction",
      "arxiv_id": null,
      "abstract": "Abstract. The past few years have witnessed the rise of neural networks (NNs) applications for hydrological time series modeling. By virtue of their capabilities, NN models can achieve unprecedented levels of performance when learning how to solve increasingly complex rainfall-runoff processes via data, making them pivotal for the development of computational hydrologic tasks such as flood predictions. The NN models should, to be considered practical, provide a probabilistic understanding of the model mechanisms and predictions and hints on what could perturb the model. In this paper, we developed two NN models, i.e., Neural Hierarchical Interpolation for Time Series Forecasting (N-HiTS) and Network-Based Expansion Analysis for Interpretable Time Series Forecasting (N-BEATS) with a probabilistic multi-quantile objective and benchmarked them with long short-term memory (LSTM) for flood prediction across two headwater streams in Georgia and North Carolina, USA. To generate a probabilistic prediction, a Multi-Quantile Loss was used to assess the 95th percentile prediction uncertainty (95 PPU) of multiple flooding events. Extensive experiments demonstrated the advantages of hierarchical interpolation and interpretable architecture, where both N-HiTS and N-BEATS provided an average accuracy improvement of ∼ 5 % over the LSTM benchmarking model. On a variety of flooding events, both N-HiTS and N-BEATS demonstrated significant performance improvements over the LSTM benchmark and showcased their probabilistic predictions by specifying a likelihood objective.",
      "authors": [
        "Mostafa Saberian",
        "Vidya Samadi",
        "Ioana Popescu"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 5,
      "year": 2026
    },
    {
      "title": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Husheng Fang",
        "Shunlin Liang",
        "Wenyuan Li",
        "Yongzhe Chen",
        "Han Ma",
        "Jianglei Xu",
        "Yichuan Ma",
        "Tao He",
        "Feng Tian",
        "Fengjiao Zhang",
        "Hui Liang"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 3,
      "year": 2026
    },
    {
      "title": "Going deeper with convolutions",
      "arxiv_id": "1409.4842",
      "abstract": "We propose a deep convolutional neural network architecture codenamed \"Inception\", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.",
      "authors": [
        "Christian Szegedy",
        "Wei Liu",
        "Yangqing Jia",
        "Pierre Sermanet",
        "Scott Reed",
        "Dragomir Anguelov",
        "Dumitru Erhan",
        "Vincent Vanhoucke",
        "Andrew Rabinovich"
      ],
      "published_date": "2014-09-17",
      "pdf_url": "https://arxiv.org/pdf/1409.4842v1",
      "citation_count": 46408,
      "year": 2014
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "arxiv_id": "1502.03167",
      "abstract": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.",
      "authors": [
        "Sergey Ioffe",
        "Christian Szegedy"
      ],
      "published_date": "2015-02-11",
      "pdf_url": "https://arxiv.org/pdf/1502.03167v3",
      "citation_count": 45918,
      "year": 2015
    },
    {
      "title": "ImageNet Large Scale Visual Recognition Challenge",
      "arxiv_id": "1409.0575",
      "abstract": "The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions.\n  This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.",
      "authors": [
        "Olga Russakovsky",
        "Jia Deng",
        "Hao Su",
        "Jonathan Krause",
        "Sanjeev Satheesh",
        "Sean Ma",
        "Zhiheng Huang",
        "Andrej Karpathy",
        "Aditya Khosla",
        "Michael Bernstein",
        "Alexander C. Berg",
        "Li Fei-Fei"
      ],
      "published_date": "2014-09-01",
      "pdf_url": "https://arxiv.org/pdf/1409.0575v3",
      "citation_count": 41748,
      "year": 2014
    },
    {
      "title": "Diffusion Transformers with Representation Autoencoders",
      "arxiv_id": "",
      "abstract": "Latent generative modeling, where a pretrained autoencoder maps pixels into a latent space for the diffusion process, has become the standard strategy for Diffusion Transformers (DiT); however, the autoencoder component has barely evolved. Most DiTs continue to rely on the original VAE encoder, which introduces several limitations: outdated backbones that compromise architectural simplicity, low-dimensional latent spaces that restrict information capacity, and weak representations that result from purely reconstruction-based training and ultimately limit generative quality. In this work, we explore replacing the VAE with pretrained representation encoders (e.g., DINO, SigLIP, MAE) paired with trained decoders, forming what we term Representation Autoencoders (RAEs). These models provide both high-quality reconstructions and semantically rich latent spaces, while allowing for a scalable transformer-based architecture. Since these latent spaces are typically high-dimensional, a key challenge is enabling diffusion transformers to operate effectively within them. We analyze the sources of this difficulty, propose theoretically motivated solutions, and validate them empirically. Our approach achieves faster convergence without auxiliary representation alignment losses. Using a DiT variant equipped with a lightweight, wide DDT head, we achieve strong image generation results on ImageNet: 1.51 FID at 256x256 (no guidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers clear advantages and should be the new default for diffusion transformer training.",
      "authors": [
        "Boyang Zheng",
        "Nanye Ma",
        "Shengbang Tong",
        "Saining Xie"
      ],
      "published_date": "2025-10-13",
      "pdf_url": "https://arxiv.org/pdf/2510.11690v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Scaling Large Language Models for Next-Generation Single-Cell Analysis",
      "arxiv_id": null,
      "abstract": "Single-cell RNA sequencing has transformed our understanding of cellular diversity, yet current single-cell foundation models (scFMs) remain limited in their scalability, flexibility across diverse tasks, and ability to natively integrate textual information. In this work, we build upon the Cell2Sentence (C2S) framework, which represents scRNA-seq profiles as textual “cell sentences,” to train Large Language Models (LLMs) on a corpus comprising over one billion tokens of transcriptomic data, biological text, and metadata. Scaling the model to 27 billion parameters yields consistent improvements in predictive and generative capabilities and supports advanced downstream tasks that require synthesis of information across multi-cellular contexts. Targeted fine-tuning with modern reinforcement learning techniques produces strong performance in perturbation response prediction, natural language interpretation, and complex biological reasoning. This predictive strength enabled a dual-context virtual screen that nominated the kinase inhibitor silmitasertib (CX-4945) as a candidate for context-selective upregulation of antigen presentation. Experimental assessment in human cell models unseen during training supported this prediction, demonstrating that C2S-Scale can effectively guide the discovery of context-conditioned biology. C2S-Scale unifies transcriptomic and textual data at unprecedented scales, surpassing both specialized single-cell models and general-purpose LLMs to provide a platform for next-generation single-cell analysis and the development of “virtual cells.”",
      "authors": [
        "S. Rizvi",
        "Daniel Levine",
        "Aakash Patel",
        "Shiyang Zhang",
        "Eric Wang",
        "Curtis Jamison Perry",
        "Ivan Vrkic",
        "Nicole Mayerli Constante",
        "Zirui Fu",
        "Sizhuang He",
        "David Zhang",
        "Cerise Tang",
        "Zhuoyang Lyu",
        "Rayyan Y Darji",
        "Chang Li",
        "Emily Sun",
        "David Jeong",
        "Lawrence Zhao",
        "J. Kwan",
        "David Braun",
        "Brian Hafler",
        "Hattie Chung",
        "R. M. Dhodapkar",
        "Paul F. Jaeger",
        "Bryan Perozzi",
        "Jeffrey Ishizuka",
        "Shekoofeh Azizi",
        "D. van Dijk"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 18,
      "year": 2025
    },
    {
      "title": "HybridVisionNet: An advanced hybrid deep learning framework for automated multi-class ocular disease diagnosis using fundus imaging",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Şafak Kılıç"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 13,
      "year": 2025
    },
    {
      "title": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights",
      "arxiv_id": null,
      "abstract": "Transferable adversarial images raise critical security concerns for computer vision systems in real-world, black-box attack scenarios. Although many transfer attacks have been proposed, existing research lacks a systematic and comprehensive evaluation. In this paper, we systemize transfer attacks into five categories around the general machine learning pipeline and provide the first comprehensive evaluation, with 23 representative attacks against 11 representative defenses, including the recent, transfer-oriented defense and the real-world Google Cloud Vision. In particular, we identify two main problems of existing evaluations: (1) for attack transferability, lack of intra-category analyses with fair hyperparameter settings, and (2) for attack stealthiness, lack of diverse measures. Our evaluation results validate that these problems have indeed caused misleading conclusions and missing points, and addressing them leads to new, <italic>consensus-challenging</italic> insights, such as (1) an early attack, DI, even outperforms all similar follow-up ones, (2) the state-of-the-art (white-box) defense, DiffPure, is even vulnerable to (black-box) transfer attacks, and (3) even under the same <inline-formula><tex-math notation=\"LaTeX\">$L_{p}$</tex-math><alternatives><mml:math><mml:msub><mml:mi>L</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math><inline-graphic xlink:href=\"zhao-ieq1-3610085.gif\"/></alternatives></inline-formula> constraint, different attacks yield dramatically different stealthiness results regarding diverse imperceptibility metrics, finer-grained measures, and a user study. We hope that our analyses will serve as guidance on properly evaluating transferable adversarial images and advance the design of attacks and defenses.",
      "authors": [
        "Zhengyu Zhao",
        "Hanwei Zhang",
        "Renjue Li",
        "R. Sicre",
        "L. Amsaleg",
        "Michael Backes",
        "Qi Li",
        "Chao Shen"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 13,
      "year": 2025
    },
    {
      "title": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot",
      "arxiv_id": null,
      "abstract": "To solve the labor shortage, robots have dramatically changed the world by combining powerful deep learning (DL) technology. Certainly, DL technology has become the key point of the widespread robot application. Efficient DL models depend on high-quality datasets and optimized architectures. However, some open datasets contain anomalous data that degrade model performance. Moreover, complex structures and high computational costs limit the adoption of DL models. This study proposes a dataset purification-based lightweight DL model construction strategy to solve these challenges. Initially, a dataset purification method is developed to filter out the anomaly data in a newly created dataset, which utilizes a lightweight cross-scale DL model OGNet to detect the anomaly data to achieve dataset purification. Subsequently, a highly efficient lightweight OGNet-based object detection (OD) model family, YOLO-OG, is presented to train the purified dataset. To evaluate the proposal, the strategy is implemented on the Empty-dish Recycling Robot. Experiments show that OGNet achieves excellent accuracy with only 0.68 M parameters and 0.35 GFLOPs. On purification Dish-10 dataset, the mean Average Precision (mAP) of YOLO-OG increases a maximum of 4.28% than original Dish-10 dataset. Meanwhile, YOLO-OG outperforms other advanced OD models, achieving the best accuracy of 99.20% mAP and the smallest 1.60 M parameters. YOLO-OG also reaches 99.86% mAP on the Dish-20 open dataset. On three other open datasets, YOLO-OG also shows excellent performance and surpasses most of the other OD models, which confirms the strong generalization ability of YOLO-OG.",
      "authors": [
        "Yifei Ge",
        "Zhuo Li",
        "Xuebin Yue",
        "Hengyi Li",
        "Lin Meng"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 10,
      "year": 2025
    },
    {
      "title": "Attention is All you Need",
      "arxiv_id": "1706.03762",
      "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan N. Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "published_date": "2017-06-12",
      "pdf_url": "https://arxiv.org/pdf/1706.03762v7",
      "citation_count": 164667,
      "year": 2017
    },
    {
      "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
      "arxiv_id": "1910.10683",
      "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",
      "authors": [
        "Colin Raffel",
        "Noam Shazeer",
        "Adam Roberts",
        "Katherine Lee",
        "Sharan Narang",
        "Michael Matena",
        "Yanqi Zhou",
        "Wei Li",
        "Peter J. Liu"
      ],
      "published_date": "2019-10-23",
      "pdf_url": "https://arxiv.org/pdf/1910.10683v4",
      "citation_count": 24191,
      "year": 2019
    },
    {
      "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "M. Heusel",
        "Hubert Ramsauer",
        "Thomas Unterthiner",
        "Bernhard Nessler",
        "Sepp Hochreiter"
      ],
      "published_date": "2017",
      "pdf_url": "",
      "citation_count": 16758,
      "year": 2017
    },
    {
      "title": "nuScenes: A Multimodal Dataset for Autonomous Driving",
      "arxiv_id": "1903.11027",
      "abstract": "Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image based benchmark datasets have driven development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. In this work we present nuTonomy scenes (nuScenes), the first dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. nuScenes comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. We define novel 3D detection and tracking metrics. We also provide careful dataset analysis as well as baselines for lidar and image based detection and tracking. Data, development kit and more information are available online.",
      "authors": [
        "Holger Caesar",
        "Varun Bankiti",
        "Alex H. Lang",
        "Sourabh Vora",
        "Venice Erin Liong",
        "Qiang Xu",
        "Anush Krishnan",
        "Yu Pan",
        "Giancarlo Baldan",
        "Oscar Beijbom"
      ],
      "published_date": "2019-03-26",
      "pdf_url": "https://arxiv.org/pdf/1903.11027v5",
      "citation_count": 7334,
      "year": 2019
    },
    {
      "title": "CARLA: An Open Urban Driving Simulator",
      "arxiv_id": "1711.03938",
      "abstract": "We introduce CARLA, an open-source simulator for autonomous driving research. CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems. In addition to open-source code and protocols, CARLA provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. The simulation platform supports flexible specification of sensor suites and environmental conditions. We use CARLA to study the performance of three approaches to autonomous driving: a classic modular pipeline, an end-to-end model trained via imitation learning, and an end-to-end model trained via reinforcement learning. The approaches are evaluated in controlled scenarios of increasing difficulty, and their performance is examined via metrics provided by CARLA, illustrating the platform's utility for autonomous driving research. The supplementary video can be viewed at https://youtu.be/Hp8Dz-Zek2E",
      "authors": [
        "Alexey Dosovitskiy",
        "German Ros",
        "Felipe Codevilla",
        "Antonio Lopez",
        "Vladlen Koltun"
      ],
      "published_date": "2017-11-10",
      "pdf_url": "https://arxiv.org/pdf/1711.03938v1",
      "citation_count": 6236,
      "year": 2017
    },
    {
      "title": "Sora as a World Model? A Complete Survey on Text-to-Video Generation",
      "arxiv_id": "2403.05131",
      "abstract": "The evolution of video generation from text, from animating MNIST to simulating the world with Sora, has progressed at a breakneck speed. Here, we systematically discuss how far text-to-video generation technology supports essential requirements in world modeling. We curate 250+ studies on text-based video synthesis and world modeling. We then observe that recent models increasingly support spatial, action, and strategic intelligences in world modeling through adherence to completeness, consistency, invention, as well as human interaction and control. We conclude that text-to-video generation is adept at world modeling, although homework in several aspects, such as the diversity-consistency trade-offs, remains to be addressed.",
      "authors": [
        "Fachrina Dewi Puspitasari",
        "Chaoning Zhang",
        "Joseph Cho",
        "Adnan Haider",
        "Noor Ul Eman",
        "Omer Amin",
        "Alexis Mankowski",
        "Muhammad Umair",
        "Jingyao Zheng",
        "Sheng Zheng",
        "Lik-Hang Lee",
        "Caiyan Qin",
        "Tae-Ho Kim",
        "Choong Seon Hong",
        "Yang Yang",
        "Heng Tao Shen"
      ],
      "published_date": "2024-03-08",
      "pdf_url": "https://arxiv.org/pdf/2403.05131v3",
      "citation_count": 66,
      "year": 2024
    },
    {
      "title": "OmniNWM: Omniscient Driving Navigation World Models",
      "arxiv_id": "",
      "abstract": "Autonomous driving world models are expected to work effectively across three core dimensions: state, action, and reward. Existing models, however, are typically restricted to limited state modalities, short video sequences, imprecise action control, and a lack of reward awareness. In this paper, we introduce OmniNWM, an omniscient panoramic navigation world model that addresses all three dimensions within a unified framework. For state, OmniNWM jointly generates panoramic videos of RGB, semantics, metric depth, and 3D occupancy. A flexible forcing strategy enables high-quality long-horizon auto-regressive generation. For action, we introduce a normalized panoramic Plucker ray-map representation that encodes input trajectories into pixel-level signals, enabling highly precise and generalizable control over panoramic video generation. Regarding reward, we move beyond learning reward functions with external image-based models: instead, we leverage the generated 3D occupancy to directly define rule-based dense rewards for driving compliance and safety. Extensive experiments demonstrate that OmniNWM achieves state-of-the-art performance in video generation, control accuracy, and long-horizon stability, while providing a reliable closed-loop evaluation framework through occupancy-grounded rewards. Project page is available at https://arlo0o.github.io/OmniNWM/.",
      "authors": [
        "Bohan Li",
        "Zhuang Ma",
        "Dalong Du",
        "Baorui Peng",
        "Zhujin Liang",
        "Zhenqiang Liu",
        "Chao Ma",
        "Yueming Jin",
        "Hao Zhao",
        "Wenjun Zeng",
        "Xin Jin"
      ],
      "published_date": "2025-10-21",
      "pdf_url": "https://arxiv.org/pdf/2510.18313v4",
      "citation_count": null,
      "year": null
    },
    {
      "title": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask",
      "arxiv_id": "",
      "abstract": "Autonomous driving relies on robust models trained on large-scale, high-quality multi-view driving videos. Although world models provide a cost-effective solution for generating realistic driving data, they often suffer from identity drift, where the same object changes its appearance or category across frames due to the absence of instance-level temporal constraints. We introduce ConsisDrive, an identity-preserving driving world model designed to enforce temporal consistency at the instance level. Our framework incorporates two key components: (1) Instance-Masked Attention, which applies instance identity masks and trajectory masks within attention blocks to ensure that visual tokens interact only with their corresponding instance features across spatial and temporal dimensions, thereby preserving object identity consistency; and (2) Instance-Masked Loss, which adaptively emphasizes foreground regions with probabilistic instance masking, reducing background noise while maintaining overall scene fidelity. By integrating these mechanisms, ConsisDrive achieves state-of-the-art driving video generation quality and demonstrates significant improvements in downstream autonomous driving tasks on the nuScenes dataset. Our project page is https://shanpoyang654.github.io/ConsisDrive/page.html.",
      "authors": [
        "Zhuoran Yang",
        "Yanyong Zhang"
      ],
      "published_date": "2026-02-03",
      "pdf_url": "https://arxiv.org/pdf/2602.03213v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving",
      "arxiv_id": "",
      "abstract": "World models have demonstrated significant promise for data synthesis in autonomous driving. However, existing methods predominantly concentrate on single-modality generation, typically focusing on either multi-camera video or LiDAR sequence synthesis. In this paper, we propose UniDriveDreamer, a single-stage unified multimodal world model for autonomous driving, which directly generates multimodal future observations without relying on intermediate representations or cascaded modules. Our framework introduces a LiDAR-specific variational autoencoder (VAE) designed to encode input LiDAR sequences, alongside a video VAE for multi-camera images. To ensure cross-modal compatibility and training stability, we propose Unified Latent Anchoring (ULA), which explicitly aligns the latent distributions of the two modalities. The aligned features are fused and processed by a diffusion transformer that jointly models their geometric correspondence and temporal evolution. Additionally, structured scene layout information is projected per modality as a conditioning signal to guide the synthesis. Extensive experiments demonstrate that UniDriveDreamer outperforms previous state-of-the-art methods in both video and LiDAR generation, while also yielding measurable improvements in downstream",
      "authors": [
        "Guosheng Zhao",
        "Yaozeng Wang",
        "Xiaofeng Wang",
        "Zheng Zhu",
        "Tingdong Yu",
        "Guan Huang",
        "Yongchen Zai",
        "Ji Jiao",
        "Changliang Xue",
        "Xiaole Wang",
        "Zhen Yang",
        "Futang Zhu",
        "Xingang Wang"
      ],
      "published_date": "2026-02-02",
      "pdf_url": "https://arxiv.org/pdf/2602.02002v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
      "arxiv_id": "",
      "abstract": "Recent video diffusion models generate photorealistic, temporally coherent videos, yet they fall short as reliable world models for autonomous driving, where structured motion and physically consistent interactions are essential. Adapting these generalist video models to driving domains has shown promise but typically requires massive domain-specific data and costly fine-tuning. We propose an efficient adaptation framework that converts generalist video diffusion models into controllable driving world models with minimal supervision. The key idea is to decouple motion learning from appearance synthesis. First, the model is adapted to predict structured motion in a simplified form: videos of skeletonized agents and scene elements, focusing learning on physical and social plausibility. Then, the same backbone is reused to synthesize realistic RGB videos conditioned on these motion sequences, effectively \"dressing\" the motion with texture and lighting. This two-stage process mirrors a reasoning-rendering paradigm: first infer dynamics, then render appearance. Our experiments show this decoupled approach is exceptionally efficient: adapting SVD, we match prior SOTA models with less than 6% of their compute. Scaling to LTX, our MAD-LTX model outperforms all open-source competitors, and supports a comprehensive suite of text, ego, and object controls. Project page: https://vita-epfl.github.io/MAD-World-Model/",
      "authors": [
        "Ahmad Rahimi",
        "Valentin Gerard",
        "Eloi Zablocki",
        "Matthieu Cord",
        "Alexandre Alahi"
      ],
      "published_date": "2026-01-14",
      "pdf_url": "https://arxiv.org/pdf/2601.09452v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "I and J",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "W. Marsden"
      ],
      "published_date": "2012",
      "pdf_url": "",
      "citation_count": 168812,
      "year": 2012
    },
    {
      "title": "ImageNet: A large-scale hierarchical image database",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Jia Deng",
        "Wei Dong",
        "R. Socher",
        "Li-Jia Li",
        "K. Li",
        "Li Fei-Fei"
      ],
      "published_date": "2009",
      "pdf_url": "",
      "citation_count": 70627,
      "year": 2009
    },
    {
      "title": "A and V",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "R. Stephenson"
      ],
      "published_date": "1962",
      "pdf_url": "",
      "citation_count": 51404,
      "year": 1962
    },
    {
      "title": "What matters for Representation Alignment: Global Information or Spatial Structure?",
      "arxiv_id": "",
      "abstract": "Representation alignment (REPA) guides generative training by distilling representations from a strong, pretrained vision encoder to intermediate diffusion features. We investigate a fundamental question: what aspect of the target representation matters for generation, its \\textit{global} \\revision{semantic} information (e.g., measured by ImageNet-1K accuracy) or its spatial structure (i.e. pairwise cosine similarity between patch tokens)? Prevalent wisdom holds that stronger global semantic performance leads to better generation as a target representation. To study this, we first perform a large-scale empirical analysis across 27 different vision encoders and different model scales. The results are surprising; spatial structure, rather than global performance, drives the generation performance of a target representation. To further study this, we introduce two straightforward modifications, which specifically accentuate the transfer of \\emph{spatial} information. We replace the standard MLP projection layer in REPA with a simple convolution layer and introduce a spatial normalization layer for the external representation. Surprisingly, our simple method (implemented in $<$4 lines of code), termed iREPA, consistently improves convergence speed of REPA, across a diverse set of vision encoders, model sizes, and training variants (such as REPA, REPA-E, Meanflow, JiT etc). %, etc. Our work motivates revisiting the fundamental working mechanism of representational alignment and how it can be leveraged for improved training of generative models. The code and project page are available at https://end2end-diffusion.github.io/irepa",
      "authors": [
        "Jaskirat Singh",
        "Xingjian Leng",
        "Zongze Wu",
        "Liang Zheng",
        "Richard Zhang",
        "Eli Shechtman",
        "Saining Xie"
      ],
      "published_date": "2025-12-11",
      "pdf_url": "https://arxiv.org/pdf/2512.10794v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "SCB-DETR: Multiscale Deformable Transformers for Occlusion-Resilient Student Learning Behavior Detection in Smart Classroom",
      "arxiv_id": null,
      "abstract": "The integration of artificial intelligence (AI) into the modern educational system is rapidly evolving, particularly in monitoring student behavior in classrooms—a task traditionally dependent on manual observation. This conventional method is notably inefficient, prompting a shift toward more advanced solutions such as computer vision. However, existing target detection models face significant challenges such as occlusion, blurring, and scale disparity, which are exacerbated by the dynamic and complex nature of classroom settings. Furthermore, these models must adeptly handle multiple target detection. To overcome these obstacles, we introduce the student classroom behavior detection with multiscale deformable transformers (SCB-DETR), an innovative approach that utilizes large convolutional kernels for upstream feature extraction, and multiscale feature fusion. This technique significantly improves the detection capabilities for multiscale and occluded targets, offering a robust solution for analyzing student behavior. SCB-DETR establishes an end-to-end system that simplifies the detection process and consistently outperforms other deep learning methods. Employing our custom student classroom behavior (SCBehavior) dataset, SCB-DETR achieves a mean Average Precision (mAP) of 0.626, which is a 1.5% improvement over the baseline model’s mAP and a 6% increase in AP50. These results demonstrate SCB-DETR’s superior performance in handling the uneven distribution of student behaviors and ensuring precise detection in dynamic classroom environments. The source code of this study is publicly available at https://github.com/CCNUZFW/SCB-DETR.",
      "authors": [
        "Zhifeng Wang",
        "Minghui Wang",
        "Chunyan Zeng",
        "Longlong Li"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 7,
      "year": 2025
    },
    {
      "title": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination",
      "arxiv_id": null,
      "abstract": "This article introduces a robust deep feature ultrasound image-based visual servoing (UIBVS) technique for an ultrasound robot focusing on automatic cardiac examination. To this end, a convolutional neural network named ultrasound-cardiac-feature-net (UCF-Net) is developed, which is trained in a supervised manner to process ultrasound images and generate a set of six image features referred to as deep ultrasound image features. To enhance the robustness of UCF-Net against the variables that affect the ultrasound image quality, such as interaction normal force, scan depth, dynamic range, power, and gain, several datasets with different sets of parameters are gathered for training. Deep ultrasound image features enable an eye-in-hand robot to interact with the human body through UIBVS. To implement UIBVS, a filtered integral quasi-super-twisting algorithm (FIQSTA) is synthesized as the primary controller. Interaction force control is also considered within a hybrid vision/force control framework, providing compliance with the body and increasing the safety of the interaction. The proof of the robustness and stability of FIQSTA is also investigated. Experimental results on a cardiac phantom for four main views, i.e., parasternal short axis, parasternal long axis, subcostal, and apical four chambers views, and a trajectory passing through the main views demonstrate the feasibility of the proposed method for cardiac examination and the superior performance of the main controller to other well-known methods, including proportional (P) controller, sliding mode controller, super-twisting algorithm (STA), and integral quasi-STA.",
      "authors": [
        "Ehsan Zakeri",
        "Amanda Spilkin",
        "Hanae Elmekki",
        "Antonela Zanuttini",
        "L. Kadem",
        "Jamal Bentahar",
        "Wen-Fang Xie",
        "Philippe Pibarot"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 5,
      "year": 2025
    },
    {
      "title": "Bio-Inspired Fine-Tuning for Selective Transfer Learning in Image Classification",
      "arxiv_id": "2601.11235",
      "abstract": "Deep learning has significantly advanced image analysis across diverse domains but often depends on large, annotated datasets for success. Transfer learning addresses this challenge by utilizing pre-trained models to tackle new tasks with limited labeled data. However, discrepancies between source and target domains can hinder effective transfer learning. We introduce BioTune, a novel adaptive fine-tuning technique utilizing evolutionary optimization. BioTune enhances transfer learning by optimally choosing which layers to freeze and adjusting learning rates for unfrozen layers. Through extensive evaluation on nine image classification datasets, spanning natural and specialized domains such as medical imaging, BioTune demonstrates superior accuracy and efficiency over state-of-the-art fine-tuning methods, including AutoRGN and LoRA, highlighting its adaptability to various data characteristics and distribution changes. Additionally, BioTune consistently achieves top performance across four different CNN architectures, underscoring its flexibility. Ablation studies provide valuable insights into the impact of BioTune's key components on overall performance. The source code is available at https://github.com/davilac/BioTune.",
      "authors": [
        "Ana Davila",
        "Jacinto Colan",
        "Yasuhisa Hasegawa"
      ],
      "published_date": "2026-01-16",
      "pdf_url": "https://arxiv.org/pdf/2601.11235v1",
      "citation_count": 3,
      "year": 2026
    },
    {
      "title": "Hand Sign Language Detection Using Deep Learning",
      "arxiv_id": "2601.08262",
      "abstract": "Hand gesture recognition is an important aspect of human-computer interaction. It forms the basis of sign language for the visually impaired people. This work proposes a novel hand gesture recognizing system for the differently-abled persons. The model uses a convolutional neural network, known as VGG-16 net, for building a trained model on a widely used image dataset by employing Python and Keras libraries. Furthermore, the result is validated by the NUS dataset, consisting of 10 classes of hand gestures, fed to the model as the validation set. Afterwards, a testing dataset of 10 classes is built by employing Google's open source Application Programming Interface (API) that captures different gestures of human hand and the efficacy is then measured by carrying out experiments. The experimental results show that by combining a transfer learning mechanism together with the image data augmentation, the VGG-16 net produced around 98% accuracy.",
      "authors": [
        "Subham Sharma",
        "Sharmila Subudhi"
      ],
      "published_date": "2026-01-13",
      "pdf_url": "https://arxiv.org/pdf/2601.08262v1",
      "citation_count": 3,
      "year": 2026
    },
    {
      "title": "Representation Learning: A Review and New Perspectives",
      "arxiv_id": "",
      "abstract": "The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.",
      "authors": [
        "Yoshua Bengio",
        "Aaron Courville",
        "Pascal Vincent"
      ],
      "published_date": "2012-06-24",
      "pdf_url": "https://arxiv.org/pdf/1206.5538v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion",
      "arxiv_id": null,
      "abstract": "",
      "authors": [],
      "published_date": "",
      "pdf_url": "",
      "citation_count": 7442,
      "year": 2010
    },
    {
      "title": "In Advances in Neural Information Processing Systems",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "D. Touretzky",
        "M. C. Mozer",
        "M. E. Hasselmo",
        "RegressionChristopher",
        "I. K.",
        "WilliamsNeural",
        "GroupAston",
        "UniversityBirmingham"
      ],
      "published_date": "1996",
      "pdf_url": "",
      "citation_count": 6307,
      "year": 1996
    },
    {
      "title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Danilo Jimenez Rezende",
        "S. Mohamed",
        "Daan Wierstra"
      ],
      "published_date": "2014",
      "pdf_url": "",
      "citation_count": 5526,
      "year": 2014
    },
    {
      "title": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation",
      "arxiv_id": "",
      "abstract": "We propose a diffusion distillation method that achieves new state-of-the-art in one-step/few-step 1024px text-to-image generation based on SDXL. Our method combines progressive and adversarial distillation to achieve a balance between quality and mode coverage. In this paper, we discuss the theoretical analysis, discriminator design, model formulation, and training techniques. We open-source our distilled SDXL-Lightning models both as LoRA and full UNet weights.",
      "authors": [
        "Shanchuan Lin",
        "Anran Wang",
        "Xiao Yang"
      ],
      "published_date": "2024-02-21",
      "pdf_url": "https://arxiv.org/pdf/2402.13929v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions",
      "arxiv_id": "",
      "abstract": "The area of portrait image animation, propelled by audio input, has witnessed notable progress in the generation of lifelike and dynamic portraits. Conventional methods are limited to utilizing either audios or facial key points to drive images into videos, while they can yield satisfactory results, certain issues exist. For instance, methods driven solely by audios can be unstable at times due to the relatively weaker audio signal, while methods driven exclusively by facial key points, although more stable in driving, can result in unnatural outcomes due to the excessive control of key point information. In addressing the previously mentioned challenges, in this paper, we introduce a novel approach which we named EchoMimic. EchoMimic is concurrently trained using both audios and facial landmarks. Through the implementation of a novel training strategy, EchoMimic is capable of generating portrait videos not only by audios and facial landmarks individually, but also by a combination of both audios and selected facial landmarks. EchoMimic has been comprehensively compared with alternative algorithms across various public datasets and our collected dataset, showcasing superior performance in both quantitative and qualitative evaluations. Additional visualization and access to the source code can be located on the EchoMimic project page.",
      "authors": [
        "Zhiyuan Chen",
        "Jiajiong Cao",
        "Zhiquan Chen",
        "Yuming Li",
        "Chenguang Ma"
      ],
      "published_date": "2024-07-11",
      "pdf_url": "https://arxiv.org/pdf/2407.08136v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "GenAD: Generative End-to-End Autonomous Driving",
      "arxiv_id": "",
      "abstract": "Directly producing planning results from raw sensors has been a long-desired solution for autonomous driving and has attracted increasing attention recently. Most existing end-to-end autonomous driving methods factorize this problem into perception, motion prediction, and planning. However, we argue that the conventional progressive pipeline still cannot comprehensively model the entire traffic evolution process, e.g., the future interaction between the ego car and other traffic participants and the structural trajectory prior. In this paper, we explore a new paradigm for end-to-end autonomous driving, where the key is to predict how the ego car and the surroundings evolve given past scenes. We propose GenAD, a generative framework that casts autonomous driving into a generative modeling problem. We propose an instance-centric scene tokenizer that first transforms the surrounding scenes into map-aware instance tokens. We then employ a variational autoencoder to learn the future trajectory distribution in a structural latent space for trajectory prior modeling. We further adopt a temporal model to capture the agent and ego movements in the latent space to generate more effective future trajectories. GenAD finally simultaneously performs motion prediction and planning by sampling distributions in the learned structural latent space conditioned on the instance tokens and using the learned temporal model to generate futures. Extensive experiments on the widely used nuScenes benchmark show that the proposed GenAD achieves state-of-the-art performance on vision-centric end-to-end autonomous driving with high efficiency. Code: https://github.com/wzzheng/GenAD.",
      "authors": [
        "Wenzhao Zheng",
        "Ruiqi Song",
        "Xianda Guo",
        "Chenming Zhang",
        "Long Chen"
      ],
      "published_date": "2024-02-18",
      "pdf_url": "https://arxiv.org/pdf/2402.11502v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model",
      "arxiv_id": "",
      "abstract": "Current Multimodal Large Language Models (MLLMs) typically integrate a pre-trained LLM with another pre-trained vision transformer through a connector, such as an MLP, endowing the LLM with visual capabilities. However, the misalignment between two embedding strategies in MLLMs -- the structural textual embeddings based on an embedding look-up table and the continuous embeddings generated directly by the vision encoder -- makes challenges for a more seamless fusion of visual and textual information. We propose Ovis, a novel MLLM architecture designed to structurally align visual and textual embeddings. Ovis integrates an additional learnable visual embedding table into the visual encoder's process. To capture rich visual semantics, each image patch indexes the visual embedding table multiple times, resulting in a final visual embedding that is a probabilistic combination of the indexed embeddings. This structural approach mirrors the method used for generating textual embeddings. Empirical evaluations on various multimodal benchmarks show that Ovis outperforms open-source MLLMs of similar parameter scales and even surpasses the proprietary model Qwen-VL-Plus overall. These results highlight the potential of Ovis' structured visual representation for advancing MLLM architectural design and promoting more effective multimodal learning. Code, datasets, and models are available at https://github.com/AIDC-AI/Ovis.",
      "authors": [
        "Shiyin Lu",
        "Yang Li",
        "Qing-Guo Chen",
        "Zhao Xu",
        "Weihua Luo",
        "Kaifu Zhang",
        "Han-Jia Ye"
      ],
      "published_date": "2024-05-31",
      "pdf_url": "https://arxiv.org/pdf/2405.20797v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Improving Video Generation with Human Feedback",
      "arxiv_id": "",
      "abstract": "Video generation has achieved significant advances through rectified flow techniques, but issues like unsmooth motion and misalignment between videos and prompts persist. In this work, we develop a systematic pipeline that harnesses human feedback to mitigate these problems and refine the video generation model. Specifically, we begin by constructing a large-scale human preference dataset focused on modern video generation models, incorporating pairwise annotations across multi-dimensions. We then introduce VideoReward, a multi-dimensional video reward model, and examine how annotations and various design choices impact its rewarding efficacy. From a unified reinforcement learning perspective aimed at maximizing reward with KL regularization, we introduce three alignment algorithms for flow-based models. These include two training-time strategies: direct preference optimization for flow (Flow-DPO) and reward weighted regression for flow (Flow-RWR), and an inference-time technique, Flow-NRG, which applies reward guidance directly to noisy videos. Experimental results indicate that VideoReward significantly outperforms existing reward models, and Flow-DPO demonstrates superior performance compared to both Flow-RWR and supervised fine-tuning methods. Additionally, Flow-NRG lets users assign custom weights to multiple objectives during inference, meeting personalized video quality needs.",
      "authors": [
        "Jie Liu",
        "Gongye Liu",
        "Jiajun Liang",
        "Ziyang Yuan",
        "Xiaokun Liu",
        "Mingwu Zheng",
        "Xiele Wu",
        "Qiulin Wang",
        "Menghan Xia",
        "Xintao Wang",
        "Xiaohong Liu",
        "Fei Yang",
        "Pengfei Wan",
        "Di Zhang",
        "Kun Gai",
        "Yujiu Yang",
        "Wanli Ouyang"
      ],
      "published_date": "2025-01-23",
      "pdf_url": "https://arxiv.org/pdf/2501.13918v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Speech Recognition with Deep Recurrent Neural Networks",
      "arxiv_id": "",
      "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \\emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.",
      "authors": [
        "Alex Graves",
        "Abdel-rahman Mohamed",
        "Geoffrey Hinton"
      ],
      "published_date": "2013-03-22",
      "pdf_url": "https://arxiv.org/pdf/1303.5778v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Learning representations by back-propagating errors",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "D. Rumelhart",
        "Geoffrey E. Hinton",
        "Ronald J. Williams"
      ],
      "published_date": "1986",
      "pdf_url": "",
      "citation_count": 30052,
      "year": 1986
    },
    {
      "title": "Bidirectional recurrent neural networks",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "M. Schuster",
        "K. Paliwal"
      ],
      "published_date": "1997",
      "pdf_url": "",
      "citation_count": 9325,
      "year": 1997
    },
    {
      "title": "Connectionist Temporal Classiﬁcation: Labelling Unsegmented Sequence Data with Recurrent Neural Networks",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Alex Graves",
        "Santiago Fern´andez",
        "Faustino J. Gomez",
        "J¨urgen Schmidhuber"
      ],
      "published_date": "0",
      "pdf_url": "",
      "citation_count": 4898,
      "year": 0
    },
    {
      "title": "2005 Special Issue: Framewise phoneme classification with bidirectional LSTM and other neural network architectures",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Alex Graves",
        "J. Schmidhuber"
      ],
      "published_date": "2005",
      "pdf_url": "",
      "citation_count": 4761,
      "year": 2005
    },
    {
      "title": "A high-performance neuroprosthesis for speech decoding and avatar control",
      "arxiv_id": null,
      "abstract": "",
      "authors": [],
      "published_date": "",
      "pdf_url": "",
      "citation_count": 393,
      "year": 2023
    },
    {
      "title": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses",
      "arxiv_id": null,
      "abstract": "The latest progress of emerging smart flexible sensing systems driven by brain-inspired artificial intelligence (AI) from both the algorithm (machine learning) and the framework (artificial synapses) level is reviewed. New enabling features such as powerful data analysis and intelligent decision-making resulting from the fusion of AI technology with flexible sensors are discussed. Promising application prospects of AI-driven smart flexible sensing systems such as more intelligent monitoring for human activities, more humanoid feeling by artificial sensory organs, and more autonomous action of soft robotics are demonstrated. The latest progress of emerging smart flexible sensing systems driven by brain-inspired artificial intelligence (AI) from both the algorithm (machine learning) and the framework (artificial synapses) level is reviewed. New enabling features such as powerful data analysis and intelligent decision-making resulting from the fusion of AI technology with flexible sensors are discussed. Promising application prospects of AI-driven smart flexible sensing systems such as more intelligent monitoring for human activities, more humanoid feeling by artificial sensory organs, and more autonomous action of soft robotics are demonstrated. The recent wave of the artificial intelligence (AI) revolution has aroused unprecedented interest in the intelligentialize of human society. As an essential component that bridges the physical world and digital signals, flexible sensors are evolving from a single sensing element to a smarter system, which is capable of highly efficient acquisition, analysis, and even perception of vast, multifaceted data. While challenging from a manual perspective, the development of intelligent flexible sensing has been remarkably facilitated owing to the rapid advances of brain-inspired AI innovations from both the algorithm (machine learning) and the framework (artificial synapses) level. This review presents the recent progress of the emerging AI-driven, intelligent flexible sensing systems. The basic concept of machine learning and artificial synapses are introduced. The new enabling features induced by the fusion of AI and flexible sensing are comprehensively reviewed, which significantly advances the applications such as flexible sensory systems, soft/humanoid robotics, and human activity monitoring. As two of the most profound innovations in the twenty-first century, the deep incorporation of flexible sensing and AI technology holds tremendous potential for creating a smarter world for human beings.",
      "authors": [
        "Tianming Sun",
        "Bin Feng",
        "Jinpeng Huo",
        "Yu Xiao",
        "Wengan Wang",
        "Jin Peng",
        "Zehua Li",
        "Chengjie Du",
        "Wenxian Wang",
        "G. Zou",
        "Lei Liu"
      ],
      "published_date": "2023",
      "pdf_url": "",
      "citation_count": 240,
      "year": 2023
    },
    {
      "title": "A high-performance speech neuroprosthesis",
      "arxiv_id": null,
      "abstract": "Speech brain–computer interfaces (BCIs) have the potential to restore rapid communication to people with paralysis by decoding neural activity evoked by attempted speech into text^ 1 , 2 or sound^ 3 , 4 . Early demonstrations, although promising, have not yet achieved accuracies sufficiently high for communication of unconstrained sentences from a large vocabulary^ 1 – 7 . Here we demonstrate a speech-to-text BCI that records spiking activity from intracortical microelectrode arrays. Enabled by these high-resolution recordings, our study participant—who can no longer speak intelligibly owing to amyotrophic lateral sclerosis—achieved a 9.1% word error rate on a 50-word vocabulary (2.7 times fewer errors than the previous state-of-the-art speech BCI^ 2 ) and a 23.8% word error rate on a 125,000-word vocabulary (the first successful demonstration, to our knowledge, of large-vocabulary decoding). Our participant’s attempted speech was decoded  at 62 words per minute, which is 3.4 times as fast as the previous record^ 8 and begins to approach the speed of natural conversation (160 words per minute^ 9 ). Finally, we highlight two aspects of the neural code for speech that are encouraging for speech BCIs: spatially intermixed tuning to speech articulators that makes accurate decoding possible from only a small region of cortex, and a detailed articulatory representation of phonemes that persists years after paralysis. These results show a feasible path forward for restoring rapid communication to people with paralysis who can no longer speak. A speech-to-text brain–computer interface that records spiking activity from intracortical microelectrode arrays enabled an individual who cannot speak intelligibly to achieve 9.1 and 23.8% word error rates on a 50- and 125,000-word vocabulary, respectively.",
      "authors": [
        "Francis R. Willett",
        "Erin M. Kunz",
        "Chaofei Fan",
        "Donald T. Avansino",
        "G. Wilson",
        "Eun Young Choi",
        "Foram B. Kamdar",
        "M. Glasser",
        "L. Hochberg",
        "S. Druckmann",
        "K. Shenoy",
        "J. Henderson"
      ],
      "published_date": "2023",
      "pdf_url": "",
      "citation_count": 225,
      "year": 2023
    },
    {
      "title": "Loss of plasticity in deep continual learning",
      "arxiv_id": null,
      "abstract": "Artificial neural networks, deep-learning methods and the backpropagation algorithm1 form the foundation of modern machine learning and artificial intelligence. These methods are almost always used in two phases, one in which the weights of the network are updated and one in which the weights are held constant while the network is used or evaluated. This contrasts with natural learning and many applications, which require continual learning. It has been unclear whether or not deep learning methods work in continual learning settings. Here we show that they do not—that standard deep-learning methods gradually lose plasticity in continual-learning settings until they learn no better than a shallow network. We show such loss of plasticity using the classic ImageNet dataset and reinforcement-learning problems across a wide range of variations in the network and the learning algorithm. Plasticity is maintained indefinitely only by algorithms that continually inject diversity into the network, such as our continual backpropagation algorithm, a variation of backpropagation in which a small fraction of less-used units are continually and randomly reinitialized. Our results indicate that methods based on gradient descent are not enough—that sustained deep learning requires a random, non-gradient component to maintain variability and plasticity. The pervasive problem of artificial neural networks losing plasticity in continual-learning settings is demonstrated and a simple solution called the continual backpropagation algorithm is described to prevent this issue.",
      "authors": [
        "Shibhansh Dohare",
        "J. F. Hernandez-Garcia",
        "Qingfeng Lan",
        "Parash Rahman",
        "A. Mahmood",
        "R. Sutton"
      ],
      "published_date": "2024",
      "pdf_url": "",
      "citation_count": 222,
      "year": 2024
    },
    {
      "title": "An analog-AI chip for energy-efficient speech recognition and transcription",
      "arxiv_id": null,
      "abstract": "A low-power chip that runs AI models using analog rather than digital computation shows comparable accuracy on speech-recognition tasks but is more than 14 times as energy efficient. Models of artificial intelligence (AI) that have billions of parameters can achieve high accuracy across a range of tasks^ 1 , 2 , but they exacerbate the poor energy efficiency of conventional general-purpose processors, such as graphics processing units or central processing units. Analog in-memory computing (analog-AI)^ 3 – 7 can provide better energy efficiency by performing matrix–vector multiplications in parallel on ‘memory tiles’. However, analog-AI has yet to demonstrate software-equivalent (SW_eq) accuracy on models that require many such tiles and efficient communication of neural-network activations between the tiles. Here we present an analog-AI chip that combines 35 million phase-change memory devices across 34 tiles, massively parallel inter-tile communication and analog, low-power peripheral circuitry that can achieve up to 12.4 tera-operations per second per watt (TOPS/W) chip-sustained performance. We demonstrate fully end-to-end SW_eq accuracy for a small keyword-spotting network and near-SW_eq accuracy on the much larger MLPerf^ 8 recurrent neural-network transducer (RNNT), with more than 45 million weights mapped onto more than 140 million phase-change memory devices across five chips.",
      "authors": [
        "S. Ambrogio",
        "P. Narayanan",
        "A. Okazaki",
        "A. Fasoli",
        "C. Mackin",
        "K. Hosokawa",
        "A. Nomura",
        "Takeo Yasuda",
        "An Chen",
        "A. Friz",
        "M. Ishii",
        "J. Luquin",
        "Y. Kohda",
        "N. Saulnier",
        "K. Brew",
        "Samuel Choi",
        "I. Ok",
        "Timothy Philip",
        "Victor Chan",
        "M. Silvestre",
        "Ishtiaq Ahsan",
        "Vijay Narayanan",
        "H. Tsai",
        "Geoffrey W. Burr"
      ],
      "published_date": "2023",
      "pdf_url": "",
      "citation_count": 184,
      "year": 2023
    },
    {
      "title": "A Mathematical Theory of Communication",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "J. Shin",
        "Sang Joon Kim"
      ],
      "published_date": "2006",
      "pdf_url": "",
      "citation_count": 72220,
      "year": 2006
    },
    {
      "title": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers",
      "arxiv_id": "",
      "abstract": "Tokenization is fundamental to how language models represent and process text, yet the behavior of widely used BPE tokenizers has received far less study than model architectures and training. In this paper, we investigate intermediate merge residues in BPE vocabularies: tokens that are frequent during merge learning so that retained in the final vocabulary, but are mostly further merged and rarely emitted when tokenizing the corpus during tokenizer usage. Such low-frequency tokens not only waste vocabulary capacity but also increase vulnerability to adversarial or atypical inputs. We present a systematic empirical characterization of this phenomenon across commonly used tokenizers and introduce LiteToken, a simple method for removing residue tokens. Because the affected tokens are rarely used, pretrained models can often accommodate the modified tokenizer without additional fine-tuning. Experiments show that LiteToken reduces token fragmentation, reduces parameters, and improves robustness to noisy or misspelled inputs, while preserving overall performance.",
      "authors": [
        "Yike Sun",
        "Haotong Yang",
        "Zhouchen Lin",
        "Muhan Zhang"
      ],
      "published_date": "2026-02-04",
      "pdf_url": "https://arxiv.org/pdf/2602.04706v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling",
      "arxiv_id": "",
      "abstract": "Scaling Large Language Models (LLMs) typically relies on increasing the number of parameters or test-time computations to boost performance. However, these strategies are impractical for edge device deployment due to limited RAM and NPU resources. Despite hardware constraints, deploying performant LLM on edge devices such as smartphone remains crucial for user experience. To address this, we propose MeKi (Memory-based Expert Knowledge Injection), a novel system that scales LLM capacity via storage space rather than FLOPs. MeKi equips each Transformer layer with token-level memory experts that injects pre-stored semantic knowledge into the generation process. To bridge the gap between training capacity and inference efficiency, we employ a re-parameterization strategy to fold parameter matrices used during training into a compact static lookup table. By offloading the knowledge to ROM, MeKi decouples model capacity from computational cost, introducing zero inference latency overhead. Extensive experiments demonstrate that MeKi significantly outperforms dense LLM baselines with identical inference speed, validating the effectiveness of memory-based scaling paradigm for on-device LLMs. Project homepage is at https://github.com/ningding-o/MeKi.",
      "authors": [
        "Ning Ding",
        "Fangcheng Liu",
        "Kyungrae Kim",
        "Linji Hao",
        "Kyeng-Hun Lee",
        "Hyeonmok Ko",
        "Yehui Tang"
      ],
      "published_date": "2026-02-03",
      "pdf_url": "https://arxiv.org/pdf/2602.03359v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram",
      "arxiv_id": "",
      "abstract": "Current genomic foundation models (GFMs) rely on extensive neural computation to implicitly approximate conserved biological motifs from single-nucleotide inputs. We propose Gengram, a conditional memory module that introduces an explicit and highly efficient lookup primitive for multi-base motifs via a genomic-specific hashing scheme, establishing genomic \"syntax\". Integrated into the backbone of state-of-the-art GFMs, Gengram achieves substantial gains (up to 14%) across several functional genomics tasks. The module demonstrates robust architectural generalization, while further inspection of Gengram's latent space reveals the emergence of meaningful representations that align closely with fundamental biological knowledge. By establishing structured motif memory as a modeling primitive, Gengram simultaneously boosts empirical performance and mechanistic interpretability, providing a scalable and biology-aligned pathway for the next generation of GFMs. The code is available at https://github.com/zhejianglab/Genos, and the model checkpoint is available at https://huggingface.co/ZhejiangLab/Gengram.",
      "authors": [
        "Huinan Xu",
        "Xuyang Feng",
        "Junhong Chen",
        "Junchen Liu",
        "Kaiwen Deng",
        "Kai Ding",
        "Shengning Long",
        "Jiaxue Shuai",
        "Zhaorong Li",
        "Shiping Liu",
        "Guirong Xue",
        "Zhan Xiao"
      ],
      "published_date": "2026-01-29",
      "pdf_url": "https://arxiv.org/pdf/2601.22203v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "L$^3$: Large Lookup Layers",
      "arxiv_id": "",
      "abstract": "Modern sparse language models typically achieve sparsity through Mixture-of-Experts (MoE) layers, which dynamically route tokens to dense MLP \"experts.\" However, dynamic hard routing has a number of drawbacks, such as potentially poor hardware efficiency and needing auxiliary losses for stable training. In contrast, the tokenizer embedding table, which is natively sparse, largely avoids these issues by selecting a single embedding per token at the cost of not having contextual information. In this work, we introduce the Large Lookup Layer (L$^3$), which unlocks a new axis of sparsity by generalizing embedding tables to model decoder layers. L$^3$ layers use static token-based routing to aggregate a set of learned embeddings per token in a context-dependent way, allowing the model to efficiently balance memory and compute by caching information in embeddings. L$^3$ has two main components: (1) a systems-friendly architecture that allows for fast training and CPU-offloaded inference with no overhead, and (2) an information-theoretic embedding allocation algorithm that effectively balances speed and quality. We empirically test L$^3$ by training transformers with up to 2.6B active parameters and find that L$^3$ strongly outperforms both dense models and iso-sparse MoEs in both language modeling and downstream tasks.",
      "authors": [
        "Albert Tseng",
        "Christopher De Sa"
      ],
      "published_date": "2026-01-29",
      "pdf_url": "https://arxiv.org/pdf/2601.21461v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Scaling Embeddings Outperforms Scaling Experts in Language Models",
      "arxiv_id": "",
      "abstract": "While Mixture-of-Experts (MoE) architectures have become the standard for sparsity scaling in large language models, they increasingly face diminishing returns and system-level bottlenecks. In this work, we explore embedding scaling as a potent, orthogonal dimension for scaling sparsity. Through a comprehensive analysis and experiments, we identify specific regimes where embedding scaling achieves a superior Pareto frontier compared to expert scaling. We systematically characterize the critical architectural factors governing this efficacy -- ranging from parameter budgeting to the interplay with model width and depth. Moreover, by integrating tailored system optimizations and speculative decoding, we effectively convert this sparsity into tangible inference speedups. Guided by these insights, we introduce LongCat-Flash-Lite, a 68.5B parameter model with ~3B activated trained from scratch. Despite allocating over 30B parameters to embeddings, LongCat-Flash-Lite not only surpasses parameter-equivalent MoE baselines but also exhibits exceptional competitiveness against existing models of comparable scale, particularly in agentic and coding domains.",
      "authors": [
        "Hong Liu",
        "Jiaqi Zhang",
        "Chao Wang",
        "Xing Hu",
        "Linkun Lyu",
        "Jiaqi Sun",
        "Xurui Yang",
        "Bo Wang",
        "Fengcun Li",
        "Yulei Qian",
        "Lingtong Si",
        "Yerui Sun",
        "Rumei Li",
        "Peng Pei",
        "Yuchen Xie",
        "Xunliang Cai"
      ],
      "published_date": "2026-01-29",
      "pdf_url": "https://arxiv.org/pdf/2601.21204v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Going Deeper with Convolutions",
      "arxiv_id": "",
      "abstract": "We propose a deep convolutional neural network architecture codenamed \"Inception\", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.",
      "authors": [
        "Christian Szegedy",
        "Wei Liu",
        "Yangqing Jia",
        "Pierre Sermanet",
        "Scott Reed",
        "Dragomir Anguelov",
        "Dumitru Erhan",
        "Vincent Vanhoucke",
        "Andrew Rabinovich"
      ],
      "published_date": "2014-09-17",
      "pdf_url": "https://arxiv.org/pdf/1409.4842v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Gradient-based learning applied to document recognition",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Yann LeCun",
        "L. Bottou",
        "Yoshua Bengio",
        "P. Haffner"
      ],
      "published_date": "1998",
      "pdf_url": "",
      "citation_count": 58683,
      "year": 1998
    },
    {
      "title": "Regression Shrinkage and Selection via the Lasso",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "R. Tibshirani"
      ],
      "published_date": "1996",
      "pdf_url": "",
      "citation_count": 50388,
      "year": 1996
    },
    {
      "title": "Microsoft COCO: Common Objects in Context",
      "arxiv_id": "1405.0312",
      "abstract": "We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.",
      "authors": [
        "Tsung-Yi Lin",
        "Michael Maire",
        "Serge Belongie",
        "Lubomir Bourdev",
        "Ross Girshick",
        "James Hays",
        "Pietro Perona",
        "Deva Ramanan",
        "C. Lawrence Zitnick",
        "Piotr Dollár"
      ],
      "published_date": "2014-05-01",
      "pdf_url": "https://arxiv.org/pdf/1405.0312v3",
      "citation_count": 50042,
      "year": 2014
    },
    {
      "title": "LifeCLEF Plant Identification Task 2015",
      "arxiv_id": "",
      "abstract": "The LifeCLEF plant identification challenge aims at evaluating plant identification methods and systems at a very large scale, close to the conditions of a real-world biodiversity monitoring scenario. The 2015 evaluation was actually conducted on a set of more than 100K images illustrating 1000 plant species living in West Europe. The main originality of this dataset is that it was built through a large-scale participatory sensing plateform initiated in 2011 and which now involves tens of thousands of contributors. This overview presents more precisely the resources and assessments of the challenge, summarizes the approaches and systems employed by the participating research groups, and provides an analysis of the main outcomes.",
      "authors": [
        "Herve Goeau",
        "Pierre Bonnet",
        "Alexis Joly"
      ],
      "published_date": "2025-09-28",
      "pdf_url": "https://arxiv.org/pdf/2509.23891v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics",
      "arxiv_id": "",
      "abstract": "The Internet of Things (IoT) has recently proliferated in both size and complexity. Using multi-source and heterogeneous IoT data aids in providing efficient data analytics for a variety of prevalent and crucial applications. To address the privacy and security concerns raised by analyzing IoT data locally or in the cloud, distributed data analytics techniques were proposed to collect and analyze data in edge or fog devices. In this context, federated learning has been recommended as an ideal distributed machine/deep learning-based technique for edge/fog computing environments. Additionally, the data analytics results are time-sensitive; they should be generated with minimal latency and high reliability. As a result, reusing efficient architectures validated through a high number of challenging test cases would be advantageous. The work proposed here presents a solution using a microservices-based architecture that allows an IoT application to be structured as a collection of fine-grained, loosely coupled, and reusable entities. The proposed solution uses the promising capabilities of federated learning to provide intelligent microservices that ensure efficient, flexible, and extensible data analytics. This solution aims to deliver cloud calculations to the edge to reduce latency and bandwidth congestion while protecting the privacy of exchanged data. The proposed approach was validated through an IoT-malware detection and classification use case. MaleVis, a publicly available dataset, was used in the experiments to analyze and validate the proposed approach. This dataset included more than 14,000 RGB-converted images, comprising 25 malware classes and one benign class. The results showed that our proposed approach outperformed existing state-of-the-art methods in terms of detection and classification performance, with a 99.24%.",
      "authors": [
        "Safa Ben Atitallah",
        "Maha Driss",
        "Henda Ben Ghezela"
      ],
      "published_date": "2025-10-22",
      "pdf_url": "https://arxiv.org/pdf/2510.20852v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Enhancing pine wilt disease detection with synthetic data and external attention-based transformers",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Sareer Ul Amin",
        "Yonghoon Jung",
        "Muhammad Fayaz",
        "Bumsoo Kim",
        "Sanghyun Seo"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 30,
      "year": 2025
    },
    {
      "title": "A comprehensive review on YOLO versions for object detection",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Ayşe Aybilge Murat",
        "M. S. Kıran"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 27,
      "year": 2025
    },
    {
      "title": "Harnessing large vision and language models in agriculture: a review",
      "arxiv_id": null,
      "abstract": "Introduction Agriculture is a cornerstone of human society but faces significant challenges, including pests, diseases, and the need for increased production efficiency. Large models, encompassing large language models, large vision models, and multimodal large language models, have shown transformative potential in various domains. This review aims to explore the potential applications of these models in agriculture to address existing problems and improve production. Methods We conduct a systematic review of the development trajectories and key capabilities of large models. A bibliometric analysis of literature from Web of Science and arXiv is performed to quantify the current research focus and identify the gap between the potential and the application of large models in the agricultural sector. Results Our analysis confirms that agriculture is an emerging but currently underrepresented field for large model research. Nevertheless, we identify and categorize promising applications, including tailored models for agricultural question-answering, robotic automation, and advanced image analysis from remote sensing and spectral data. These applications demonstrate significant potential to solve complex, nuanced agricultural tasks. Discussion This review culminates in a pragmatic framework to guide the choice between large and traditional models, balancing data availability against deployment constraints. We also highlight critical challenges, including data acquisition, infrastructure barriers, and the significant ethical considerations for responsible deployment. We conclude that while tailored large models are poised to greatly enhance agricultural efficiency and yield, realizing this future requires a concerted effort to overcome the existing technical, infrastructural, and ethical hurdles.",
      "authors": [
        "Hongyan Zhu",
        "Shuai Qin",
        "Min Su",
        "Chengzhi Lin",
        "Anjie Li",
        "Junfeng Gao"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 22,
      "year": 2025
    },
    {
      "title": "Multi-axis vision transformer for medical image segmentation",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Abdul Rehman Khan",
        "Asifullah Khan"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 8,
      "year": 2025
    },
    {
      "title": "A comprehensive review of facial beauty prediction using deep learning techniques",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "D. E. Boukhari",
        "F. Dornaika",
        "A. Chemsa",
        "Abdelmalik Taleb-Ahmed"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 7,
      "year": 2025
    },
    {
      "title": "Active learning framework leveraging transcriptomics identifies modulators of disease phenotypes.",
      "arxiv_id": null,
      "abstract": "Phenotypic drug screening remains constrained by the vastness of chemical space and technical challenges scaling experimental workflows. To overcome these barriers, computational methods have been developed to prioritize compounds, but they rely on either single-task models lacking generalizability or heuristic-based genomic proxies that resist optimization. We designed an active deep-learning framework that leverages omics to enable scalable, optimizable identification of compounds that induce complex phenotypes. Our generalizable algorithm outperformed state-of-the-art models on classical recall, translating to a 13-17x increase in phenotypic hit-rate across two hematological discovery campaigns. Combining this algorithm with a lab-in-the-loop signature refinement step, we achieved an additional two-fold increase in hit-rate and molecular insights. In sum, our framework enables efficient phenotypic hit identification campaigns, with broad potential to accelerate drug discovery.",
      "authors": [
        "Benjamin DeMeo",
        "Charlotte Nesbitt",
        "S. A. Miller",
        "Daniel B. Burkhardt",
        "Inna Lipchina",
        "Doris Fu",
        "Peter Holderreith",
        "David Kim",
        "Sergey Kolchenko",
        "Artur Szałata",
        "Ishan Gupta",
        "Christine Kerr",
        "Thomas Pfefer",
        "Raziel Rojas-Rodriguez",
        "Sunil Kuppassani",
        "Laurens Kruidenier",
        "Parul B Doshi",
        "Mahdi Zamanighomi",
        "James J. Collins",
        "A. Shalek",
        "F. Theis",
        "Mauricio Cortes"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 6,
      "year": 2025
    },
    {
      "title": "Distinctive Image Features from Scale-Invariant Keypoints",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "D. Lowe"
      ],
      "published_date": "2004",
      "pdf_url": "",
      "citation_count": 49312,
      "year": 2004
    },
    {
      "title": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training",
      "arxiv_id": "",
      "abstract": "We present LLaVA-OneVision-1.5, a novel family of Large Multimodal Models (LMMs) that achieve state-of-the-art performance with significantly reduced computational and financial costs. Different from the existing works, LLaVA-OneVision-1.5 provides an open, efficient, and reproducible framework for building high-quality vision-language models entirely from scratch. The LLaVA-OneVision-1.5 release comprises three primary components: (1) Large-Scale Curated Datasets: We construct an 85M concept-balanced pretraining dataset LLaVA-OneVision-1.5-Mid-Traning and a meticulously curated 22M instruction dataset LLaVA-OneVision-1.5-Instruct. (2) Efficient Training Framework: We develop a complete end-to-end efficient training framework leveraging an offline parallel data packing strategy to facilitate the training of LLaVA-OneVision-1.5 within a $16,000 budget. (3) State-of-the-art Performance: Experimental results demonstrate that LLaVA-OneVision-1.5 yields exceptionally competitive performance across a broad range of downstream tasks. Specifically, LLaVA-OneVision-1.5-8B outperforms Qwen2.5-VL-7B on 18 of 27 benchmarks, and LLaVA-OneVision-1.5-4B surpasses Qwen2.5-VL-3B on all 27 benchmarks. (4) RL-based Post-training: We unlock the model's latent potential through a lightweight RL stage, effectively eliciting robust chain-of-thought reasoning to significantly boost performance on complex multimodal reasoning tasks.",
      "authors": [
        "Xiang An",
        "Yin Xie",
        "Kaicheng Yang",
        "Wenkang Zhang",
        "Xiuwei Zhao",
        "Zheng Cheng",
        "Yirui Wang",
        "Songcen Xu",
        "Changrui Chen",
        "Didi Zhu",
        "Chunsheng Wu",
        "Huajie Tan",
        "Chunyuan Li",
        "Jing Yang",
        "Jie Yu",
        "Xiyao Wang",
        "Bin Qin",
        "Yumeng Wang",
        "Zizhen Yan",
        "Ziyong Feng",
        "Ziwei Liu",
        "Bo Li",
        "Jiankang Deng"
      ],
      "published_date": "2025-09-28",
      "pdf_url": "https://arxiv.org/pdf/2509.23661v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications",
      "arxiv_id": "",
      "abstract": "Amid growing efforts to leverage advances in large language models (LLMs) and vision-language models (VLMs) for robotics, Vision-Language-Action (VLA) models have recently gained significant attention. By unifying vision, language, and action data at scale, which have traditionally been studied separately, VLA models aim to learn policies that generalise across diverse tasks, objects, embodiments, and environments. This generalisation capability is expected to enable robots to solve novel downstream tasks with minimal or no additional task-specific data, facilitating more flexible and scalable real-world deployment. Unlike previous surveys that focus narrowly on action representations or high-level model architectures, this work offers a comprehensive, full-stack review, integrating both software and hardware components of VLA systems. In particular, this paper provides a systematic review of VLAs, covering their strategy and architectural transition, architectures and building blocks, modality-specific processing techniques, and learning paradigms. In addition, to support the deployment of VLAs in real-world robotic applications, we also review commonly used robot platforms, data collection strategies, publicly available datasets, data augmentation methods, and evaluation benchmarks. Throughout this comprehensive survey, this paper aims to offer practical guidance for the robotics community in applying VLAs to real-world robotic systems. All references categorized by training approach, evaluation method, modality, and dataset are available in the table on our project website: https://vla-survey.github.io .",
      "authors": [
        "Kento Kawaharazuka",
        "Jihoon Oh",
        "Jun Yamada",
        "Ingmar Posner",
        "Yuke Zhu"
      ],
      "published_date": "2025-10-08",
      "pdf_url": "https://arxiv.org/pdf/2510.07077v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Aligning machine and human visual representations across abstraction levels",
      "arxiv_id": null,
      "abstract": "Deep neural networks have achieved success across a wide range of applications, including as models of human behaviour and neural representations in vision tasks1,2. However, neural network training and human learning differ in fundamental ways, and neural networks often fail to generalize as robustly as humans do3,4, raising questions regarding the similarity of their underlying representations. We need to determine what is missing for modern learning systems to exhibit more human-aligned behaviour. Here we highlight a key misalignment between vision models and humans: whereas human conceptual knowledge is hierarchically organized from fine- to coarse-scale distinctions (for example, ref. 5), model representations do not accurately capture all these levels of abstraction. To address this misalignment, we first train a teacher model to imitate human judgements, then transfer human-aligned structure from its representations to refine the representations of pretrained state-of-the-art vision foundation models via fine-tuning. These human-aligned models more accurately approximate human behaviour and uncertainty across a wide range of similarity tasks, including a dataset of human judgements spanning multiple levels of semantic abstractions. They also perform better on a diverse set of machine learning tasks, increasing generalization and out-of-distribution robustness. Thus, infusing neural networks with additional human knowledge yields a best-of-both-worlds representation that is both more consistent with human cognitive judgements and more practically useful, paving the way towards more robust, interpretable and human-aligned artificial intelligence systems. Aligning foundation models with human judgments enables them to more accurately approximate human behaviour and uncertainty across various levels of visual abstraction, while additionally improving their generalization performance.",
      "authors": [
        "Lukas Muttenthaler",
        "Klaus Greff",
        "Frieda Born",
        "Bernhard Spitzer",
        "Simon Kornblith",
        "M. C. Mozer",
        "Klaus-Robert Muller",
        "Thomas Unterthiner",
        "Andrew Kyle Lampinen"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 26,
      "year": 2025
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "arxiv_id": "2010.11929",
      "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly",
        "Jakob Uszkoreit",
        "Neil Houlsby"
      ],
      "published_date": "2020-10-22",
      "pdf_url": "https://arxiv.org/pdf/2010.11929v2",
      "citation_count": 56617,
      "year": 2020
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "arxiv_id": "2103.00020",
      "abstract": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",
      "authors": [
        "Alec Radford",
        "Jong Wook Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark",
        "Gretchen Krueger",
        "Ilya Sutskever"
      ],
      "published_date": "2021-02-26",
      "pdf_url": "https://arxiv.org/pdf/2103.00020v1",
      "citation_count": 42549,
      "year": 2021
    },
    {
      "title": "GENERATIVE ADVERSARIAL NETS",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Individualized Treat",
        "Jinsung Yoon"
      ],
      "published_date": "2018",
      "pdf_url": "",
      "citation_count": 39432,
      "year": 2018
    },
    {
      "title": "Improved Mean Flows: On the Challenges of Fastforward Generative Models",
      "arxiv_id": "",
      "abstract": "MeanFlow (MF) has recently been established as a framework for one-step generative modeling. However, its ``fastforward'' nature introduces key challenges in both the training objective and the guidance mechanism. First, the original MF's training target depends not only on the underlying ground-truth fields but also on the network itself. To address this issue, we recast the objective as a loss on the instantaneous velocity $v$, re-parameterized by a network that predicts the average velocity $u$. Our reformulation yields a more standard regression problem and improves the training stability. Second, the original MF fixes the classifier-free guidance scale during training, which sacrifices flexibility. We tackle this issue by formulating guidance as explicit conditioning variables, thereby retaining flexibility at test time. The diverse conditions are processed through in-context conditioning, which reduces model size and benefits performance. Overall, our $\\textbf{improved MeanFlow}$ ($\\textbf{iMF}$) method, trained entirely from scratch, achieves $\\textbf{1.72}$ FID with a single function evaluation (1-NFE) on ImageNet 256$\\times$256. iMF substantially outperforms prior methods of this kind and closes the gap with multi-step methods while using no distillation. We hope our work will further advance fastforward generative modeling as a stand-alone paradigm.",
      "authors": [
        "Zhengyang Geng",
        "Yiyang Lu",
        "Zongze Wu",
        "Eli Shechtman",
        "J. Zico Kolter",
        "Kaiming He"
      ],
      "published_date": "2025-12-01",
      "pdf_url": "https://arxiv.org/pdf/2512.02012v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training",
      "arxiv_id": "",
      "abstract": "Pixel-space generative models are often more difficult to train and generally underperform compared to their latent-space counterparts, leaving a persistent performance and efficiency gap. In this paper, we introduce a novel two-stage training framework that closes this gap for pixel-space diffusion and consistency models. In the first stage, we pre-train encoders to capture meaningful semantics from clean images while aligning them with points along the same deterministic sampling trajectory, which evolves points from the prior to the data distribution. In the second stage, we integrate the encoder with a randomly initialized decoder and fine-tune the complete model end-to-end for both diffusion and consistency models. Our framework achieves state-of-the-art (SOTA) performance on ImageNet. Specifically, our diffusion model reaches an FID of 1.58 on ImageNet-256 and 2.35 on ImageNet-512 with 75 number of function evaluations (NFE) surpassing prior pixel-space methods and VAE-based counterparts by a large margin in both generation quality and training efficiency. In a direct comparison, our model significantly outperforms DiT while using only around 30\\% of its training compute.",
      "authors": [
        "Jiachen Lei",
        "Keli Liu",
        "Julius Berner",
        "Haiming Yu",
        "Hongkai Zheng",
        "Jiahong Wu",
        "Xiangxiang Chu"
      ],
      "published_date": "2025-10-14",
      "pdf_url": "https://arxiv.org/pdf/2510.12586v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
      "arxiv_id": "2512.11749",
      "abstract": "Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain. By leveraging a standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 on GenEval and 85.78 on DPG-Bench. This performance validates the intrinsic representational power of VFMs for generative tasks. We fully open-source the project, including the autoencoder and generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation.",
      "authors": [
        "Minglei Shi",
        "Haolin Wang",
        "Borui Zhang",
        "Wenzhao Zheng",
        "Bohan Zeng",
        "Ziyang Yuan",
        "Xiaoshi Wu",
        "Yuanxing Zhang",
        "Huan Yang",
        "Xintao Wang",
        "Pengfei Wan",
        "Kun Gai",
        "Jie Zhou",
        "Jiwen Lu"
      ],
      "published_date": "2025-12-12",
      "pdf_url": "https://arxiv.org/pdf/2512.11749v1",
      "citation_count": 5,
      "year": 2025
    },
    {
      "title": "PixelDiT: Pixel Diffusion Transformers for Image Generation",
      "arxiv_id": "",
      "abstract": "Latent-space modeling has been the standard for Diffusion Transformers (DiTs). However, it relies on a two-stage pipeline where the pretrained autoencoder introduces lossy reconstruction, leading to error accumulation while hindering joint optimization. To address these issues, we propose PixelDiT, a single-stage, end-to-end model that eliminates the need for the autoencoder and learns the diffusion process directly in the pixel space. PixelDiT adopts a fully transformer-based architecture shaped by a dual-level design: a patch-level DiT that captures global semantics and a pixel-level DiT that refines texture details, enabling efficient training of a pixel-space diffusion model while preserving fine details. Our analysis reveals that effective pixel-level token modeling is essential to the success of pixel diffusion. PixelDiT achieves 1.61 FID on ImageNet 256x256, surpassing existing pixel generative models by a large margin. We further extend PixelDiT to text-to-image generation and pretrain it at the 1024x1024 resolution in pixel space. It achieves 0.74 on GenEval and 83.5 on DPG-bench, approaching the best latent diffusion models.",
      "authors": [
        "Yongsheng Yu",
        "Wei Xiong",
        "Weili Nie",
        "Yichen Sheng",
        "Shiqiu Liu",
        "Jiebo Luo"
      ],
      "published_date": "2025-11-25",
      "pdf_url": "https://arxiv.org/pdf/2511.20645v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models",
      "arxiv_id": "",
      "abstract": "Unified multimodal models (UMMs) aim to jointly perform multimodal understanding and generation within a single framework. We present TUNA, a native UMM that builds a unified continuous visual representation by cascading a VAE encoder with a representation encoder. This unified representation space allows end-to-end processing of images and videos for both understanding and generation tasks. Compared to prior UMMs with decoupled representations, TUNA's unified visual space avoids representation format mismatches introduced by separate encoders, outperforming decoupled alternatives in both understanding and generation. Moreover, we observe that stronger pretrained representation encoders consistently yield better performance across all multimodal tasks, highlighting the importance of the representation encoder. Finally, in this unified setting, jointly training on both understanding and generation data allows the two tasks to benefit from each other rather than interfere. Our extensive experiments on multimodal understanding and generation benchmarks show that TUNA achieves state-of-the-art results in image and video understanding, image and video generation, and image editing, demonstrating the effectiveness and scalability of its unified representation design.",
      "authors": [
        "Zhiheng Liu",
        "Weiming Ren",
        "Haozhe Liu",
        "Zijian Zhou",
        "Shoufa Chen",
        "Haonan Qiu",
        "Xiaoke Huang",
        "Zhaochong An",
        "Fanny Yang",
        "Aditya Patel",
        "Viktar Atliha",
        "Tony Ng",
        "Xiao Han",
        "Chuyan Zhu",
        "Chenyang Zhang",
        "Ding Liu",
        "Juan-Manuel Perez-Rua",
        "Sen He",
        "Jürgen Schmidhuber",
        "Wenhu Chen",
        "Ping Luo",
        "Wei Liu",
        "Tao Xiang",
        "Jonas Schult",
        "Yuren Cong"
      ],
      "published_date": "2025-12-01",
      "pdf_url": "https://arxiv.org/pdf/2512.02014v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "arxiv_id": "1810.04805",
      "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\n  BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "published_date": "2018-10-11",
      "pdf_url": "https://arxiv.org/pdf/1810.04805v2",
      "citation_count": 109629,
      "year": 2019
    },
    {
      "title": "A comprehensive review of object detection with traditional and deep learning methods",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Vrushali Pagire",
        "M. Chavali",
        "Ashish Kale"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 19,
      "year": 2025
    },
    {
      "title": "Diffusion Language Models are Super Data Learners",
      "arxiv_id": "",
      "abstract": "Under strictly controlled pre-training settings, we observe a Crossover: when unique data is limited, diffusion language models (DLMs) consistently surpass autoregressive (AR) models by training for more epochs. The crossover shifts later with more or higher-quality data, earlier with larger models, and persists across dense and sparse architectures. We attribute the gains to three compounding factors: (1) any-order modeling, (2) super-dense compute from iterative bidirectional denoising, and (3) built-in Monte Carlo augmentation; input or parameter noise improves AR under data constraint but cannot close the gap. At scale, a 1.7B DLM trained with a ~1.5T-token compute budget on 10B unique Python tokens overtakes an AR coder trained with strictly matched settings. In addition, a 1B-parameter DLM achieves > 56% accuracy on HellaSwag and > 33% on MMLU using only 1B tokens, without any special tricks, just by repeating standard pre-training data. We also show that rising validation cross-entropy does not imply degraded downstream performance in this regime.",
      "authors": [
        "Jinjie Ni",
        "Qian Liu",
        "Longxu Dou",
        "Chao Du",
        "Zili Wang",
        "Hang Yan",
        "Tianyu Pang",
        "Michael Qizhe Shieh"
      ],
      "published_date": "2025-11-05",
      "pdf_url": "https://arxiv.org/pdf/2511.03276v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas",
      "arxiv_id": "",
      "abstract": "Diffusion Language Models (DLMs) present a compelling alternative to autoregressive models, offering flexible, any-order infilling without specialized prompting design. However, their practical utility is blocked by a critical limitation: the requirement of a fixed-length masked sequence for generation. This constraint severely degrades code infilling performance when the predefined mask size mismatches the ideal completion length. To address this, we propose DreamOn, a novel diffusion framework that enables dynamic, variable-length generation. DreamOn augments the diffusion process with two length control states, allowing the model to autonomously expand or contract the output length based solely on its own predictions. We integrate this mechanism into existing DLMs with minimal modifications to the training objective and no architectural changes. Built upon Dream-Coder-7B and DiffuCoder-7B, DreamOn achieves infilling performance on par with state-of-the-art autoregressive models on HumanEval-Infilling and SantaCoder-FIM and matches oracle performance achieved with ground-truth length. Our work removes a fundamental barrier to the practical deployment of DLMs, significantly advancing their flexibility and applicability for variable-length generation. Our code is available at https://github.com/DreamLM/DreamOn.",
      "authors": [
        "Zirui Wu",
        "Lin Zheng",
        "Zhihui Xie",
        "Jiacheng Ye",
        "Jiahui Gao",
        "Shansan Gong",
        "Yansong Feng",
        "Zhenguo Li",
        "Wei Bi",
        "Guorui Zhou",
        "Lingpeng Kong"
      ],
      "published_date": "2026-02-01",
      "pdf_url": "https://arxiv.org/pdf/2602.01326v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "FLEX: Continuous Agent Evolution via Forward Learning from Experience",
      "arxiv_id": "",
      "abstract": "Autonomous agents driven by Large Language Models (LLMs) have revolutionized reasoning and problem-solving but remain static after training, unable to grow with experience as intelligent beings do during deployment. We introduce Forward Learning with EXperience (FLEX), a gradient-free learning paradigm that enables LLM agents to continuously evolve through accumulated experience. Specifically, FLEX cultivates scalable and inheritable evolution by constructing a structured experience library through continual reflection on successes and failures during interaction with the environment. FLEX delivers substantial improvements on mathematical reasoning, chemical retrosynthesis, and protein fitness prediction (up to 23% on AIME25, 10% on USPTO50k, and 14% on ProteinGym). We further identify a clear scaling law of experiential growth and the phenomenon of experience inheritance across agents, marking a step toward scalable and inheritable continuous agent evolution. Project Page: https://flex-gensi-thuair.github.io.",
      "authors": [
        "Zhicheng Cai",
        "Xinyuan Guo",
        "Yu Pei",
        "Jiangtao Feng",
        "Jinsong Su",
        "Jiangjie Chen",
        "Ya-Qin Zhang",
        "Wei-Ying Ma",
        "Mingxuan Wang",
        "Hao Zhou"
      ],
      "published_date": "2025-11-09",
      "pdf_url": "https://arxiv.org/pdf/2511.06449v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning",
      "arxiv_id": "",
      "abstract": "Large Language Models (LLMs) are increasingly being explored for building Agents capable of active environmental interaction (e.g., via tool use) to solve complex problems. Reinforcement Learning (RL) is considered a key technology with significant potential for training such Agents; however, the effective application of RL to LLM Agents is still in its nascent stages and faces considerable challenges. Currently, this emerging field lacks in-depth exploration into RL approaches specifically tailored for the LLM Agent context, alongside a scarcity of flexible and easily extensible training frameworks designed for this purpose. To help advance this area, this paper first revisits and clarifies Reinforcement Learning methodologies for LLM Agents by systematically extending the Markov Decision Process (MDP) framework to comprehensively define the key components of an LLM Agent. Secondly, we introduce Agent-R1, a modular, flexible, and user-friendly training framework for RL-based LLM Agents, designed for straightforward adaptation across diverse task scenarios and interactive environments. We conducted experiments on Multihop QA benchmark tasks, providing initial validation for the effectiveness of our proposed methods and framework.",
      "authors": [
        "Mingyue Cheng",
        "Jie Ouyang",
        "Shuo Yu",
        "Ruiran Yan",
        "Yucong Luo",
        "Zirui Liu",
        "Daoyu Wang",
        "Qi Liu",
        "Enhong Chen"
      ],
      "published_date": "2025-11-18",
      "pdf_url": "https://arxiv.org/pdf/2511.14460v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "nuScenes: A multimodal dataset for autonomous driving",
      "arxiv_id": "",
      "abstract": "Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image based benchmark datasets have driven development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. In this work we present nuTonomy scenes (nuScenes), the first dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. nuScenes comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. We define novel 3D detection and tracking metrics. We also provide careful dataset analysis as well as baselines for lidar and image based detection and tracking. Data, development kit and more information are available online.",
      "authors": [
        "Holger Caesar",
        "Varun Bankiti",
        "Alex H. Lang",
        "Sourabh Vora",
        "Venice Erin Liong",
        "Qiang Xu",
        "Anush Krishnan",
        "Yu Pan",
        "Giancarlo Baldan",
        "Oscar Beijbom"
      ],
      "published_date": "2019-03-26",
      "pdf_url": "https://arxiv.org/pdf/1903.11027v5",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Squeeze-and-Excitation Networks",
      "arxiv_id": "",
      "abstract": "The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the \"Squeeze-and-Excitation\" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of ~25%. Models and code are available at https://github.com/hujie-frank/SENet.",
      "authors": [
        "Jie Hu",
        "Li Shen",
        "Samuel Albanie",
        "Gang Sun",
        "Enhua Wu"
      ],
      "published_date": "2017-09-05",
      "pdf_url": "https://arxiv.org/pdf/1709.01507v4",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Decoupled Weight Decay Regularization",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "I. Loshchilov",
        "F. Hutter"
      ],
      "published_date": "2017",
      "pdf_url": "",
      "citation_count": 30284,
      "year": 2017
    },
    {
      "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
      "arxiv_id": "",
      "abstract": "Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.\n  To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.",
      "authors": [
        "Mingxing Tan",
        "Quoc V. Le"
      ],
      "published_date": "2019-05-28",
      "pdf_url": "https://arxiv.org/pdf/1905.11946v5",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control",
      "arxiv_id": "",
      "abstract": "Recent advances in illumination control extend image-based methods to video, yet still facing a trade-off between lighting fidelity and temporal consistency. Moving beyond relighting, a key step toward generative modeling of real-world scenes is the joint control of camera trajectory and illumination, since visual dynamics are inherently shaped by both geometry and lighting. To this end, we present Light-X, a video generation framework that enables controllable rendering from monocular videos with both viewpoint and illumination control. 1) We propose a disentangled design that decouples geometry and lighting signals: geometry and motion are captured via dynamic point clouds projected along user-defined camera trajectories, while illumination cues are provided by a relit frame consistently projected into the same geometry. These explicit, fine-grained cues enable effective disentanglement and guide high-quality illumination. 2) To address the lack of paired multi-view and multi-illumination videos, we introduce Light-Syn, a degradation-based pipeline with inverse-mapping that synthesizes training pairs from in-the-wild monocular footage. This strategy yields a dataset covering static, dynamic, and AI-generated scenes, ensuring robust training. Extensive experiments show that Light-X outperforms baseline methods in joint camera-illumination control and surpasses prior video relighting methods under both text- and background-conditioned settings.",
      "authors": [
        "Tianqi Liu",
        "Zhaoxi Chen",
        "Zihao Huang",
        "Shaocong Xu",
        "Saining Zhang",
        "Chongjie Ye",
        "Bohan Li",
        "Zhiguo Cao",
        "Wei Li",
        "Hao Zhao",
        "Ziwei Liu"
      ],
      "published_date": "2025-12-04",
      "pdf_url": "https://arxiv.org/pdf/2512.05115v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World",
      "arxiv_id": "",
      "abstract": "World models have become crucial for autonomous driving, as they learn how scenarios evolve over time to address the long-tail challenges of the real world. However, current approaches relegate world models to limited roles: they operate within ostensibly unified architectures that still keep world prediction and motion planning as decoupled processes. To bridge this gap, we propose DriveLaW, a novel paradigm that unifies video generation and motion planning. By directly injecting the latent representation from its video generator into the planner, DriveLaW ensures inherent consistency between high-fidelity future generation and reliable trajectory planning. Specifically, DriveLaW consists of two core components: DriveLaW-Video, our powerful world model that generates high-fidelity forecasting with expressive latent representations, and DriveLaW-Act, a diffusion planner that generates consistent and reliable trajectories from the latent of DriveLaW-Video, with both components optimized by a three-stage progressive training strategy. The power of our unified paradigm is demonstrated by new state-of-the-art results across both tasks. DriveLaW not only advances video prediction significantly, surpassing best-performing work by 33.3% in FID and 1.8% in FVD, but also achieves a new record on the NAVSIM planning benchmark.",
      "authors": [
        "Tianze Xia",
        "Yongkang Li",
        "Lijun Zhou",
        "Jingfeng Yao",
        "Kaixin Xiong",
        "Haiyang Sun",
        "Bing Wang",
        "Kun Ma",
        "Guang Chen",
        "Hangjun Ye",
        "Wenyu Liu",
        "Xinggang Wang"
      ],
      "published_date": "2025-12-29",
      "pdf_url": "https://arxiv.org/pdf/2512.23421v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "DVGT: Driving Visual Geometry Transformer",
      "arxiv_id": "",
      "abstract": "Perceiving and reconstructing 3D scene geometry from visual inputs is crucial for autonomous driving. However, there still lacks a driving-targeted dense geometry perception model that can adapt to different scenarios and camera configurations. To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed multi-view visual inputs. We first extract visual features for each image using a DINO backbone, and employ alternating intra-view local attention, cross-view spatial attention, and cross-frame temporal attention to infer geometric relations across images. We then use multiple heads to decode a global point map in the ego coordinate of the first frame and the ego poses for each frame. Unlike conventional methods that rely on precise camera parameters, DVGT is free of explicit 3D geometric priors, enabling flexible processing of arbitrary camera configurations. DVGT directly predicts metric-scaled geometry from image sequences, eliminating the need for post-alignment with external sensors. Trained on a large mixture of driving datasets including nuScenes, OpenScene, Waymo, KITTI, and DDAD, DVGT significantly outperforms existing models on various scenarios. Code is available at https://github.com/wzzheng/DVGT.",
      "authors": [
        "Sicheng Zuo",
        "Zixun Xie",
        "Wenzhao Zheng",
        "Shaoqing Xu",
        "Fang Li",
        "Shengyin Jiang",
        "Long Chen",
        "Zhi-Xin Yang",
        "Jiwen Lu"
      ],
      "published_date": "2025-12-18",
      "pdf_url": "https://arxiv.org/pdf/2512.16919v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Adding Conditional Control to Text-to-Image Diffusion Models",
      "arxiv_id": "",
      "abstract": "We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with \"zero convolutions\" (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, eg, edges, depth, segmentation, human pose, etc, with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small (<50k) and large (>1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models.",
      "authors": [
        "Lvmin Zhang",
        "Anyi Rao",
        "Maneesh Agrawala"
      ],
      "published_date": "2023-02-10",
      "pdf_url": "https://arxiv.org/pdf/2302.05543v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Scaling Instruction-Finetuned Language Models",
      "arxiv_id": "",
      "abstract": "Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.",
      "authors": [
        "Hyung Won Chung",
        "Le Hou",
        "Shayne Longpre",
        "Barret Zoph",
        "Yi Tay",
        "William Fedus",
        "Yunxuan Li",
        "Xuezhi Wang",
        "Mostafa Dehghani",
        "Siddhartha Brahma",
        "Albert Webson",
        "Shixiang Shane Gu",
        "Zhuyun Dai",
        "Mirac Suzgun",
        "Xinyun Chen",
        "Aakanksha Chowdhery",
        "Alex Castro-Ros",
        "Marie Pellat",
        "Kevin Robinson",
        "Dasha Valter",
        "Sharan Narang",
        "Gaurav Mishra",
        "Adams Yu",
        "Vincent Zhao",
        "Yanping Huang",
        "Andrew Dai",
        "Hongkun Yu",
        "Slav Petrov",
        "Ed H. Chi",
        "Jeff Dean",
        "Jacob Devlin",
        "Adam Roberts",
        "Denny Zhou",
        "Quoc V. Le",
        "Jason Wei"
      ],
      "published_date": "2022-10-20",
      "pdf_url": "https://arxiv.org/pdf/2210.11416v5",
      "citation_count": null,
      "year": null
    },
    {
      "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
      "arxiv_id": "",
      "abstract": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .",
      "authors": [
        "Robin Rombach",
        "Andreas Blattmann",
        "Dominik Lorenz",
        "Patrick Esser",
        "Björn Ommer"
      ],
      "published_date": "2021-12-20",
      "pdf_url": "https://arxiv.org/pdf/2112.10752v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "AUTO-ENCODING VARIATIONAL BAYES",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Romain Lopez",
        "Pierre Boyeau",
        "N. Yosef",
        "Michael I. Jordan",
        "J. Regier"
      ],
      "published_date": "2020",
      "pdf_url": "",
      "citation_count": 21145,
      "year": 2020
    },
    {
      "title": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
      "arxiv_id": "",
      "abstract": "While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called \"perceptual losses\"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.",
      "authors": [
        "Richard Zhang",
        "Phillip Isola",
        "Alexei A. Efros",
        "Eli Shechtman",
        "Oliver Wang"
      ],
      "published_date": "2018-01-11",
      "pdf_url": "https://arxiv.org/pdf/1801.03924v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset",
      "arxiv_id": "",
      "abstract": "The research community has increasing interest in autonomous driving research, despite the resource intensity of obtaining representative real world data. Existing self-driving datasets are limited in the scale and variation of the environments they capture, even though generalization within and between operating regions is crucial to the overall viability of the technology. In an effort to help align the research community's contributions with real-world self-driving problems, we introduce a new large scale, high quality, diverse dataset. Our new dataset consists of 1150 scenes that each span 20 seconds, consisting of well synchronized and calibrated high quality LiDAR and camera data captured across a range of urban and suburban geographies. It is 15x more diverse than the largest camera+LiDAR dataset available based on our proposed diversity metric. We exhaustively annotated this data with 2D (camera image) and 3D (LiDAR) bounding boxes, with consistent identifiers across frames. Finally, we provide strong baselines for 2D as well as 3D detection and tracking tasks. We further study the effects of dataset size and generalization across geographies on 3D detection methods. Find data, code and more up-to-date information at http://www.waymo.com/open.",
      "authors": [
        "Pei Sun",
        "Henrik Kretzschmar",
        "Xerxes Dotiwalla",
        "Aurelien Chouard",
        "Vijaysai Patnaik",
        "Paul Tsui",
        "James Guo",
        "Yin Zhou",
        "Yuning Chai",
        "Benjamin Caine",
        "Vijay Vasudevan",
        "Wei Han",
        "Jiquan Ngiam",
        "Hang Zhao",
        "Aleksei Timofeev",
        "Scott Ettinger",
        "Maxim Krivokon",
        "Amy Gao",
        "Aditya Joshi",
        "Sheng Zhao",
        "Shuyang Cheng",
        "Yu Zhang",
        "Jonathon Shlens",
        "Zhifeng Chen",
        "Dragomir Anguelov"
      ],
      "published_date": "2019-12-10",
      "pdf_url": "https://arxiv.org/pdf/1912.04838v7",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Histograms of oriented gradients for human detection",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Navneet Dalal",
        "B. Triggs"
      ],
      "published_date": "2005",
      "pdf_url": "",
      "citation_count": 35233,
      "year": 2005
    },
    {
      "title": "Generative Adversarial Networks",
      "arxiv_id": "",
      "abstract": "Generative Adversarial Networks (GANs) are very popular frameworks for generating high-quality data, and are immensely used in both the academia and industry in many domains. Arguably, their most substantial impact has been in the area of computer vision, where they achieve state-of-the-art image generation. This chapter gives an introduction to GANs, by discussing their principle mechanism and presenting some of their inherent problems during training and evaluation. We focus on these three issues: (1) mode collapse, (2) vanishing gradients, and (3) generation of low-quality images. We then list some architecture-variant and loss-variant GANs that remedy the above challenges. Lastly, we present two utilization examples of GANs for real-world applications: Data augmentation and face images generation.",
      "authors": [
        "Gilad Cohen",
        "Raja Giryes"
      ],
      "published_date": "2022-03-01",
      "pdf_url": "https://arxiv.org/pdf/2203.00667v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Generalization of Diffusion Models Arises with a Balanced Representation Space",
      "arxiv_id": "",
      "abstract": "Diffusion models excel at generating high-quality, diverse samples, yet they risk memorizing training data when overfit to the training objective. We analyze the distinctions between memorization and generalization in diffusion models through the lens of representation learning. By investigating a two-layer ReLU denoising autoencoder (DAE), we prove that (i) memorization corresponds to the model storing raw training samples in the learned weights for encoding and decoding, yielding localized \"spiky\" representations, whereas (ii) generalization arises when the model captures local data statistics, producing \"balanced\" representations. Furthermore, we validate these theoretical findings on real-world unconditional and text-to-image diffusion models, demonstrating that the same representation structures emerge in deep generative models with significant practical implications. Building on these insights, we propose a representation-based method for detecting memorization and a training-free editing technique that allows precise control via representation steering. Together, our results highlight that learning good representations is central to novel and meaningful generative modeling.",
      "authors": [
        "Zekai Zhang",
        "Xiao Li",
        "Xiang Li",
        "Lianghe Shi",
        "Meng Wu",
        "Molei Tao",
        "Qing Qu"
      ],
      "published_date": "2025-12-24",
      "pdf_url": "https://arxiv.org/pdf/2512.20963v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Stable Velocity: A Variance Perspective on Flow Matching",
      "arxiv_id": "",
      "abstract": "While flow matching is elegant, its reliance on single-sample conditional velocities leads to high-variance training targets that destabilize optimization and slow convergence. By explicitly characterizing this variance, we identify 1) a high-variance regime near the prior, where optimization is challenging, and 2) a low-variance regime near the data distribution, where conditional and marginal velocities nearly coincide. Leveraging this insight, we propose Stable Velocity, a unified framework that improves both training and sampling. For training, we introduce Stable Velocity Matching (StableVM), an unbiased variance-reduction objective, along with Variance-Aware Representation Alignment (VA-REPA), which adaptively strengthen auxiliary supervision in the low-variance regime. For inference, we show that dynamics in the low-variance regime admit closed-form simplifications, enabling Stable Velocity Sampling (StableVS), a finetuning-free acceleration. Extensive experiments on ImageNet $256\\times256$ and large pretrained text-to-image and text-to-video models, including SD3.5, Flux, Qwen-Image, and Wan2.2, demonstrate consistent improvements in training efficiency and more than $2\\times$ faster sampling within the low-variance regime without degrading sample quality. Our code is available at https://github.com/linYDTHU/StableVelocity.",
      "authors": [
        "Donglin Yang",
        "Yongxing Zhang",
        "Xin Yu",
        "Liang Hou",
        "Xin Tao",
        "Pengfei Wan",
        "Xiaojuan Qi",
        "Renjie Liao"
      ],
      "published_date": "2026-02-05",
      "pdf_url": "https://arxiv.org/pdf/2602.05435v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Laminating Representation Autoencoders for Efficient Diffusion",
      "arxiv_id": "",
      "abstract": "Recent work has shown that diffusion models can generate high-quality images by operating directly on SSL patch features rather than pixel-space latents. However, the dense patch grids from encoders like DINOv2 contain significant redundancy, making diffusion needlessly expensive. We introduce FlatDINO, a variational autoencoder that compresses this representation into a one-dimensional sequence of just 32 continuous tokens -an 8x reduction in sequence length and 48x compression in total dimensionality. On ImageNet 256x256, a DiT-XL trained on FlatDINO latents achieves a gFID of 1.80 with classifier-free guidance while requiring 8x fewer FLOPs per forward pass and up to 4.5x fewer FLOPs per training step compared to diffusion on uncompressed DINOv2 features. These are preliminary results and this work is in progress.",
      "authors": [
        "Ramón Calvo-González",
        "François Fleuret"
      ],
      "published_date": "2026-02-04",
      "pdf_url": "https://arxiv.org/pdf/2602.04873v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Adaptive 1D Video Diffusion Autoencoder",
      "arxiv_id": "",
      "abstract": "Recent video generation models largely rely on video autoencoders that compress pixel-space videos into latent representations. However, existing video autoencoders suffer from three major limitations: (1) fixed-rate compression that wastes tokens on simple videos, (2) inflexible CNN architectures that prevent variable-length latent modeling, and (3) deterministic decoders that struggle to recover appropriate details from compressed latents. To address these issues, we propose One-Dimensional Diffusion Video Autoencoder (One-DVA), a transformer-based framework for adaptive 1D encoding and diffusion-based decoding. The encoder employs query-based vision transformers to extract spatiotemporal features and produce latent representations, while a variable-length dropout mechanism dynamically adjusts the latent length. The decoder is a pixel-space diffusion transformer that reconstructs videos with the latents as input conditions. With a two-stage training strategy, One-DVA achieves performance comparable to 3D-CNN VAEs on reconstruction metrics at identical compression ratios. More importantly, it supports adaptive compression and thus can achieve higher compression ratios. To better support downstream latent generation, we further regularize the One-DVA latent distribution for generative modeling and fine-tune its decoder to mitigate artifacts caused by the generation process.",
      "authors": [
        "Yao Teng",
        "Minxuan Lin",
        "Xian Liu",
        "Shuai Wang",
        "Xiao Yang",
        "Xihui Liu"
      ],
      "published_date": "2026-02-04",
      "pdf_url": "https://arxiv.org/pdf/2602.04220v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Test-Time Conditioning with Representation-Aligned Visual Features",
      "arxiv_id": "",
      "abstract": "While representation alignment with self-supervised models has been shown to improve diffusion model training, its potential for enhancing inference-time conditioning remains largely unexplored. We introduce Representation-Aligned Guidance (REPA-G), a framework that leverages these aligned representations, with rich semantic properties, to enable test-time conditioning from features in generation. By optimizing a similarity objective (the potential) at inference, we steer the denoising process toward a conditioned representation extracted from a pre-trained feature extractor. Our method provides versatile control at multiple scales, ranging from fine-grained texture matching via single patches to broad semantic guidance using global image feature tokens. We further extend this to multi-concept composition, allowing for the faithful combination of distinct concepts. REPA-G operates entirely at inference time, offering a flexible and precise alternative to often ambiguous text prompts or coarse class labels. We theoretically justify how this guidance enables sampling from the potential-induced tilted distribution. Quantitative results on ImageNet and COCO demonstrate that our approach achieves high-quality, diverse generations. Code is available at https://github.com/valeoai/REPA-G.",
      "authors": [
        "Nicolas Sereyjol-Garros",
        "Ellington Kirby",
        "Victor Letzelter",
        "Victor Besnier",
        "Nermin Samet"
      ],
      "published_date": "2026-02-03",
      "pdf_url": "https://arxiv.org/pdf/2602.03753v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Bio-inspired fine-tuning for selective transfer learning in image classification",
      "arxiv_id": "",
      "abstract": "Deep learning has significantly advanced image analysis across diverse domains but often depends on large, annotated datasets for success. Transfer learning addresses this challenge by utilizing pre-trained models to tackle new tasks with limited labeled data. However, discrepancies between source and target domains can hinder effective transfer learning. We introduce BioTune, a novel adaptive fine-tuning technique utilizing evolutionary optimization. BioTune enhances transfer learning by optimally choosing which layers to freeze and adjusting learning rates for unfrozen layers. Through extensive evaluation on nine image classification datasets, spanning natural and specialized domains such as medical imaging, BioTune demonstrates superior accuracy and efficiency over state-of-the-art fine-tuning methods, including AutoRGN and LoRA, highlighting its adaptability to various data characteristics and distribution changes. Additionally, BioTune consistently achieves top performance across four different CNN architectures, underscoring its flexibility. Ablation studies provide valuable insights into the impact of BioTune's key components on overall performance. The source code is available at https://github.com/davilac/BioTune.",
      "authors": [
        "Ana Davila",
        "Jacinto Colan",
        "Yasuhisa Hasegawa"
      ],
      "published_date": "2026-01-16",
      "pdf_url": "https://arxiv.org/pdf/2601.11235v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "VGG Induced Deep Hand Sign Language Detection",
      "arxiv_id": "",
      "abstract": "Hand gesture recognition is an important aspect of human-computer interaction. It forms the basis of sign language for the visually impaired people. This work proposes a novel hand gesture recognizing system for the differently-abled persons. The model uses a convolutional neural network, known as VGG-16 net, for building a trained model on a widely used image dataset by employing Python and Keras libraries. Furthermore, the result is validated by the NUS dataset, consisting of 10 classes of hand gestures, fed to the model as the validation set. Afterwards, a testing dataset of 10 classes is built by employing Google's open source Application Programming Interface (API) that captures different gestures of human hand and the efficacy is then measured by carrying out experiments. The experimental results show that by combining a transfer learning mechanism together with the image data augmentation, the VGG-16 net produced around 98% accuracy.",
      "authors": [
        "Subham Sharma",
        "Sharmila Subudhi"
      ],
      "published_date": "2026-01-13",
      "pdf_url": "https://arxiv.org/pdf/2601.08262v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "MediaPipe: A Framework for Building Perception Pipelines",
      "arxiv_id": "",
      "abstract": "Building applications that perceive the world around them is challenging. A developer needs to (a) select and develop corresponding machine learning algorithms and models, (b) build a series of prototypes and demos, (c) balance resource consumption against the quality of the solutions, and finally (d) identify and mitigate problematic cases. The MediaPipe framework addresses all of these challenges. A developer can use MediaPipe to build prototypes by combining existing perception components, to advance them to polished cross-platform applications and measure system performance and resource consumption on target platforms. We show that these features enable a developer to focus on the algorithm or model development and use MediaPipe as an environment for iteratively improving their application with results reproducible across different devices and platforms. MediaPipe will be open-sourced at https://github.com/google/mediapipe.",
      "authors": [
        "Camillo Lugaresi",
        "Jiuqiang Tang",
        "Hadon Nash",
        "Chris McClanahan",
        "Esha Uboweja",
        "Michael Hays",
        "Fan Zhang",
        "Chuo-Ling Chang",
        "Ming Guang Yong",
        "Juhyun Lee",
        "Wan-Teh Chang",
        "Wei Hua",
        "Manfred Georg",
        "Matthias Grundmann"
      ],
      "published_date": "2019-06-14",
      "pdf_url": "https://arxiv.org/pdf/1906.08172v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Hand Pose Estimation and Hand Shape Classification Using Multi-layered Randomized Decision Forests",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Cem Keskin",
        "Mustafa Furkan Kıraç",
        "Yunus Emre Kara",
        "L. Akarun"
      ],
      "published_date": "2012",
      "pdf_url": "",
      "citation_count": 322,
      "year": 2012
    },
    {
      "title": "A robust static hand gesture recognition system using geometry based normalizations and Krawtchouk moments",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "S. P. Priyal",
        "P. Bora"
      ],
      "published_date": "2013",
      "pdf_url": "",
      "citation_count": 133,
      "year": 2013
    },
    {
      "title": "Hand signal classification system for sign language communication in Virtual Reality",
      "arxiv_id": null,
      "abstract": "Sign language is an essential form of communication for people with hearing disabilities. However, effective communication traditionally required the use of trained translators. With recent advances in AI and virtual reality, technology seems to provide a viable means of facilitating communication with sign-language users. This paper describes a solution developed around the detection and classification of the user’s hand signals and shows its integration within a system meant to facilitate communication with hearing impaired individuals. The system relies on the capabilities of a virtual reality headset to capture the user’s hand signals and then employs a machine learning model in order to correctly classify them. Within this paper, we discuss some of the relevant technical aspects of the system as well as provide insight into its observed capabilities.",
      "authors": [
        "Octavian Dudas",
        "C. Nandra",
        "C. Mocan",
        "D. Gorgan"
      ],
      "published_date": "2023",
      "pdf_url": "",
      "citation_count": 1,
      "year": 2023
    },
    {
      "title": "Real-Time Static Hand Sign Recognition System using MediaPipe and Fully Connected Neural Network",
      "arxiv_id": null,
      "abstract": "About 5% of the world's population faces the challenges of being deaf and mute, according to the World Health Organization. This highlights the urgent need for communication technologies to connect individuals with speech and hearing impairments to the wider community, where Sign Language Recognition (SLR) is a pioneer. However, the development of sign language recognition systems is often challenged by the scarce availability of suitable datasets, as each spoken language has its own distinct sign language. Additionally, the high computational power required to process this data complicates the progress, making it hard to reach the masses. This paper presents a real-time static sign recognition system that not only creates robust and customizable datasets but also preprocesses the data, and recognizes static hand signs, making it adaptable to any static sign language. Currently based off the American Sign Language (ASL), the model uses MediaPipe and a fully connected neural network (FCNN) that is trained and tested on 10 signs with 200 images per sign and later with 500 images per sign to include diversity. The proposed system demonstrates high accuracy and fast recognition, offering a practical solution for enhancing communication for speech and hearing-impaired individuals in real time.",
      "authors": [
        "Avinash Dhiran",
        "Anurag Kumbhare",
        "Achal Patil",
        "Mrugank Vichare",
        "Dhananjay Patel"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 0,
      "year": 2025
    },
    {
      "title": "Real-Time Gesture Recognition to Aid Communication in Children with Motor Impairments",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Saransh Mishra",
        "Pavan Nair",
        "Pushpalatha M",
        "Poornima S"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 0,
      "year": 2025
    },
    {
      "title": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "A. Dempster",
        "N. Laird",
        "D. Rubin"
      ],
      "published_date": "1977",
      "pdf_url": "",
      "citation_count": 53653,
      "year": 1977
    },
    {
      "title": "Visualizing Data using t-SNE",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "L. Maaten",
        "Geoffrey E. Hinton"
      ],
      "published_date": "2008",
      "pdf_url": "",
      "citation_count": 47177,
      "year": 2008
    },
    {
      "title": "Learning Multiple Layers of Features from Tiny Images",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "A. Krizhevsky"
      ],
      "published_date": "2009",
      "pdf_url": "",
      "citation_count": 40351,
      "year": 2009
    },
    {
      "title": "LLM Social Simulations Are a Promising Research Method",
      "arxiv_id": "",
      "abstract": "Accurate and verifiable large language model (LLM) simulations of human research subjects promise an accessible data source for understanding human behavior and training new AI systems. However, results to date have been limited, and few social scientists have adopted this method. In this position paper, we argue that the promise of LLM social simulations can be achieved by addressing five tractable challenges. We ground our argument in a review of empirical comparisons between LLMs and human research subjects, commentaries on the topic, and related work. We identify promising directions, including context-rich prompting and fine-tuning with social science datasets. We believe that LLM social simulations can already be used for pilot and exploratory studies, and more widespread use may soon be possible with rapidly advancing LLM capabilities. Researchers should prioritize developing conceptual models and iterative evaluations to make the best use of new AI systems.",
      "authors": [
        "Jacy Reese Anthis",
        "Ryan Liu",
        "Sean M. Richardson",
        "Austin C. Kozlowski",
        "Bernard Koch",
        "James Evans",
        "Erik Brynjolfsson",
        "Michael Bernstein"
      ],
      "published_date": "2025-04-03",
      "pdf_url": "https://arxiv.org/pdf/2504.02234v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Federated Contrastive Learning With Feature-Based Distillation for Human Activity Recognition",
      "arxiv_id": null,
      "abstract": "This article proposes a federated contrastive learning with feature-based distillation (FCLFD) framework tailored for human activity recognition (HAR). The FCLFD system integrates a central server with multiple mobile users to address a diverse range of HAR challenges. The framework encompasses two pivotal elements: a contrastive student–teacher (CST) architecture with feature-based distillation and an average weight scheme (AWS). The CST framework facilitates the transfer of comprehensive knowledge from a teacher model to a student model through feature-based distillation and contrastive learning, with both models sharing an identical architecture. Each participating user periodically uploads the weights of its student model to the central server, where the AWS deployed on the server calculates the average weights based on contributions from all connected users. The aggregated weights are then redistributed to each user, who updates their teacher model accordingly. Experimental evaluations demonstrate that when 50 users are connected, the proposed FCLFD scheme obtains the highest $F_{1}$ values of 89.01 and 94.19, outperforming several state-of-the-art federated learning algorithms on the wireless sensor data mining (WISDM) and PAMAP2 datasets.",
      "authors": [
        "Zhiwen Xiao",
        "Huagang Tong"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 47,
      "year": 2025
    },
    {
      "title": "Diffuse and Disperse: Image Generation with Representation Regularization",
      "arxiv_id": "",
      "abstract": "The development of diffusion-based generative models over the past decade has largely proceeded independently of progress in representation learning. These diffusion models typically rely on regression-based objectives and generally lack explicit regularization. In this work, we propose \\textit{Dispersive Loss}, a simple plug-and-play regularizer that effectively improves diffusion-based generative models. Our loss function encourages internal representations to disperse in the hidden space, analogous to contrastive self-supervised learning, with the key distinction that it requires no positive sample pairs and therefore does not interfere with the sampling process used for regression. Compared to the recent method of representation alignment (REPA), our approach is self-contained and minimalist, requiring no pre-training, no additional parameters, and no external data. We evaluate Dispersive Loss on the ImageNet dataset across a range of models and report consistent improvements over widely used and strong baselines. We hope our work will help bridge the gap between generative modeling and representation learning.",
      "authors": [
        "Runqian Wang",
        "Kaiming He"
      ],
      "published_date": "2025-06-10",
      "pdf_url": "https://arxiv.org/pdf/2506.09027v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Deep Autoencoder Neural Networks: A Comprehensive Review and New Perspectives",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Ibomoiye Domor Mienye",
        "Theo G. Swart"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 41,
      "year": 2025
    },
    {
      "title": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems",
      "arxiv_id": "",
      "abstract": "As scaling large language models faces prohibitive costs, multi-agent systems emerge as a promising alternative, though challenged by static knowledge assumptions and coordination inefficiencies. We introduces Knowledge-Aware Bayesian Bandits (KABB), a novel framework that enhances multi-agent system coordination through semantic understanding and dynamic adaptation. The framework features three key innovations: a three-dimensional knowledge distance model for deep semantic understanding, a dual-adaptation mechanism for continuous expert optimization, and a knowledge-aware Thompson Sampling strategy for efficient expert selection. Extensive evaluation demonstrates KABB achieves an optimal cost-performance balance, maintaining high performance while keeping computational demands relatively low in multi-agent coordination.",
      "authors": [
        "Jusheng Zhang",
        "Zimeng Huang",
        "Yijia Fan",
        "Ningyuan Liu",
        "Mingyan Li",
        "Zhuojie Yang",
        "Jiawei Yao",
        "Jian Wang",
        "Keze Wang"
      ],
      "published_date": "2025-02-11",
      "pdf_url": "https://arxiv.org/pdf/2502.07350v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
      "arxiv_id": "",
      "abstract": "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .",
      "authors": [
        "Olaf Ronneberger",
        "Philipp Fischer",
        "Thomas Brox"
      ],
      "published_date": "2015-05-18",
      "pdf_url": "https://arxiv.org/pdf/1505.04597v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Improved Distribution Matching Distillation for Fast Image Synthesis",
      "arxiv_id": "",
      "abstract": "Recent approaches have shown promises distilling diffusion models into efficient one-step generators. Among them, Distribution Matching Distillation (DMD) produces one-step generators that match their teacher in distribution, without enforcing a one-to-one correspondence with the sampling trajectories of their teachers. However, to ensure stable training, DMD requires an additional regression loss computed using a large set of noise-image pairs generated by the teacher with many steps of a deterministic sampler. This is costly for large-scale text-to-image synthesis and limits the student's quality, tying it too closely to the teacher's original sampling paths. We introduce DMD2, a set of techniques that lift this limitation and improve DMD training. First, we eliminate the regression loss and the need for expensive dataset construction. We show that the resulting instability is due to the fake critic not estimating the distribution of generated samples accurately and propose a two time-scale update rule as a remedy. Second, we integrate a GAN loss into the distillation procedure, discriminating between generated samples and real images. This lets us train the student model on real data, mitigating the imperfect real score estimation from the teacher model, and enhancing quality. Lastly, we modify the training procedure to enable multi-step sampling. We identify and address the training-inference input mismatch problem in this setting, by simulating inference-time generator samples during training time. Taken together, our improvements set new benchmarks in one-step image generation, with FID scores of 1.28 on ImageNet-64x64 and 8.35 on zero-shot COCO 2014, surpassing the original teacher despite a 500X reduction in inference cost. Further, we show our approach can generate megapixel images by distilling SDXL, demonstrating exceptional visual quality among few-step methods.",
      "authors": [
        "Tianwei Yin",
        "Michaël Gharbi",
        "Taesung Park",
        "Richard Zhang",
        "Eli Shechtman",
        "Fredo Durand",
        "William T. Freeman"
      ],
      "published_date": "2024-05-23",
      "pdf_url": "https://arxiv.org/pdf/2405.14867v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation",
      "arxiv_id": "",
      "abstract": "Diffusion models are the main driver of progress in image and video synthesis, but suffer from slow inference speed. Distillation methods, like the recently introduced adversarial diffusion distillation (ADD) aim to shift the model from many-shot to single-step inference, albeit at the cost of expensive and difficult optimization due to its reliance on a fixed pretrained DINOv2 discriminator. We introduce Latent Adversarial Diffusion Distillation (LADD), a novel distillation approach overcoming the limitations of ADD. In contrast to pixel-based ADD, LADD utilizes generative features from pretrained latent diffusion models. This approach simplifies training and enhances performance, enabling high-resolution multi-aspect ratio image synthesis. We apply LADD to Stable Diffusion 3 (8B) to obtain SD3-Turbo, a fast model that matches the performance of state-of-the-art text-to-image generators using only four unguided sampling steps. Moreover, we systematically investigate its scaling behavior and demonstrate LADD's effectiveness in various applications such as image editing and inpainting.",
      "authors": [
        "Axel Sauer",
        "Frederic Boesel",
        "Tim Dockhorn",
        "Andreas Blattmann",
        "Patrick Esser",
        "Robin Rombach"
      ],
      "published_date": "2024-03-18",
      "pdf_url": "https://arxiv.org/pdf/2403.12015v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Evolutionary optimization of model merging recipes",
      "arxiv_id": "2403.13187",
      "abstract": "Large language models (LLMs) have become increasingly capable, but their development often requires substantial computational resources. While model merging has emerged as a cost-effective promising approach for creating new models by combining existing ones, it currently relies on human intuition and domain knowledge, limiting its potential. Here, we propose an evolutionary approach that overcomes this limitation by automatically discovering effective combinations of diverse open-source models, harnessing their collective intelligence without requiring extensive additional training data or compute. Our approach operates in both parameter space and data flow space, allowing for optimization beyond just the weights of the individual models. This approach even facilitates cross-domain merging, generating models like a Japanese LLM with Math reasoning capabilities. Surprisingly, our Japanese Math LLM achieved state-of-the-art performance on a variety of established Japanese LLM benchmarks, even surpassing models with significantly more parameters, despite not being explicitly trained for such tasks. Furthermore, a culturally-aware Japanese VLM generated through our approach demonstrates its effectiveness in describing Japanese culture-specific content, outperforming previous Japanese VLMs. This work not only contributes new state-of-the-art models back to the open-source community, but also introduces a new paradigm for automated model composition, paving the way for exploring alternative, efficient approaches to foundation model development.",
      "authors": [
        "Takuya Akiba",
        "Makoto Shing",
        "Yujin Tang",
        "Qi Sun",
        "David Ha"
      ],
      "published_date": "2024-03-19",
      "pdf_url": "https://arxiv.org/pdf/2403.13187v2",
      "citation_count": 184,
      "year": 2024
    },
    {
      "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
      "arxiv_id": "",
      "abstract": "Current video diffusion models achieve impressive generation quality but struggle in interactive applications due to bidirectional attention dependencies. The generation of a single frame requires the model to process the entire sequence, including the future. We address this limitation by adapting a pretrained bidirectional diffusion transformer to an autoregressive transformer that generates frames on-the-fly. To further reduce latency, we extend distribution matching distillation (DMD) to videos, distilling 50-step diffusion model into a 4-step generator. To enable stable and high-quality distillation, we introduce a student initialization scheme based on teacher's ODE trajectories, as well as an asymmetric distillation strategy that supervises a causal student model with a bidirectional teacher. This approach effectively mitigates error accumulation in autoregressive generation, allowing long-duration video synthesis despite training on short clips. Our model achieves a total score of 84.27 on the VBench-Long benchmark, surpassing all previous video generation models. It enables fast streaming generation of high-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our approach also enables streaming video-to-video translation, image-to-video, and dynamic prompting in a zero-shot manner.",
      "authors": [
        "Tianwei Yin",
        "Qiang Zhang",
        "Richard Zhang",
        "William T. Freeman",
        "Fredo Durand",
        "Eli Shechtman",
        "Xun Huang"
      ],
      "published_date": "2024-12-10",
      "pdf_url": "https://arxiv.org/pdf/2412.07772v4",
      "citation_count": null,
      "year": null
    },
    {
      "title": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment",
      "arxiv_id": "",
      "abstract": "We propose Pure and Lightning ID customization (PuLID), a novel tuning-free ID customization method for text-to-image generation. By incorporating a Lightning T2I branch with a standard diffusion one, PuLID introduces both contrastive alignment loss and accurate ID loss, minimizing disruption to the original model and ensuring high ID fidelity. Experiments show that PuLID achieves superior performance in both ID fidelity and editability. Another attractive property of PuLID is that the image elements (e.g., background, lighting, composition, and style) before and after the ID insertion are kept as consistent as possible. Codes and models are available at https://github.com/ToTheBeginning/PuLID",
      "authors": [
        "Zinan Guo",
        "Yanze Wu",
        "Zhuowei Chen",
        "Lang Chen",
        "Peng Zhang",
        "Qian He"
      ],
      "published_date": "2024-04-24",
      "pdf_url": "https://arxiv.org/pdf/2404.16022v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Denoising Diffusion Probabilistic Models",
      "arxiv_id": "",
      "abstract": "We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion",
      "authors": [
        "Jonathan Ho",
        "Ajay Jain",
        "Pieter Abbeel"
      ],
      "published_date": "2020-06-19",
      "pdf_url": "https://arxiv.org/pdf/2006.11239v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "HunyuanVideo: A Systematic Framework For Large Video Generative Models",
      "arxiv_id": "",
      "abstract": "Recent advancements in video generation have significantly impacted daily life for both individuals and industries. However, the leading video generation models remain closed-source, resulting in a notable performance gap between industry capabilities and those available to the public. In this report, we introduce HunyuanVideo, an innovative open-source video foundation model that demonstrates performance in video generation comparable to, or even surpassing, that of leading closed-source models. HunyuanVideo encompasses a comprehensive framework that integrates several key elements, including data curation, advanced architectural design, progressive model scaling and training, and an efficient infrastructure tailored for large-scale model training and inference. As a result, we successfully trained a video generative model with over 13 billion parameters, making it the largest among all open-source models. We conducted extensive experiments and implemented a series of targeted designs to ensure high visual quality, motion dynamics, text-video alignment, and advanced filming techniques. According to evaluations by professionals, HunyuanVideo outperforms previous state-of-the-art models, including Runway Gen-3, Luma 1.6, and three top-performing Chinese video generative models. By releasing the code for the foundation model and its applications, we aim to bridge the gap between closed-source and open-source communities. This initiative will empower individuals within the community to experiment with their ideas, fostering a more dynamic and vibrant video generation ecosystem. The code is publicly available at https://github.com/Tencent/HunyuanVideo.",
      "authors": [
        "Weijie Kong",
        "Qi Tian",
        "Zijian Zhang",
        "Rox Min",
        "Zuozhuo Dai",
        "Jin Zhou",
        "Jiangfeng Xiong",
        "Xin Li",
        "Bo Wu",
        "Jianwei Zhang",
        "Kathrina Wu",
        "Qin Lin",
        "Junkun Yuan",
        "Yanxin Long",
        "Aladdin Wang",
        "Andong Wang",
        "Changlin Li",
        "Duojun Huang",
        "Fang Yang",
        "Hao Tan",
        "Hongmei Wang",
        "Jacob Song",
        "Jiawang Bai",
        "Jianbing Wu",
        "Jinbao Xue",
        "Joey Wang",
        "Kai Wang",
        "Mengyang Liu",
        "Pengyu Li",
        "Shuai Li",
        "Weiyan Wang",
        "Wenqing Yu",
        "Xinchi Deng",
        "Yang Li",
        "Yi Chen",
        "Yutao Cui",
        "Yuanbo Peng",
        "Zhentao Yu",
        "Zhiyu He",
        "Zhiyong Xu",
        "Zixiang Zhou",
        "Zunnan Xu",
        "Yangyu Tao",
        "Qinglin Lu",
        "Songtao Liu",
        "Dax Zhou",
        "Hongfa Wang",
        "Yong Yang",
        "Di Wang",
        "Yuhong Liu",
        "Jie Jiang",
        "Caesar Zhong"
      ],
      "published_date": "2024-12-03",
      "pdf_url": "https://arxiv.org/pdf/2412.03603v6",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency",
      "arxiv_id": "",
      "abstract": "With the introduction of diffusion-based video generation techniques, audio-conditioned human video generation has recently achieved significant breakthroughs in both the naturalness of motion and the synthesis of portrait details. Due to the limited control of audio signals in driving human motion, existing methods often add auxiliary spatial signals to stabilize movements, which may compromise the naturalness and freedom of motion. In this paper, we propose an end-to-end audio-only conditioned video diffusion model named Loopy. Specifically, we designed an inter- and intra-clip temporal module and an audio-to-latents module, enabling the model to leverage long-term motion information from the data to learn natural motion patterns and improving audio-portrait movement correlation. This method removes the need for manually specified spatial motion templates used in existing methods to constrain motion during inference. Extensive experiments show that Loopy outperforms recent audio-driven portrait diffusion models, delivering more lifelike and high-quality results across various scenarios.",
      "authors": [
        "Jianwen Jiang",
        "Chao Liang",
        "Jiaqi Yang",
        "Gaojie Lin",
        "Tianyun Zhong",
        "Yanbo Zheng"
      ],
      "published_date": "2024-09-04",
      "pdf_url": "https://arxiv.org/pdf/2409.02634v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models",
      "arxiv_id": "",
      "abstract": "End-to-end human animation, such as audio-driven talking human generation, has undergone notable advancements in the recent few years. However, existing methods still struggle to scale up as large general video generation models, limiting their potential in real applications. In this paper, we propose OmniHuman, a Diffusion Transformer-based framework that scales up data by mixing motion-related conditions into the training phase. To this end, we introduce two training principles for these mixed conditions, along with the corresponding model architecture and inference strategy. These designs enable OmniHuman to fully leverage data-driven motion generation, ultimately achieving highly realistic human video generation. More importantly, OmniHuman supports various portrait contents (face close-up, portrait, half-body, full-body), supports both talking and singing, handles human-object interactions and challenging body poses, and accommodates different image styles. Compared to existing end-to-end audio-driven methods, OmniHuman not only produces more realistic videos, but also offers greater flexibility in inputs. It also supports multiple driving modalities (audio-driven, video-driven and combined driving signals). Video samples are provided on the ttfamily project page (https://omnihuman-lab.github.io)",
      "authors": [
        "Gaojie Lin",
        "Jianwen Jiang",
        "Jiaqi Yang",
        "Zerong Zheng",
        "Chao Liang"
      ],
      "published_date": "2025-02-03",
      "pdf_url": "https://arxiv.org/pdf/2502.01061v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation",
      "arxiv_id": "",
      "abstract": "Recent advances in latent diffusion-based generative models for portrait image animation, such as Hallo, have achieved impressive results in short-duration video synthesis. In this paper, we present updates to Hallo, introducing several design enhancements to extend its capabilities. First, we extend the method to produce long-duration videos. To address substantial challenges such as appearance drift and temporal artifacts, we investigate augmentation strategies within the image space of conditional motion frames. Specifically, we introduce a patch-drop technique augmented with Gaussian noise to enhance visual consistency and temporal coherence over long duration. Second, we achieve 4K resolution portrait video generation. To accomplish this, we implement vector quantization of latent codes and apply temporal alignment techniques to maintain coherence across the temporal dimension. By integrating a high-quality decoder, we realize visual synthesis at 4K resolution. Third, we incorporate adjustable semantic textual labels for portrait expressions as conditional inputs. This extends beyond traditional audio cues to improve controllability and increase the diversity of the generated content. To the best of our knowledge, Hallo2, proposed in this paper, is the first method to achieve 4K resolution and generate hour-long, audio-driven portrait image animations enhanced with textual prompts. We have conducted extensive experiments to evaluate our method on publicly available datasets, including HDTF, CelebV, and our introduced \"Wild\" dataset. The experimental results demonstrate that our approach achieves state-of-the-art performance in long-duration portrait video animation, successfully generating rich and controllable content at 4K resolution for duration extending up to tens of minutes. Project page https://fudan-generative-vision.github.io/hallo2",
      "authors": [
        "Jiahao Cui",
        "Hui Li",
        "Yao Yao",
        "Hao Zhu",
        "Hanlin Shang",
        "Kaihui Cheng",
        "Hang Zhou",
        "Siyu Zhu",
        "Jingdong Wang"
      ],
      "published_date": "2024-10-10",
      "pdf_url": "https://arxiv.org/pdf/2410.07718v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation",
      "arxiv_id": "",
      "abstract": "Recent work on human animation usually involves audio, pose, or movement maps conditions, thereby achieves vivid animation quality. However, these methods often face practical challenges due to extra control conditions, cumbersome condition injection modules, or limitation to head region driving. Hence, we ask if it is possible to achieve striking half-body human animation while simplifying unnecessary conditions. To this end, we propose a half-body human animation method, dubbed EchoMimicV2, that leverages a novel Audio-Pose Dynamic Harmonization strategy, including Pose Sampling and Audio Diffusion, to enhance half-body details, facial and gestural expressiveness, and meanwhile reduce conditions redundancy. To compensate for the scarcity of half-body data, we utilize Head Partial Attention to seamlessly accommodate headshot data into our training framework, which can be omitted during inference, providing a free lunch for animation. Furthermore, we design the Phase-specific Denoising Loss to guide motion, detail, and low-level quality for animation in specific phases, respectively. Besides, we also present a novel benchmark for evaluating the effectiveness of half-body human animation. Extensive experiments and analyses demonstrate that EchoMimicV2 surpasses existing methods in both quantitative and qualitative evaluations.",
      "authors": [
        "Rang Meng",
        "Xingyu Zhang",
        "Yuming Li",
        "Chenguang Ma"
      ],
      "published_date": "2024-11-15",
      "pdf_url": "https://arxiv.org/pdf/2411.10061v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Feature Pyramid Networks for Object Detection",
      "arxiv_id": "",
      "abstract": "Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.",
      "authors": [
        "Tsung-Yi Lin",
        "Piotr Dollár",
        "Ross Girshick",
        "Kaiming He",
        "Bharath Hariharan",
        "Serge Belongie"
      ],
      "published_date": "2016-12-09",
      "pdf_url": "https://arxiv.org/pdf/1612.03144v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation",
      "arxiv_id": "1406.1078",
      "abstract": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
      "authors": [
        "Kyunghyun Cho",
        "Bart van Merrienboer",
        "Caglar Gulcehre",
        "Dzmitry Bahdanau",
        "Fethi Bougares",
        "Holger Schwenk",
        "Yoshua Bengio"
      ],
      "published_date": "2014-06-03",
      "pdf_url": "https://arxiv.org/pdf/1406.1078v3",
      "citation_count": 25490,
      "year": 2014
    },
    {
      "title": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving",
      "arxiv_id": "2312.09245",
      "abstract": "Large language models (LLMs) have opened up new possibilities for intelligent agents, endowing them with human-like thinking and cognitive abilities. In this work, we delve into the potential of large language models (LLMs) in autonomous driving (AD). We introduce DriveMLM, an LLM-based AD framework that can perform close-loop autonomous driving in realistic simulators. To this end, (1) we bridge the gap between the language decisions and the vehicle control commands by standardizing the decision states according to the off-the-shelf motion planning module. (2) We employ a multimodal LLM (MLLM) to model the behavior planning module of a module AD system, which uses driving rules, user commands, and inputs from various sensors (e.g., camera, lidar) as input and makes driving decisions and provide explanations; This model can plug-and-play in existing AD systems such as Autopilot and Apollo for close-loop driving. (3) We design an effective data engine to collect a dataset that includes decision state and corresponding explanation annotation for model training and evaluation. We conduct extensive experiments and show that replacing the decision-making modules of the Autopilot and Apollo with DriveMLM resulted in significant improvements of 3.2 and 4.7 points on the CARLA Town05 Long respectively, demonstrating the effectiveness of our model. We hope this work can serve as a baseline for autonomous driving with LLMs.",
      "authors": [
        "Erfei Cui",
        "Wenhai Wang",
        "Zhiqi Li",
        "Jiangwei Xie",
        "Haoming Zou",
        "Hanming Deng",
        "Gen Luo",
        "Lewei Lu",
        "Xizhou Zhu",
        "Jifeng Dai"
      ],
      "published_date": "2023-12-14",
      "pdf_url": "https://arxiv.org/pdf/2312.09245v3",
      "citation_count": 221,
      "year": 2023
    },
    {
      "title": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability",
      "arxiv_id": "",
      "abstract": "World models can foresee the outcomes of different actions, which is of paramount importance for autonomous driving. Nevertheless, existing driving world models still have limitations in generalization to unseen environments, prediction fidelity of critical details, and action controllability for flexible application. In this paper, we present Vista, a generalizable driving world model with high fidelity and versatile controllability. Based on a systematic diagnosis of existing methods, we introduce several key ingredients to address these limitations. To accurately predict real-world dynamics at high resolution, we propose two novel losses to promote the learning of moving instances and structural information. We also devise an effective latent replacement approach to inject historical frames as priors for coherent long-horizon rollouts. For action controllability, we incorporate a versatile set of controls from high-level intentions (command, goal point) to low-level maneuvers (trajectory, angle, and speed) through an efficient learning strategy. After large-scale training, the capabilities of Vista can seamlessly generalize to different scenarios. Extensive experiments on multiple datasets show that Vista outperforms the most advanced general-purpose video generator in over 70% of comparisons and surpasses the best-performing driving world model by 55% in FID and 27% in FVD. Moreover, for the first time, we utilize the capacity of Vista itself to establish a generalizable reward for real-world action evaluation without accessing the ground truth actions.",
      "authors": [
        "Shenyuan Gao",
        "Jiazhi Yang",
        "Li Chen",
        "Kashyap Chitta",
        "Yihang Qiu",
        "Andreas Geiger",
        "Jun Zhang",
        "Hongyang Li"
      ],
      "published_date": "2024-05-27",
      "pdf_url": "https://arxiv.org/pdf/2405.17398v5",
      "citation_count": null,
      "year": null
    },
    {
      "title": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving",
      "arxiv_id": "",
      "abstract": "Recently, the diffusion model has emerged as a powerful generative technique for robotic policy learning, capable of modeling multi-mode action distributions. Leveraging its capability for end-to-end autonomous driving is a promising direction. However, the numerous denoising steps in the robotic diffusion policy and the more dynamic, open-world nature of traffic scenes pose substantial challenges for generating diverse driving actions at a real-time speed. To address these challenges, we propose a novel truncated diffusion policy that incorporates prior multi-mode anchors and truncates the diffusion schedule, enabling the model to learn denoising from anchored Gaussian distribution to the multi-mode driving action distribution. Additionally, we design an efficient cascade diffusion decoder for enhanced interaction with conditional scene context. The proposed model, DiffusionDrive, demonstrates 10$\\times$ reduction in denoising steps compared to vanilla diffusion policy, delivering superior diversity and quality in just 2 steps. On the planning-oriented NAVSIM dataset, with the aligned ResNet-34 backbone, DiffusionDrive achieves 88.1 PDMS without bells and whistles, setting a new record, while running at a real-time speed of 45 FPS on an NVIDIA 4090. Qualitative results on challenging scenarios further confirm that DiffusionDrive can robustly generate diverse plausible driving actions. Code and model will be available at https://github.com/hustvl/DiffusionDrive.",
      "authors": [
        "Bencheng Liao",
        "Shaoyu Chen",
        "Haoran Yin",
        "Bo Jiang",
        "Cheng Wang",
        "Sixu Yan",
        "Xinbang Zhang",
        "Xiangyu Li",
        "Ying Zhang",
        "Qian Zhang",
        "Xinggang Wang"
      ],
      "published_date": "2024-11-22",
      "pdf_url": "https://arxiv.org/pdf/2411.15139v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "How Far is Video Generation from World Model: A Physical Law Perspective",
      "arxiv_id": "",
      "abstract": "OpenAI's Sora highlights the potential of video generation for developing world models that adhere to fundamental physical laws. However, the ability of video generation models to discover such laws purely from visual data without human priors can be questioned. A world model learning the true law should give predictions robust to nuances and correctly extrapolate on unseen scenarios. In this work, we evaluate across three key scenarios: in-distribution, out-of-distribution, and combinatorial generalization. We developed a 2D simulation testbed for object movement and collisions to generate videos deterministically governed by one or more classical mechanics laws. This provides an unlimited supply of data for large-scale experimentation and enables quantitative evaluation of whether the generated videos adhere to physical laws. We trained diffusion-based video generation models to predict object movements based on initial frames. Our scaling experiments show perfect generalization within the distribution, measurable scaling behavior for combinatorial generalization, but failure in out-of-distribution scenarios. Further experiments reveal two key insights about the generalization mechanisms of these models: (1) the models fail to abstract general physical rules and instead exhibit \"case-based\" generalization behavior, i.e., mimicking the closest training example; (2) when generalizing to new cases, models are observed to prioritize different factors when referencing training data: color > size > velocity > shape. Our study suggests that scaling alone is insufficient for video generation models to uncover fundamental physical laws, despite its role in Sora's broader success. See our project page at https://phyworld.github.io",
      "authors": [
        "Bingyi Kang",
        "Yang Yue",
        "Rui Lu",
        "Zhijie Lin",
        "Yang Zhao",
        "Kaixin Wang",
        "Gao Huang",
        "Jiashi Feng"
      ],
      "published_date": "2024-11-04",
      "pdf_url": "https://arxiv.org/pdf/2411.02385v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "EMMA: End-to-End Multimodal Model for Autonomous Driving",
      "arxiv_id": "",
      "abstract": "We introduce EMMA, an End-to-end Multimodal Model for Autonomous driving. Built upon a multi-modal large language model foundation like Gemini, EMMA directly maps raw camera sensor data into various driving-specific outputs, including planner trajectories, perception objects, and road graph elements. EMMA maximizes the utility of world knowledge from the pre-trained large language models, by representing all non-sensor inputs (e.g. navigation instructions and ego vehicle status) and outputs (e.g. trajectories and 3D locations) as natural language text. This approach allows EMMA to jointly process various driving tasks in a unified language space, and generate the outputs for each task using task-specific prompts. Empirically, we demonstrate EMMA's effectiveness by achieving state-of-the-art performance in motion planning on nuScenes as well as competitive results on the Waymo Open Motion Dataset (WOMD). EMMA also yields competitive results for camera-primary 3D object detection on the Waymo Open Dataset (WOD). We show that co-training EMMA with planner trajectories, object detection, and road graph tasks yields improvements across all three domains, highlighting EMMA's potential as a generalist model for autonomous driving applications. We hope that our results will inspire research to further evolve the state of the art in autonomous driving model architectures.",
      "authors": [
        "Jyh-Jing Hwang",
        "Runsheng Xu",
        "Hubert Lin",
        "Wei-Chih Hung",
        "Jingwei Ji",
        "Kristy Choi",
        "Di Huang",
        "Tong He",
        "Paul Covington",
        "Benjamin Sapp",
        "Yin Zhou",
        "James Guo",
        "Dragomir Anguelov",
        "Mingxing Tan"
      ],
      "published_date": "2024-10-30",
      "pdf_url": "https://arxiv.org/pdf/2410.23262v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Language Models are Few-Shot Learners",
      "arxiv_id": "",
      "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
      "authors": [
        "Tom B. Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell",
        "Sandhini Agarwal",
        "Ariel Herbert-Voss",
        "Gretchen Krueger",
        "Tom Henighan",
        "Rewon Child",
        "Aditya Ramesh",
        "Daniel M. Ziegler",
        "Jeffrey Wu",
        "Clemens Winter",
        "Christopher Hesse",
        "Mark Chen",
        "Eric Sigler",
        "Mateusz Litwin",
        "Scott Gray",
        "Benjamin Chess",
        "Jack Clark",
        "Christopher Berner",
        "Sam McCandlish",
        "Alec Radford",
        "Ilya Sutskever",
        "Dario Amodei"
      ],
      "published_date": "2020-05-28",
      "pdf_url": "https://arxiv.org/pdf/2005.14165v4",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling",
      "arxiv_id": "",
      "abstract": "We introduce InternVL 2.5, an advanced multimodal large language model (MLLM) series that builds upon InternVL 2.0, maintaining its core model architecture while introducing significant enhancements in training and testing strategies as well as data quality. In this work, we delve into the relationship between model scaling and performance, systematically exploring the performance trends in vision encoders, language models, dataset sizes, and test-time configurations. Through extensive evaluations on a wide range of benchmarks, including multi-discipline reasoning, document understanding, multi-image / video understanding, real-world comprehension, multimodal hallucination detection, visual grounding, multilingual capabilities, and pure language processing, InternVL 2.5 exhibits competitive performance, rivaling leading commercial models such as GPT-4o and Claude-3.5-Sonnet. Notably, our model is the first open-source MLLMs to surpass 70% on the MMMU benchmark, achieving a 3.7-point improvement through Chain-of-Thought (CoT) reasoning and showcasing strong potential for test-time scaling. We hope this model contributes to the open-source community by setting new standards for developing and applying multimodal AI systems. HuggingFace demo see https://huggingface.co/spaces/OpenGVLab/InternVL",
      "authors": [
        "Zhe Chen",
        "Weiyun Wang",
        "Yue Cao",
        "Yangzhou Liu",
        "Zhangwei Gao",
        "Erfei Cui",
        "Jinguo Zhu",
        "Shenglong Ye",
        "Hao Tian",
        "Zhaoyang Liu",
        "Lixin Gu",
        "Xuehui Wang",
        "Qingyun Li",
        "Yiming Ren",
        "Zixuan Chen",
        "Jiapeng Luo",
        "Jiahao Wang",
        "Tan Jiang",
        "Bo Wang",
        "Conghui He",
        "Botian Shi",
        "Xingcheng Zhang",
        "Han Lv",
        "Yi Wang",
        "Wenqi Shao",
        "Pei Chu",
        "Zhongying Tu",
        "Tong He",
        "Zhiyong Wu",
        "Huipeng Deng",
        "Jiaye Ge",
        "Kai Chen",
        "Kaipeng Zhang",
        "Limin Wang",
        "Min Dou",
        "Lewei Lu",
        "Xizhou Zhu",
        "Tong Lu",
        "Dahua Lin",
        "Yu Qiao",
        "Jifeng Dai",
        "Wenhai Wang"
      ],
      "published_date": "2024-12-06",
      "pdf_url": "https://arxiv.org/pdf/2412.05271v5",
      "citation_count": null,
      "year": null
    },
    {
      "title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models",
      "arxiv_id": "",
      "abstract": "We introduce InternVL3, a significant advancement in the InternVL series featuring a native multimodal pre-training paradigm. Rather than adapting a text-only large language model (LLM) into a multimodal large language model (MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and linguistic capabilities from both diverse multimodal data and pure-text corpora during a single pre-training stage. This unified training paradigm effectively addresses the complexities and alignment challenges commonly encountered in conventional post-hoc training pipelines for MLLMs. To further improve performance and scalability, InternVL3 incorporates variable visual position encoding (V2PE) to support extended multimodal contexts, employs advanced post-training techniques such as supervised fine-tuning (SFT) and mixed preference optimization (MPO), and adopts test-time scaling strategies alongside an optimized training infrastructure. Extensive empirical evaluations demonstrate that InternVL3 delivers superior performance across a wide range of multi-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the MMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its capabilities remain highly competitive with leading proprietary models, including ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also maintaining strong pure-language proficiency. In pursuit of open-science principles, we will publicly release both the training data and model weights to foster further research and development in next-generation MLLMs.",
      "authors": [
        "Jinguo Zhu",
        "Weiyun Wang",
        "Zhe Chen",
        "Zhaoyang Liu",
        "Shenglong Ye",
        "Lixin Gu",
        "Hao Tian",
        "Yuchen Duan",
        "Weijie Su",
        "Jie Shao",
        "Zhangwei Gao",
        "Erfei Cui",
        "Xuehui Wang",
        "Yue Cao",
        "Yangzhou Liu",
        "Xingguang Wei",
        "Hongjie Zhang",
        "Haomin Wang",
        "Weiye Xu",
        "Hao Li",
        "Jiahao Wang",
        "Nianchen Deng",
        "Songze Li",
        "Yinan He",
        "Tan Jiang",
        "Jiapeng Luo",
        "Yi Wang",
        "Conghui He",
        "Botian Shi",
        "Xingcheng Zhang",
        "Wenqi Shao",
        "Junjun He",
        "Yingtong Xiong",
        "Wenwen Qu",
        "Peng Sun",
        "Penglong Jiao",
        "Han Lv",
        "Lijun Wu",
        "Kaipeng Zhang",
        "Huipeng Deng",
        "Jiaye Ge",
        "Kai Chen",
        "Limin Wang",
        "Min Dou",
        "Lewei Lu",
        "Xizhou Zhu",
        "Tong Lu",
        "Dahua Lin",
        "Yu Qiao",
        "Jifeng Dai",
        "Wenhai Wang"
      ],
      "published_date": "2025-04-14",
      "pdf_url": "https://arxiv.org/pdf/2504.10479v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models",
      "arxiv_id": "2407.11691",
      "abstract": "We present VLMEvalKit: an open-source toolkit for evaluating large multi-modality models based on PyTorch. The toolkit aims to provide a user-friendly and comprehensive framework for researchers and developers to evaluate existing multi-modality models and publish reproducible evaluation results. In VLMEvalKit, we implement over 200+ different large multi-modality models, including both proprietary APIs and open-source models, as well as more than 80 different multi-modal benchmarks. By implementing a single interface, new models can be easily added to the toolkit, while the toolkit automatically handles the remaining workloads, including data preparation, distributed inference, prediction post-processing, and metric calculation. Although the toolkit is currently mainly used for evaluating large vision-language models, its design is compatible with future updates that incorporate additional modalities, such as audio and video. Based on the evaluation results obtained with the toolkit, we host OpenVLM Leaderboard, a comprehensive leaderboard to track the progress of multi-modality learning research. The toolkit is released on https://github.com/open-compass/VLMEvalKit and is actively maintained.",
      "authors": [
        "Haodong Duan",
        "Xinyu Fang",
        "Junming Yang",
        "Xiangyu Zhao",
        "Yuxuan Qiao",
        "Mo Li",
        "Amit Agarwal",
        "Zhe Chen",
        "Lin Chen",
        "Yuan Liu",
        "Yubo Ma",
        "Hailong Sun",
        "Yifan Zhang",
        "Shiyin Lu",
        "Tack Hwa Wong",
        "Weiyun Wang",
        "Peiheng Zhou",
        "Xiaozhe Li",
        "Chaoyou Fu",
        "Junbo Cui",
        "Jixuan Chen",
        "Enxin Song",
        "Song Mao",
        "Shengyuan Ding",
        "Tianhao Liang",
        "Zicheng Zhang",
        "Xiaoyi Dong",
        "Yuhang Zang",
        "Pan Zhang",
        "Jiaqi Wang",
        "Dahua Lin",
        "Kai Chen"
      ],
      "published_date": "2024-07-16",
      "pdf_url": "https://arxiv.org/pdf/2407.11691v4",
      "citation_count": 375,
      "year": 2024
    },
    {
      "title": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step",
      "arxiv_id": "",
      "abstract": "Large language models have demonstrated substantial advancements in reasoning capabilities. However, current Vision-Language Models (VLMs) often struggle to perform systematic and structured reasoning, especially when handling complex visual question-answering tasks. In this work, we introduce LLaVA-CoT, a large VLM designed to conduct autonomous multistage reasoning. Unlike chain-of-thought prompting, LLaVA-CoT independently engages in sequential stages of summarization, visual interpretation, logical reasoning, and conclusion generation. This structured approach enables LLaVA-CoT to achieve marked improvements on reasoning-intensive tasks. To accomplish this, we construct the LLaVA-CoT-100k dataset, integrating samples from various visual question answering sources and providing structured reasoning annotations. Besides, we propose a test-time stage-wise retracing search method (SWIRES), which enables effective and efficient test-time scaling. Remarkably, with only 100k training samples and test-time scaling, LLaVA-CoT not only outperforms its base model by 9.4% on a wide range of multimodal reasoning benchmarks, but also surpasses the performance of larger and even closed-source models, such as Gemini-1.5-pro, GPT-4o-mini, and Llama-3.2-90B-Vision-Instruct. The code, dataset, and pre-trained weights are publicly available at https://github.com/PKU-YuanGroup/LLaVA-CoT.",
      "authors": [
        "Guowei Xu",
        "Peng Jin",
        "Ziang Wu",
        "Hao Li",
        "Yibing Song",
        "Lichao Sun",
        "Li Yuan"
      ],
      "published_date": "2024-11-15",
      "pdf_url": "https://arxiv.org/pdf/2411.10440v6",
      "citation_count": null,
      "year": null
    },
    {
      "title": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency",
      "arxiv_id": "",
      "abstract": "We introduce InternVL 3.5, a new family of open-source multimodal models that significantly advances versatility, reasoning capability, and inference efficiency along the InternVL series. A key innovation is the Cascade Reinforcement Learning (Cascade RL) framework, which enhances reasoning through a two-stage process: offline RL for stable convergence and online RL for refined alignment. This coarse-to-fine training strategy leads to substantial improvements on downstream reasoning tasks, e.g., MMMU and MathVista. To optimize efficiency, we propose a Visual Resolution Router (ViR) that dynamically adjusts the resolution of visual tokens without compromising performance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD) strategy separates the vision encoder and language model across different GPUs, effectively balancing computational load. These contributions collectively enable InternVL3.5 to achieve up to a +16.0\\% gain in overall reasoning performance and a 4.05$\\times$ inference speedup compared to its predecessor, i.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as GUI interaction and embodied agency. Notably, our largest model, i.e., InternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs across general multimodal, reasoning, text, and agentic tasks -- narrowing the performance gap with leading commercial models like GPT-5. All models and code are publicly released.",
      "authors": [
        "Weiyun Wang",
        "Zhangwei Gao",
        "Lixin Gu",
        "Hengjun Pu",
        "Long Cui",
        "Xingguang Wei",
        "Zhaoyang Liu",
        "Linglin Jing",
        "Shenglong Ye",
        "Jie Shao",
        "Zhaokai Wang",
        "Zhe Chen",
        "Hongjie Zhang",
        "Ganlin Yang",
        "Haomin Wang",
        "Qi Wei",
        "Jinhui Yin",
        "Wenhao Li",
        "Erfei Cui",
        "Guanzhou Chen",
        "Zichen Ding",
        "Changyao Tian",
        "Zhenyu Wu",
        "Jingjing Xie",
        "Zehao Li",
        "Bowen Yang",
        "Yuchen Duan",
        "Xuehui Wang",
        "Zhi Hou",
        "Haoran Hao",
        "Tianyi Zhang",
        "Songze Li",
        "Xiangyu Zhao",
        "Haodong Duan",
        "Nianchen Deng",
        "Bin Fu",
        "Yinan He",
        "Yi Wang",
        "Conghui He",
        "Botian Shi",
        "Junjun He",
        "Yingtong Xiong",
        "Han Lv",
        "Lijun Wu",
        "Wenqi Shao",
        "Kaipeng Zhang",
        "Huipeng Deng",
        "Biqing Qi",
        "Jiaye Ge",
        "Qipeng Guo",
        "Wenwei Zhang",
        "Songyang Zhang",
        "Maosong Cao",
        "Junyao Lin",
        "Kexian Tang",
        "Jianfei Gao",
        "Haian Huang",
        "Yuzhe Gu",
        "Chengqi Lyu",
        "Huanze Tang",
        "Rui Wang",
        "Haijun Lv",
        "Wanli Ouyang",
        "Limin Wang",
        "Min Dou",
        "Xizhou Zhu",
        "Tong Lu",
        "Dahua Lin",
        "Jifeng Dai",
        "Weijie Su",
        "Bowen Zhou",
        "Kai Chen",
        "Yu Qiao",
        "Wenhai Wang",
        "Gen Luo"
      ],
      "published_date": "2025-08-25",
      "pdf_url": "https://arxiv.org/pdf/2508.18265v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Proximal Policy Optimization Algorithms",
      "arxiv_id": "",
      "abstract": "We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.",
      "authors": [
        "John Schulman",
        "Filip Wolski",
        "Prafulla Dhariwal",
        "Alec Radford",
        "Oleg Klimov"
      ],
      "published_date": "2017-07-20",
      "pdf_url": "https://arxiv.org/pdf/1707.06347v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Flow-GRPO: Training Flow Matching Models via Online RL",
      "arxiv_id": "",
      "abstract": "We propose Flow-GRPO, the first method to integrate online policy gradient reinforcement learning (RL) into flow matching models. Our approach uses two key strategies: (1) an ODE-to-SDE conversion that transforms a deterministic Ordinary Differential Equation (ODE) into an equivalent Stochastic Differential Equation (SDE) that matches the original model's marginal distribution at all timesteps, enabling statistical sampling for RL exploration; and (2) a Denoising Reduction strategy that reduces training denoising steps while retaining the original number of inference steps, significantly improving sampling efficiency without sacrificing performance. Empirically, Flow-GRPO is effective across multiple text-to-image tasks. For compositional generation, RL-tuned SD3.5-M generates nearly perfect object counts, spatial relations, and fine-grained attributes, increasing GenEval accuracy from $63\\%$ to $95\\%$. In visual text rendering, accuracy improves from $59\\%$ to $92\\%$, greatly enhancing text generation. Flow-GRPO also achieves substantial gains in human preference alignment. Notably, very little reward hacking occurred, meaning rewards did not increase at the cost of appreciable image quality or diversity degradation.",
      "authors": [
        "Jie Liu",
        "Gongye Liu",
        "Jiajun Liang",
        "Yangguang Li",
        "Jiaheng Liu",
        "Xintao Wang",
        "Pengfei Wan",
        "Di Zhang",
        "Wanli Ouyang"
      ],
      "published_date": "2025-05-08",
      "pdf_url": "https://arxiv.org/pdf/2505.05470v5",
      "citation_count": null,
      "year": null
    },
    {
      "title": "DanceGRPO: Unleashing GRPO on Visual Generation",
      "arxiv_id": "",
      "abstract": "Recent advances in generative AI have revolutionized visual content creation, yet aligning model outputs with human preferences remains a critical challenge. While Reinforcement Learning (RL) has emerged as a promising approach for fine-tuning generative models, existing methods like DDPO and DPOK face fundamental limitations - particularly their inability to maintain stable optimization when scaling to large and diverse prompt sets, severely restricting their practical utility. This paper presents DanceGRPO, a framework that addresses these limitations through an innovative adaptation of Group Relative Policy Optimization (GRPO) for visual generation tasks. Our key insight is that GRPO's inherent stability mechanisms uniquely position it to overcome the optimization challenges that plague prior RL-based approaches on visual generation. DanceGRPO establishes several significant advances: First, it demonstrates consistent and stable policy optimization across multiple modern generative paradigms, including both diffusion models and rectified flows. Second, it maintains robust performance when scaling to complex, real-world scenarios encompassing three key tasks and four foundation models. Third, it shows remarkable versatility in optimizing for diverse human preferences as captured by five distinct reward models assessing image/video aesthetics, text-image alignment, video motion quality, and binary feedback. Our comprehensive experiments reveal that DanceGRPO outperforms baseline methods by up to 181\\% across multiple established benchmarks, including HPS-v2.1, CLIP Score, VideoAlign, and GenEval. Our results establish DanceGRPO as a robust and versatile solution for scaling Reinforcement Learning from Human Feedback (RLHF) tasks in visual generation, offering new insights into harmonizing reinforcement learning and visual synthesis.",
      "authors": [
        "Zeyue Xue",
        "Jie Wu",
        "Yu Gao",
        "Fangyuan Kong",
        "Lingting Zhu",
        "Mengzhao Chen",
        "Zhiheng Liu",
        "Wei Liu",
        "Qiushan Guo",
        "Weilin Huang",
        "Ping Luo"
      ],
      "published_date": "2025-05-12",
      "pdf_url": "https://arxiv.org/pdf/2505.07818v4",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Seedance 1.0: Exploring the Boundaries of Video Generation Models",
      "arxiv_id": "",
      "abstract": "Notable breakthroughs in diffusion modeling have propelled rapid improvements in video generation, yet current foundational model still face critical challenges in simultaneously balancing prompt following, motion plausibility, and visual quality. In this report, we introduce Seedance 1.0, a high-performance and inference-efficient video foundation generation model that integrates several core technical improvements: (i) multi-source data curation augmented with precision and meaningful video captioning, enabling comprehensive learning across diverse scenarios; (ii) an efficient architecture design with proposed training paradigm, which allows for natively supporting multi-shot generation and jointly learning of both text-to-video and image-to-video tasks. (iii) carefully-optimized post-training approaches leveraging fine-grained supervised fine-tuning, and video-specific RLHF with multi-dimensional reward mechanisms for comprehensive performance improvements; (iv) excellent model acceleration achieving ~10x inference speedup through multi-stage distillation strategies and system-level optimizations. Seedance 1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds (NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance 1.0 stands out with high-quality and fast video generation having superior spatiotemporal fluidity with structural stability, precise instruction adherence in complex multi-subject contexts, native multi-shot narrative coherence with consistent subject representation.",
      "authors": [
        "Yu Gao",
        "Haoyuan Guo",
        "Tuyen Hoang",
        "Weilin Huang",
        "Lu Jiang",
        "Fangyuan Kong",
        "Huixia Li",
        "Jiashi Li",
        "Liang Li",
        "Xiaojie Li",
        "Xunsong Li",
        "Yifu Li",
        "Shanchuan Lin",
        "Zhijie Lin",
        "Jiawei Liu",
        "Shu Liu",
        "Xiaonan Nie",
        "Zhiwu Qing",
        "Yuxi Ren",
        "Li Sun",
        "Zhi Tian",
        "Rui Wang",
        "Sen Wang",
        "Guoqiang Wei",
        "Guohong Wu",
        "Jie Wu",
        "Ruiqi Xia",
        "Fei Xiao",
        "Xuefeng Xiao",
        "Jiangqiao Yan",
        "Ceyuan Yang",
        "Jianchao Yang",
        "Runkai Yang",
        "Tao Yang",
        "Yihang Yang",
        "Zilyu Ye",
        "Xuejiao Zeng",
        "Yan Zeng",
        "Heng Zhang",
        "Yang Zhao",
        "Xiaozheng Zheng",
        "Peihao Zhu",
        "Jiaxin Zou",
        "Feilong Zuo"
      ],
      "published_date": "2025-06-10",
      "pdf_url": "https://arxiv.org/pdf/2506.09113v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "SkyReels-V2: Infinite-length Film Generative Model",
      "arxiv_id": "",
      "abstract": "Recent advances in video generation have been driven by diffusion models and autoregressive frameworks, yet critical challenges persist in harmonizing prompt adherence, visual quality, motion dynamics, and duration: compromises in motion dynamics to enhance temporal visual quality, constrained video duration (5-10 seconds) to prioritize resolution, and inadequate shot-aware generation stemming from general-purpose MLLMs' inability to interpret cinematic grammar, such as shot composition, actor expressions, and camera motions. These intertwined limitations hinder realistic long-form synthesis and professional film-style generation. To address these limitations, we propose SkyReels-V2, an Infinite-length Film Generative Model, that synergizes Multi-modal Large Language Model (MLLM), Multi-stage Pretraining, Reinforcement Learning, and Diffusion Forcing Framework. Firstly, we design a comprehensive structural representation of video that combines the general descriptions by the Multi-modal LLM and the detailed shot language by sub-expert models. Aided with human annotation, we then train a unified Video Captioner, named SkyCaptioner-V1, to efficiently label the video data. Secondly, we establish progressive-resolution pretraining for the fundamental video generation, followed by a four-stage post-training enhancement: Initial concept-balanced Supervised Fine-Tuning (SFT) improves baseline quality; Motion-specific Reinforcement Learning (RL) training with human-annotated and synthetic distortion data addresses dynamic artifacts; Our diffusion forcing framework with non-decreasing noise schedules enables long-video synthesis in an efficient search space; Final high-quality SFT refines visual fidelity. All the code and models are available at https://github.com/SkyworkAI/SkyReels-V2.",
      "authors": [
        "Guibin Chen",
        "Dixuan Lin",
        "Jiangping Yang",
        "Chunze Lin",
        "Junchen Zhu",
        "Mingyuan Fan",
        "Hao Zhang",
        "Sheng Chen",
        "Zheng Chen",
        "Chengcheng Ma",
        "Weiming Xiong",
        "Wei Wang",
        "Nuo Pang",
        "Kang Kang",
        "Zhiheng Xu",
        "Yuzhe Jin",
        "Yupeng Liang",
        "Yubing Song",
        "Peng Zhao",
        "Boyuan Xu",
        "Di Qiu",
        "Debang Li",
        "Zhengcong Fei",
        "Yang Li",
        "Yahui Zhou"
      ],
      "published_date": "2025-04-17",
      "pdf_url": "https://arxiv.org/pdf/2504.13074v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Unified Reward Model for Multimodal Understanding and Generation",
      "arxiv_id": "",
      "abstract": "Recent advances in human preference alignment have significantly enhanced multimodal generation and understanding. A key approach is training reward models to guide preference optimization. However, existing models are often task-specific, limiting their adaptability across diverse visual applications. We also argue that jointly learning to assess multiple tasks may foster a synergistic effect, where improved image understanding enhances image generation assessment, and refined image evaluation benefits video assessment through better frame analysis. To this end, this paper proposes UnifiedReward, the first unified reward model for multimodal understanding and generation assessment, enabling both pairwise ranking and pointwise scoring, which can be employed for vision model preference alignment. Specifically, (1) we first develop UnifiedReward on our constructed large-scale human preference dataset, including both image and video generation/understanding tasks. (2) Then, it is utilized to automatically construct high-quality preference pair data based on the vision models, fine-gradually filtering their outputs through pair ranking and point sifting. (3) Finally, these data are used for their preference alignment through Direct Preference Optimization (DPO). Experimental results demonstrate that joint learning to assess diverse visual tasks can lead to substantial mutual benefits and we apply our pipeline to both image and video understanding/generation tasks, significantly improving the performance in each domain.",
      "authors": [
        "Yibin Wang",
        "Yuhang Zang",
        "Hao Li",
        "Cheng Jin",
        "Jiaqi Wang"
      ],
      "published_date": "2025-03-07",
      "pdf_url": "https://arxiv.org/pdf/2503.05236v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "5分で分かる!? 有名論文ナナメ読み：Jacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "知秀 柴田"
      ],
      "published_date": "2020",
      "pdf_url": "",
      "citation_count": 13132,
      "year": 2020
    },
    {
      "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
      "arxiv_id": "",
      "abstract": "We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research.\n  The dataset is freely available at https://stanford-qa.com",
      "authors": [
        "Pranav Rajpurkar",
        "Jian Zhang",
        "Konstantin Lopyrev",
        "Percy Liang"
      ],
      "published_date": "2016-06-16",
      "pdf_url": "https://arxiv.org/pdf/1606.05250v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Neural Machine Translation of Rare Words with Subword Units",
      "arxiv_id": "",
      "abstract": "Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.",
      "authors": [
        "Rico Sennrich",
        "Barry Haddow",
        "Alexandra Birch"
      ],
      "published_date": "2015-08-31",
      "pdf_url": "https://arxiv.org/pdf/1508.07909v5",
      "citation_count": null,
      "year": null
    },
    {
      "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing",
      "arxiv_id": "",
      "abstract": "This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at https://github.com/google/sentencepiece.",
      "authors": [
        "Taku Kudo",
        "John Richardson"
      ],
      "published_date": "2018-08-19",
      "pdf_url": "https://arxiv.org/pdf/1808.06226v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "GPT-4 Technical Report",
      "arxiv_id": "",
      "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",
      "authors": [
        "OpenAI",
        "Josh Achiam",
        "Steven Adler",
        "Sandhini Agarwal",
        "Lama Ahmad",
        "Ilge Akkaya",
        "Florencia Leoni Aleman",
        "Diogo Almeida",
        "Janko Altenschmidt",
        "Sam Altman",
        "Shyamal Anadkat",
        "Red Avila",
        "Igor Babuschkin",
        "Suchir Balaji",
        "Valerie Balcom",
        "Paul Baltescu",
        "Haiming Bao",
        "Mohammad Bavarian",
        "Jeff Belgum",
        "Irwan Bello",
        "Jake Berdine",
        "Gabriel Bernadett-Shapiro",
        "Christopher Berner",
        "Lenny Bogdonoff",
        "Oleg Boiko",
        "Madelaine Boyd",
        "Anna-Luisa Brakman",
        "Greg Brockman",
        "Tim Brooks",
        "Miles Brundage",
        "Kevin Button",
        "Trevor Cai",
        "Rosie Campbell",
        "Andrew Cann",
        "Brittany Carey",
        "Chelsea Carlson",
        "Rory Carmichael",
        "Brooke Chan",
        "Che Chang",
        "Fotis Chantzis",
        "Derek Chen",
        "Sully Chen",
        "Ruby Chen",
        "Jason Chen",
        "Mark Chen",
        "Ben Chess",
        "Chester Cho",
        "Casey Chu",
        "Hyung Won Chung",
        "Dave Cummings",
        "Jeremiah Currier",
        "Yunxing Dai",
        "Cory Decareaux",
        "Thomas Degry",
        "Noah Deutsch",
        "Damien Deville",
        "Arka Dhar",
        "David Dohan",
        "Steve Dowling",
        "Sheila Dunning",
        "Adrien Ecoffet",
        "Atty Eleti",
        "Tyna Eloundou",
        "David Farhi",
        "Liam Fedus",
        "Niko Felix",
        "Simón Posada Fishman",
        "Juston Forte",
        "Isabella Fulford",
        "Leo Gao",
        "Elie Georges",
        "Christian Gibson",
        "Vik Goel",
        "Tarun Gogineni",
        "Gabriel Goh",
        "Rapha Gontijo-Lopes",
        "Jonathan Gordon",
        "Morgan Grafstein",
        "Scott Gray",
        "Ryan Greene",
        "Joshua Gross",
        "Shixiang Shane Gu",
        "Yufei Guo",
        "Chris Hallacy",
        "Jesse Han",
        "Jeff Harris",
        "Yuchen He",
        "Mike Heaton",
        "Johannes Heidecke",
        "Chris Hesse",
        "Alan Hickey",
        "Wade Hickey",
        "Peter Hoeschele",
        "Brandon Houghton",
        "Kenny Hsu",
        "Shengli Hu",
        "Xin Hu",
        "Joost Huizinga",
        "Shantanu Jain",
        "Shawn Jain",
        "Joanne Jang",
        "Angela Jiang",
        "Roger Jiang",
        "Haozhun Jin",
        "Denny Jin",
        "Shino Jomoto",
        "Billie Jonn",
        "Heewoo Jun",
        "Tomer Kaftan",
        "Łukasz Kaiser",
        "Ali Kamali",
        "Ingmar Kanitscheider",
        "Nitish Shirish Keskar",
        "Tabarak Khan",
        "Logan Kilpatrick",
        "Jong Wook Kim",
        "Christina Kim",
        "Yongjik Kim",
        "Jan Hendrik Kirchner",
        "Jamie Kiros",
        "Matt Knight",
        "Daniel Kokotajlo",
        "Łukasz Kondraciuk",
        "Andrew Kondrich",
        "Aris Konstantinidis",
        "Kyle Kosic",
        "Gretchen Krueger",
        "Vishal Kuo",
        "Michael Lampe",
        "Ikai Lan",
        "Teddy Lee",
        "Jan Leike",
        "Jade Leung",
        "Daniel Levy",
        "Chak Ming Li",
        "Rachel Lim",
        "Molly Lin",
        "Stephanie Lin",
        "Mateusz Litwin",
        "Theresa Lopez",
        "Ryan Lowe",
        "Patricia Lue",
        "Anna Makanju",
        "Kim Malfacini",
        "Sam Manning",
        "Todor Markov",
        "Yaniv Markovski",
        "Bianca Martin",
        "Katie Mayer",
        "Andrew Mayne",
        "Bob McGrew",
        "Scott Mayer McKinney",
        "Christine McLeavey",
        "Paul McMillan",
        "Jake McNeil",
        "David Medina",
        "Aalok Mehta",
        "Jacob Menick",
        "Luke Metz",
        "Andrey Mishchenko",
        "Pamela Mishkin",
        "Vinnie Monaco",
        "Evan Morikawa",
        "Daniel Mossing",
        "Tong Mu",
        "Mira Murati",
        "Oleg Murk",
        "David Mély",
        "Ashvin Nair",
        "Reiichiro Nakano",
        "Rajeev Nayak",
        "Arvind Neelakantan",
        "Richard Ngo",
        "Hyeonwoo Noh",
        "Long Ouyang",
        "Cullen O'Keefe",
        "Jakub Pachocki",
        "Alex Paino",
        "Joe Palermo",
        "Ashley Pantuliano",
        "Giambattista Parascandolo",
        "Joel Parish",
        "Emy Parparita",
        "Alex Passos",
        "Mikhail Pavlov",
        "Andrew Peng",
        "Adam Perelman",
        "Filipe de Avila Belbute Peres",
        "Michael Petrov",
        "Henrique Ponde de Oliveira Pinto",
        "Michael",
        "Pokorny",
        "Michelle Pokrass",
        "Vitchyr H. Pong",
        "Tolly Powell",
        "Alethea Power",
        "Boris Power",
        "Elizabeth Proehl",
        "Raul Puri",
        "Alec Radford",
        "Jack Rae",
        "Aditya Ramesh",
        "Cameron Raymond",
        "Francis Real",
        "Kendra Rimbach",
        "Carl Ross",
        "Bob Rotsted",
        "Henri Roussez",
        "Nick Ryder",
        "Mario Saltarelli",
        "Ted Sanders",
        "Shibani Santurkar",
        "Girish Sastry",
        "Heather Schmidt",
        "David Schnurr",
        "John Schulman",
        "Daniel Selsam",
        "Kyla Sheppard",
        "Toki Sherbakov",
        "Jessica Shieh",
        "Sarah Shoker",
        "Pranav Shyam",
        "Szymon Sidor",
        "Eric Sigler",
        "Maddie Simens",
        "Jordan Sitkin",
        "Katarina Slama",
        "Ian Sohl",
        "Benjamin Sokolowsky",
        "Yang Song",
        "Natalie Staudacher",
        "Felipe Petroski Such",
        "Natalie Summers",
        "Ilya Sutskever",
        "Jie Tang",
        "Nikolas Tezak",
        "Madeleine B. Thompson",
        "Phil Tillet",
        "Amin Tootoonchian",
        "Elizabeth Tseng",
        "Preston Tuggle",
        "Nick Turley",
        "Jerry Tworek",
        "Juan Felipe Cerón Uribe",
        "Andrea Vallone",
        "Arun Vijayvergiya",
        "Chelsea Voss",
        "Carroll Wainwright",
        "Justin Jay Wang",
        "Alvin Wang",
        "Ben Wang",
        "Jonathan Ward",
        "Jason Wei",
        "CJ Weinmann",
        "Akila Welihinda",
        "Peter Welinder",
        "Jiayi Weng",
        "Lilian Weng",
        "Matt Wiethoff",
        "Dave Willner",
        "Clemens Winter",
        "Samuel Wolrich",
        "Hannah Wong",
        "Lauren Workman",
        "Sherwin Wu",
        "Jeff Wu",
        "Michael Wu",
        "Kai Xiao",
        "Tao Xu",
        "Sarah Yoo",
        "Kevin Yu",
        "Qiming Yuan",
        "Wojciech Zaremba",
        "Rowan Zellers",
        "Chong Zhang",
        "Marvin Zhang",
        "Shengjia Zhao",
        "Tianhao Zheng",
        "Juntang Zhuang",
        "William Zhuk",
        "Barret Zoph"
      ],
      "published_date": "2023-03-15",
      "pdf_url": "https://arxiv.org/pdf/2303.08774v6",
      "citation_count": null,
      "year": null
    },
    {
      "title": "LLaMA: Open and Efficient Foundation Language Models",
      "arxiv_id": "",
      "abstract": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro",
        "Faisal Azhar",
        "Aurelien Rodriguez",
        "Armand Joulin",
        "Edouard Grave",
        "Guillaume Lample"
      ],
      "published_date": "2023-02-27",
      "pdf_url": "https://arxiv.org/pdf/2302.13971v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
      "arxiv_id": "2201.11903",
      "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
      "authors": [
        "Jason Wei",
        "Xuezhi Wang",
        "Dale Schuurmans",
        "Maarten Bosma",
        "Brian Ichter",
        "Fei Xia",
        "Ed Chi",
        "Quoc Le",
        "Denny Zhou"
      ],
      "published_date": "2022-01-28",
      "pdf_url": "https://arxiv.org/pdf/2201.11903v6",
      "citation_count": 15233,
      "year": 2022
    },
    {
      "title": "Training Verifiers to Solve Math Word Problems",
      "arxiv_id": "",
      "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
      "authors": [
        "Karl Cobbe",
        "Vineet Kosaraju",
        "Mohammad Bavarian",
        "Mark Chen",
        "Heewoo Jun",
        "Lukasz Kaiser",
        "Matthias Plappert",
        "Jerry Tworek",
        "Jacob Hilton",
        "Reiichiro Nakano",
        "Christopher Hesse",
        "John Schulman"
      ],
      "published_date": "2021-10-27",
      "pdf_url": "https://arxiv.org/pdf/2110.14168v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Crystal structure of the nucleosome core particle at 2.8 Å resolution",
      "arxiv_id": null,
      "abstract": "",
      "authors": [],
      "published_date": "",
      "pdf_url": "",
      "citation_count": 9266,
      "year": 1997
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "arxiv_id": "",
      "abstract": "Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.",
      "authors": [
        "Albert Gu",
        "Tri Dao"
      ],
      "published_date": "2023-12-01",
      "pdf_url": "https://arxiv.org/pdf/2312.00752v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "A catalogue of splice junction sequences.",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Stephen M. Mount"
      ],
      "published_date": "1982",
      "pdf_url": "",
      "citation_count": 3139,
      "year": 1982
    },
    {
      "title": "Origin of the Genetic Code",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "F. Crick"
      ],
      "published_date": "1967",
      "pdf_url": "",
      "citation_count": 2299,
      "year": 1967
    },
    {
      "title": "SGDR: Stochastic Gradient Descent with Warm Restarts",
      "arxiv_id": "",
      "abstract": "Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14% and 16.21%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "published_date": "2016-08-13",
      "pdf_url": "https://arxiv.org/pdf/1608.03983v5",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
      "arxiv_id": "",
      "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.",
      "authors": [
        "Noam Shazeer",
        "Azalia Mirhoseini",
        "Krzysztof Maziarz",
        "Andy Davis",
        "Quoc Le",
        "Geoffrey Hinton",
        "Jeff Dean"
      ],
      "published_date": "2017-01-23",
      "pdf_url": "https://arxiv.org/pdf/1701.06538v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Compression of individual sequences via variable-rate coding",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "J. Ziv",
        "A. Lempel"
      ],
      "published_date": "1978",
      "pdf_url": "",
      "citation_count": 3732,
      "year": 1978
    },
    {
      "title": "Pointer Sentinel Mixture Models",
      "arxiv_id": "",
      "abstract": "Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.",
      "authors": [
        "Stephen Merity",
        "Caiming Xiong",
        "James Bradbury",
        "Richard Socher"
      ],
      "published_date": "2016-09-26",
      "pdf_url": "https://arxiv.org/pdf/1609.07843v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Measuring Massive Multitask Language Understanding",
      "arxiv_id": "",
      "abstract": "We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",
      "authors": [
        "Dan Hendrycks",
        "Collin Burns",
        "Steven Basart",
        "Andy Zou",
        "Mantas Mazeika",
        "Dawn Song",
        "Jacob Steinhardt"
      ],
      "published_date": "2020-09-07",
      "pdf_url": "https://arxiv.org/pdf/2009.03300v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Let's Verify Step by Step",
      "arxiv_id": "",
      "abstract": "In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning. However, even state-of-the-art models still regularly produce logical mistakes. To train more reliable models, we can turn either to outcome supervision, which provides feedback for a final result, or process supervision, which provides feedback for each intermediate reasoning step. Given the importance of training reliable models, and given the high cost of human feedback, it is important to carefully compare the both methods. Recent work has already begun this comparison, but many questions still remain. We conduct our own investigation, finding that process supervision significantly outperforms outcome supervision for training models to solve problems from the challenging MATH dataset. Our process-supervised model solves 78% of problems from a representative subset of the MATH test set. Additionally, we show that active learning significantly improves the efficacy of process supervision. To support related research, we also release PRM800K, the complete dataset of 800,000 step-level human feedback labels used to train our best reward model.",
      "authors": [
        "Hunter Lightman",
        "Vineet Kosaraju",
        "Yura Burda",
        "Harri Edwards",
        "Bowen Baker",
        "Teddy Lee",
        "Jan Leike",
        "John Schulman",
        "Ilya Sutskever",
        "Karl Cobbe"
      ],
      "published_date": "2023-05-31",
      "pdf_url": "https://arxiv.org/pdf/2305.20050v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark",
      "arxiv_id": "",
      "abstract": "We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are \"Google-proof\"). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",
      "authors": [
        "David Rein",
        "Betty Li Hou",
        "Asa Cooper Stickland",
        "Jackson Petty",
        "Richard Yuanzhe Pang",
        "Julien Dirani",
        "Julian Michael",
        "Samuel R. Bowman"
      ],
      "published_date": "2023-11-20",
      "pdf_url": "https://arxiv.org/pdf/2311.12022v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
      "arxiv_id": "",
      "abstract": "Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.",
      "authors": [
        "Dmitry Lepikhin",
        "HyoukJoong Lee",
        "Yuanzhong Xu",
        "Dehao Chen",
        "Orhan Firat",
        "Yanping Huang",
        "Maxim Krikun",
        "Noam Shazeer",
        "Zhifeng Chen"
      ],
      "published_date": "2020-06-30",
      "pdf_url": "https://arxiv.org/pdf/2006.16668v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation",
      "arxiv_id": "1311.2524",
      "abstract": "Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012---achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.",
      "authors": [
        "Ross Girshick",
        "Jeff Donahue",
        "Trevor Darrell",
        "Jitendra Malik"
      ],
      "published_date": "2013-11-11",
      "pdf_url": "https://arxiv.org/pdf/1311.2524v5",
      "citation_count": 28359,
      "year": 2013
    },
    {
      "title": "MuMu-LLaMA: Multi-modal music understanding and generation via large language models",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Shansong Liu",
        "Atin Sakkeer Hussain",
        "Qilong Wu",
        "Chenshuo Sun",
        "Ying Shan"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 12,
      "year": 2025
    },
    {
      "title": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy",
      "arxiv_id": "",
      "abstract": "The success of diffusion models has raised concerns about the generation of unsafe or harmful content, prompting concept erasure approaches that fine-tune modules to suppress specific concepts while preserving general generative capabilities. However, as the number of erased concepts grows, these methods often become inefficient and ineffective, since each concept requires a separate set of fine-tuned parameters and may degrade the overall generation quality. In this work, we propose a supertype-subtype concept hierarchy that organizes erased concepts into a parent-child structure. Each erased concept is treated as a child node, and semantically related concepts (e.g., macaw, and bald eagle) are grouped under a shared parent node, referred to as a supertype concept (e.g., bird). Rather than erasing concepts individually, we introduce an effective and efficient group-wise suppression method, where semantically similar concepts are grouped and erased jointly by sharing a single set of learnable parameters. During the erasure phase, standard diffusion regularization is applied to preserve denoising process in unmasked regions. To mitigate the degradation of supertype generation caused by excessive erasure of semantically related subtypes, we propose a novel method called Supertype-Preserving Low-Rank Adaptation (SuPLoRA), which encodes the supertype concept information in the frozen down-projection matrix and updates only the up-projection matrix during erasure. Theoretical analysis demonstrates the effectiveness of SuPLoRA in mitigating generation performance degradation. We construct a more challenging benchmark that requires simultaneous erasure of concepts across diverse domains, including celebrities, objects, and pornographic content.",
      "authors": [
        "Jiahang Tu",
        "Ye Li",
        "Yiming Wu",
        "Hanbin Zhao",
        "Chao Zhang",
        "Hui Qian"
      ],
      "published_date": "2026-01-06",
      "pdf_url": "https://arxiv.org/pdf/2601.03305v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism",
      "arxiv_id": "2512.21452",
      "abstract": "Ground Penetrating Radar (GPR) has emerged as a pivotal tool for non-destructive evaluation of subsurface road defects. However, conventional GPR image interpretation remains heavily reliant on subjective expertise, introducing inefficiencies and inaccuracies. This study introduces a comprehensive framework to address these limitations: (1) A DCGAN-based data augmentation strategy synthesizes high-fidelity GPR images to mitigate data scarcity while preserving defect morphology under complex backgrounds; (2) A novel Multi-modal Chain and Global Attention Network (MCGA-Net) is proposed, integrating Multi-modal Chain Feature Fusion (MCFF) for hierarchical multi-scale defect representation and Global Attention Mechanism (GAM) for context-aware feature enhancement; (3) MS COCO transfer learning fine-tunes the backbone network, accelerating convergence and improving generalization. Ablation and comparison experiments validate the framework's efficacy. MCGA-Net achieves Precision (92.8%), Recall (92.5%), and mAP@50 (95.9%). In the detection of Gaussian noise, weak signals and small targets, MCGA-Net maintains robustness and outperforms other models. This work establishes a new paradigm for automated GPR-based defect detection, balancing computational efficiency with high accuracy in complex subsurface environments.",
      "authors": [
        "Haotian Lv",
        "Yuhui Zhang",
        "Jiangbo Dai",
        "Hanli Wu",
        "Jiaji Wang",
        "Dawei Wang"
      ],
      "published_date": "2025-12-25",
      "pdf_url": "https://arxiv.org/pdf/2512.21452v1",
      "citation_count": 6,
      "year": 2025
    },
    {
      "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
      "arxiv_id": "",
      "abstract": "In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in $\\textbf{2B}$ and $\\textbf{8B}$ parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of $\\textbf{77.8}$ on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.",
      "authors": [
        "Mingxin Li",
        "Yanzhao Zhang",
        "Dingkun Long",
        "Keqin Chen",
        "Sibo Song",
        "Shuai Bai",
        "Zhibo Yang",
        "Pengjun Xie",
        "An Yang",
        "Dayiheng Liu",
        "Jingren Zhou",
        "Junyang Lin"
      ],
      "published_date": "2026-01-08",
      "pdf_url": "https://arxiv.org/pdf/2601.04720v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness",
      "arxiv_id": null,
      "abstract": "Screen-shooting watermarking is an effective means of protecting screen content from unauthorized capture and illegal dissemination. However, existing methods are primarily designed for full-image capture, making them ineffective for partial screen-shooting prevalent in real-world scenarios. To address this limitation, we propose <inline-formula> <tex-math notation=\"LaTeX\">$\\textsf {FPSMark}$ </tex-math></inline-formula>, a flexible watermarking method tailored for partial screen-shooting that embeds consistent watermarks in multiple uniformly distributed cover blocks. Specifically, considering that robustness requirements vary according to the layout of each image, we model the mathematical relationship between the watermark block count and robustness, proving the flexibility of <inline-formula> <tex-math notation=\"LaTeX\">$\\textsf {FPSMark}$ </tex-math></inline-formula> in ensuring partial screen-shooting robustness. Moreover, partial screen-shooting disrupts watermark synchronization, posing challenges for precise watermark localization. To overcome this, we design an intrinsic signal localization network optimized with a hybrid loss. The localization network exploits the inherent distinctions between the watermark and non-watermark features, while the hybrid loss constrains the network at three dimensions: pixel-level, region-level, and sample-level. Experimental results demonstrate the superiority of <inline-formula> <tex-math notation=\"LaTeX\">$\\textsf {FPSMark}$ </tex-math></inline-formula>, showing robust performance across partial capture percentages. Its extraction accuracy exceeds 98% even with only half of the image captured, and it achieves 82% accuracy at a 40% capture ratio, whereas existing methods achieve only around 50% under the same conditions.",
      "authors": [
        "Mingyue Chen",
        "Xin Liao",
        "Han Fang",
        "Jinlin Guo",
        "Yanxiang Chen",
        "Xiaoshuai Wu"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 5,
      "year": 2025
    },
    {
      "title": "Pattern Recognition and Machine Learning",
      "arxiv_id": null,
      "abstract": "Probability Distributions.- Linear Models for Regression.- Linear Models for Classification.- Neural Networks.- Kernel Methods.- Sparse Kernel Machines.- Graphical Models.- Mixture Models and EM.- Approximate Inference.- Sampling Methods.- Continuous Latent Variables.- Sequential Data.- Combining Models.",
      "authors": [
        "Radford M. Neal"
      ],
      "published_date": "2006",
      "pdf_url": "",
      "citation_count": 38837,
      "year": 2006
    },
    {
      "title": "Bagging Predictors",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "L. Breiman"
      ],
      "published_date": "1996",
      "pdf_url": "",
      "citation_count": 25699,
      "year": 1996
    },
    {
      "title": "Automatic Image-Based Plant Disease Severity Estimation Using Deep Learning",
      "arxiv_id": null,
      "abstract": "Automatic and accurate estimation of disease severity is essential for food security, disease management, and yield loss prediction. Deep learning, the latest breakthrough in computer vision, is promising for fine-grained disease severity classification, as the method avoids the labor-intensive feature engineering and threshold-based segmentation. Using the apple black rot images in the PlantVillage dataset, which are further annotated by botanists with four severity stages as ground truth, a series of deep convolutional neural networks are trained to diagnose the severity of the disease. The performances of shallow networks trained from scratch and deep models fine-tuned by transfer learning are evaluated systemically in this paper. The best model is the deep VGG16 model trained with transfer learning, which yields an overall accuracy of 90.4% on the hold-out test set. The proposed deep learning model may have great potential in disease control for modern agriculture.",
      "authors": [
        "Guan Wang",
        "Yu Sun",
        "Jianxin Wang"
      ],
      "published_date": "2017",
      "pdf_url": "",
      "citation_count": 638,
      "year": 2017
    },
    {
      "title": "Plant identification using deep neural networks via optimization of transfer learning parameters",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Mostafa Mehdipour-Ghazi",
        "B. Yanikoglu",
        "E. Aptoula"
      ],
      "published_date": "2017",
      "pdf_url": "",
      "citation_count": 476,
      "year": 2017
    },
    {
      "title": "New perspectives on plant disease characterization based on deep learning",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Sue Han Lee",
        "H. Goëau",
        "P. Bonnet",
        "A. Joly"
      ],
      "published_date": "2020",
      "pdf_url": "",
      "citation_count": 264,
      "year": 2020
    },
    {
      "title": "Deep Learning for Plant Identification in Natural Environment",
      "arxiv_id": null,
      "abstract": "Plant image identification has become an interdisciplinary focus in both botanical taxonomy and computer vision. The first plant image dataset collected by mobile phone in natural scene is presented, which contains 10,000 images of 100 ornamental plant species in Beijing Forestry University campus. A 26-layer deep learning model consisting of 8 residual building blocks is designed for large-scale plant classification in natural environment. The proposed model achieves a recognition rate of 91.78% on the BJFU100 dataset, demonstrating that deep learning is a promising technology for smart forestry.",
      "authors": [
        "Yu Sun",
        "Yuan Liu",
        "Guan Wang",
        "Haiyan Zhang"
      ],
      "published_date": "2017",
      "pdf_url": "",
      "citation_count": 262,
      "year": 2017
    },
    {
      "title": "Going deeper in the automated identification of Herbarium specimens",
      "arxiv_id": null,
      "abstract": "Hundreds of herbarium collections have accumulated a valuable heritage and knowledge of plants over several centuries. Recent initiatives started ambitious preservation plans to digitize this information and make it available to botanists and the general public through web portals. However, thousands of sheets are still unidentified at the species level while numerous sheets should be reviewed and updated following more recent taxonomic knowledge. These annotations and revisions require an unrealistic amount of work for botanists to carry out in a reasonable time. Computer vision and machine learning approaches applied to herbarium sheets are promising but are still not well studied compared to automated species identification from leaf scans or pictures of plants in the field. In this work, we propose to study and evaluate the accuracy with which herbarium images can be potentially exploited for species identification with deep learning technology. In addition, we propose to study if the combination of herbarium sheets with photos of plants in the field is relevant in terms of accuracy, and finally, we explore if herbarium images from one region that has one specific flora can be used to do transfer learning to another region with other species; for example, on a region under-represented in terms of collected data. This is, to our knowledge, the first study that uses deep learning to analyze a big dataset with thousands of species from herbaria. Results show the potential of Deep Learning on herbarium species identification, particularly by training and testing across different datasets from different herbaria. This could potentially lead to the creation of a semi, or even fully automated system to help taxonomists and experts with their annotation, classification, and revision works.",
      "authors": [
        "Jose Carranza-Rojas",
        "Hervé Goeau",
        "P. Bonnet",
        "Erick Mata-Montero",
        "A. Joly"
      ],
      "published_date": "2017",
      "pdf_url": "",
      "citation_count": 243,
      "year": 2017
    },
    {
      "title": "Densely Connected Convolutional Networks",
      "arxiv_id": "",
      "abstract": "Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .",
      "authors": [
        "Gao Huang",
        "Zhuang Liu",
        "Laurens van der Maaten",
        "Kilian Q. Weinberger"
      ],
      "published_date": "2016-08-25",
      "pdf_url": "https://arxiv.org/pdf/1608.06993v5",
      "citation_count": null,
      "year": null
    },
    {
      "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks",
      "arxiv_id": "",
      "abstract": "In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3.\n  The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters",
      "authors": [
        "Mark Sandler",
        "Andrew Howard",
        "Menglong Zhu",
        "Andrey Zhmoginov",
        "Liang-Chieh Chen"
      ],
      "published_date": "2018-01-13",
      "pdf_url": "https://arxiv.org/pdf/1801.04381v4",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Communication-Efficient Learning of Deep Networks from Decentralized Data",
      "arxiv_id": "",
      "abstract": "Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning.\n  We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.",
      "authors": [
        "H. Brendan McMahan",
        "Eider Moore",
        "Daniel Ramage",
        "Seth Hampson",
        "Blaise Agüera y Arcas"
      ],
      "published_date": "2016-02-17",
      "pdf_url": "https://arxiv.org/pdf/1602.05629v4",
      "citation_count": null,
      "year": null
    },
    {
      "title": "A Comprehensive Survey on Transfer Learning",
      "arxiv_id": "",
      "abstract": "Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning researches, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey paper reviews more than forty representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over twenty representative transfer learning models are used for experiments. The models are performed on three different datasets, i.e., Amazon Reviews, Reuters-21578, and Office-31. And the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.",
      "authors": [
        "Fuzhen Zhuang",
        "Zhiyuan Qi",
        "Keyu Duan",
        "Dongbo Xi",
        "Yongchun Zhu",
        "Hengshu Zhu",
        "Hui Xiong",
        "Qing He"
      ],
      "published_date": "2019-11-07",
      "pdf_url": "https://arxiv.org/pdf/1911.02685v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Privacy Preserved and Decentralized Smartphone Recommendation System",
      "arxiv_id": null,
      "abstract": "The increasing popularity of mobile phones has led to an abundance of online reviews, making it challenging for consumers to make well-informed purchasing decisions. This study proposes a novel recommendation system-based mobile phone rating classification approach using federated learning and Term Frequency-Inverse Document Frequency (TF-IDF) features. We created a novel dataset by scraping over 13,000 mobile phone reviews from Flipkart’s website. The proposed approach involves the development of a federated deep neural network (FDNN) to classify the newly created Flipkart dataset. This approach includes data cleaning, balancing, TF-IDF feature extraction, and prediction using federated learning. We employ two clients and one server and conduct three rounds of experiments. The experimental results demonstrate that the proposed approach achieved an accuracy of 96.68% on the aggregated server side while maintaining the security of customer data on their local devices. The proposed approach has the potential to assist consumers in making well-informed purchasing decisions and can be extended to other e-commerce platforms with large datasets of online reviews.",
      "authors": [
        "A. Khan",
        "Maha Driss",
        "Wadii Boulila",
        "G. A. Sampedro",
        "Sidra Abbas",
        "Chitapong Wechtaisong"
      ],
      "published_date": "2024",
      "pdf_url": "",
      "citation_count": 54,
      "year": 2024
    },
    {
      "title": "A Survey on Heterogeneity Taxonomy, Security and Privacy Preservation in the Integration of IoT, Wireless Sensor Networks and Federated Learning",
      "arxiv_id": null,
      "abstract": "Federated learning (FL) is a machine learning (ML) technique that enables collaborative model training without sharing raw data, making it ideal for Internet of Things (IoT) applications where data are distributed across devices and privacy is a concern. Wireless Sensor Networks (WSNs) play a crucial role in IoT systems by collecting data from the physical environment. This paper presents a comprehensive survey of the integration of FL, IoT, and WSNs. It covers FL basics, strategies, and types and discusses the integration of FL, IoT, and WSNs in various domains. The paper addresses challenges related to heterogeneity in FL and summarizes state-of-the-art research in this area. It also explores security and privacy considerations and performance evaluation methodologies. The paper outlines the latest achievements and potential research directions in FL, IoT, and WSNs and emphasizes the significance of the surveyed topics within the context of current technological advancements.",
      "authors": [
        "Tesfahunegn Minwuyelet Mengistu",
        "Taewoon Kim",
        "Jenn-Wei Lin"
      ],
      "published_date": "2024",
      "pdf_url": "",
      "citation_count": 46,
      "year": 2024
    },
    {
      "title": "A privacy-preserving federated learning with a secure collaborative for malware detection models using Internet of Things resources",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "A. Alamer"
      ],
      "published_date": "2024",
      "pdf_url": "",
      "citation_count": 25,
      "year": 2024
    },
    {
      "title": "Graph-based deep learning techniques for remote sensing applications: Techniques, taxonomy, and applications - A comprehensive review",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Manel Khazri Khlifi",
        "Wadii Boulila",
        "I. Farah"
      ],
      "published_date": "2023",
      "pdf_url": "",
      "citation_count": 25,
      "year": 2023
    },
    {
      "title": "Federated Learning Architectures: A Performance Evaluation With Crop Yield Prediction Application",
      "arxiv_id": "2408.02998",
      "abstract": "Federated learning has become an emerging technology for data analysis for IoT applications. This paper implements centralized and decentralized federated learning frameworks for crop yield prediction based on Long Short-Term Memory Network. For centralized federated learning, multiple clients and one server is considered, where the clients exchange their model updates with the server that works as the aggregator to build the global model. For the decentralized framework, a collaborative network is formed among the devices either using ring topology or using mesh topology. In this network, each device receives model updates from the neighbour devices, and performs aggregation to build the upgraded model. The performance of the centralized and decentralized federated learning frameworks are evaluated in terms of prediction accuracy, precision, recall, F1-Score, and training time. The experimental results present that $\\geq$97% and $>$97.5% prediction accuracy are achieved using the centralized and decentralized federated learning-based frameworks respectively. The results also show that the using centralized federated learning the response time can be reduced by $\\sim$75% than the cloud-only framework. Finally, the future research directions of the use of federated learning in crop yield prediction are explored in this paper.",
      "authors": [
        "Anwesha Mukherjee",
        "Rajkumar Buyya"
      ],
      "published_date": "2024-08-06",
      "pdf_url": "https://arxiv.org/pdf/2408.02998v1",
      "citation_count": 18,
      "year": 2024
    },
    {
      "title": "Segment Anything",
      "arxiv_id": "",
      "abstract": "We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.",
      "authors": [
        "Alexander Kirillov",
        "Eric Mintun",
        "Nikhila Ravi",
        "Hanzi Mao",
        "Chloe Rolland",
        "Laura Gustafson",
        "Tete Xiao",
        "Spencer Whitehead",
        "Alexander C. Berg",
        "Wan-Yen Lo",
        "Piotr Dollár",
        "Ross Girshick"
      ],
      "published_date": "2023-04-05",
      "pdf_url": "https://arxiv.org/pdf/2304.02643v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Visual Instruction Tuning",
      "arxiv_id": "",
      "abstract": "Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Qingyang Wu",
        "Yong Jae Lee"
      ],
      "published_date": "2023-04-17",
      "pdf_url": "https://arxiv.org/pdf/2304.08485v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Improved Baselines with Visual Instruction Tuning",
      "arxiv_id": "",
      "abstract": "Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this note, we show that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~1 day on a single 8-A100 node. We hope this can make state-of-the-art LMM research more accessible. Code and model will be publicly available.",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Yuheng Li",
        "Yong Jae Lee"
      ],
      "published_date": "2023-10-05",
      "pdf_url": "https://arxiv.org/pdf/2310.03744v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset",
      "arxiv_id": "2405.18842",
      "abstract": "With the rapid advancement of Vision Language Models (VLMs), VLM-based Image Quality Assessment (IQA) seeks to describe image quality linguistically to align with human expression and capture the multifaceted nature of IQA tasks. However, current methods are still far from practical usage. First, prior works focus narrowly on specific sub-tasks or settings, which do not align with diverse real-world applications. Second, their performance is sub-optimal due to limitations in dataset coverage, scale, and quality. To overcome these challenges, we introduce the enhanced Depicted image Quality Assessment model (DepictQA-Wild). Our method includes a multi-functional IQA task paradigm that encompasses both assessment and comparison tasks, brief and detailed responses, full-reference and non-reference scenarios. We introduce a ground-truth-informed dataset construction approach to enhance data quality, and scale up the dataset to 495K under the brief-detail joint framework. Consequently, we construct a comprehensive, large-scale, and high-quality dataset, named DQ-495K. We also retain image resolution during training to better handle resolution-related quality issues, and estimate a confidence score that is helpful to filter out low-quality responses. Experimental results demonstrate that DepictQA-Wild significantly outperforms traditional score-based methods, prior VLM-based IQA models, and proprietary GPT-4V in distortion identification, instant rating, and reasoning tasks. Our advantages are further confirmed by real-world applications including assessing the web-downloaded images and ranking model-processed images. Codes, datasets, and model weights have been released in https://depictqa.github.io/.",
      "authors": [
        "Zhiyuan You",
        "Jinjin Gu",
        "Xin Cai",
        "Zheyuan Li",
        "Kaiwen Zhu",
        "Chao Dong",
        "Tianfan Xue"
      ],
      "published_date": "2024-05-29",
      "pdf_url": "https://arxiv.org/pdf/2405.18842v3",
      "citation_count": 38,
      "year": 2024
    },
    {
      "title": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs",
      "arxiv_id": "",
      "abstract": "Vision language models (VLMs) have achieved impressive performance across a variety of computer vision tasks. However, the multimodal reasoning capability has not been fully explored in existing models. In this paper, we propose a Chain-of-Focus (CoF) method that allows VLMs to perform adaptive focusing and zooming in on key image regions based on obtained visual cues and the given questions, achieving efficient multimodal reasoning. To enable this CoF capability, we present a two-stage training pipeline, including supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we construct the MM-CoF dataset, comprising 3K samples derived from a visual agent designed to adaptively identify key regions to solve visual tasks with different image resolutions and questions. We use MM-CoF to fine-tune the Qwen2.5-VL model for cold start. In the RL stage, we leverage the outcome accuracies and formats as rewards to update the Qwen2.5-VL model, enabling further refining the search and reasoning strategy of models without human priors. Our model achieves significant improvements on multiple benchmarks. On the V* benchmark that requires strong visual reasoning capability, our model outperforms existing VLMs by 5% among 8 image resolutions ranging from 224 to 4K, demonstrating the effectiveness of the proposed CoF method and facilitating the more efficient deployment of VLMs in practical applications.",
      "authors": [
        "Xintong Zhang",
        "Zhi Gao",
        "Bofei Zhang",
        "Pengxiang Li",
        "Xiaowen Zhang",
        "Yang Liu",
        "Tao Yuan",
        "Yuwei Wu",
        "Yunde Jia",
        "Song-Chun Zhu",
        "Qing Li"
      ],
      "published_date": "2025-05-21",
      "pdf_url": "https://arxiv.org/pdf/2505.15436v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views",
      "arxiv_id": "",
      "abstract": "Though recent advances in vision-language models (VLMs) have achieved remarkable progress across a wide range of multimodal tasks, understanding 3D spatial relationships from limited views remains a significant challenge. Previous reasoning methods typically rely on pure text (e.g., topological cognitive maps) or on 2D visual cues. However, their limited representational capacity hinders performance in specific tasks that require 3D spatial imagination. To address this limitation, we propose 3DThinker, a framework that can effectively exploits the rich geometric information embedded within images while reasoning, like humans do. Our framework is the first to enable 3D mentaling during reasoning without any 3D prior input, and it does not rely on explicitly labeled 3D data for training. Specifically, our training consists of two stages. First, we perform supervised training to align the 3D latent generated by VLM while reasoning with that of a 3D foundation model (e.g., VGGT). Then, we optimize the entire reasoning trajectory solely based on outcome signals, thereby refining the underlying 3D mentaling. Extensive experiments across multiple benchmarks show that 3DThinker consistently outperforms strong baselines and offers a new perspective toward unifying 3D representations into multimodal reasoning. Our code will be available at https://github.com/zhangquanchen/3DThinker.",
      "authors": [
        "Zhangquan Chen",
        "Manyuan Zhang",
        "Xinlei Yu",
        "Xufang Luo",
        "Mingze Sun",
        "Zihao Pan",
        "Yan Feng",
        "Peng Pei",
        "Xunliang Cai",
        "Ruqi Huang"
      ],
      "published_date": "2025-10-21",
      "pdf_url": "https://arxiv.org/pdf/2510.18632v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning",
      "arxiv_id": "",
      "abstract": "Universal multimodal embedding models are foundational to various tasks. Existing approaches typically employ in-batch negative mining by measuring the similarity of query-candidate pairs. However, these methods often struggle to capture subtle semantic differences among candidates and lack diversity in negative samples. Moreover, the embeddings exhibit limited discriminative ability in distinguishing false and hard negatives. In this paper, we leverage the advanced understanding capabilities of MLLMs to enhance representation learning and present a novel Universal Multimodal Embedding (UniME-V2) model. Our approach first constructs a potential hard negative set through global retrieval. We then introduce the MLLM-as-a-Judge mechanism, which utilizes MLLMs to assess the semantic alignment of query-candidate pairs and generate soft semantic matching scores. These scores serve as a foundation for hard negative mining, mitigating the impact of false negatives and enabling the identification of diverse, high-quality hard negatives. Furthermore, the semantic matching scores are used as soft labels to mitigate the rigid one-to-one mapping constraint. By aligning the similarity matrix with the soft semantic matching score matrix, the model learns semantic distinctions among candidates, significantly enhancing its discriminative capacity. To further improve performance, we propose UniME-V2-Reranker, a reranking model trained on our mined hard negatives through a joint pairwise and listwise optimization approach. We conduct comprehensive experiments on the MMEB benchmark and multiple retrieval tasks, demonstrating that our method achieves state-of-the-art performance on average across all tasks.",
      "authors": [
        "Tiancheng Gu",
        "Kaicheng Yang",
        "Kaichen Zhang",
        "Xiang An",
        "Ziyong Feng",
        "Yueyi Zhang",
        "Weidong Cai",
        "Jiankang Deng",
        "Lidong Bing"
      ],
      "published_date": "2025-10-15",
      "pdf_url": "https://arxiv.org/pdf/2510.13515v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe",
      "arxiv_id": "",
      "abstract": "Recent advancements in large reasoning models have fueled growing interest in extending such capabilities to multimodal domains. However, despite notable progress in visual reasoning, the lack of transparent and reproducible data curation and training strategies remains a major barrier to scalable research. In this work, we introduce OpenMMReasoner, a fully transparent two-stage recipe for multimodal reasoning spanning supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we construct an 874K-sample cold-start dataset with rigorous step-by-step validation, providing a strong foundation for reasoning capabilities. The subsequent RL stage leverages a 74K-sample dataset across diverse domains to further sharpen and stabilize these abilities, resulting in a more robust and efficient learning process. Extensive evaluations demonstrate that our training recipe not only surpasses strong baselines but also highlights the critical role of data quality and training design in shaping multimodal reasoning performance. Notably, our method achieves a 11.6% improvement over the Qwen2.5-VL-7B-Instruct baseline across nine multimodal reasoning benchmarks, establishing a solid empirical foundation for future large-scale multimodal reasoning research. We open-sourced all our codes, pipeline, and data at https://github.com/EvolvingLMMs-Lab/OpenMMReasoner.",
      "authors": [
        "Kaichen Zhang",
        "Keming Wu",
        "Zuhao Yang",
        "Bo Li",
        "Kairui Hu",
        "Bin Wang",
        "Ziwei Liu",
        "Xingxuan Li",
        "Lidong Bing"
      ],
      "published_date": "2025-11-20",
      "pdf_url": "https://arxiv.org/pdf/2511.16334v4",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Vision-Language-Action (VLA) Models: Concepts, Progress, Applications and Challenges",
      "arxiv_id": "",
      "abstract": "Vision-Language-Action (VLA) models mark a transformative advancement in artificial intelligence, aiming to unify perception, natural language understanding, and embodied action within a single computational framework. This foundational review presents a comprehensive synthesis of recent advancements in Vision-Language-Action models, systematically organized across five thematic pillars that structure the landscape of this rapidly evolving field. We begin by establishing the conceptual foundations of VLA systems, tracing their evolution from cross-modal learning architectures to generalist agents that tightly integrate vision-language models (VLMs), action planners, and hierarchical controllers. Our methodology adopts a rigorous literature review framework, covering over 80 VLA models published in the past three years. Key progress areas include architectural innovations, efficient training strategies, and real-time inference accelerations. We explore diverse application domains such as autonomous vehicles, medical and industrial robotics, precision agriculture, humanoid robotics, and augmented reality. We analyzed challenges and propose solutions including agentic adaptation and cross-embodiment planning. Furthermore, we outline a forward-looking roadmap where VLA models, VLMs, and agentic AI converge to strengthen socially aligned, adaptive, and general-purpose embodied agents. This work, is expected to serve as a foundational reference for advancing intelligent, real-world robotics and artificial general intelligence. The project repository is available on GitHub as https://github.com/Applied-AI-Research-Lab/Vision-Language-Action-Models-Concepts-Progress-Applications-and-Challenges. [Index Terms: Vision Language Action, VLA, Vision Language Models, VLMs, Action Tokenization, NLP]",
      "authors": [
        "Ranjan Sapkota",
        "Yang Cao",
        "Konstantinos I. Roumeliotis",
        "Manoj Karkee"
      ],
      "published_date": "2025-05-07",
      "pdf_url": "https://arxiv.org/pdf/2505.04769v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Leave No Observation Behind: Real-time Correction for VLA Action Chunks",
      "arxiv_id": "",
      "abstract": "To improve efficiency and temporal coherence, Vision-Language-Action (VLA) models often predict action chunks; however, this action chunking harms reactivity under inference delay and long horizons. We introduce Asynchronous Action Chunk Correction (A2C2), which is a lightweight real-time chunk correction head that runs every control step and adds a time-aware correction to any off-the-shelf VLA's action chunk. The module combines the latest observation, the predicted action from VLA (base action), a positional feature that encodes the index of the base action within the chunk, and some features from the base policy, then outputs a per-step correction. This preserves the base model's competence while restoring closed-loop responsiveness. The approach requires no retraining of the base policy and is orthogonal to asynchronous execution schemes such as Real Time Chunking (RTC). On the dynamic Kinetix task suite (12 tasks) and LIBERO Spatial, our method yields consistent success rate improvements across increasing delays and execution horizons (+23% point and +7% point respectively, compared to RTC), and also improves robustness for long horizons even with zero injected delay. Since the correction head is small and fast, there is minimal overhead compared to the inference of large VLA models. These results indicate that A2C2 is an effective, plug-in mechanism for deploying high-capacity chunking policies in real-time control.",
      "authors": [
        "Kohei Sendai",
        "Maxime Alvarez",
        "Tatsuya Matsushima",
        "Yutaka Matsuo",
        "Yusuke Iwasawa"
      ],
      "published_date": "2025-09-27",
      "pdf_url": "https://arxiv.org/pdf/2509.23224v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Latent Chain-of-Thought World Modeling for End-to-End Driving",
      "arxiv_id": "",
      "abstract": "Recent Vision-Language-Action (VLA) models for autonomous driving explore inference-time reasoning as a way to improve driving performance and safety in challenging scenarios. Most prior work uses natural language to express chain-of-thought (CoT) reasoning before producing driving actions. However, text may not be the most efficient representation for reasoning. In this work, we present Latent-CoT-Drive (LCDrive): a model that expresses CoT in a latent language that captures possible outcomes of the driving actions being considered. Our approach unifies CoT reasoning and decision making by representing both in an action-aligned latent space. Instead of natural language, the model reasons by interleaving (1) action-proposal tokens, which use the same vocabulary as the model's output actions; and (2) world model tokens, which are grounded in a learned latent world model and express future outcomes of these actions. We cold start latent CoT by supervising the model's action proposals and world model tokens based on ground-truth future rollouts of the scene. We then post-train with closed-loop reinforcement learning to strengthen reasoning capabilities. On a large-scale end-to-end driving benchmark, LCDrive achieves faster inference, better trajectory quality, and larger improvements from interactive reinforcement learning compared to both non-reasoning and text-reasoning baselines.",
      "authors": [
        "Shuhan Tan",
        "Kashyap Chitta",
        "Yuxiao Chen",
        "Ran Tian",
        "Yurong You",
        "Yan Wang",
        "Wenjie Luo",
        "Yulong Cao",
        "Philipp Krahenbuhl",
        "Marco Pavone",
        "Boris Ivanovic"
      ],
      "published_date": "2025-12-11",
      "pdf_url": "https://arxiv.org/pdf/2512.10226v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks",
      "arxiv_id": "",
      "abstract": "Built upon language and vision foundation models with strong generalization ability and trained on large-scale robotic data, Vision-Language-Action (VLA) models have recently emerged as a promising approach to learning generalist robotic policies. However, a key drawback of existing VLAs is their extremely high inference costs. In this paper, we propose HyperVLA to address this problem. Unlike existing monolithic VLAs that activate the whole model during both training and inference, HyperVLA uses a novel hypernetwork (HN)-based architecture that activates only a small task-specific policy during inference, while still retaining the high model capacity needed to accommodate diverse multi-task behaviors during training. Successfully training an HN-based VLA is nontrivial so HyperVLA contains several key algorithm design features that improve its performance, including properly utilizing the prior knowledge from existing vision foundation models, HN normalization, and an action generation strategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even higher success rate for both zero-shot generalization and few-shot adaptation, while significantly reducing inference costs. Compared to OpenVLA, a state-of-the-art VLA model, HyperVLA reduces the number of activated parameters at test time by $90\\times$, and accelerates inference speed by $120\\times$. Code is publicly available at https://github.com/MasterXiong/HyperVLA",
      "authors": [
        "Zheng Xiong",
        "Kang Li",
        "Zilin Wang",
        "Matthew Jackson",
        "Jakob Foerster",
        "Shimon Whiteson"
      ],
      "published_date": "2025-10-06",
      "pdf_url": "https://arxiv.org/pdf/2510.04898v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models",
      "arxiv_id": "",
      "abstract": "Vision-Language-Action (VLA) models pretrained on large-scale multimodal datasets have emerged as powerful foundations for robotic perception and control. However, their massive scale, often billions of parameters, poses significant challenges for real-time deployment, as inference becomes computationally expensive and latency-sensitive in dynamic environments. To address this, we propose Token Expand-and-Merge-VLA (TEAM-VLA), a training-free token compression framework that accelerates VLA inference while preserving task performance. TEAM-VLA introduces a dynamic token expansion mechanism that identifies and samples additional informative tokens in the spatial vicinity of attention-highlighted regions, enhancing contextual completeness. These expanded tokens are then selectively merged in deeper layers under action-aware guidance, effectively reducing redundancy while maintaining semantic coherence. By coupling expansion and merging within a single feed-forward pass, TEAM-VLA achieves a balanced trade-off between efficiency and effectiveness, without any retraining or parameter updates. Extensive experiments on LIBERO benchmark demonstrate that TEAM-VLA consistently improves inference speed while maintaining or even surpassing the task success rate of full VLA models. The code is public available on \\href{https://github.com/Jasper-aaa/TEAM-VLA}{https://github.com/Jasper-aaa/TEAM-VLA}",
      "authors": [
        "Yifan Ye",
        "Jiaqi Ma",
        "Jun Cen",
        "Zhihe Lu"
      ],
      "published_date": "2025-12-10",
      "pdf_url": "https://arxiv.org/pdf/2512.09927v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "TransSIL: A Silhouette Cue-Aware Image Classification Framework for Bird Ecological Monitoring Systems",
      "arxiv_id": null,
      "abstract": "How to automatically recognize the bird species has caused concerns in ecological systems and intelligent ecological monitoring system due to the increasing threat to bird ecological society and bird species diversity. However, traditional monitoring systems are susceptible to specific challenges when operating in complex environments, such as complex environments, multifarious postures, and backlight scenarios. To effectively address these challenges, we present a novel bird ecological intelligent detection system (TransSIL) for fine-grained bird image classification (FBIC) in diverse ecological to learn discriminative features by explicitly incorporating silhouette structural information alongside critical visual cues. Specifically, the approach begins with a silhouette token construction module to estimate the bird silhouette and extract silhouette tokens. Then, a silhouette relationship mining module is developed to fuse visual and silhouette tokens and capture long-range dependencies between them. In addition, to learn bird distinctive features at multiple levels, a critical cues awareness module is embedded within TransSIL. The performance of TransSIL was evaluated on two bird datasets, such as CUB200-2011 and NABirds. The framework demonstrates significant improvements over existing ecological intelligent surveillance methods. By utilizing silhouette and visual dependencies, we anticipate that our approach will ultimately contribute to the conservation of avian ecological societies.",
      "authors": [
        "Hai Liu",
        "Yu Song",
        "Tingting Liu",
        "Lin Chen",
        "Zhaoli Zhang",
        "Xiaolan Yang",
        "Neal N. Xiong"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 6,
      "year": 2026
    },
    {
      "title": "CBRFormer: rendering technology-based transformer for refinement segmentation of bridge crack images",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Honghu Chu",
        "Jiahao Gai",
        "Weiwei Chen",
        "Jun Ma"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 6,
      "year": 2026
    },
    {
      "title": "A Fast and Compact Threshold Switch-Based Cellular Nonlinear Network Cell",
      "arxiv_id": null,
      "abstract": "In this work, we introduce a high speed and area efficient Cellular Nonlinear Network (CNN) cell, featuring two circuit variants that utilize threshold switches. The threshold switch (TS) model employed represents a current-controlled nanoscale negative differential resistance (NDR) device which exhibits an S-shaped DC I-V curve as a fingerprint. The proposed cell can be considered as the dual of the standard isolated CNN cell where the bistable cell characteristics, originating from the N-shaped voltage-controlled resistor, is implemented through the S-shaped current-controlled TSs. Similarly, the dynamics induced by the parallel capacitor accompanying the nonlinear resistor in the standard cell version are implemented through the internal inductive dynamics of the TSs, resulting in area and speed efficiency. The proposed CNN cell employs a DC voltage source, two bias resistors and 2 TSs, and essentially, features a differential-mode operation which helps to endow it with a symmetric DC I-V characteristic, as is the case for the standard CNN cell. The differential-mode approach further introduces design flexibility as the cell DC I-V characteristic can be adjusted by tuning circuit parameters. We demonstrate the functionality of the proposed cell by implementing image processing tasks ranging from edge detection and thresholding to logic AND and OR operations.",
      "authors": [
        "A. S. Demirkol",
        "A. Ascoli",
        "I. Messaris",
        "V. Ntinas",
        "D. Prousalis",
        "R. Tetzlaff"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 3,
      "year": 2026
    },
    {
      "title": "Deep contrastive learning enables genome-wide virtual screening.",
      "arxiv_id": null,
      "abstract": "Recent breakthroughs in protein structure prediction have opened new avenues for genome-wide drug discovery, yet existing virtual screening methods remain computationally prohibitive. We present DrugCLIP, a contrastive learning framework that achieves ultrafast and accurate virtual screening, up to 10 million times faster than docking, while consistently outperforming various baselines on in silico benchmarks. In wet-lab validations, DrugCLIP achieved a 15% hit rate for norepinephrine transporter, and structures of two identified inhibitors were determined in complex with the target protein. For thyroid hormone receptor interactor 12, a target that lacks holo structures and small-molecule binders, DrugCLIP achieved a 17.5% hit rate using only AlphaFold2-predicted structures. Finally, we released GenomeScreenDB, an open-access database providing precomputed results for ~10,000 human proteins screened against 500 million compounds, pioneering a drug discovery paradigm in the post-AlphaFold era.",
      "authors": [
        "Yinjun Jia",
        "Bowen Gao",
        "Jiaxin Tan",
        "Jiqing Zheng",
        "Xin Hong",
        "Wenyu Zhu",
        "Haichuan Tan",
        "Yuan Xiao",
        "Liping Tan",
        "Hongyi Cai",
        "Yanwen Huang",
        "Zhiheng Deng",
        "Xiangwei Wu",
        "Yue Jin",
        "Yafei Yuan",
        "Jiekang Tian",
        "Wei He",
        "Weiying Ma",
        "Ya-Qin Zhang",
        "Lei Liu",
        "Chuangye Yan",
        "Wei Zhang",
        "Yanyan Lan"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 9,
      "year": 2026
    },
    {
      "title": "LLaVA-based semantic feature modulation diffusion model for underwater image enhancement",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Guodong Fan",
        "Shengning Zhou",
        "Zhen Hua",
        "Jinjiang Li",
        "Jingchun Zhou"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 8,
      "year": 2026
    },
    {
      "title": "3DGS-Drag: Dragging Gaussians for Intuitive Point-Based 3D Editing",
      "arxiv_id": "",
      "abstract": "The transformative potential of 3D content creation has been progressively unlocked through advancements in generative models. Recently, intuitive drag editing with geometric changes has attracted significant attention in 2D editing yet remains challenging for 3D scenes. In this paper, we introduce 3DGS-Drag -- a point-based 3D editing framework that provides efficient, intuitive drag manipulation of real 3D scenes. Our approach bridges the gap between deformation-based and 2D-editing-based 3D editing methods, addressing their limitations to geometry-related content editing. We leverage two key innovations: deformation guidance utilizing 3D Gaussian Splatting for consistent geometric modifications and diffusion guidance for content correction and visual quality enhancement. A progressive editing strategy further supports aggressive 3D drag edits. Our method enables a wide range of edits, including motion change, shape adjustment, inpainting, and content extension. Experimental results demonstrate the effectiveness of 3DGS-Drag in various scenes, achieving state-of-the-art performance in geometry-related 3D content editing. Notably, the editing is efficient, taking 10 to 20 minutes on a single RTX 4090 GPU.",
      "authors": [
        "Jiahua Dong",
        "Yu-Xiong Wang"
      ],
      "published_date": "2026-01-12",
      "pdf_url": "https://arxiv.org/pdf/2601.07963v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "LILaC: Late Interacting in Layered Component Graph for Open-domain Multimodal Multihop Retrieval",
      "arxiv_id": "",
      "abstract": "Multimodal document retrieval aims to retrieve query-relevant components from documents composed of textual, tabular, and visual elements. An effective multimodal retriever needs to handle two main challenges: (1) mitigate the effect of irrelevant contents caused by fixed, single-granular retrieval units, and (2) support multihop reasoning by effectively capturing semantic relationships among components within and across documents. To address these challenges, we propose LILaC, a multimodal retrieval framework featuring two core innovations. First, we introduce a layered component graph, explicitly representing multimodal information at two layers - each representing coarse and fine granularity - facilitating efficient yet precise reasoning. Second, we develop a late-interaction-based subgraph retrieval method, an edge-based approach that initially identifies coarse-grained nodes for efficient candidate generation, then performs fine-grained reasoning via late interaction. Extensive experiments demonstrate that LILaC achieves state-of-the-art retrieval performance on all five benchmarks, notably without additional fine-tuning. We make the artifacts publicly available at github.com/joohyung00/lilac.",
      "authors": [
        "Joohyung Yun",
        "Doyup Lee",
        "Wook-Shin Han"
      ],
      "published_date": "2026-02-04",
      "pdf_url": "https://arxiv.org/pdf/2602.04263v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Bidirectional Normalizing Flow: From Data to Noise and Back",
      "arxiv_id": "",
      "abstract": "Normalizing Flows (NFs) have been established as a principled framework for generative modeling. Standard NFs consist of a forward process and a reverse process: the forward process maps data to noise, while the reverse process generates samples by inverting it. Typical NF forward transformations are constrained by explicit invertibility, ensuring that the reverse process can serve as their exact analytic inverse. Recent developments in TARFlow and its variants have revitalized NF methods by combining Transformers and autoregressive flows, but have also exposed causal decoding as a major bottleneck. In this work, we introduce Bidirectional Normalizing Flow ($\\textbf{BiFlow}$), a framework that removes the need for an exact analytic inverse. BiFlow learns a reverse model that approximates the underlying noise-to-data inverse mapping, enabling more flexible loss functions and architectures. Experiments on ImageNet demonstrate that BiFlow, compared to its causal decoding counterpart, improves generation quality while accelerating sampling by up to two orders of magnitude. BiFlow yields state-of-the-art results among NF-based methods and competitive performance among single-evaluation (\"1-NFE\") methods. Following recent encouraging progress on NFs, we hope our work will draw further attention to this classical paradigm.",
      "authors": [
        "Yiyang Lu",
        "Qiao Sun",
        "Xianbang Wang",
        "Zhicheng Jiang",
        "Hanhong Zhao",
        "Kaiming He"
      ],
      "published_date": "2025-12-11",
      "pdf_url": "https://arxiv.org/pdf/2512.10953v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "One-step Latent-free Image Generation with Pixel Mean Flows",
      "arxiv_id": "",
      "abstract": "Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose \"pixel MeanFlow\" (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models.",
      "authors": [
        "Yiyang Lu",
        "Susie Lu",
        "Qiao Sun",
        "Hanhong Zhao",
        "Zhicheng Jiang",
        "Xianbang Wang",
        "Tianhong Li",
        "Zhengyang Geng",
        "Kaiming He"
      ],
      "published_date": "2026-01-29",
      "pdf_url": "https://arxiv.org/pdf/2601.22158v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Meta Flow Maps enable scalable reward alignment",
      "arxiv_id": "",
      "abstract": "Controlling generative models is computationally expensive. This is because optimal alignment with a reward function--whether via inference-time steering or fine-tuning--requires estimating the value function. This task demands access to the conditional posterior $p_{1|t}(x_1|x_t)$, the distribution of clean data $x_1$ consistent with an intermediate state $x_t$, a requirement that typically compels methods to resort to costly trajectory simulations. To address this bottleneck, we introduce Meta Flow Maps (MFMs), a framework extending consistency models and flow maps into the stochastic regime. MFMs are trained to perform stochastic one-step posterior sampling, generating arbitrarily many i.i.d. draws of clean data $x_1$ from any intermediate state. Crucially, these samples provide a differentiable reparametrization that unlocks efficient value function estimation. We leverage this capability to solve bottlenecks in both paradigms: enabling inference-time steering without inner rollouts, and facilitating unbiased, off-policy fine-tuning to general rewards. Empirically, our single-particle steered-MFM sampler outperforms a Best-of-1000 baseline on ImageNet across multiple rewards at a fraction of the compute.",
      "authors": [
        "Peter Potaptchik",
        "Adhi Saravanan",
        "Abbas Mammadov",
        "Alvaro Prat",
        "Michael S. Albergo",
        "Yee Whye Teh"
      ],
      "published_date": "2026-01-20",
      "pdf_url": "https://arxiv.org/pdf/2601.14430v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective",
      "arxiv_id": "",
      "abstract": "Sequential prediction from streaming observations is a fundamental problem in stochastic dynamical systems, where inherent uncertainty often leads to multiple plausible futures. While diffusion and flow-matching models are capable of modeling complex, multi-modal trajectories, their deployment in real-time streaming environments typically relies on repeated sampling from a non-informative initial distribution, incurring substantial inference latency and potential system backlogs. In this work, we introduce Sequential Flow Matching, a principled framework grounded in Bayesian filtering. By treating streaming inference as learning a probability flow that transports the predictive distribution from one time step to the next, our approach naturally aligns with the recursive structure of Bayesian belief updates. We provide theoretical justification that initializing generation from the previous posterior offers a principled warm start that can accelerate sampling compared to naïve re-sampling. Across a wide range of forecasting, decision-making and state estimation tasks, our method achieves performance competitive with full-step diffusion while requiring only one or very few sampling steps, therefore with faster sampling. It suggests that framing sequential inference via Bayesian filtering provides a new and principled perspective towards efficient real-time deployment of flow-based models.",
      "authors": [
        "Yinan Huang",
        "Hans Hao-Hsun Hsu",
        "Junran Wang",
        "Bo Dai",
        "Pan Li"
      ],
      "published_date": "2026-02-05",
      "pdf_url": "https://arxiv.org/pdf/2602.05319v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Generative Modeling via Drifting",
      "arxiv_id": "",
      "abstract": "Generative modeling can be formulated as learning a mapping f such that its pushforward distribution matches the data distribution. The pushforward behavior can be carried out iteratively at inference time, for example in diffusion and flow-based models. In this paper, we propose a new paradigm called Drifting Models, which evolve the pushforward distribution during training and naturally admit one-step inference. We introduce a drifting field that governs the sample movement and achieves equilibrium when the distributions match. This leads to a training objective that allows the neural network optimizer to evolve the distribution. In experiments, our one-step generator achieves state-of-the-art results on ImageNet at 256 x 256 resolution, with an FID of 1.54 in latent space and 1.61 in pixel space. We hope that our work opens up new opportunities for high-quality one-step generation.",
      "authors": [
        "Mingyang Deng",
        "He Li",
        "Tianhong Li",
        "Yilun Du",
        "Kaiming He"
      ],
      "published_date": "2026-02-04",
      "pdf_url": "https://arxiv.org/pdf/2602.04770v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "arxiv_id": "",
      "abstract": "This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.",
      "authors": [
        "Ting Chen",
        "Simon Kornblith",
        "Mohammad Norouzi",
        "Geoffrey Hinton"
      ],
      "published_date": "2020-02-13",
      "pdf_url": "https://arxiv.org/pdf/2002.05709v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning",
      "arxiv_id": "",
      "abstract": "Recent studies have demonstrated significant progress in aligning text-to-image diffusion models with human preference via Reinforcement Learning from Human Feedback. However, while existing methods achieve high scores on automated reward metrics, they often lead to Preference Mode Collapse (PMC)-a specific form of reward hacking where models converge on narrow, high-scoring outputs (e.g., images with monolithic styles or pervasive overexposure), severely degrading generative diversity. In this work, we introduce and quantify this phenomenon, proposing DivGenBench, a novel benchmark designed to measure the extent of PMC. We posit that this collapse is driven by over-optimization along the reward model's inherent biases. Building on this analysis, we propose Directional Decoupling Alignment (D$^2$-Align), a novel framework that mitigates PMC by directionally correcting the reward signal. Specifically, our method first learns a directional correction within the reward model's embedding space while keeping the model frozen. This correction is then applied to the reward signal during the optimization process, preventing the model from collapsing into specific modes and thereby maintaining diversity. Our comprehensive evaluation, combining qualitative analysis with quantitative metrics for both quality and diversity, reveals that D$^2$-Align achieves superior alignment with human preference.",
      "authors": [
        "Chubin Chen",
        "Sujie Hu",
        "Jiashu Zhu",
        "Meiqi Wu",
        "Jintao Chen",
        "Yanxun Li",
        "Nisha Huang",
        "Chengyu Fang",
        "Jiahong Wu",
        "Xiangxiang Chu",
        "Xiu Li"
      ],
      "published_date": "2025-12-30",
      "pdf_url": "https://arxiv.org/pdf/2512.24146v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints",
      "arxiv_id": "",
      "abstract": "Video generation models have achieved remarkable progress, particularly excelling in realistic scenarios; however, their performance degrades notably in imaginative scenarios. These prompts often involve rarely co-occurring concepts with long-distance semantic relationships, falling outside training distributions. Existing methods typically apply test-time scaling for improving video quality, but their fixed search spaces and static reward designs limit adaptability to imaginative scenarios. To fill this gap, we propose ImagerySearch, a prompt-guided adaptive test-time search strategy that dynamically adjusts both the inference search space and reward function according to semantic relationships in the prompt. This enables more coherent and visually plausible videos in challenging imaginative settings. To evaluate progress in this direction, we introduce LDT-Bench, the first dedicated benchmark for long-distance semantic prompts, consisting of 2,839 diverse concept pairs and an automated protocol for assessing creative generation capabilities. Extensive experiments show that ImagerySearch consistently outperforms strong video generation baselines and existing test-time scaling approaches on LDT-Bench, and achieves competitive improvements on VBench, demonstrating its effectiveness across diverse prompt types. We will release LDT-Bench and code to facilitate future research on imaginative video generation.",
      "authors": [
        "Meiqi Wu",
        "Jiashu Zhu",
        "Xiaokun Feng",
        "Chubin Chen",
        "Chen Zhu",
        "Bingze Song",
        "Fangyuan Mao",
        "Jiahong Wu",
        "Xiangxiang Chu",
        "Kaiqi Huang"
      ],
      "published_date": "2025-10-16",
      "pdf_url": "https://arxiv.org/pdf/2510.14847v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders",
      "arxiv_id": "",
      "abstract": "Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.",
      "authors": [
        "Shengbang Tong",
        "Boyang Zheng",
        "Ziteng Wang",
        "Bingda Tang",
        "Nanye Ma",
        "Ellis Brown",
        "Jihan Yang",
        "Rob Fergus",
        "Yann LeCun",
        "Saining Xie"
      ],
      "published_date": "2026-01-22",
      "pdf_url": "https://arxiv.org/pdf/2601.16208v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Simulating the Visual World with Artificial Intelligence: A Roadmap",
      "arxiv_id": "",
      "abstract": "The landscape of video generation is shifting, from a focus on generating visually appealing clips to building virtual environments that support interaction and maintain physical plausibility. These developments point toward the emergence of video foundation models that function not only as visual generators but also as implicit world models, models that simulate the physical dynamics, agent-environment interactions, and task planning that govern real or imagined worlds. This survey provides a systematic overview of this evolution, conceptualizing modern video foundation models as the combination of two core components: an implicit world model and a video renderer. The world model encodes structured knowledge about the world, including physical laws, interaction dynamics, and agent behavior. It serves as a latent simulation engine that enables coherent visual reasoning, long-term temporal consistency, and goal-driven planning. The video renderer transforms this latent simulation into realistic visual observations, effectively producing videos as a \"window\" into the simulated world. We trace the progression of video generation through four generations, in which the core capabilities advance step by step, ultimately culminating in a world model, built upon a video generation model, that embodies intrinsic physical plausibility, real-time multimodal interaction, and planning capabilities spanning multiple spatiotemporal scales. For each generation, we define its core characteristics, highlight representative works, and examine their application domains such as robotics, autonomous driving, and interactive gaming. Finally, we discuss open challenges and design principles for next-generation world models, including the role of agent intelligence in shaping and evaluating these systems. An up-to-date list of related works is maintained at this link.",
      "authors": [
        "Jingtong Yue",
        "Ziqi Huang",
        "Zhaoxi Chen",
        "Xintao Wang",
        "Pengfei Wan",
        "Ziwei Liu"
      ],
      "published_date": "2025-11-11",
      "pdf_url": "https://arxiv.org/pdf/2511.08585v4",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks",
      "arxiv_id": "",
      "abstract": "World models have emerged as a critical frontier in AI research, aiming to enhance large models by infusing them with physical dynamics and world knowledge. The core objective is to enable agents to understand, predict, and interact with complex environments. However, current research landscape remains fragmented, with approaches predominantly focused on injecting world knowledge into isolated tasks, such as visual prediction, 3D estimation, or symbol grounding, rather than establishing a unified definition or framework. While these task-specific integrations yield performance gains, they often lack the systematic coherence required for holistic world understanding. In this paper, we analyze the limitations of such fragmented approaches and propose a unified design specification for world models. We suggest that a robust world model should not be a loose collection of capabilities but a normative framework that integrally incorporates interaction, perception, symbolic reasoning, and spatial representation. This work aims to provide a structured perspective to guide future research toward more general, robust, and principled models of the world.",
      "authors": [
        "Bohan Zeng",
        "Kaixin Zhu",
        "Daili Hua",
        "Bozhou Li",
        "Chengzhuo Tong",
        "Yuran Wang",
        "Xinyi Huang",
        "Yifan Dai",
        "Zixiang Zhang",
        "Yifan Yang",
        "Zhou Liu",
        "Hao Liang",
        "Xiaochen Ma",
        "Ruichuan An",
        "Tianyi Bai",
        "Hongcheng Gao",
        "Junbo Niu",
        "Yang Shi",
        "Xinlong Chen",
        "Yue Ding",
        "Minglei Shi",
        "Kai Zeng",
        "Yiwen Tang",
        "Yuanxing Zhang",
        "Pengfei Wan",
        "Xintao Wang",
        "Wentao Zhang"
      ],
      "published_date": "2026-02-02",
      "pdf_url": "https://arxiv.org/pdf/2602.01630v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "RecTok: Reconstruction Distillation along Rectified Flow",
      "arxiv_id": "",
      "abstract": "Visual tokenizers play a crucial role in diffusion models. The dimensionality of latent space governs both reconstruction fidelity and the semantic expressiveness of the latent feature. However, a fundamental trade-off is inherent between dimensionality and generation quality, constraining existing methods to low-dimensional latent spaces. Although recent works have leveraged vision foundation models to enrich the semantics of visual tokenizers and accelerate convergence, high-dimensional tokenizers still underperform their low-dimensional counterparts. In this work, we propose RecTok, which overcomes the limitations of high-dimensional visual tokenizers through two key innovations: flow semantic distillation and reconstruction--alignment distillation. Our key insight is to make the forward flow in flow matching semantically rich, which serves as the training space of diffusion transformers, rather than focusing on the latent space as in previous works. Specifically, our method distills the semantic information in VFMs into the forward flow trajectories in flow matching. And we further enhance the semantics by introducing a masked feature reconstruction loss. Our RecTok achieves superior image reconstruction, generation quality, and discriminative performance. It achieves state-of-the-art results on the gFID-50K under both with and without classifier-free guidance settings, while maintaining a semantically rich latent space structure. Furthermore, as the latent dimensionality increases, we observe consistent improvements. Code and model are available at https://shi-qingyu.github.io/rectok.github.io.",
      "authors": [
        "Qingyu Shi",
        "Size Wu",
        "Jinbin Bai",
        "Kaidong Yu",
        "Yujing Wang",
        "Yunhai Tong",
        "Xiangtai Li",
        "Xuelong Li"
      ],
      "published_date": "2025-12-15",
      "pdf_url": "https://arxiv.org/pdf/2512.13421v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "RePack then Refine: Efficient Diffusion Transformer with Vision Foundation Model",
      "arxiv_id": "",
      "abstract": "Semantic-rich features from Vision Foundation Models (VFMs) have been leveraged to enhance Latent Diffusion Models (LDMs). However, raw VFM features are typically high-dimensional and redundant, increasing the difficulty of learning and reducing training efficiency for Diffusion Transformers (DiTs). In this paper, we propose Repack then Refine, a three-stage framework that brings the semantic-rich VFM features to DiT while further accelerating learning efficiency. Specifically, the RePack module projects the high-dimensional features onto a compact, low-dimensional manifold. This filters out the redundancy while preserving essential structural information. A standard DiT is then trained for generative modeling on this highly compressed latent space. Finally, to restore the high-frequency details lost due to the compression in RePack, we propose a Latent-Guided Refiner, which is trained lastly for enhancing the image details. On ImageNet-1K, RePack-DiT-XL/1 achieves an FID of 1.82 in only 64 training epochs. With the Refiner module, performance further improves to an FID of 1.65, significantly surpassing latest LDMs in terms of convergence efficiency. Our results demonstrate that packing VFM features, followed by targeted refinement, is a highly effective strategy for balancing generative fidelity with training efficiency.",
      "authors": [
        "Guanfang Dong",
        "Luke Schultz",
        "Negar Hassanpour",
        "Chao Gao"
      ],
      "published_date": "2025-12-12",
      "pdf_url": "https://arxiv.org/pdf/2512.12083v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Diffusion Models Beat GANs on Image Synthesis",
      "arxiv_id": "",
      "abstract": "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion",
      "authors": [
        "Prafulla Dhariwal",
        "Alex Nichol"
      ],
      "published_date": "2021-05-11",
      "pdf_url": "https://arxiv.org/pdf/2105.05233v4",
      "citation_count": null,
      "year": null
    },
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "arxiv_id": "",
      "abstract": "The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.",
      "authors": [
        "Maxime Oquab",
        "Timothée Darcet",
        "Théo Moutakanni",
        "Huy Vo",
        "Marc Szafraniec",
        "Vasil Khalidov",
        "Pierre Fernandez",
        "Daniel Haziza",
        "Francisco Massa",
        "Alaaeldin El-Nouby",
        "Mahmoud Assran",
        "Nicolas Ballas",
        "Wojciech Galuba",
        "Russell Howes",
        "Po-Yao Huang",
        "Shang-Wen Li",
        "Ishan Misra",
        "Michael Rabbat",
        "Vasu Sharma",
        "Gabriel Synnaeve",
        "Hu Xu",
        "Hervé Jegou",
        "Julien Mairal",
        "Patrick Labatut",
        "Armand Joulin",
        "Piotr Bojanowski"
      ],
      "published_date": "2023-04-14",
      "pdf_url": "https://arxiv.org/pdf/2304.07193v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss",
      "arxiv_id": "",
      "abstract": "Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAEs in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leaving existing pixel diffusion methods lagging behind latent diffusion models. We propose PixelGen, a simple pixel diffusion framework with perceptual supervision. Instead of modeling the full image manifold, PixelGen introduces two complementary perceptual losses to guide diffusion model towards learning a more meaningful perceptual manifold. An LPIPS loss facilitates learning better local patterns, while a DINO-based perceptual loss strengthens global semantics. With perceptual supervision, PixelGen surpasses strong latent diffusion baselines. It achieves an FID of 5.11 on ImageNet-256 without classifier-free guidance using only 80 training epochs, and demonstrates favorable scaling performance on large-scale text-to-image generation with a GenEval score of 0.79. PixelGen requires no VAEs, no latent representations, and no auxiliary stages, providing a simpler yet more powerful generative paradigm. Codes are publicly available at https://github.com/Zehong-Ma/PixelGen.",
      "authors": [
        "Zehong Ma",
        "Ruihan Xu",
        "Shiliang Zhang"
      ],
      "published_date": "2026-02-02",
      "pdf_url": "https://arxiv.org/pdf/2602.02493v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
      "arxiv_id": "",
      "abstract": "Recent years have seen remarkable progress in both multimodal understanding models and image generation models. Despite their respective successes, these two domains have evolved independently, leading to distinct architectural paradigms: While autoregressive-based architectures have dominated multimodal understanding, diffusion-based models have become the cornerstone of image generation. Recently, there has been growing interest in developing unified frameworks that integrate these tasks. The emergence of GPT-4o's new capabilities exemplifies this trend, highlighting the potential for unification. However, the architectural differences between the two domains pose significant challenges. To provide a clear overview of current efforts toward unification, we present a comprehensive survey aimed at guiding future research. First, we introduce the foundational concepts and recent advancements in multimodal understanding and text-to-image generation models. Next, we review existing unified models, categorizing them into three main architectural paradigms: diffusion-based, autoregressive-based, and hybrid approaches that fuse autoregressive and diffusion mechanisms. For each category, we analyze the structural designs and innovations introduced by related works. Additionally, we compile datasets and benchmarks tailored for unified models, offering resources for future exploration. Finally, we discuss the key challenges facing this nascent field, including tokenization strategy, cross-modal attention, and data. As this area is still in its early stages, we anticipate rapid advancements and will regularly update this survey. Our goal is to inspire further research and provide a valuable reference for the community. The references associated with this survey are available on GitHub (https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models).",
      "authors": [
        "Shanshan Zhao",
        "Xinjie Zhang",
        "Jintao Guo",
        "Jiakui Hu",
        "Lunhao Duan",
        "Minghao Fu",
        "Yong Xien Chng",
        "Guo-Hua Wang",
        "Qing-Guo Chen",
        "Zhao Xu",
        "Weihua Luo",
        "Kaifu Zhang"
      ],
      "published_date": "2025-05-05",
      "pdf_url": "https://arxiv.org/pdf/2505.02567v6",
      "citation_count": null,
      "year": null
    },
    {
      "title": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery",
      "arxiv_id": "",
      "abstract": "Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner. To evaluate and probe this capability, we develop MentisOculi, a procedural, stratified suite of multi-step reasoning problems amenable to visual solution, tuned to challenge frontier models. Evaluating visual strategies ranging from latent tokens to explicit generated imagery, we find they generally fail to improve performance. Analysis of UMMs specifically exposes a critical limitation: While they possess the textual reasoning capacity to solve a task and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to leverage even ground-truth visualizations. Our findings suggest that despite their inherent appeal, visual thoughts do not yet benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and close this gap across diverse model families.",
      "authors": [
        "Jana Zeller",
        "Thaddäus Wiedemer",
        "Fanfei Li",
        "Thomas Klein",
        "Prasanna Mayilvahanan",
        "Matthias Bethge",
        "Felix Wichmann",
        "Ryan Cotterell",
        "Wieland Brendel"
      ],
      "published_date": "2026-02-02",
      "pdf_url": "https://arxiv.org/pdf/2602.02465v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation",
      "arxiv_id": "",
      "abstract": "This paper presents a family of advanced vision encoder, named OpenVision 3, that learns a single, unified visual representation that can serve both image understanding and image generation. Our core architecture is simple: we feed VAE-compressed image latents to a ViT encoder and train its output to support two complementary roles. First, the encoder output is passed to the ViT-VAE decoder to reconstruct the original image, encouraging the representation to capture generative structure. Second, the same representation is optimized with contrastive learning and image-captioning objectives, strengthening semantic features. By jointly optimizing reconstruction- and semantics-driven signals in a shared latent space, the encoder learns representations that synergize and generalize well across both regimes. We validate this unified design through extensive downstream evaluations with the encoder frozen. For multimodal understanding, we plug the encoder into the LLaVA-1.5 framework: it performs comparably with a standard CLIP vision encoder (e.g., 62.4 vs 62.2 on SeedBench, and 83.7 vs 82.9 on POPE). For generation, we test it under the RAE framework: ours substantially surpasses the standard CLIP-based encoder (e.g., gFID: 1.89 vs 2.54 on ImageNet). We hope this work can spur future research on unified modeling.",
      "authors": [
        "Letian Zhang",
        "Sucheng Ren",
        "Yanqing Liu",
        "Xianhang Li",
        "Zeyu Wang",
        "Yuyin Zhou",
        "Huaxiu Yao",
        "Zeyu Zheng",
        "Weili Nie",
        "Guilin Liu",
        "Zhiding Yu",
        "Cihang Xie"
      ],
      "published_date": "2026-01-21",
      "pdf_url": "https://arxiv.org/pdf/2601.15369v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Distributed Representations of Words and Phrases and their Compositionality",
      "arxiv_id": "",
      "abstract": "The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
      "authors": [
        "Tomas Mikolov",
        "Ilya Sutskever",
        "Kai Chen",
        "Greg Corrado",
        "Jeffrey Dean"
      ],
      "published_date": "2013-10-16",
      "pdf_url": "https://arxiv.org/pdf/1310.4546v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "GloVe: Global Vectors for Word Representation",
      "arxiv_id": null,
      "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
      "authors": [
        "Jeffrey Pennington",
        "R. Socher",
        "Christopher D. Manning"
      ],
      "published_date": "2014",
      "pdf_url": "",
      "citation_count": 33934,
      "year": 2014
    },
    {
      "title": "Deep Contextualized Word Representations",
      "arxiv_id": "1802.05365",
      "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
      "authors": [
        "Matthew E. Peters",
        "Mark Neumann",
        "Mohit Iyyer",
        "Matt Gardner",
        "Christopher Clark",
        "Kenton Lee",
        "Luke Zettlemoyer"
      ],
      "published_date": "2018-02-15",
      "pdf_url": "https://arxiv.org/pdf/1802.05365v2",
      "citation_count": 12014,
      "year": 2018
    },
    {
      "title": "Protein Language Models: Is Scaling Necessary?",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Quentin Fournier",
        "Robert M. Vernon",
        "Almer M. van der Sloot",
        "Benjamin Schulz",
        "Sarath Chandar",
        "C. Langmead"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 26,
      "year": 2026
    },
    {
      "title": "Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis",
      "arxiv_id": "",
      "abstract": "Modern models for common NLP tasks often employ machine learning techniques and train on journalistic, social media, or other culturally-derived text. These have recently been scrutinized for racial and gender biases, rooting from inherent bias in their training text. These biases are often sub-optimal and recent work poses methods to rectify them; however, these biases may shed light on actual racial or gender gaps in the culture(s) that produced the training text, thereby helping us understand cultural context through big data. This paper presents an approach for quantifying gender bias in word embeddings, and then using them to characterize statistical gender gaps in education, politics, economics, and health. We validate these metrics on 2018 Twitter data spanning 51 U.S. regions and 99 countries. We correlate state and country word embedding biases with 18 international and 5 U.S.-based statistical gender gaps, characterizing regularities and predictive strength.",
      "authors": [
        "Scott Friedman",
        "Sonja Schmer-Galunder",
        "Anthony Chen",
        "Jeffrey Rye"
      ],
      "published_date": "2026-01-23",
      "pdf_url": "https://arxiv.org/pdf/2601.17203v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation",
      "arxiv_id": null,
      "abstract": "The Transformer architecture has demonstrated remarkable results in 3D medical image segmentation due to its capability of modeling global relationships. However, it poses a significant computational burden when processing high-dimensional medical images. Mamba, as a State Space Model (SSM), has recently emerged as a notable approach for modeling long-range dependencies in sequential data. Although a substantial amount of Mamba-based research has focused on natural language and 2D image processing, few studies explore the capability of Mamba on 3D medical images. In this paper, we propose SegMamba-V2, a novel 3D medical image segmentation model, to effectively capture long-range dependencies within whole-volume features at each scale. To achieve this goal, we first devise a hierarchical scale downsampling strategy to enhance the receptive field and mitigate information loss during downsampling. Furthermore, we design a novel tri-orientated spatial Mamba block that extends the global dependency modeling process from one plane to three orthogonal planes to improve feature representation capability. Moreover, we collect and annotate a large-scale dataset (named CRC-2000) with fine-grained categories to facilitate benchmarking evaluation in 3D colorectal cancer (CRC) segmentation. We evaluate the effectiveness of our SegMamba-V2 on CRC-2000 and three other large-scale 3D medical image segmentation datasets, covering various modalities, organs, and segmentation targets. Experimental results demonstrate that our Segmamba-V2 outperforms state-of-the-art methods by a significant margin, which indicates the universality and effectiveness of the proposed model on 3D medical image segmentation tasks. The code for SegMamba-V2 is publicly available at: https://github.com/ge-xing/SegMamba-V2",
      "authors": [
        "Zhaohu Xing",
        "Tian Ye",
        "Yijun Yang",
        "D. Cai",
        "Baowen Gai",
        "Xiao-Jian Wu",
        "Feng Gao",
        "Lei Zhu"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 13,
      "year": 2026
    },
    {
      "title": "Generative Classifiers Avoid Shortcut Solutions",
      "arxiv_id": "",
      "abstract": "Discriminative approaches to classification often learn shortcuts that hold in-distribution but fail even under minor distribution shift. This failure mode stems from an overreliance on features that are spuriously correlated with the label. We show that generative classifiers, which use class-conditional generative models, can avoid this issue by modeling all features, both core and spurious, instead of mainly spurious ones. These generative classifiers are simple to train, avoiding the need for specialized augmentations, strong regularization, extra hyperparameters, or knowledge of the specific spurious correlations to avoid. We find that diffusion-based and autoregressive generative classifiers achieve state-of-the-art performance on five standard image and text distribution shift benchmarks and reduce the impact of spurious correlations in realistic applications, such as medical or satellite datasets. Finally, we carefully analyze a Gaussian toy setting to understand the inductive biases of generative classifiers, as well as the data properties that determine when generative classifiers outperform discriminative ones.",
      "authors": [
        "Alexander C. Li",
        "Ananya Kumar",
        "Deepak Pathak"
      ],
      "published_date": "2025-12-31",
      "pdf_url": "https://arxiv.org/pdf/2512.25034v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Edge Large AI Model Agent-Empowered Cognitive Multimodal Semantic Communication",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Y. Sun",
        "Yinqiu Liu",
        "Shaoyong Guo",
        "Xuesong Qiu",
        "Jiewei Chen",
        "Jiakai Hao",
        "Dusist Niyato"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 10,
      "year": 2026
    },
    {
      "title": "Language Models are Unsupervised Multitask Learners",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Alec Radford",
        "Jeff Wu",
        "R. Child",
        "D. Luan",
        "Dario Amodei",
        "I. Sutskever"
      ],
      "published_date": "2019",
      "pdf_url": "",
      "citation_count": 27076,
      "year": 2019
    },
    {
      "title": "The Llama 3 Herd of Models",
      "arxiv_id": "",
      "abstract": "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",
      "authors": [
        "Aaron Grattafiori",
        "Abhimanyu Dubey",
        "Abhinav Jauhri",
        "Abhinav Pandey",
        "Abhishek Kadian",
        "Ahmad Al-Dahle",
        "Aiesha Letman",
        "Akhil Mathur",
        "Alan Schelten",
        "Alex Vaughan",
        "Amy Yang",
        "Angela Fan",
        "Anirudh Goyal",
        "Anthony Hartshorn",
        "Aobo Yang",
        "Archi Mitra",
        "Archie Sravankumar",
        "Artem Korenev",
        "Arthur Hinsvark",
        "Arun Rao",
        "Aston Zhang",
        "Aurelien Rodriguez",
        "Austen Gregerson",
        "Ava Spataru",
        "Baptiste Roziere",
        "Bethany Biron",
        "Binh Tang",
        "Bobbie Chern",
        "Charlotte Caucheteux",
        "Chaya Nayak",
        "Chloe Bi",
        "Chris Marra",
        "Chris McConnell",
        "Christian Keller",
        "Christophe Touret",
        "Chunyang Wu",
        "Corinne Wong",
        "Cristian Canton Ferrer",
        "Cyrus Nikolaidis",
        "Damien Allonsius",
        "Daniel Song",
        "Danielle Pintz",
        "Danny Livshits",
        "Danny Wyatt",
        "David Esiobu",
        "Dhruv Choudhary",
        "Dhruv Mahajan",
        "Diego Garcia-Olano",
        "Diego Perino",
        "Dieuwke Hupkes",
        "Egor Lakomkin",
        "Ehab AlBadawy",
        "Elina Lobanova",
        "Emily Dinan",
        "Eric Michael Smith",
        "Filip Radenovic",
        "Francisco Guzmán",
        "Frank Zhang",
        "Gabriel Synnaeve",
        "Gabrielle Lee",
        "Georgia Lewis Anderson",
        "Govind Thattai",
        "Graeme Nail",
        "Gregoire Mialon",
        "Guan Pang",
        "Guillem Cucurell",
        "Hailey Nguyen",
        "Hannah Korevaar",
        "Hu Xu",
        "Hugo Touvron",
        "Iliyan Zarov",
        "Imanol Arrieta Ibarra",
        "Isabel Kloumann",
        "Ishan Misra",
        "Ivan Evtimov",
        "Jack Zhang",
        "Jade Copet",
        "Jaewon Lee",
        "Jan Geffert",
        "Jana Vranes",
        "Jason Park",
        "Jay Mahadeokar",
        "Jeet Shah",
        "Jelmer van der Linde",
        "Jennifer Billock",
        "Jenny Hong",
        "Jenya Lee",
        "Jeremy Fu",
        "Jianfeng Chi",
        "Jianyu Huang",
        "Jiawen Liu",
        "Jie Wang",
        "Jiecao Yu",
        "Joanna Bitton",
        "Joe Spisak",
        "Jongsoo Park",
        "Joseph Rocca",
        "Joshua Johnstun",
        "Joshua Saxe",
        "Junteng Jia",
        "Kalyan Vasuden Alwala",
        "Karthik Prasad",
        "Kartikeya Upasani",
        "Kate Plawiak",
        "Ke Li",
        "Kenneth Heafield",
        "Kevin Stone",
        "Khalid El-Arini",
        "Krithika Iyer",
        "Kshitiz Malik",
        "Kuenley Chiu",
        "Kunal Bhalla",
        "Kushal Lakhotia",
        "Lauren Rantala-Yeary",
        "Laurens van der Maaten",
        "Lawrence Chen",
        "Liang Tan",
        "Liz Jenkins",
        "Louis Martin",
        "Lovish Madaan",
        "Lubo Malo",
        "Lukas Blecher",
        "Lukas Landzaat",
        "Luke de Oliveira",
        "Madeline Muzzi",
        "Mahesh Pasupuleti",
        "Mannat Singh",
        "Manohar Paluri",
        "Marcin Kardas",
        "Maria Tsimpoukelli",
        "Mathew Oldham",
        "Mathieu Rita",
        "Maya Pavlova",
        "Melanie Kambadur",
        "Mike Lewis",
        "Min Si",
        "Mitesh Kumar Singh",
        "Mona Hassan",
        "Naman Goyal",
        "Narjes Torabi",
        "Nikolay Bashlykov",
        "Nikolay Bogoychev",
        "Niladri Chatterji",
        "Ning Zhang",
        "Olivier Duchenne",
        "Onur Çelebi",
        "Patrick Alrassy",
        "Pengchuan Zhang",
        "Pengwei Li",
        "Petar Vasic",
        "Peter Weng",
        "Prajjwal Bhargava",
        "Pratik Dubal",
        "Praveen Krishnan",
        "Punit Singh Koura",
        "Puxin Xu",
        "Qing He",
        "Qingxiao Dong",
        "Ragavan Srinivasan",
        "Raj Ganapathy",
        "Ramon Calderer",
        "Ricardo Silveira Cabral",
        "Robert Stojnic",
        "Roberta Raileanu",
        "Rohan Maheswari",
        "Rohit Girdhar",
        "Rohit Patel",
        "Romain Sauvestre",
        "Ronnie Polidoro",
        "Roshan Sumbaly",
        "Ross Taylor",
        "Ruan Silva",
        "Rui Hou",
        "Rui Wang",
        "Saghar Hosseini",
        "Sahana Chennabasappa",
        "Sanjay Singh",
        "Sean Bell",
        "Seohyun Sonia Kim",
        "Sergey Edunov",
        "Shaoliang Nie",
        "Sharan Narang",
        "Sharath Raparthy",
        "Sheng Shen",
        "Shengye Wan",
        "Shruti Bhosale",
        "Shun Zhang",
        "Simon Vandenhende",
        "Soumya Batra",
        "Spencer Whitman",
        "Sten Sootla",
        "Stephane Collot",
        "Suchin Gururangan",
        "Sydney Borodinsky",
        "Tamar Herman",
        "Tara Fowler",
        "Tarek Sheasha",
        "Thomas Georgiou",
        "Thomas Scialom",
        "Tobias Speckbacher",
        "Todor Mihaylov",
        "Tong Xiao",
        "Ujjwal Karn",
        "Vedanuj Goswami",
        "Vibhor Gupta",
        "Vignesh Ramanathan",
        "Viktor Kerkez",
        "Vincent Gonguet",
        "Virginie Do",
        "Vish Vogeti",
        "Vítor Albiero",
        "Vladan Petrovic",
        "Weiwei Chu",
        "Wenhan Xiong",
        "Wenyin Fu",
        "Whitney Meers",
        "Xavier Martinet",
        "Xiaodong Wang",
        "Xiaofang Wang",
        "Xiaoqing Ellen Tan",
        "Xide Xia",
        "Xinfeng Xie",
        "Xuchao Jia",
        "Xuewei Wang",
        "Yaelle Goldschlag",
        "Yashesh Gaur",
        "Yasmine Babaei",
        "Yi Wen",
        "Yiwen Song",
        "Yuchen Zhang",
        "Yue Li",
        "Yuning Mao",
        "Zacharie Delpierre Coudert",
        "Zheng Yan",
        "Zhengxing Chen",
        "Zoe Papakipos",
        "Aaditya Singh",
        "Aayushi Srivastava",
        "Abha Jain",
        "Adam Kelsey",
        "Adam Shajnfeld",
        "Adithya Gangidi",
        "Adolfo Victoria",
        "Ahuva Goldstand",
        "Ajay Menon",
        "Ajay Sharma",
        "Alex Boesenberg",
        "Alexei Baevski",
        "Allie Feinstein",
        "Amanda Kallet",
        "Amit Sangani",
        "Amos Teo",
        "Anam Yunus",
        "Andrei Lupu",
        "Andres Alvarado",
        "Andrew Caples",
        "Andrew Gu",
        "Andrew Ho",
        "Andrew Poulton",
        "Andrew Ryan",
        "Ankit Ramchandani",
        "Annie Dong",
        "Annie Franco",
        "Anuj Goyal",
        "Aparajita Saraf",
        "Arkabandhu Chowdhury",
        "Ashley Gabriel",
        "Ashwin Bharambe",
        "Assaf Eisenman",
        "Azadeh Yazdan",
        "Beau James",
        "Ben Maurer",
        "Benjamin Leonhardi",
        "Bernie Huang",
        "Beth Loyd",
        "Beto De Paola",
        "Bhargavi Paranjape",
        "Bing Liu",
        "Bo Wu",
        "Boyu Ni",
        "Braden Hancock",
        "Bram Wasti",
        "Brandon Spence",
        "Brani Stojkovic",
        "Brian Gamido",
        "Britt Montalvo",
        "Carl Parker",
        "Carly Burton",
        "Catalina Mejia",
        "Ce Liu",
        "Changhan Wang",
        "Changkyu Kim",
        "Chao Zhou",
        "Chester Hu",
        "Ching-Hsiang Chu",
        "Chris Cai",
        "Chris Tindal",
        "Christoph Feichtenhofer",
        "Cynthia Gao",
        "Damon Civin",
        "Dana Beaty",
        "Daniel Kreymer",
        "Daniel Li",
        "David Adkins",
        "David Xu",
        "Davide Testuggine",
        "Delia David",
        "Devi Parikh",
        "Diana Liskovich",
        "Didem Foss",
        "Dingkang Wang",
        "Duc Le",
        "Dustin Holland",
        "Edward Dowling",
        "Eissa Jamil",
        "Elaine Montgomery",
        "Eleonora Presani",
        "Emily Hahn",
        "Emily Wood",
        "Eric-Tuan Le",
        "Erik Brinkman",
        "Esteban Arcaute",
        "Evan Dunbar",
        "Evan Smothers",
        "Fei Sun",
        "Felix Kreuk",
        "Feng Tian",
        "Filippos Kokkinos",
        "Firat Ozgenel",
        "Francesco Caggioni",
        "Frank Kanayet",
        "Frank Seide",
        "Gabriela Medina Florez",
        "Gabriella Schwarz",
        "Gada Badeer",
        "Georgia Swee",
        "Gil Halpern",
        "Grant Herman",
        "Grigory Sizov",
        "Guangyi",
        "Zhang",
        "Guna Lakshminarayanan",
        "Hakan Inan",
        "Hamid Shojanazeri",
        "Han Zou",
        "Hannah Wang",
        "Hanwen Zha",
        "Haroun Habeeb",
        "Harrison Rudolph",
        "Helen Suk",
        "Henry Aspegren",
        "Hunter Goldman",
        "Hongyuan Zhan",
        "Ibrahim Damlaj",
        "Igor Molybog",
        "Igor Tufanov",
        "Ilias Leontiadis",
        "Irina-Elena Veliche",
        "Itai Gat",
        "Jake Weissman",
        "James Geboski",
        "James Kohli",
        "Janice Lam",
        "Japhet Asher",
        "Jean-Baptiste Gaya",
        "Jeff Marcus",
        "Jeff Tang",
        "Jennifer Chan",
        "Jenny Zhen",
        "Jeremy Reizenstein",
        "Jeremy Teboul",
        "Jessica Zhong",
        "Jian Jin",
        "Jingyi Yang",
        "Joe Cummings",
        "Jon Carvill",
        "Jon Shepard",
        "Jonathan McPhie",
        "Jonathan Torres",
        "Josh Ginsburg",
        "Junjie Wang",
        "Kai Wu",
        "Kam Hou U",
        "Karan Saxena",
        "Kartikay Khandelwal",
        "Katayoun Zand",
        "Kathy Matosich",
        "Kaushik Veeraraghavan",
        "Kelly Michelena",
        "Keqian Li",
        "Kiran Jagadeesh",
        "Kun Huang",
        "Kunal Chawla",
        "Kyle Huang",
        "Lailin Chen",
        "Lakshya Garg",
        "Lavender A",
        "Leandro Silva",
        "Lee Bell",
        "Lei Zhang",
        "Liangpeng Guo",
        "Licheng Yu",
        "Liron Moshkovich",
        "Luca Wehrstedt",
        "Madian Khabsa",
        "Manav Avalani",
        "Manish Bhatt",
        "Martynas Mankus",
        "Matan Hasson",
        "Matthew Lennie",
        "Matthias Reso",
        "Maxim Groshev",
        "Maxim Naumov",
        "Maya Lathi",
        "Meghan Keneally",
        "Miao Liu",
        "Michael L. Seltzer",
        "Michal Valko",
        "Michelle Restrepo",
        "Mihir Patel",
        "Mik Vyatskov",
        "Mikayel Samvelyan",
        "Mike Clark",
        "Mike Macey",
        "Mike Wang",
        "Miquel Jubert Hermoso",
        "Mo Metanat",
        "Mohammad Rastegari",
        "Munish Bansal",
        "Nandhini Santhanam",
        "Natascha Parks",
        "Natasha White",
        "Navyata Bawa",
        "Nayan Singhal",
        "Nick Egebo",
        "Nicolas Usunier",
        "Nikhil Mehta",
        "Nikolay Pavlovich Laptev",
        "Ning Dong",
        "Norman Cheng",
        "Oleg Chernoguz",
        "Olivia Hart",
        "Omkar Salpekar",
        "Ozlem Kalinli",
        "Parkin Kent",
        "Parth Parekh",
        "Paul Saab",
        "Pavan Balaji",
        "Pedro Rittner",
        "Philip Bontrager",
        "Pierre Roux",
        "Piotr Dollar",
        "Polina Zvyagina",
        "Prashant Ratanchandani",
        "Pritish Yuvraj",
        "Qian Liang",
        "Rachad Alao",
        "Rachel Rodriguez",
        "Rafi Ayub",
        "Raghotham Murthy",
        "Raghu Nayani",
        "Rahul Mitra",
        "Rangaprabhu Parthasarathy",
        "Raymond Li",
        "Rebekkah Hogan",
        "Robin Battey",
        "Rocky Wang",
        "Russ Howes",
        "Ruty Rinott",
        "Sachin Mehta",
        "Sachin Siby",
        "Sai Jayesh Bondu",
        "Samyak Datta",
        "Sara Chugh",
        "Sara Hunt",
        "Sargun Dhillon",
        "Sasha Sidorov",
        "Satadru Pan",
        "Saurabh Mahajan",
        "Saurabh Verma",
        "Seiji Yamamoto",
        "Sharadh Ramaswamy",
        "Shaun Lindsay",
        "Shaun Lindsay",
        "Sheng Feng",
        "Shenghao Lin",
        "Shengxin Cindy Zha",
        "Shishir Patil",
        "Shiva Shankar",
        "Shuqiang Zhang",
        "Shuqiang Zhang",
        "Sinong Wang",
        "Sneha Agarwal",
        "Soji Sajuyigbe",
        "Soumith Chintala",
        "Stephanie Max",
        "Stephen Chen",
        "Steve Kehoe",
        "Steve Satterfield",
        "Sudarshan Govindaprasad",
        "Sumit Gupta",
        "Summer Deng",
        "Sungmin Cho",
        "Sunny Virk",
        "Suraj Subramanian",
        "Sy Choudhury",
        "Sydney Goldman",
        "Tal Remez",
        "Tamar Glaser",
        "Tamara Best",
        "Thilo Koehler",
        "Thomas Robinson",
        "Tianhe Li",
        "Tianjun Zhang",
        "Tim Matthews",
        "Timothy Chou",
        "Tzook Shaked",
        "Varun Vontimitta",
        "Victoria Ajayi",
        "Victoria Montanez",
        "Vijai Mohan",
        "Vinay Satish Kumar",
        "Vishal Mangla",
        "Vlad Ionescu",
        "Vlad Poenaru",
        "Vlad Tiberiu Mihailescu",
        "Vladimir Ivanov",
        "Wei Li",
        "Wenchen Wang",
        "Wenwen Jiang",
        "Wes Bouaziz",
        "Will Constable",
        "Xiaocheng Tang",
        "Xiaojian Wu",
        "Xiaolan Wang",
        "Xilun Wu",
        "Xinbo Gao",
        "Yaniv Kleinman",
        "Yanjun Chen",
        "Ye Hu",
        "Ye Jia",
        "Ye Qi",
        "Yenda Li",
        "Yilin Zhang",
        "Ying Zhang",
        "Yossi Adi",
        "Youngjin Nam",
        "Yu",
        "Wang",
        "Yu Zhao",
        "Yuchen Hao",
        "Yundi Qian",
        "Yunlu Li",
        "Yuzi He",
        "Zach Rait",
        "Zachary DeVito",
        "Zef Rosnbrick",
        "Zhaoduo Wen",
        "Zhenyu Yang",
        "Zhiwei Zhao",
        "Zhiyu Ma"
      ],
      "published_date": "2024-07-31",
      "pdf_url": "https://arxiv.org/pdf/2407.21783v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "A Survey on Diffusion Language Models",
      "arxiv_id": "",
      "abstract": "Diffusion Language Models (DLMs) are rapidly emerging as a powerful and promising alternative to the dominant autoregressive (AR) paradigm. By generating tokens in parallel through an iterative denoising process, DLMs possess inherent advantages in reducing inference latency and capturing bidirectional context, thereby enabling fine-grained control over the generation process. While achieving a several-fold speed-up, recent advancements have allowed DLMs to show performance comparable to their autoregressive counterparts, making them a compelling choice for various natural language processing tasks. In this survey, we provide a holistic overview of the current DLM landscape. We trace its evolution and relationship with other paradigms, such as autoregressive and masked language models, and cover both foundational principles and state-of-the-art models. Our work offers an up-to-date, comprehensive taxonomy and an in-depth analysis of current techniques, from pre-training strategies to advanced post-training methods. Another contribution of this survey is a thorough review of DLM inference strategies and optimizations, including improvements in decoding parallelism, caching mechanisms, and generation quality. We also highlight the latest approaches to multimodal extensions of DLMs and delineate their applications across various practical scenarios. Furthermore, our discussion addresses the limitations and challenges of DLMs, including efficiency, long-sequence handling, and infrastructure requirements, while outlining future research directions to sustain progress in this rapidly evolving field. Project GitHub is available at https://github.com/VILA-Lab/Awesome-DLMs.",
      "authors": [
        "Tianyi Li",
        "Mingda Chen",
        "Bowei Guo",
        "Zhiqiang Shen"
      ],
      "published_date": "2025-08-14",
      "pdf_url": "https://arxiv.org/pdf/2508.10875v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Training Optimal Large Diffusion Language Models",
      "arxiv_id": "",
      "abstract": "We introduce Quokka, the first systematic scaling law for diffusion language models (DLMs), encompassing both compute-constrained and data-constrained regimes, and studying the key modeling and optimization designs. Quokka is a good friend of Chinchilla and provides wider scopes. We hope the results would bring short-term practical guidance in DLMs training and long-term inspirations for the whole AI community.",
      "authors": [
        "Jinjie Ni",
        "Qian Liu",
        "Chao Du",
        "Longxu Dou",
        "Hang Yan",
        "Zili Wang",
        "Tianyu Pang",
        "Michael Qizhe Shieh"
      ],
      "published_date": "2025-09-28",
      "pdf_url": "https://arxiv.org/pdf/2510.03280v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models",
      "arxiv_id": "",
      "abstract": "Masked diffusion large language models (dLLMs) are emerging as promising alternatives to autoregressive LLMs, offering competitive performance while supporting unique generation capabilities such as inpainting. We explore how inpainting can inform RL algorithm design for dLLMs. Aligning LLMs with reinforcement learning faces an exploration challenge: sparse reward signals and sample waste when models fail to discover correct solutions. While this inefficiency affects LLMs broadly, dLLMs offer a distinctive opportunity--their inpainting ability can guide exploration. We introduce IGPO (Inpainting Guided Policy Optimization), an RL framework that strategically inserts partial ground-truth reasoning traces during online sampling. Unlike providing full solutions, inpainting steers exploration toward promising trajectory spaces while preserving self-generated reasoning, bridging supervised fine-tuning and reinforcement learning. We apply IGPO to group-based optimization methods such as GRPO, where exploration failures cause zero advantages and gradients. IGPO restores meaningful gradients while improving sample efficiency. We also propose supervised fine-tuning on synthetically rewritten concise traces that better align with dLLM generation patterns. With additional techniques including entropy-based filtering, our training recipe yields substantial gains across three mathematical benchmarks--GSM8K, Math500, and AMC--achieving new state-of-the-art results for full-attention masked dLLMs.",
      "authors": [
        "Siyan Zhao",
        "Mengchen Liu",
        "Jing Huang",
        "Miao Liu",
        "Chenyu Wang",
        "Bo Liu",
        "Yuandong Tian",
        "Guan Pang",
        "Sean Bell",
        "Aditya Grover",
        "Feiyu Chen"
      ],
      "published_date": "2025-09-12",
      "pdf_url": "https://arxiv.org/pdf/2509.10396v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Encoder-Decoder Diffusion Language Models for Efficient Training and Inference",
      "arxiv_id": "",
      "abstract": "Discrete diffusion models enable parallel token sampling for faster inference than autoregressive approaches. However, prior diffusion models use a decoder-only architecture, which requires sampling algorithms that invoke the full network at every denoising step and incur high computational cost. Our key insight is that discrete diffusion models perform two types of computation: 1) representing clean tokens and 2) denoising corrupted tokens, which enables us to use separate modules for each task. We propose an encoder-decoder architecture to accelerate discrete diffusion inference, which relies on an encoder to represent clean tokens and a lightweight decoder to iteratively refine a noised sequence. We also show that this architecture enables faster training of block diffusion models, which partition sequences into blocks for better quality and are commonly used in diffusion language model inference. We introduce a framework for Efficient Encoder-Decoder Diffusion (E2D2), consisting of an architecture with specialized training and sampling algorithms, and we show that E2D2 achieves superior trade-offs between generation quality and inference throughput on summarization, translation, and mathematical reasoning tasks. We provide the code, model weights, and blog post on the project page: https://m-arriola.com/e2d2",
      "authors": [
        "Marianne Arriola",
        "Yair Schiff",
        "Hao Phung",
        "Aaron Gokaslan",
        "Volodymyr Kuleshov"
      ],
      "published_date": "2025-10-26",
      "pdf_url": "https://arxiv.org/pdf/2510.22852v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model",
      "arxiv_id": "",
      "abstract": "Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \\~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.",
      "authors": [
        "Chenghao Fan",
        "Wen Heng",
        "Bo Li",
        "Sichen Liu",
        "Yuxuan Song",
        "Jing Su",
        "Xiaoye Qu",
        "Kai Shen",
        "Wei Wei"
      ],
      "published_date": "2026-01-22",
      "pdf_url": "https://arxiv.org/pdf/2601.15892v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications",
      "arxiv_id": "",
      "abstract": "Memory emerges as the core module in the Large Language Model (LLM)-based agents for long-horizon complex tasks (e.g., multi-turn dialogue, game playing, scientific discovery), where memory can enable knowledge accumulation, iterative reasoning and self-evolution. Among diverse paradigms, graph stands out as a powerful structure for agent memory due to the intrinsic capabilities to model relational dependencies, organize hierarchical information, and support efficient retrieval. This survey presents a comprehensive review of agent memory from the graph-based perspective. First, we introduce a taxonomy of agent memory, including short-term vs. long-term memory, knowledge vs. experience memory, non-structural vs. structural memory, with an implementation view of graph-based memory. Second, according to the life cycle of agent memory, we systematically analyze the key techniques in graph-based agent memory, covering memory extraction for transforming the data into the contents, storage for organizing the data efficiently, retrieval for retrieving the relevant contents from memory to support reasoning, and evolution for updating the contents in the memory. Third, we summarize the open-sourced libraries and benchmarks that support the development and evaluation of self-evolving agent memory. We also explore diverse application scenarios. Finally, we identify critical challenges and future research directions. This survey aims to offer actionable insights to advance the development of more efficient and reliable graph-based agent memory systems. All the related resources, including research papers, open-source data, and projects, are collected for the community in https://github.com/DEEP-PolyU/Awesome-GraphMemory.",
      "authors": [
        "Chang Yang",
        "Chuang Zhou",
        "Yilin Xiao",
        "Su Dong",
        "Luyao Zhuang",
        "Yujing Zhang",
        "Zhu Wang",
        "Zijin Hong",
        "Zheng Yuan",
        "Zhishang Xiang",
        "Shengyuan Chen",
        "Huachi Zhou",
        "Qinggang Zhang",
        "Ninghao Liu",
        "Jinsong Su",
        "Xinrun Wang",
        "Yi Chang",
        "Xiao Huang"
      ],
      "published_date": "2026-02-05",
      "pdf_url": "https://arxiv.org/pdf/2602.05665v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search",
      "arxiv_id": "",
      "abstract": "Inference-time scaling strategies, particularly Monte Carlo Tree Search (MCTS), have significantly enhanced the reasoning capabilities of Large Language Models (LLMs). However, current approaches remain predominantly stateless, discarding successful reasoning patterns after each problem instance and failing to mimic the empirical accumulation of wisdom characteristic of human problem-solving. To bridge this gap, we introduce Empirical-MCTS, a dual-loop framework that transforms stateless search into a continuous, non-parametric learning process. The framework unifies local exploration with global memory optimization through two novel mechanisms: Pairwise-Experience-Evolutionary Meta-Prompting (PE-EMP) and a Memory Optimization Agent. PE-EMP functions as a reflexive optimizer within the local search, utilizing pairwise feedback to dynamically synthesize adaptive criteria and evolve meta-prompts (system prompts) in real-time. Simultaneously, the Memory Optimization Agent manages a global repository as a dynamic policy prior, employing atomic operations to distill high-quality insights across problems. Extensive evaluations on complex reasoning benchmarks, including AIME25, ARC-AGI-2, and MathArena Apex, demonstrate that Empirical-MCTS significantly outperforms both stateless MCTS strategies and standalone experience-driven agents. These results underscore the critical necessity of coupling structured search with empirical accumulation for mastering complex, open-ended reasoning tasks.",
      "authors": [
        "Hao Lu",
        "Haoyuan Huang",
        "Yulin Zhou",
        "Chen Li",
        "Ningxin Zhu"
      ],
      "published_date": "2026-02-04",
      "pdf_url": "https://arxiv.org/pdf/2602.04248v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking",
      "arxiv_id": "",
      "abstract": "Test-time evolution of agent memory serves as a pivotal paradigm for achieving AGI by bolstering complex reasoning through experience accumulation. However, even during benign task evolution, agent safety alignment remains vulnerable-a phenomenon known as Agent Memory Misevolution. To evaluate this phenomenon, we construct the Trust-Memevo benchmark to assess multi-dimensional trustworthiness during benign task evolution, revealing an overall decline in trustworthiness across various task domains and evaluation settings. To address this issue, we propose TAME, a dual-memory evolutionary framework that separately evolves executor memory to improve task performance by distilling generalizable methodologies, and evaluator memory to refine assessments of both safety and task utility based on historical feedback. Through a closed loop of memory filtering, draft generation, trustworthy refinement, execution, and dual-track memory updating, TAME preserves trustworthiness without sacrificing utility. Experiments demonstrate that TAME mitigates misevolution, achieving a joint improvement in both trustworthiness and task performance.",
      "authors": [
        "Yu Cheng",
        "Jiuan Zhou",
        "Yongkang Hu",
        "Yihang Chen",
        "Huichi Zhou",
        "Mingang Chen",
        "Zhizhong Zhang",
        "Kun Shao",
        "Yuan Xie",
        "Zhaoxia Yin"
      ],
      "published_date": "2026-02-03",
      "pdf_url": "https://arxiv.org/pdf/2602.03224v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents",
      "arxiv_id": "",
      "abstract": "LLM-driven agents demonstrate strong performance in sequential decision-making but often rely on on-the-fly reasoning, re-deriving solutions even in recurring scenarios. This insufficient experience reuse leads to computational redundancy and execution instability. To bridge this gap, we propose ProcMEM, a framework that enables agents to autonomously learn procedural memory from interaction experiences without parameter updates. By formalizing a Skill-MDP, ProcMEM transforms passive episodic narratives into executable Skills defined by activation, execution, and termination conditions to ensure executability. To achieve reliable reusability without capability degradation, we introduce Non-Parametric PPO, which leverages semantic gradients for high-quality candidate generation and a PPO Gate for robust Skill verification. Through score-based maintenance, ProcMEM sustains compact, high-quality procedural memory. Experimental results across in-domain, cross-task, and cross-agent scenarios demonstrate that ProcMEM achieves superior reuse rates and significant performance gains with extreme memory compression. Visualized evolutionary trajectories and Skill distributions further reveal how ProcMEM transparently accumulates, refines, and reuses procedural knowledge to facilitate long-term autonomy.",
      "authors": [
        "Qirui Mi",
        "Zhijian Ma",
        "Mengyue Yang",
        "Haoxuan Li",
        "Yisen Wang",
        "Haifeng Zhang",
        "Jun Wang"
      ],
      "published_date": "2026-02-02",
      "pdf_url": "https://arxiv.org/pdf/2602.01869v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting",
      "arxiv_id": "",
      "abstract": "Time series forecasting has traditionally been formulated as a model-centric, static, and single-pass prediction problem that maps historical observations to future values. While this paradigm has driven substantial progress, it proves insufficient in adaptive and multi-turn settings where forecasting requires informative feature extraction, reasoning-driven inference, iterative refinement, and continual adaptation over time. In this paper, we argue for agentic time series forecasting (ATSF), which reframes forecasting as an agentic process composed of perception, planning, action, reflection, and memory. Rather than focusing solely on predictive models, ATSF emphasizes organizing forecasting as an agentic workflow that can interact with tools, incorporate feedback from outcomes, and evolve through experience accumulation. We outline three representative implementation paradigms -- workflow-based design, agentic reinforcement learning, and a hybrid agentic workflow paradigm -- and discuss the opportunities and challenges that arise when shifting from model-centric prediction to agentic forecasting. Together, this position aims to establish agentic forecasting as a foundation for future research at the intersection of time series forecasting.",
      "authors": [
        "Mingyue Cheng",
        "Xiaoyu Tao",
        "Qi Liu",
        "Ze Guo",
        "Enhong Chen"
      ],
      "published_date": "2026-02-02",
      "pdf_url": "https://arxiv.org/pdf/2602.01776v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Training language models to follow instructions with human feedback",
      "arxiv_id": "",
      "abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",
      "authors": [
        "Long Ouyang",
        "Jeff Wu",
        "Xu Jiang",
        "Diogo Almeida",
        "Carroll L. Wainwright",
        "Pamela Mishkin",
        "Chong Zhang",
        "Sandhini Agarwal",
        "Katarina Slama",
        "Alex Ray",
        "John Schulman",
        "Jacob Hilton",
        "Fraser Kelton",
        "Luke Miller",
        "Maddie Simens",
        "Amanda Askell",
        "Peter Welinder",
        "Paul Christiano",
        "Jan Leike",
        "Ryan Lowe"
      ],
      "published_date": "2022-03-04",
      "pdf_url": "https://arxiv.org/pdf/2203.02155v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "PaLM: Scaling Language Modeling with Pathways",
      "arxiv_id": "",
      "abstract": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",
      "authors": [
        "Aakanksha Chowdhery",
        "Sharan Narang",
        "Jacob Devlin",
        "Maarten Bosma",
        "Gaurav Mishra",
        "Adam Roberts",
        "Paul Barham",
        "Hyung Won Chung",
        "Charles Sutton",
        "Sebastian Gehrmann",
        "Parker Schuh",
        "Kensen Shi",
        "Sasha Tsvyashchenko",
        "Joshua Maynez",
        "Abhishek Rao",
        "Parker Barnes",
        "Yi Tay",
        "Noam Shazeer",
        "Vinodkumar Prabhakaran",
        "Emily Reif",
        "Nan Du",
        "Ben Hutchinson",
        "Reiner Pope",
        "James Bradbury",
        "Jacob Austin",
        "Michael Isard",
        "Guy Gur-Ari",
        "Pengcheng Yin",
        "Toju Duke",
        "Anselm Levskaya",
        "Sanjay Ghemawat",
        "Sunipa Dev",
        "Henryk Michalewski",
        "Xavier Garcia",
        "Vedant Misra",
        "Kevin Robinson",
        "Liam Fedus",
        "Denny Zhou",
        "Daphne Ippolito",
        "David Luan",
        "Hyeontaek Lim",
        "Barret Zoph",
        "Alexander Spiridonov",
        "Ryan Sepassi",
        "David Dohan",
        "Shivani Agrawal",
        "Mark Omernick",
        "Andrew M. Dai",
        "Thanumalayan Sankaranarayana Pillai",
        "Marie Pellat",
        "Aitor Lewkowycz",
        "Erica Moreira",
        "Rewon Child",
        "Oleksandr Polozov",
        "Katherine Lee",
        "Zongwei Zhou",
        "Xuezhi Wang",
        "Brennan Saeta",
        "Mark Diaz",
        "Orhan Firat",
        "Michele Catasta",
        "Jason Wei",
        "Kathy Meier-Hellstern",
        "Douglas Eck",
        "Jeff Dean",
        "Slav Petrov",
        "Noah Fiedel"
      ],
      "published_date": "2022-04-05",
      "pdf_url": "https://arxiv.org/pdf/2204.02311v5",
      "citation_count": null,
      "year": null
    },
    {
      "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
      "arxiv_id": "",
      "abstract": "While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io",
      "authors": [
        "Shunyu Yao",
        "Jeffrey Zhao",
        "Dian Yu",
        "Nan Du",
        "Izhak Shafran",
        "Karthik Narasimhan",
        "Yuan Cao"
      ],
      "published_date": "2022-10-06",
      "pdf_url": "https://arxiv.org/pdf/2210.03629v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "From System 1 to System 2: A Survey of Reasoning Large Language Models",
      "arxiv_id": "",
      "abstract": "Achieving human-level intelligence requires refining the transition from the fast, intuitive System 1 to the slower, more deliberate System 2 reasoning. While System 1 excels in quick, heuristic decisions, System 2 relies on logical reasoning for more accurate judgments and reduced biases. Foundational Large Language Models (LLMs) excel at fast decision-making but lack the depth for complex reasoning, as they have not yet fully embraced the step-by-step analysis characteristic of true System 2 thinking. Recently, reasoning LLMs like OpenAI's o1/o3 and DeepSeek's R1 have demonstrated expert-level performance in fields such as mathematics and coding, closely mimicking the deliberate reasoning of System 2 and showcasing human-like cognitive abilities. This survey begins with a brief overview of the progress in foundational LLMs and the early development of System 2 technologies, exploring how their combination has paved the way for reasoning LLMs. Next, we discuss how to construct reasoning LLMs, analyzing their features, the core methods enabling advanced reasoning, and the evolution of various reasoning LLMs. Additionally, we provide an overview of reasoning benchmarks, offering an in-depth comparison of the performance of representative reasoning LLMs. Finally, we explore promising directions for advancing reasoning LLMs and maintain a real-time \\href{https://github.com/zzli2022/Awesome-Slow-Reason-System}{GitHub Repository} to track the latest developments. We hope this survey will serve as a valuable resource to inspire innovation and drive progress in this rapidly evolving field.",
      "authors": [
        "Zhong-Zhi Li",
        "Duzhen Zhang",
        "Ming-Liang Zhang",
        "Jiaxin Zhang",
        "Zengyan Liu",
        "Yuxuan Yao",
        "Haotian Xu",
        "Junhao Zheng",
        "Pei-Jie Wang",
        "Xiuyi Chen",
        "Yingying Zhang",
        "Fei Yin",
        "Jiahua Dong",
        "Zhiwei Li",
        "Bao-Long Bi",
        "Ling-Rui Mei",
        "Junfeng Fang",
        "Xiao Liang",
        "Zhijiang Guo",
        "Le Song",
        "Cheng-Lin Liu"
      ],
      "published_date": "2025-02-24",
      "pdf_url": "https://arxiv.org/pdf/2502.17419v6",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Deep Research Agents: A Systematic Examination And Roadmap",
      "arxiv_id": "",
      "abstract": "The rapid progress of Large Language Models (LLMs) has given rise to a new category of autonomous AI systems, referred to as Deep Research (DR) agents. These agents are designed to tackle complex, multi-turn informational research tasks by leveraging a combination of dynamic reasoning, adaptive long-horizon planning, multi-hop information retrieval, iterative tool use, and the generation of structured analytical reports. In this paper, we conduct a detailed analysis of the foundational technologies and architectural components that constitute Deep Research agents. We begin by reviewing information acquisition strategies, contrasting API-based retrieval methods with browser-based exploration. We then examine modular tool-use frameworks, including code execution, multimodal input processing, and the integration of Model Context Protocols (MCPs) to support extensibility and ecosystem development. To systematize existing approaches, we propose a taxonomy that differentiates between static and dynamic workflows, and we classify agent architectures based on planning strategies and agent composition, including single-agent and multi-agent configurations. We also provide a critical evaluation of current benchmarks, highlighting key limitations such as restricted access to external knowledge, sequential execution inefficiencies, and misalignment between evaluation metrics and the practical objectives of DR agents. Finally, we outline open challenges and promising directions for future research. A curated and continuously updated repository of DR agent research is available at: {https://github.com/ai-agents-2030/awesome-deep-research-agent}.",
      "authors": [
        "Yuxuan Huang",
        "Yihang Chen",
        "Haozheng Zhang",
        "Kang Li",
        "Huichi Zhou",
        "Meng Fang",
        "Linyi Yang",
        "Xiaoguang Li",
        "Lifeng Shang",
        "Songcen Xu",
        "Jianye Hao",
        "Kun Shao",
        "Jun Wang"
      ],
      "published_date": "2025-06-22",
      "pdf_url": "https://arxiv.org/pdf/2506.18096v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Table-R1: Inference-Time Scaling for Table Reasoning",
      "arxiv_id": "",
      "abstract": "In this work, we present the first study to explore inference-time scaling on table reasoning tasks. We develop and evaluate two post-training strategies to enable inference-time scaling: distillation from frontier model reasoning traces and reinforcement learning with verifiable rewards (RLVR). For distillation, we introduce a large-scale dataset of reasoning traces generated by DeepSeek-R1, which we use to fine-tune LLMs into the Table-R1-SFT model. For RLVR, we propose task-specific verifiable reward functions and apply the GRPO algorithm to obtain the Table-R1-Zero model. We evaluate our Table-R1-series models across diverse table reasoning tasks, including short-form QA, fact verification, and free-form QA. Notably, the Table-R1-Zero model matches or exceeds the performance of GPT-4.1 and DeepSeek-R1, while using only a 7B-parameter LLM. It also demonstrates strong generalization to out-of-domain datasets. Extensive ablation and qualitative analyses reveal the benefits of instruction tuning, model architecture choices, and cross-task generalization, as well as emergence of essential table reasoning skills during RL training.",
      "authors": [
        "Zheyuan Yang",
        "Lyuhao Chen",
        "Arman Cohan",
        "Yilun Zhao"
      ],
      "published_date": "2025-05-29",
      "pdf_url": "https://arxiv.org/pdf/2505.23621v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis",
      "arxiv_id": "",
      "abstract": "Synthesizing informative commercial reports from massive and noisy web sources is critical for high-stakes business decisions. Although current deep research agents achieve notable progress, their reports still remain limited in terms of quality, reliability, and coverage. In this work, we propose Mind2Report, a cognitive deep research agent that emulates the commercial analyst to synthesize expert-level reports. Specifically, it first probes fine-grained intent, then searches web sources and records distilled information on the fly, and subsequently iteratively synthesizes the report. We design Mind2Report as a training-free agentic workflow that augments general large language models (LLMs) with dynamic memory to support these long-form cognitive processes. To rigorously evaluate Mind2Report, we further construct QRC-Eval comprising 200 real-world commercial tasks and establish a holistic evaluation strategy to assess report quality, reliability, and coverage. Experiments demonstrate that Mind2Report outperforms leading baselines, including OpenAI and Gemini deep research agents. Although this is a preliminary study, we expect it to serve as a foundation for advancing the future design of commercial deep research agents. Our code and data are available at https://github.com/Melmaphother/Mind2Report.",
      "authors": [
        "Mingyue Cheng",
        "Daoyu Wang",
        "Qi Liu",
        "Shuo Yu",
        "Xiaoyu Tao",
        "Yuqian Wang",
        "Chengzhong Chu",
        "Yu Duan",
        "Mingkang Long",
        "Enhong Chen"
      ],
      "published_date": "2026-01-08",
      "pdf_url": "https://arxiv.org/pdf/2601.04879v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware Sequence-Level Policy Optimization",
      "arxiv_id": "",
      "abstract": "Academic paper search is a fundamental task in scientific research, yet most existing approaches rely on rigid, predefined workflows that struggle with complex, conditional queries. To address this limitation, we propose PaperScout, an autonomous agent that reformulates paper search as a sequential decision-making process. Unlike static workflows, PaperScout dynamically decides whether, when, and how to invoke search and expand tools based on accumulated retrieval context. However, training such agents presents a fundamental challenge: standard reinforcement learning methods, typically designed for single-turn tasks, suffer from a granularity mismatch when applied to multi-turn agentic tasks, where token-level optimization diverges from the granularity of sequence-level interactions, leading to noisy credit assignment. We introduce Proximal Sequence Policy Optimization (PSPO), a process-aware, sequence-level policy optimization method that aligns optimization with agent-environment interaction. Comprehensive experiments on both synthetic and real-world benchmarks demonstrate that PaperScout significantly outperforms strong workflow-driven and RL baselines in both recall and relevance, validating the effectiveness of our adaptive agentic framework and optimization strategy.",
      "authors": [
        "Tingyue Pan",
        "Jie Ouyang",
        "Mingyue Cheng",
        "Qingchuan Li",
        "Zirui Liu",
        "Mingfan Pan",
        "Shuo Yu",
        "Qi Liu"
      ],
      "published_date": "2026-01-15",
      "pdf_url": "https://arxiv.org/pdf/2601.10029v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Gradient-Guided Learning Network for Infrared Small Target Detection",
      "arxiv_id": "",
      "abstract": "Recently, infrared small target detection has attracted extensive attention. However, due to the small size and the lack of intrinsic features of infrared small targets, the existing methods generally have the problem of inaccurate edge positioning and the target is easily submerged by the background. Therefore, we propose an innovative gradient-guided learning network (GGL-Net). Specifically, we are the first to explore the introduction of gradient magnitude images into the deep learning-based infrared small target detection method, which is conducive to emphasizing the edge details and alleviating the problem of inaccurate edge positioning of small targets. On this basis, we propose a novel dual-branch feature extraction network that utilizes the proposed gradient supplementary module (GSM) to encode raw gradient information into deeper network layers and embeds attention mechanisms reasonably to enhance feature extraction ability. In addition, we construct a two-way guidance fusion module (TGFM), which fully considers the characteristics of feature maps at different levels. It can facilitate the effective fusion of multi-scale feature maps and extract richer semantic information and detailed information through reasonable two-way guidance. Extensive experiments prove that GGL-Net has achieves state-of-the-art results on the public real NUAA-SIRST dataset and the public synthetic NUDT-SIRST dataset. Our code has been integrated into https://github.com/YuChuang1205/MSDA-Net",
      "authors": [
        "Jinmiao Zhao",
        "Chuang Yu",
        "Zelin Shi",
        "Yunpeng Liu",
        "Yingdi Zhang"
      ],
      "published_date": "2025-12-10",
      "pdf_url": "https://arxiv.org/pdf/2512.09497v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "A Retrieval-Augmented Generation Approach to Extracting Algorithmic Logic from Neural Networks",
      "arxiv_id": "",
      "abstract": "Reusing existing neural-network components is central to research efficiency, yet discovering, extracting, and validating such modules across thousands of open-source repositories remains difficult. We introduce NN-RAG, a retrieval-augmented generation system that converts large, heterogeneous PyTorch codebases into a searchable and executable library of validated neural modules. Unlike conventional code search or clone-detection tools, NN-RAG performs scope-aware dependency resolution, import-preserving reconstruction, and validator-gated promotion -- ensuring that every retrieved block is scope-closed, compilable, and runnable. Applied to 19 major repositories, the pipeline extracted 1,289 candidate blocks, validated 941 (73.0%), and demonstrated that over 80% are structurally unique. Through multi-level de-duplication (exact, lexical, structural), we find that NN-RAG contributes the overwhelming majority of unique architectures to the LEMUR dataset, supplying approximately 72% of all novel network structures. Beyond quantity, NN-RAG uniquely enables cross-repository migration of architectural patterns, automatically identifying reusable modules in one project and regenerating them, dependency-complete, in another context. To our knowledge, no other open-source system provides this capability at scale. The framework's neutral specifications further allow optional integration with language models for synthesis or dataset registration without redistributing third-party code. Overall, NN-RAG transforms fragmented vision code into a reproducible, provenance-tracked substrate for algorithmic discovery, offering a first open-source solution that both quantifies and expands the diversity of executable neural architectures across repositories.",
      "authors": [
        "Waleed Khalid",
        "Dmitry Ignatov",
        "Radu Timofte"
      ],
      "published_date": "2025-12-03",
      "pdf_url": "https://arxiv.org/pdf/2512.04329v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation",
      "arxiv_id": "",
      "abstract": "Brain lesion segmentation remains challenging due to small, low-contrast lesions, anisotropic sampling, and cross-slice discontinuities. We propose CenterMamba-SAM, an end-to-end framework that freezes a pretrained backbone and trains only lightweight adapters for efficient fine-tuning. At its core is the CenterMamba encoder, which employs a novel 3x3 corner-axis-center short-sequence scanning strategy to enable center-prioritized, axis-reinforced, and diagonally compensated information aggregation. This design enhances sensitivity to weak boundaries and tiny foci while maintaining sparse yet effective feature representation. A memory-driven structural prompt generator maintains a prototype bank across neighboring slices, enabling automatic synthesis of reliable prompts without user interaction, thereby improving inter-slice coherence. The memory-augmented multi-scale decoder integrates memory attention modules at multiple levels, combining deep supervision with progressive refinement to restore fine details while preserving global consistency. Extensive experiments on public benchmarks demonstrate that CenterMamba-SAM achieves state-of-the-art performance in brain lesion segmentation.",
      "authors": [
        "Yu Tian",
        "Zhongheng Yang",
        "Chenshi Liu",
        "Yiyun Su",
        "Ziwei Hong",
        "Zexi Gong",
        "Jingyuan Xu"
      ],
      "published_date": "2025-11-03",
      "pdf_url": "https://arxiv.org/pdf/2511.01243v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "IMobileTransformer: A fusion-based lightweight model for rice disease identification",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Yang Lu",
        "Haoyang Zhou",
        "Peng Wang",
        "Erzhi Wang",
        "Gongfa Li",
        "Tongjian Yu"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 11,
      "year": 2025
    },
    {
      "title": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers",
      "arxiv_id": "",
      "abstract": "Open-vocabulary detectors achieve impressive performance on COCO, but often fail to generalize to real-world datasets with out-of-distribution classes not typically found in their pre-training. Rather than simply fine-tuning a heavy-weight vision-language model (VLM) for new domains, we introduce RF-DETR, a light-weight specialist detection transformer that discovers accuracy-latency Pareto curves for any target dataset with weight-sharing neural architecture search (NAS). Our approach fine-tunes a pre-trained base network on a target dataset and evaluates thousands of network configurations with different accuracy-latency tradeoffs without re-training. Further, we revisit the \"tunable knobs\" for NAS to improve the transferability of DETRs to diverse target domains. Notably, RF-DETR significantly improves over prior state-of-the-art real-time methods on COCO and Roboflow100-VL. RF-DETR (nano) achieves 48.0 AP on COCO, beating D-FINE (nano) by 5.3 AP at similar latency, and RF-DETR (2x-large) outperforms GroundingDINO (tiny) by 1.2 AP on Roboflow100-VL while running 20x as fast. To the best of our knowledge, RF-DETR (2x-large) is the first real-time detector to surpass 60 AP on COCO. Our code is available at https://github.com/roboflow/rf-detr",
      "authors": [
        "Isaac Robinson",
        "Peter Robicheaux",
        "Matvei Popov",
        "Deva Ramanan",
        "Neehar Peri"
      ],
      "published_date": "2025-11-12",
      "pdf_url": "https://arxiv.org/pdf/2511.09554v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Jinhui Yi",
        "Gina Lopez",
        "S. Hadir",
        "Jan Weyler",
        "Lasse Klingbeil",
        "Marion Deichmann",
        "Juergen Gall",
        "S. J. Seidel"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 5,
      "year": 2025
    },
    {
      "title": "Deep learning models for efficient geotechnical predictions: reducing training effort and data requirements with transfer learning",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Haoding Xu",
        "Xuzhen He",
        "Shaoheng Dai",
        "Caihui Zhu",
        "F. Shan",
        "Qin Zhao",
        "Faning Dang",
        "Daichao Sheng"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 4,
      "year": 2026
    },
    {
      "title": "Image quality assessment: from error visibility to structural similarity",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Zhou Wang",
        "A. Bovik",
        "H. Sheikh",
        "Eero P. Simoncelli"
      ],
      "published_date": "2004",
      "pdf_url": "",
      "citation_count": 54381,
      "year": 2004
    },
    {
      "title": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction",
      "arxiv_id": "",
      "abstract": "Camera redirection aims to replay a dynamic scene from a single monocular video under a user-specified camera trajectory. However, large-angle redirection is inherently ill-posed: a monocular video captures only a narrow spatio-temporal view of a dynamic 3D scene, providing highly partial observations of the underlying 4D world. The key challenge is therefore to recover a complete and coherent representation from this limited input, with consistent geometry and motion. While recent diffusion-based methods achieve impressive results, they often break down under large-angle viewpoint changes far from the original trajectory, where missing visual grounding leads to severe geometric ambiguity and temporal inconsistency. To address this, we present FreeOrbit4D, an effective training-free framework that tackles this geometric ambiguity by recovering a geometry-complete 4D proxy as structural grounding for video generation. We obtain this proxy by decoupling foreground and background reconstructions: we unproject the monocular video into a static background and geometry-incomplete foreground point clouds in a unified global space, then leverage an object-centric multi-view diffusion model to synthesize multi-view images and reconstruct geometry-complete foreground point clouds in canonical object space. By aligning the canonical foreground point cloud to the global scene space via dense pixel-synchronized 3D--3D correspondences and projecting the geometry-complete 4D proxy onto target camera viewpoints, we provide geometric scaffolds that guide a conditional video diffusion model. Extensive experiments show that FreeOrbit4D produces more faithful redirected videos under challenging large-angle trajectories, and our geometry-complete 4D proxy further opens a potential avenue for practical applications such as edit propagation and 4D data generation. Project page and code will be released soon.",
      "authors": [
        "Wei Cao",
        "Hao Zhang",
        "Fengrui Tian",
        "Yulun Wu",
        "Yingying Li",
        "Shenlong Wang",
        "Ning Yu",
        "Yaoyao Liu"
      ],
      "published_date": "2026-01-26",
      "pdf_url": "https://arxiv.org/pdf/2601.18993v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos",
      "arxiv_id": "",
      "abstract": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io",
      "authors": [
        "Yuxue Yang",
        "Lue Fan",
        "Ziqi Shi",
        "Junran Peng",
        "Feng Wang",
        "Zhaoxiang Zhang"
      ],
      "published_date": "2026-01-01",
      "pdf_url": "https://arxiv.org/pdf/2601.00393v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Scalable Diffusion Models with Transformers",
      "arxiv_id": "",
      "abstract": "We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops -- through increased transformer depth/width or increased number of input tokens -- consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512x512 and 256x256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.",
      "authors": [
        "William Peebles",
        "Saining Xie"
      ],
      "published_date": "2022-12-19",
      "pdf_url": "https://arxiv.org/pdf/2212.09748v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding",
      "arxiv_id": "",
      "abstract": "Position encoding recently has shown effective in the transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding(RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: \\url{https://huggingface.co/docs/transformers/model_doc/roformer}.",
      "authors": [
        "Jianlin Su",
        "Yu Lu",
        "Shengfeng Pan",
        "Ahmed Murtadha",
        "Bo Wen",
        "Yunfeng Liu"
      ],
      "published_date": "2021-04-20",
      "pdf_url": "https://arxiv.org/pdf/2104.09864v5",
      "citation_count": null,
      "year": null
    },
    {
      "title": "The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey",
      "arxiv_id": "",
      "abstract": "The Driving World Model (DWM), which focuses on predicting scene evolution during the driving process, has emerged as a promising paradigm in the pursuit of autonomous driving (AD). DWMs enable AD systems to better perceive, understand, and interact with dynamic driving environments. In this survey, we provide a comprehensive overview of the latest progress in DWM. First, we review the DWM ecosystem, which is constructed using mainstream simulators, high-impact datasets, and various metrics that evaluate DWMs across multiple dimensions. We then categorize existing approaches based on the modalities of the predicted scenes, including video, point cloud, occupancy, latent feature, and traffic map, and summarize their specific applications in AD research. In addition, the performance of representative approaches across generating and driving tasks is presented. Finally, we discuss the potential limitations of current research and propose future directions. This survey provides valuable insights into the development and application of DWM, fostering its broader adoption in AD. The relevant papers are collected at https://github.com/LMD0311/Awesome-World-Model.",
      "authors": [
        "Sifan Tu",
        "Xin Zhou",
        "Dingkang Liang",
        "Xingyu Jiang",
        "Yumeng Zhang",
        "Xiaofan Li",
        "Xiang Bai"
      ],
      "published_date": "2025-02-14",
      "pdf_url": "https://arxiv.org/pdf/2502.10498v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Vision meets robotics: The KITTI dataset",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Andreas Geiger",
        "Philip Lenz",
        "C. Stiller",
        "R. Urtasun"
      ],
      "published_date": "2013",
      "pdf_url": "",
      "citation_count": 9865,
      "year": 2013
    },
    {
      "title": "Least-Squares Estimation of Transformation Parameters Between Two Point Patterns",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "S. Umeyama"
      ],
      "published_date": "1991",
      "pdf_url": "",
      "citation_count": 2615,
      "year": 1991
    },
    {
      "title": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations",
      "arxiv_id": "",
      "abstract": "High-definition (HD) maps provide essential semantic information of road structures for autonomous driving systems, yet current HD map construction methods require calibrated multi-camera setups and either implicit or explicit 2D-to-BEV transformations, making them fragile when sensors fail or camera configurations vary across vehicle fleets. We introduce FlexMap, unlike prior methods that are fixed to a specific N-camera rig, our approach adapts to variable camera configurations without any architectural changes or per-configuration retraining. Our key innovation eliminates explicit geometric projections by using a geometry-aware foundation model with cross-frame attention to implicitly encode 3D scene understanding in feature space. FlexMap features two core components: a spatial-temporal enhancement module that separates cross-view spatial reasoning from temporal dynamics, and a camera-aware decoder with latent camera tokens, enabling view-adaptive attention without the need for projection matrices. Experiments demonstrate that FlexMap outperforms existing methods across multiple configurations while maintaining robustness to missing views and sensor variations, enabling more practical real-world deployment.",
      "authors": [
        "Run Wang",
        "Chaoyi Zhou",
        "Amir Salarpour",
        "Xi Liu",
        "Zhi-Qi Cheng",
        "Feng Luo",
        "Mert D. Pesé",
        "Siyu Huang"
      ],
      "published_date": "2026-01-29",
      "pdf_url": "https://arxiv.org/pdf/2601.22376v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Deep Learning",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Xingbang Hao",
        "Guigang Zhang",
        "Shang Ma"
      ],
      "published_date": "2016",
      "pdf_url": "",
      "citation_count": 71055,
      "year": 2016
    },
    {
      "title": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction",
      "arxiv_id": "",
      "abstract": "We introduce MapAnything, a unified transformer-based feed-forward model that ingests one or more images along with optional geometric inputs such as camera intrinsics, poses, depth, or partial reconstructions, and then directly regresses the metric 3D scene geometry and cameras. MapAnything leverages a factored representation of multi-view scene geometry, i.e., a collection of depth maps, local ray maps, camera poses, and a metric scale factor that effectively upgrades local reconstructions into a globally consistent metric frame. Standardizing the supervision and training across diverse datasets, along with flexible input augmentation, enables MapAnything to address a broad range of 3D vision tasks in a single feed-forward pass, including uncalibrated structure-from-motion, calibrated multi-view stereo, monocular depth estimation, camera localization, depth completion, and more. We provide extensive experimental analyses and model ablations demonstrating that MapAnything outperforms or matches specialist feed-forward models while offering more efficient joint training behavior, thus paving the way toward a universal 3D reconstruction backbone.",
      "authors": [
        "Nikhil Keetha",
        "Norman Müller",
        "Johannes Schönberger",
        "Lorenzo Porzi",
        "Yuchen Zhang",
        "Tobias Fischer",
        "Arno Knapitsch",
        "Duncan Zauss",
        "Ethan Weber",
        "Nelson Antunes",
        "Jonathon Luiten",
        "Manuel Lopez-Antequera",
        "Samuel Rota Bulò",
        "Christian Richardt",
        "Deva Ramanan",
        "Sebastian Scherer",
        "Peter Kontschieder"
      ],
      "published_date": "2025-09-16",
      "pdf_url": "https://arxiv.org/pdf/2509.13414v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Seedream 4.0: Toward Next-generation Multimodal Image Generation",
      "arxiv_id": "",
      "abstract": "We introduce Seedream 4.0, an efficient and high-performance multimodal image generation system that unifies text-to-image (T2I) synthesis, image editing, and multi-image composition within a single framework. We develop a highly efficient diffusion transformer with a powerful VAE which also can reduce the number of image tokens considerably. This allows for efficient training of our model, and enables it to fast generate native high-resolution images (e.g., 1K-4K). Seedream 4.0 is pretrained on billions of text-image pairs spanning diverse taxonomies and knowledge-centric concepts. Comprehensive data collection across hundreds of vertical scenarios, coupled with optimized strategies, ensures stable and large-scale training, with strong generalization. By incorporating a carefully fine-tuned VLM model, we perform multi-modal post-training for training both T2I and image editing tasks jointly. For inference acceleration, we integrate adversarial distillation, distribution matching, and quantization, as well as speculative decoding. It achieves an inference time of up to 1.8 seconds for generating a 2K image (without a LLM/VLM as PE model). Comprehensive evaluations reveal that Seedream 4.0 can achieve state-of-the-art results on both T2I and multimodal image editing. In particular, it demonstrates exceptional multimodal capabilities in complex tasks, including precise image editing and in-context reasoning, and also allows for multi-image reference, and can generate multiple output images. This extends traditional T2I systems into an more interactive and multidimensional creative tool, pushing the boundary of generative AI for both creativity and professional applications. We further scale our model and data as Seedream 4.5. Seedream 4.0 and Seedream 4.5 are accessible on Volcano Engine https://www.volcengine.com/experience/ark?launch=seedream.",
      "authors": [
        "Team Seedream",
        ":",
        "Yunpeng Chen",
        "Yu Gao",
        "Lixue Gong",
        "Meng Guo",
        "Qiushan Guo",
        "Zhiyao Guo",
        "Xiaoxia Hou",
        "Weilin Huang",
        "Yixuan Huang",
        "Xiaowen Jian",
        "Huafeng Kuang",
        "Zhichao Lai",
        "Fanshi Li",
        "Liang Li",
        "Xiaochen Lian",
        "Chao Liao",
        "Liyang Liu",
        "Wei Liu",
        "Yanzuo Lu",
        "Zhengxiong Luo",
        "Tongtong Ou",
        "Guang Shi",
        "Yichun Shi",
        "Shiqi Sun",
        "Yu Tian",
        "Zhi Tian",
        "Peng Wang",
        "Rui Wang",
        "Xun Wang",
        "Ye Wang",
        "Guofeng Wu",
        "Jie Wu",
        "Wenxu Wu",
        "Yonghui Wu",
        "Xin Xia",
        "Xuefeng Xiao",
        "Shuang Xu",
        "Xin Yan",
        "Ceyuan Yang",
        "Jianchao Yang",
        "Zhonghua Zhai",
        "Chenlin Zhang",
        "Heng Zhang",
        "Qi Zhang",
        "Xinyu Zhang",
        "Yuwei Zhang",
        "Shijia Zhao",
        "Wenliang Zhao",
        "Wenjia Zhu"
      ],
      "published_date": "2025-09-24",
      "pdf_url": "https://arxiv.org/pdf/2509.20427v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation",
      "arxiv_id": "",
      "abstract": "Recently, GPT-4o has garnered significant attention for its strong performance in image generation, yet open-source models still lag behind. Several studies have explored distilling image data from GPT-4o to enhance open-source models, achieving notable progress. However, a key question remains: given that real-world image datasets already constitute a natural source of high-quality data, why should we use GPT-4o-generated synthetic data? In this work, we identify two key advantages of synthetic images. First, they can complement rare scenarios in real-world datasets, such as surreal fantasy or multi-reference image generation, which frequently occur in user queries. Second, they provide clean and controllable supervision. Real-world data often contains complex background noise and inherent misalignment between text descriptions and image content, whereas synthetic images offer pure backgrounds and long-tailed supervision signals, facilitating more accurate text-to-image alignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale synthetic dataset generated by GPT-4o, harnessing the power of synthetic image data to address blind spots in real-world coverage. Using this dataset, we fine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o. In addition, we propose two new evaluation benchmarks for a more accurate and challenging assessment of image generation capabilities: GenEval++, which increases instruction complexity to mitigate score saturation, and Imagine-Bench, which focuses on evaluating both the understanding and generation of imaginative content. Echo-4o demonstrates strong performance across standard benchmarks. Moreover, applying Echo-4o-Image to other foundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains across multiple metrics, highlighting the datasets strong transferability.",
      "authors": [
        "Junyan Ye",
        "Dongzhi Jiang",
        "Zihao Wang",
        "Leqi Zhu",
        "Zhenghao Hu",
        "Zilong Huang",
        "Jun He",
        "Zhiyuan Yan",
        "Jinghua Yu",
        "Hongsheng Li",
        "Conghui He",
        "Weijia Li"
      ],
      "published_date": "2025-08-13",
      "pdf_url": "https://arxiv.org/pdf/2508.09987v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding",
      "arxiv_id": "",
      "abstract": "We introduce Lumina-DiMOO, an open-source foundational model for seamless multi-modal generation and understanding. Lumina-DiMOO sets itself apart from prior unified models by utilizing a fully discrete diffusion modeling to handle inputs and outputs across various modalities. This innovative approach allows Lumina-DiMOO to achieve higher sampling efficiency compared to previous autoregressive (AR) or hybrid AR-Diffusion paradigms and adeptly support a broad spectrum of multi-modal tasks, including text-to-image generation, image-to-image generation (e.g., image editing, subject-driven generation, and image inpainting, etc.), as well as image understanding. Lumina-DiMOO achieves state-of-the-art performance on multiple benchmarks, surpassing existing open-source unified multi-modal models. To foster further advancements in multi-modal and discrete diffusion model research, we release our code and checkpoints to the community. Project Page: https://synbol.github.io/Lumina-DiMOO.",
      "authors": [
        "Yi Xin",
        "Qi Qin",
        "Siqi Luo",
        "Kaiwen Zhu",
        "Juncheng Yan",
        "Yan Tai",
        "Jiayi Lei",
        "Yuewen Cao",
        "Keqi Wang",
        "Yibin Wang",
        "Jinbin Bai",
        "Qian Yu",
        "Dengyang Jiang",
        "Yuandong Pu",
        "Haoxing Chen",
        "Le Zhuo",
        "Junjun He",
        "Gen Luo",
        "Tianbin Li",
        "Ming Hu",
        "Jin Ye",
        "Shenglong Ye",
        "Bo Zhang",
        "Chang Xu",
        "Wenhai Wang",
        "Hongsheng Li",
        "Guangtao Zhai",
        "Tianfan Xue",
        "Bin Fu",
        "Xiaohong Liu",
        "Yu Qiao",
        "Yihao Liu"
      ],
      "published_date": "2025-10-07",
      "pdf_url": "https://arxiv.org/pdf/2510.06308v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale",
      "arxiv_id": "",
      "abstract": "Prevailing autoregressive (AR) models for text-to-image generation either rely on heavy, computationally-intensive diffusion models to process continuous image tokens, or employ vector quantization (VQ) to obtain discrete tokens with quantization loss. In this paper, we push the autoregressive paradigm forward with NextStep-1, a 14B autoregressive model paired with a 157M flow matching head, training on discrete text tokens and continuous image tokens with next-token prediction objectives. NextStep-1 achieves state-of-the-art performance for autoregressive models in text-to-image generation tasks, exhibiting strong capabilities in high-fidelity image synthesis. Furthermore, our method shows strong performance in image editing, highlighting the power and versatility of our unified approach. To facilitate open research, we will release our code and models to the community.",
      "authors": [
        "NextStep Team",
        "Chunrui Han",
        "Guopeng Li",
        "Jingwei Wu",
        "Quan Sun",
        "Yan Cai",
        "Yuang Peng",
        "Zheng Ge",
        "Deyu Zhou",
        "Haomiao Tang",
        "Hongyu Zhou",
        "Kenkun Liu",
        "Ailin Huang",
        "Bin Wang",
        "Changxin Miao",
        "Deshan Sun",
        "En Yu",
        "Fukun Yin",
        "Gang Yu",
        "Hao Nie",
        "Haoran Lv",
        "Hanpeng Hu",
        "Jia Wang",
        "Jian Zhou",
        "Jianjian Sun",
        "Kaijun Tan",
        "Kang An",
        "Kangheng Lin",
        "Liang Zhao",
        "Mei Chen",
        "Peng Xing",
        "Rui Wang",
        "Shiyu Liu",
        "Shutao Xia",
        "Tianhao You",
        "Wei Ji",
        "Xianfang Zeng",
        "Xin Han",
        "Xuelin Zhang",
        "Yana Wei",
        "Yanming Xu",
        "Yimin Jiang",
        "Yingming Wang",
        "Yu Zhou",
        "Yucheng Han",
        "Ziyang Meng",
        "Binxing Jiao",
        "Daxin Jiang",
        "Xiangyu Zhang",
        "Yibo Zhu"
      ],
      "published_date": "2025-08-14",
      "pdf_url": "https://arxiv.org/pdf/2508.10711v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "arxiv_id": "",
      "abstract": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "published_date": "2019-07-26",
      "pdf_url": "https://arxiv.org/pdf/1907.11692v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning",
      "arxiv_id": "2501.12948",
      "abstract": "General reasoning represents a long-standing and formidable challenge in artificial intelligence. Recent breakthroughs, exemplified by large language models (LLMs) and chain-of-thought prompting, have achieved considerable success on foundational reasoning tasks. However, this success is heavily contingent upon extensive human-annotated demonstrations, and models' capabilities are still insufficient for more complex problems. Here we show that the reasoning abilities of LLMs can be incentivized through pure reinforcement learning (RL), obviating the need for human-labeled reasoning trajectories. The proposed RL framework facilitates the emergent development of advanced reasoning patterns, such as self-reflection, verification, and dynamic strategy adaptation. Consequently, the trained model achieves superior performance on verifiable tasks such as mathematics, coding competitions, and STEM fields, surpassing its counterparts trained via conventional supervised learning on human demonstrations. Moreover, the emergent reasoning patterns exhibited by these large-scale models can be systematically harnessed to guide and enhance the reasoning capabilities of smaller models.",
      "authors": [
        "DeepSeek-AI",
        "Daya Guo",
        "Dejian Yang",
        "Haowei Zhang",
        "Junxiao Song",
        "Peiyi Wang",
        "Qihao Zhu",
        "Runxin Xu",
        "Ruoyu Zhang",
        "Shirong Ma",
        "Xiao Bi",
        "Xiaokang Zhang",
        "Xingkai Yu",
        "Yu Wu",
        "Z. F. Wu",
        "Zhibin Gou",
        "Zhihong Shao",
        "Zhuoshu Li",
        "Ziyi Gao",
        "Aixin Liu",
        "Bing Xue",
        "Bingxuan Wang",
        "Bochao Wu",
        "Bei Feng",
        "Chengda Lu",
        "Chenggang Zhao",
        "Chengqi Deng",
        "Chenyu Zhang",
        "Chong Ruan",
        "Damai Dai",
        "Deli Chen",
        "Dongjie Ji",
        "Erhang Li",
        "Fangyun Lin",
        "Fucong Dai",
        "Fuli Luo",
        "Guangbo Hao",
        "Guanting Chen",
        "Guowei Li",
        "H. Zhang",
        "Han Bao",
        "Hanwei Xu",
        "Haocheng Wang",
        "Honghui Ding",
        "Huajian Xin",
        "Huazuo Gao",
        "Hui Qu",
        "Hui Li",
        "Jianzhong Guo",
        "Jiashi Li",
        "Jiawei Wang",
        "Jingchang Chen",
        "Jingyang Yuan",
        "Junjie Qiu",
        "Junlong Li",
        "J. L. Cai",
        "Jiaqi Ni",
        "Jian Liang",
        "Jin Chen",
        "Kai Dong",
        "Kai Hu",
        "Kaige Gao",
        "Kang Guan",
        "Kexin Huang",
        "Kuai Yu",
        "Lean Wang",
        "Lecong Zhang",
        "Liang Zhao",
        "Litong Wang",
        "Liyue Zhang",
        "Lei Xu",
        "Leyi Xia",
        "Mingchuan Zhang",
        "Minghua Zhang",
        "Minghui Tang",
        "Meng Li",
        "Miaojun Wang",
        "Mingming Li",
        "Ning Tian",
        "Panpan Huang",
        "Peng Zhang",
        "Qiancheng Wang",
        "Qinyu Chen",
        "Qiushi Du",
        "Ruiqi Ge",
        "Ruisong Zhang",
        "Ruizhe Pan",
        "Runji Wang",
        "R. J. Chen",
        "R. L. Jin",
        "Ruyi Chen",
        "Shanghao Lu",
        "Shangyan Zhou",
        "Shanhuang Chen",
        "Shengfeng Ye",
        "Shiyu Wang",
        "Shuiping Yu",
        "Shunfeng Zhou",
        "Shuting Pan",
        "S. S. Li",
        "Shuang Zhou",
        "Shaoqing Wu",
        "Shengfeng Ye",
        "Tao Yun",
        "Tian Pei",
        "Tianyu Sun",
        "T. Wang",
        "Wangding Zeng",
        "Wanjia Zhao",
        "Wen Liu",
        "Wenfeng Liang",
        "Wenjun Gao",
        "Wenqin Yu",
        "Wentao Zhang",
        "W. L. Xiao",
        "Wei An",
        "Xiaodong Liu",
        "Xiaohan Wang",
        "Xiaokang Chen",
        "Xiaotao Nie",
        "Xin Cheng",
        "Xin Liu",
        "Xin Xie",
        "Xingchao Liu",
        "Xinyu Yang",
        "Xinyuan Li",
        "Xuecheng Su",
        "Xuheng Lin",
        "X. Q. Li",
        "Xiangyue Jin",
        "Xiaojin Shen",
        "Xiaosha Chen",
        "Xiaowen Sun",
        "Xiaoxiang Wang",
        "Xinnan Song",
        "Xinyi Zhou",
        "Xianzu Wang",
        "Xinxia Shan",
        "Y. K. Li",
        "Y. Q. Wang",
        "Y. X. Wei",
        "Yang Zhang",
        "Yanhong Xu",
        "Yao Li",
        "Yao Zhao",
        "Yaofeng Sun",
        "Yaohui Wang",
        "Yi Yu",
        "Yichao Zhang",
        "Yifan Shi",
        "Yiliang Xiong",
        "Ying He",
        "Yishi Piao",
        "Yisong Wang",
        "Yixuan Tan",
        "Yiyang Ma",
        "Yiyuan Liu",
        "Yongqiang Guo",
        "Yuan Ou",
        "Yuduan Wang",
        "Yue Gong",
        "Yuheng Zou",
        "Yujia He",
        "Yunfan Xiong",
        "Yuxiang Luo",
        "Yuxiang You",
        "Yuxuan Liu",
        "Yuyang Zhou",
        "Y. X. Zhu",
        "Yanhong Xu",
        "Yanping Huang",
        "Yaohui Li",
        "Yi Zheng",
        "Yuchen Zhu",
        "Yunxian Ma",
        "Ying Tang",
        "Yukun Zha",
        "Yuting Yan",
        "Z. Z. Ren",
        "Zehui Ren",
        "Zhangli Sha",
        "Zhe Fu",
        "Zhean Xu",
        "Zhenda Xie",
        "Zhengyan Zhang",
        "Zhewen Hao",
        "Zhicheng Ma",
        "Zhigang Yan",
        "Zhiyu Wu",
        "Zihui Gu",
        "Zijia Zhu",
        "Zijun Liu",
        "Zilin Li",
        "Ziwei Xie",
        "Ziyang Song",
        "Zizheng Pan",
        "Zhen Huang",
        "Zhipeng Xu",
        "Zhongyu Zhang",
        "Zhen Zhang"
      ],
      "published_date": "2025-01-22",
      "pdf_url": "https://arxiv.org/pdf/2501.12948v2",
      "citation_count": 5472,
      "year": 2025
    },
    {
      "title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning",
      "arxiv_id": "",
      "abstract": "Efficiently acquiring external knowledge and up-to-date information is essential for effective reasoning and text generation in large language models (LLMs). Prompting advanced LLMs with reasoning capabilities to use search engines during inference is often suboptimal, as the LLM might not fully possess the capability on how to interact optimally with the search engine. This paper introduces Search-R1, an extension of reinforcement learning (RL) for reasoning frameworks where the LLM learns to autonomously generate (multiple) search queries during step-by-step reasoning with real-time retrieval. Search-R1 optimizes LLM reasoning trajectories with multi-turn search interactions, leveraging retrieved token masking for stable RL training and a simple outcome-based reward function. Experiments on seven question-answering datasets show that Search-R1 improves performance by 41% (Qwen2.5-7B) and 20% (Qwen2.5-3B) over various RAG baselines under the same setting. This paper further provides empirical insights into RL optimization methods, LLM choices, and response length dynamics in retrieval-augmented reasoning. The code and model checkpoints are available at https://github.com/PeterGriffinJin/Search-R1.",
      "authors": [
        "Bowen Jin",
        "Hansi Zeng",
        "Zhenrui Yue",
        "Jinsung Yoon",
        "Sercan Arik",
        "Dong Wang",
        "Hamed Zamani",
        "Jiawei Han"
      ],
      "published_date": "2025-03-12",
      "pdf_url": "https://arxiv.org/pdf/2503.09516v5",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Toward expert-level medical question answering with large language models",
      "arxiv_id": null,
      "abstract": "Large language models (LLMs) have shown promise in medical question answering, with Med-PaLM being the first to exceed a ‘passing’ score in United States Medical Licensing Examination style questions. However, challenges remain in long-form medical question answering and handling real-world workflows. Here, we present Med-PaLM 2, which bridges these gaps with a combination of base LLM improvements, medical domain fine-tuning and new strategies for improving reasoning and grounding through ensemble refinement and chain of retrieval. Med-PaLM 2 scores up to 86.5% on the MedQA dataset, improving upon Med-PaLM by over 19%, and demonstrates dramatic performance increases across MedMCQA, PubMedQA and MMLU clinical topics datasets. Our detailed human evaluations framework shows that physicians prefer Med-PaLM 2 answers to those from other physicians on eight of nine clinical axes. Med-PaLM 2 also demonstrates significant improvements over its predecessor across all evaluation metrics, particularly on new adversarial datasets designed to probe LLM limitations (P < 0.001). In a pilot study using real-world medical questions, specialists preferred Med-PaLM 2 answers to generalist physician answers 65% of the time. While specialist answers were still preferred overall, both specialists and generalists rated Med-PaLM 2 to be as safe as physician answers, demonstrating its growing potential in real-world medical applications. With an improved framework for model development and evaluation, a large language model is shown to provide answers to medical questions that are comparable or preferred with respect to those provided by human physicians.",
      "authors": [
        "Karan Singhal",
        "Tao Tu",
        "Juraj Gottweis",
        "R. Sayres",
        "Ellery Wulczyn",
        "Mohamed Amin",
        "Le Hou",
        "Kevin Clark",
        "Stephen R. Pfohl",
        "Heather Cole-Lewis",
        "Darlene Neal",
        "Q. Rashid",
        "Mike Schaekermann",
        "Amy Wang",
        "Dev Dash",
        "Jonathan H. Chen",
        "Nigam H. Shah",
        "Sami Lachgar",
        "P. Mansfield",
        "Sushant Prakash",
        "Bradley Green",
        "Ewa Dominowska",
        "Blaise Agüera y Arcas",
        "Nenad Tomašev",
        "Yun Liu",
        "Renee Wong",
        "Christopher Semturs",
        "S. Mahdavi",
        "Joelle K. Barral",
        "Dale R. Webster",
        "G. Corrado",
        "Yossi Matias",
        "Shekoofeh Azizi",
        "A. Karthikesalingam",
        "Vivek Natarajan"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 590,
      "year": 2025
    },
    {
      "title": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training",
      "arxiv_id": "",
      "abstract": "Supervised fine-tuning (SFT) and reinforcement learning (RL) are widely used post-training techniques for foundation models. However, their roles in enhancing model generalization capabilities remain unclear. This paper studies the difference between SFT and RL on generalization and memorization, focusing on text-based rule variants and visual variants. We introduce GeneralPoints, an arithmetic reasoning card game, and adopt V-IRL, a real-world navigation environment, to assess how models trained with SFT and RL generalize to unseen variants in both textual and visual domains. We show that RL, especially when trained with an outcome-based reward, generalizes across both rule-based textual and visual variants. SFT, in contrast, tends to memorize training data and struggles to generalize out-of-distribution scenarios. Further analysis reveals that RL improves the model's underlying visual recognition capabilities, contributing to its enhanced generalization in the visual domain. Despite RL's superior generalization, we show that SFT remains essential for effective RL training; SFT stabilizes the model's output format, enabling subsequent RL to achieve its performance gains. These findings demonstrates the capability of RL for acquiring generalizable knowledge in complex, multi-modal tasks.",
      "authors": [
        "Tianzhe Chu",
        "Yuexiang Zhai",
        "Jihan Yang",
        "Shengbang Tong",
        "Saining Xie",
        "Dale Schuurmans",
        "Quoc V. Le",
        "Sergey Levine",
        "Yi Ma"
      ],
      "published_date": "2025-01-28",
      "pdf_url": "https://arxiv.org/pdf/2501.17161v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Equivariant Diffusion for Crystal Structure Prediction",
      "arxiv_id": "",
      "abstract": "In addressing the challenge of Crystal Structure Prediction (CSP), symmetry-aware deep learning models, particularly diffusion models, have been extensively studied, which treat CSP as a conditional generation task. However, ensuring permutation, rotation, and periodic translation equivariance during diffusion process remains incompletely addressed. In this work, we propose EquiCSP, a novel equivariant diffusion-based generative model. We not only address the overlooked issue of lattice permutation equivariance in existing models, but also develop a unique noising algorithm that rigorously maintains periodic translation equivariance throughout both training and inference processes. Our experiments indicate that EquiCSP significantly surpasses existing models in terms of generating accurate structures and demonstrates faster convergence during the training process.",
      "authors": [
        "Peijia Lin",
        "Pin Chen",
        "Rui Jiao",
        "Qing Mo",
        "Jianhuan Cen",
        "Wenbing Huang",
        "Yang Liu",
        "Dan Huang",
        "Yutong Lu"
      ],
      "published_date": "2025-12-08",
      "pdf_url": "https://arxiv.org/pdf/2512.07289v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling",
      "arxiv_id": "",
      "abstract": "This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.",
      "authors": [
        "Wenqiang Sun",
        "Haiyu Zhang",
        "Haoyuan Wang",
        "Junta Wu",
        "Zehan Wang",
        "Zhenwei Wang",
        "Yunhong Wang",
        "Jun Zhang",
        "Tengfei Wang",
        "Chunchao Guo"
      ],
      "published_date": "2025-12-16",
      "pdf_url": "https://arxiv.org/pdf/2512.14614v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Native and Compact Structured Latents for 3D Generation",
      "arxiv_id": "",
      "abstract": "Recent advancements in 3D generative modeling have significantly improved the generation realism, yet the field is still hampered by existing representations, which struggle to capture assets with complex topologies and detailed appearance. This paper present an approach for learning a structured latent representation from native 3D data to address this challenge. At its core is a new sparse voxel structure called O-Voxel, an omni-voxel representation that encodes both geometry and appearance. O-Voxel can robustly model arbitrary topology, including open, non-manifold, and fully-enclosed surfaces, while capturing comprehensive surface attributes beyond texture color, such as physically-based rendering parameters. Based on O-Voxel, we design a Sparse Compression VAE which provides a high spatial compression rate and a compact latent space. We train large-scale flow-matching models comprising 4B parameters for 3D generation using diverse public 3D asset datasets. Despite their scale, inference remains highly efficient. Meanwhile, the geometry and material quality of our generated assets far exceed those of existing models. We believe our approach offers a significant advancement in 3D generative modeling.",
      "authors": [
        "Jianfeng Xiang",
        "Xiaoxue Chen",
        "Sicheng Xu",
        "Ruicheng Wang",
        "Zelong Lv",
        "Yu Deng",
        "Hongyuan Zhu",
        "Yue Dong",
        "Hao Zhao",
        "Nicholas Jing Yuan",
        "Jiaolong Yang"
      ],
      "published_date": "2025-12-16",
      "pdf_url": "https://arxiv.org/pdf/2512.14692v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "and as an in",
      "arxiv_id": null,
      "abstract": "",
      "authors": [],
      "published_date": "0",
      "pdf_url": "",
      "citation_count": 79428,
      "year": 0
    },
    {
      "title": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?",
      "arxiv_id": "",
      "abstract": "A long-standing challenge in AI is to develop agents capable of solving a wide range of physical tasks and generalizing to new, unseen tasks and environments. A popular recent approach involves training a world model from state-action trajectories and subsequently use it with a planning algorithm to solve new tasks. Planning is commonly performed in the input space, but a recent family of methods has introduced planning algorithms that optimize in the learned representation space of the world model, with the promise that abstracting irrelevant details yields more efficient planning. In this work, we characterize models from this family as JEPA-WMs and investigate the technical choices that make algorithms from this class work. We propose a comprehensive study of several key components with the objective of finding the optimal approach within the family. We conducted experiments using both simulated environments and real-world robotic data, and studied how the model architecture, the training objective, and the planning algorithm affect planning success. We combine our findings to propose a model that outperforms two established baselines, DINO-WM and V-JEPA-2-AC, in both navigation and manipulation tasks. Code, data and checkpoints are available at https://github.com/facebookresearch/jepa-wms.",
      "authors": [
        "Basile Terver",
        "Tsung-Yen Yang",
        "Jean Ponce",
        "Adrien Bardes",
        "Yann LeCun"
      ],
      "published_date": "2025-12-30",
      "pdf_url": "https://arxiv.org/pdf/2512.24497v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Progressive Learned Image Transmission for Semantic Communication Using Hierarchical VAE",
      "arxiv_id": null,
      "abstract": "We present a novel framework called progressive learned image transmission (PLIT) that utilizes a hierarchical variational autoencoder (VAE) to catalyze semantic communication. PLIT employs autoregressive generation through bottom-up and top-down paths to create several feature representations of the transmitted image, effectively capturing contextual information. In this paper, we investigate the progressive transmission of these representations, particularly in the context of successive refinement. In this scenario, the representations are sent to the receiver in phases, with representations in later phases serving to enhance the image quality. Diverging from previous works, our proposed PLIT offers improved flexibility as it is able to dynamically determine the transmission rate. Specifically, PLIT can transform each representation into different numbers of channel symbols, guided by the hierarchical VAE’s learned priors that indicate the entropy of each representation. In addition, we devise a rate attention mechanism to help adjust the encoding strategy to realize different transmission rates. Furthermore, we introduce a spatial grouping strategy to reduce communication overhead for rate matching without compromising image fidelity. Extensive experiments show that our proposed approach outperforms existing baseline methods in terms of rate-distortion performance and maintains robust performance against channel noise.",
      "authors": [
        "Guangyi Zhang",
        "Hanlei Li",
        "Yunlong Cai",
        "Qiyu Hu",
        "Guanding Yu",
        "Zhijing Qin"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 5,
      "year": 2025
    },
    {
      "title": "STORM: Search-Guided Generative World Models for Robotic Manipulation",
      "arxiv_id": "",
      "abstract": "We present STORM (Search-Guided Generative World Models), a novel framework for spatio-temporal reasoning in robotic manipulation that unifies diffusion-based action generation, conditional video prediction, and search-based planning. Unlike prior Vision-Language-Action (VLA) models that rely on abstract latent dynamics or delegate reasoning to language components, STORM grounds planning in explicit visual rollouts, enabling interpretable and foresight-driven decision-making. A diffusion-based VLA policy proposes diverse candidate actions, a generative video world model simulates their visual and reward outcomes, and Monte Carlo Tree Search (MCTS) selectively refines plans through lookahead evaluation. Experiments on the SimplerEnv manipulation benchmark demonstrate that STORM achieves a new state-of-the-art average success rate of 51.0 percent, outperforming strong baselines such as CogACT. Reward-augmented video prediction substantially improves spatio-temporal fidelity and task relevance, reducing Frechet Video Distance by over 75 percent. Moreover, STORM exhibits robust re-planning and failure recovery behavior, highlighting the advantages of search-guided generative world models for long-horizon robotic manipulation.",
      "authors": [
        "Wenjun Lin",
        "Jensen Zhang",
        "Kaitong Cai",
        "Keze Wang"
      ],
      "published_date": "2025-12-20",
      "pdf_url": "https://arxiv.org/pdf/2512.18477v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Evolutionary Optimization of Model Merging Recipes",
      "arxiv_id": "",
      "abstract": "Large language models (LLMs) have become increasingly capable, but their development often requires substantial computational resources. While model merging has emerged as a cost-effective promising approach for creating new models by combining existing ones, it currently relies on human intuition and domain knowledge, limiting its potential. Here, we propose an evolutionary approach that overcomes this limitation by automatically discovering effective combinations of diverse open-source models, harnessing their collective intelligence without requiring extensive additional training data or compute. Our approach operates in both parameter space and data flow space, allowing for optimization beyond just the weights of the individual models. This approach even facilitates cross-domain merging, generating models like a Japanese LLM with Math reasoning capabilities. Surprisingly, our Japanese Math LLM achieved state-of-the-art performance on a variety of established Japanese LLM benchmarks, even surpassing models with significantly more parameters, despite not being explicitly trained for such tasks. Furthermore, a culturally-aware Japanese VLM generated through our approach demonstrates its effectiveness in describing Japanese culture-specific content, outperforming previous Japanese VLMs. This work not only contributes new state-of-the-art models back to the open-source community, but also introduces a new paradigm for automated model composition, paving the way for exploring alternative, efficient approaches to foundation model development.",
      "authors": [
        "Takuya Akiba",
        "Makoto Shing",
        "Yujin Tang",
        "Qi Sun",
        "David Ha"
      ],
      "published_date": "2024-03-19",
      "pdf_url": "https://arxiv.org/pdf/2403.13187v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
      "arxiv_id": "",
      "abstract": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
      "authors": [
        "Kyunghyun Cho",
        "Bart van Merrienboer",
        "Caglar Gulcehre",
        "Dzmitry Bahdanau",
        "Fethi Bougares",
        "Holger Schwenk",
        "Yoshua Bengio"
      ],
      "published_date": "2014-06-03",
      "pdf_url": "https://arxiv.org/pdf/1406.1078v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving",
      "arxiv_id": "",
      "abstract": "Large language models (LLMs) have opened up new possibilities for intelligent agents, endowing them with human-like thinking and cognitive abilities. In this work, we delve into the potential of large language models (LLMs) in autonomous driving (AD). We introduce DriveMLM, an LLM-based AD framework that can perform close-loop autonomous driving in realistic simulators. To this end, (1) we bridge the gap between the language decisions and the vehicle control commands by standardizing the decision states according to the off-the-shelf motion planning module. (2) We employ a multimodal LLM (MLLM) to model the behavior planning module of a module AD system, which uses driving rules, user commands, and inputs from various sensors (e.g., camera, lidar) as input and makes driving decisions and provide explanations; This model can plug-and-play in existing AD systems such as Autopilot and Apollo for close-loop driving. (3) We design an effective data engine to collect a dataset that includes decision state and corresponding explanation annotation for model training and evaluation. We conduct extensive experiments and show that replacing the decision-making modules of the Autopilot and Apollo with DriveMLM resulted in significant improvements of 3.2 and 4.7 points on the CARLA Town05 Long respectively, demonstrating the effectiveness of our model. We hope this work can serve as a baseline for autonomous driving with LLMs.",
      "authors": [
        "Erfei Cui",
        "Wenhai Wang",
        "Zhiqi Li",
        "Jiangwei Xie",
        "Haoming Zou",
        "Hanming Deng",
        "Gen Luo",
        "Lewei Lu",
        "Xizhou Zhu",
        "Jifeng Dai"
      ],
      "published_date": "2023-12-14",
      "pdf_url": "https://arxiv.org/pdf/2312.09245v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models",
      "arxiv_id": "",
      "abstract": "We present VLMEvalKit: an open-source toolkit for evaluating large multi-modality models based on PyTorch. The toolkit aims to provide a user-friendly and comprehensive framework for researchers and developers to evaluate existing multi-modality models and publish reproducible evaluation results. In VLMEvalKit, we implement over 200+ different large multi-modality models, including both proprietary APIs and open-source models, as well as more than 80 different multi-modal benchmarks. By implementing a single interface, new models can be easily added to the toolkit, while the toolkit automatically handles the remaining workloads, including data preparation, distributed inference, prediction post-processing, and metric calculation. Although the toolkit is currently mainly used for evaluating large vision-language models, its design is compatible with future updates that incorporate additional modalities, such as audio and video. Based on the evaluation results obtained with the toolkit, we host OpenVLM Leaderboard, a comprehensive leaderboard to track the progress of multi-modality learning research. The toolkit is released on https://github.com/open-compass/VLMEvalKit and is actively maintained.",
      "authors": [
        "Haodong Duan",
        "Xinyu Fang",
        "Junming Yang",
        "Xiangyu Zhao",
        "Yuxuan Qiao",
        "Mo Li",
        "Amit Agarwal",
        "Zhe Chen",
        "Lin Chen",
        "Yuan Liu",
        "Yubo Ma",
        "Hailong Sun",
        "Yifan Zhang",
        "Shiyin Lu",
        "Tack Hwa Wong",
        "Weiyun Wang",
        "Peiheng Zhou",
        "Xiaozhe Li",
        "Chaoyou Fu",
        "Junbo Cui",
        "Jixuan Chen",
        "Enxin Song",
        "Song Mao",
        "Shengyuan Ding",
        "Tianhao Liang",
        "Zicheng Zhang",
        "Xiaoyi Dong",
        "Yuhang Zang",
        "Pan Zhang",
        "Jiaqi Wang",
        "Dahua Lin",
        "Kai Chen"
      ],
      "published_date": "2024-07-16",
      "pdf_url": "https://arxiv.org/pdf/2407.11691v4",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "arxiv_id": "",
      "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
      "authors": [
        "Jason Wei",
        "Xuezhi Wang",
        "Dale Schuurmans",
        "Maarten Bosma",
        "Brian Ichter",
        "Fei Xia",
        "Ed Chi",
        "Quoc Le",
        "Denny Zhou"
      ],
      "published_date": "2022-01-28",
      "pdf_url": "https://arxiv.org/pdf/2201.11903v6",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
      "arxiv_id": "",
      "abstract": "Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012---achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.",
      "authors": [
        "Ross Girshick",
        "Jeff Donahue",
        "Trevor Darrell",
        "Jitendra Malik"
      ],
      "published_date": "2013-11-11",
      "pdf_url": "https://arxiv.org/pdf/1311.2524v5",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism",
      "arxiv_id": "",
      "abstract": "Ground Penetrating Radar (GPR) has emerged as a pivotal tool for non-destructive evaluation of subsurface road defects. However, conventional GPR image interpretation remains heavily reliant on subjective expertise, introducing inefficiencies and inaccuracies. This study introduces a comprehensive framework to address these limitations: (1) A DCGAN-based data augmentation strategy synthesizes high-fidelity GPR images to mitigate data scarcity while preserving defect morphology under complex backgrounds; (2) A novel Multi-modal Chain and Global Attention Network (MCGA-Net) is proposed, integrating Multi-modal Chain Feature Fusion (MCFF) for hierarchical multi-scale defect representation and Global Attention Mechanism (GAM) for context-aware feature enhancement; (3) MS COCO transfer learning fine-tunes the backbone network, accelerating convergence and improving generalization. Ablation and comparison experiments validate the framework's efficacy. MCGA-Net achieves Precision (92.8%), Recall (92.5%), and mAP@50 (95.9%). In the detection of Gaussian noise, weak signals and small targets, MCGA-Net maintains robustness and outperforms other models. This work establishes a new paradigm for automated GPR-based defect detection, balancing computational efficiency with high accuracy in complex subsurface environments.",
      "authors": [
        "Haotian Lv",
        "Yuhui Zhang",
        "Jiangbo Dai",
        "Hanli Wu",
        "Jiaji Wang",
        "Dawei Wang"
      ],
      "published_date": "2025-12-25",
      "pdf_url": "https://arxiv.org/pdf/2512.21452v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Federated Learning Architectures: A Performance Evaluation with Crop Yield Prediction Application",
      "arxiv_id": "",
      "abstract": "Federated learning has become an emerging technology for data analysis for IoT applications. This paper implements centralized and decentralized federated learning frameworks for crop yield prediction based on Long Short-Term Memory Network. For centralized federated learning, multiple clients and one server is considered, where the clients exchange their model updates with the server that works as the aggregator to build the global model. For the decentralized framework, a collaborative network is formed among the devices either using ring topology or using mesh topology. In this network, each device receives model updates from the neighbour devices, and performs aggregation to build the upgraded model. The performance of the centralized and decentralized federated learning frameworks are evaluated in terms of prediction accuracy, precision, recall, F1-Score, and training time. The experimental results present that $\\geq$97% and $>$97.5% prediction accuracy are achieved using the centralized and decentralized federated learning-based frameworks respectively. The results also show that the using centralized federated learning the response time can be reduced by $\\sim$75% than the cloud-only framework. Finally, the future research directions of the use of federated learning in crop yield prediction are explored in this paper.",
      "authors": [
        "Anwesha Mukherjee",
        "Rajkumar Buyya"
      ],
      "published_date": "2024-08-06",
      "pdf_url": "https://arxiv.org/pdf/2408.02998v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Enhancing Descriptive Image Quality Assessment with A Large-scale Multi-modal Dataset",
      "arxiv_id": "",
      "abstract": "With the rapid advancement of Vision Language Models (VLMs), VLM-based Image Quality Assessment (IQA) seeks to describe image quality linguistically to align with human expression and capture the multifaceted nature of IQA tasks. However, current methods are still far from practical usage. First, prior works focus narrowly on specific sub-tasks or settings, which do not align with diverse real-world applications. Second, their performance is sub-optimal due to limitations in dataset coverage, scale, and quality. To overcome these challenges, we introduce the enhanced Depicted image Quality Assessment model (DepictQA-Wild). Our method includes a multi-functional IQA task paradigm that encompasses both assessment and comparison tasks, brief and detailed responses, full-reference and non-reference scenarios. We introduce a ground-truth-informed dataset construction approach to enhance data quality, and scale up the dataset to 495K under the brief-detail joint framework. Consequently, we construct a comprehensive, large-scale, and high-quality dataset, named DQ-495K. We also retain image resolution during training to better handle resolution-related quality issues, and estimate a confidence score that is helpful to filter out low-quality responses. Experimental results demonstrate that DepictQA-Wild significantly outperforms traditional score-based methods, prior VLM-based IQA models, and proprietary GPT-4V in distortion identification, instant rating, and reasoning tasks. Our advantages are further confirmed by real-world applications including assessing the web-downloaded images and ranking model-processed images. Codes, datasets, and model weights have been released in https://depictqa.github.io/.",
      "authors": [
        "Zhiyuan You",
        "Jinjin Gu",
        "Xin Cai",
        "Zheyuan Li",
        "Kaiwen Zhu",
        "Chao Dong",
        "Tianfan Xue"
      ],
      "published_date": "2024-05-29",
      "pdf_url": "https://arxiv.org/pdf/2405.18842v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Deep contextualized word representations",
      "arxiv_id": "",
      "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
      "authors": [
        "Matthew E. Peters",
        "Mark Neumann",
        "Mohit Iyyer",
        "Matt Gardner",
        "Christopher Clark",
        "Kenton Lee",
        "Luke Zettlemoyer"
      ],
      "published_date": "2018-02-15",
      "pdf_url": "https://arxiv.org/pdf/1802.05365v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
      "arxiv_id": "",
      "abstract": "General reasoning represents a long-standing and formidable challenge in artificial intelligence. Recent breakthroughs, exemplified by large language models (LLMs) and chain-of-thought prompting, have achieved considerable success on foundational reasoning tasks. However, this success is heavily contingent upon extensive human-annotated demonstrations, and models' capabilities are still insufficient for more complex problems. Here we show that the reasoning abilities of LLMs can be incentivized through pure reinforcement learning (RL), obviating the need for human-labeled reasoning trajectories. The proposed RL framework facilitates the emergent development of advanced reasoning patterns, such as self-reflection, verification, and dynamic strategy adaptation. Consequently, the trained model achieves superior performance on verifiable tasks such as mathematics, coding competitions, and STEM fields, surpassing its counterparts trained via conventional supervised learning on human demonstrations. Moreover, the emergent reasoning patterns exhibited by these large-scale models can be systematically harnessed to guide and enhance the reasoning capabilities of smaller models.",
      "authors": [
        "DeepSeek-AI",
        "Daya Guo",
        "Dejian Yang",
        "Haowei Zhang",
        "Junxiao Song",
        "Peiyi Wang",
        "Qihao Zhu",
        "Runxin Xu",
        "Ruoyu Zhang",
        "Shirong Ma",
        "Xiao Bi",
        "Xiaokang Zhang",
        "Xingkai Yu",
        "Yu Wu",
        "Z. F. Wu",
        "Zhibin Gou",
        "Zhihong Shao",
        "Zhuoshu Li",
        "Ziyi Gao",
        "Aixin Liu",
        "Bing Xue",
        "Bingxuan Wang",
        "Bochao Wu",
        "Bei Feng",
        "Chengda Lu",
        "Chenggang Zhao",
        "Chengqi Deng",
        "Chenyu Zhang",
        "Chong Ruan",
        "Damai Dai",
        "Deli Chen",
        "Dongjie Ji",
        "Erhang Li",
        "Fangyun Lin",
        "Fucong Dai",
        "Fuli Luo",
        "Guangbo Hao",
        "Guanting Chen",
        "Guowei Li",
        "H. Zhang",
        "Han Bao",
        "Hanwei Xu",
        "Haocheng Wang",
        "Honghui Ding",
        "Huajian Xin",
        "Huazuo Gao",
        "Hui Qu",
        "Hui Li",
        "Jianzhong Guo",
        "Jiashi Li",
        "Jiawei Wang",
        "Jingchang Chen",
        "Jingyang Yuan",
        "Junjie Qiu",
        "Junlong Li",
        "J. L. Cai",
        "Jiaqi Ni",
        "Jian Liang",
        "Jin Chen",
        "Kai Dong",
        "Kai Hu",
        "Kaige Gao",
        "Kang Guan",
        "Kexin Huang",
        "Kuai Yu",
        "Lean Wang",
        "Lecong Zhang",
        "Liang Zhao",
        "Litong Wang",
        "Liyue Zhang",
        "Lei Xu",
        "Leyi Xia",
        "Mingchuan Zhang",
        "Minghua Zhang",
        "Minghui Tang",
        "Meng Li",
        "Miaojun Wang",
        "Mingming Li",
        "Ning Tian",
        "Panpan Huang",
        "Peng Zhang",
        "Qiancheng Wang",
        "Qinyu Chen",
        "Qiushi Du",
        "Ruiqi Ge",
        "Ruisong Zhang",
        "Ruizhe Pan",
        "Runji Wang",
        "R. J. Chen",
        "R. L. Jin",
        "Ruyi Chen",
        "Shanghao Lu",
        "Shangyan Zhou",
        "Shanhuang Chen",
        "Shengfeng Ye",
        "Shiyu Wang",
        "Shuiping Yu",
        "Shunfeng Zhou",
        "Shuting Pan",
        "S. S. Li",
        "Shuang Zhou",
        "Shaoqing Wu",
        "Shengfeng Ye",
        "Tao Yun",
        "Tian Pei",
        "Tianyu Sun",
        "T. Wang",
        "Wangding Zeng",
        "Wanjia Zhao",
        "Wen Liu",
        "Wenfeng Liang",
        "Wenjun Gao",
        "Wenqin Yu",
        "Wentao Zhang",
        "W. L. Xiao",
        "Wei An",
        "Xiaodong Liu",
        "Xiaohan Wang",
        "Xiaokang Chen",
        "Xiaotao Nie",
        "Xin Cheng",
        "Xin Liu",
        "Xin Xie",
        "Xingchao Liu",
        "Xinyu Yang",
        "Xinyuan Li",
        "Xuecheng Su",
        "Xuheng Lin",
        "X. Q. Li",
        "Xiangyue Jin",
        "Xiaojin Shen",
        "Xiaosha Chen",
        "Xiaowen Sun",
        "Xiaoxiang Wang",
        "Xinnan Song",
        "Xinyi Zhou",
        "Xianzu Wang",
        "Xinxia Shan",
        "Y. K. Li",
        "Y. Q. Wang",
        "Y. X. Wei",
        "Yang Zhang",
        "Yanhong Xu",
        "Yao Li",
        "Yao Zhao",
        "Yaofeng Sun",
        "Yaohui Wang",
        "Yi Yu",
        "Yichao Zhang",
        "Yifan Shi",
        "Yiliang Xiong",
        "Ying He",
        "Yishi Piao",
        "Yisong Wang",
        "Yixuan Tan",
        "Yiyang Ma",
        "Yiyuan Liu",
        "Yongqiang Guo",
        "Yuan Ou",
        "Yuduan Wang",
        "Yue Gong",
        "Yuheng Zou",
        "Yujia He",
        "Yunfan Xiong",
        "Yuxiang Luo",
        "Yuxiang You",
        "Yuxuan Liu",
        "Yuyang Zhou",
        "Y. X. Zhu",
        "Yanhong Xu",
        "Yanping Huang",
        "Yaohui Li",
        "Yi Zheng",
        "Yuchen Zhu",
        "Yunxian Ma",
        "Ying Tang",
        "Yukun Zha",
        "Yuting Yan",
        "Z. Z. Ren",
        "Zehui Ren",
        "Zhangli Sha",
        "Zhe Fu",
        "Zhean Xu",
        "Zhenda Xie",
        "Zhengyan Zhang",
        "Zhewen Hao",
        "Zhicheng Ma",
        "Zhigang Yan",
        "Zhiyu Wu",
        "Zihui Gu",
        "Zijia Zhu",
        "Zijun Liu",
        "Zilin Li",
        "Ziwei Xie",
        "Ziyang Song",
        "Zizheng Pan",
        "Zhen Huang",
        "Zhipeng Xu",
        "Zhongyu Zhang",
        "Zhen Zhang"
      ],
      "published_date": "2025-01-22",
      "pdf_url": "https://arxiv.org/pdf/2501.12948v2",
      "citation_count": null,
      "year": null
    }
  ],
  "top_k": 5,
  "depth": 4,
  "knowledge_graph": {
    "entities": [
      {
        "name": "Attention Is All You Need",
        "type": "Thesis",
        "arxiv_id": "1706.03762"
      },
      {
        "name": "Deep Residual Learning for Image Recognition",
        "type": "Thesis",
        "arxiv_id": "1512.03385"
      },
      {
        "name": "Adam: A Method for Stochastic Optimization",
        "type": "Thesis",
        "arxiv_id": "1412.6980"
      },
      {
        "name": "Long Short-Term Memory",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Dropout: a simple way to prevent neural networks from overfitting",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Rethinking the Inception Architecture for Computer Vision",
        "type": "Thesis",
        "arxiv_id": "1512.00567"
      },
      {
        "name": "A comprehensive review of recommender systems: Transitioning from theory to practice",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
        "type": "Thesis",
        "arxiv_id": "2602.03242"
      },
      {
        "name": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Enhanced Transformer-BiLSTM Deep Learning Framework for Day-Ahead Energy Price Forecasting",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "ImageNet classification with deep convolutional neural networks",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
        "type": "Thesis",
        "arxiv_id": "1409.1556"
      },
      {
        "name": "Et al",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
        "type": "Thesis",
        "arxiv_id": "1506.01497"
      },
      {
        "name": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "DiffusionEngine: Diffusion model is scalable data engine for object detection",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Enhancing hyperspectral image prediction with contrastive learning in low-label regimes",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Auto-Encoding Variational Bayes",
        "type": "Thesis",
        "arxiv_id": "1312.6114"
      },
      {
        "name": "Supporting Online Material for Reducing the Dimensionality of Data with Neural Networks",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Speech recognition with deep recurrent neural networks",
        "type": "Thesis",
        "arxiv_id": "1303.5778"
      },
      {
        "name": "Weakly Supervised Image Dehazing via Physics-Based Decomposition",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
        "type": "Thesis",
        "arxiv_id": "2601.07372"
      },
      {
        "name": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Going deeper with convolutions",
        "type": "Thesis",
        "arxiv_id": "1409.4842"
      },
      {
        "name": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
        "type": "Thesis",
        "arxiv_id": "1502.03167"
      },
      {
        "name": "ImageNet Large Scale Visual Recognition Challenge",
        "type": "Thesis",
        "arxiv_id": "1409.0575"
      },
      {
        "name": "Diffusion Transformers with Representation Autoencoders",
        "type": "Thesis",
        "arxiv_id": "2510.11690"
      },
      {
        "name": "Scaling Large Language Models for Next-Generation Single-Cell Analysis",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "HybridVisionNet: An advanced hybrid deep learning framework for automated multi-class ocular disease diagnosis using fundus imaging",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Attention is All you Need",
        "type": "Thesis",
        "arxiv_id": "1706.03762"
      },
      {
        "name": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "type": "Thesis",
        "arxiv_id": "1910.10683"
      },
      {
        "name": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "nuScenes: A Multimodal Dataset for Autonomous Driving",
        "type": "Thesis",
        "arxiv_id": "1903.11027"
      },
      {
        "name": "CARLA: An Open Urban Driving Simulator",
        "type": "Thesis",
        "arxiv_id": "1711.03938"
      },
      {
        "name": "Sora as a World Model? A Complete Survey on Text-to-Video Generation",
        "type": "Thesis",
        "arxiv_id": "2403.05131"
      },
      {
        "name": "OmniNWM: Omniscient Driving Navigation World Models",
        "type": "Thesis",
        "arxiv_id": "2510.18313"
      },
      {
        "name": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask",
        "type": "Thesis",
        "arxiv_id": "2602.03213"
      },
      {
        "name": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving",
        "type": "Thesis",
        "arxiv_id": "2602.02002"
      },
      {
        "name": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
        "type": "Thesis",
        "arxiv_id": "2601.09452"
      },
      {
        "name": "I and J",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "ImageNet: A large-scale hierarchical image database",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "A and V",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "What matters for Representation Alignment: Global Information or Spatial Structure?",
        "type": "Thesis",
        "arxiv_id": "2512.10794"
      },
      {
        "name": "SCB-DETR: Multiscale Deformable Transformers for Occlusion-Resilient Student Learning Behavior Detection in Smart Classroom",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Bio-Inspired Fine-Tuning for Selective Transfer Learning in Image Classification",
        "type": "Thesis",
        "arxiv_id": "2601.11235"
      },
      {
        "name": "Hand Sign Language Detection Using Deep Learning",
        "type": "Thesis",
        "arxiv_id": "2601.08262"
      },
      {
        "name": "Representation Learning: A Review and New Perspectives",
        "type": "Thesis",
        "arxiv_id": "1206.5538"
      },
      {
        "name": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "In Advances in Neural Information Processing Systems",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation",
        "type": "Thesis",
        "arxiv_id": "2402.13929"
      },
      {
        "name": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions",
        "type": "Thesis",
        "arxiv_id": "2407.08136"
      },
      {
        "name": "GenAD: Generative End-to-End Autonomous Driving",
        "type": "Thesis",
        "arxiv_id": "2402.11502"
      },
      {
        "name": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model",
        "type": "Thesis",
        "arxiv_id": "2405.20797"
      },
      {
        "name": "Improving Video Generation with Human Feedback",
        "type": "Thesis",
        "arxiv_id": "2501.13918"
      },
      {
        "name": "Speech Recognition with Deep Recurrent Neural Networks",
        "type": "Thesis",
        "arxiv_id": "1303.5778"
      },
      {
        "name": "Learning representations by back-propagating errors",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Bidirectional recurrent neural networks",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Connectionist Temporal Classiﬁcation: Labelling Unsegmented Sequence Data with Recurrent Neural Networks",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "2005 Special Issue: Framewise phoneme classification with bidirectional LSTM and other neural network architectures",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "A high-performance neuroprosthesis for speech decoding and avatar control",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "A high-performance speech neuroprosthesis",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Loss of plasticity in deep continual learning",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "An analog-AI chip for energy-efficient speech recognition and transcription",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "A Mathematical Theory of Communication",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers",
        "type": "Thesis",
        "arxiv_id": "2602.04706"
      },
      {
        "name": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling",
        "type": "Thesis",
        "arxiv_id": "2602.03359"
      },
      {
        "name": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram",
        "type": "Thesis",
        "arxiv_id": "2601.22203"
      },
      {
        "name": "L$^3$: Large Lookup Layers",
        "type": "Thesis",
        "arxiv_id": "2601.21461"
      },
      {
        "name": "Scaling Embeddings Outperforms Scaling Experts in Language Models",
        "type": "Thesis",
        "arxiv_id": "2601.21204"
      },
      {
        "name": "Going Deeper with Convolutions",
        "type": "Thesis",
        "arxiv_id": "1409.4842"
      },
      {
        "name": "Gradient-based learning applied to document recognition",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Regression Shrinkage and Selection via the Lasso",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Microsoft COCO: Common Objects in Context",
        "type": "Thesis",
        "arxiv_id": "1405.0312"
      },
      {
        "name": "LifeCLEF Plant Identification Task 2015",
        "type": "Thesis",
        "arxiv_id": "2509.23891"
      },
      {
        "name": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics",
        "type": "Thesis",
        "arxiv_id": "2510.20852"
      },
      {
        "name": "Enhancing pine wilt disease detection with synthetic data and external attention-based transformers",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "A comprehensive review on YOLO versions for object detection",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Harnessing large vision and language models in agriculture: a review",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Multi-axis vision transformer for medical image segmentation",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "A comprehensive review of facial beauty prediction using deep learning techniques",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Active learning framework leveraging transcriptomics identifies modulators of disease phenotypes.",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Distinctive Image Features from Scale-Invariant Keypoints",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training",
        "type": "Thesis",
        "arxiv_id": "2509.23661"
      },
      {
        "name": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications",
        "type": "Thesis",
        "arxiv_id": "2510.07077"
      },
      {
        "name": "Aligning machine and human visual representations across abstraction levels",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
        "type": "Thesis",
        "arxiv_id": "2010.11929"
      },
      {
        "name": "Learning Transferable Visual Models From Natural Language Supervision",
        "type": "Thesis",
        "arxiv_id": "2103.00020"
      },
      {
        "name": "GENERATIVE ADVERSARIAL NETS",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Improved Mean Flows: On the Challenges of Fastforward Generative Models",
        "type": "Thesis",
        "arxiv_id": "2512.02012"
      },
      {
        "name": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training",
        "type": "Thesis",
        "arxiv_id": "2510.12586"
      },
      {
        "name": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
        "type": "Thesis",
        "arxiv_id": "2512.11749"
      },
      {
        "name": "PixelDiT: Pixel Diffusion Transformers for Image Generation",
        "type": "Thesis",
        "arxiv_id": "2511.20645"
      },
      {
        "name": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models",
        "type": "Thesis",
        "arxiv_id": "2512.02014"
      },
      {
        "name": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "type": "Thesis",
        "arxiv_id": "1810.04805"
      },
      {
        "name": "A comprehensive review of object detection with traditional and deep learning methods",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Diffusion Language Models are Super Data Learners",
        "type": "Thesis",
        "arxiv_id": "2511.03276"
      },
      {
        "name": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas",
        "type": "Thesis",
        "arxiv_id": "2602.01326"
      },
      {
        "name": "FLEX: Continuous Agent Evolution via Forward Learning from Experience",
        "type": "Thesis",
        "arxiv_id": "2511.06449"
      },
      {
        "name": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning",
        "type": "Thesis",
        "arxiv_id": "2511.14460"
      },
      {
        "name": "nuScenes: A multimodal dataset for autonomous driving",
        "type": "Thesis",
        "arxiv_id": "1903.11027"
      },
      {
        "name": "Squeeze-and-Excitation Networks",
        "type": "Thesis",
        "arxiv_id": "1709.01507"
      },
      {
        "name": "Decoupled Weight Decay Regularization",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
        "type": "Thesis",
        "arxiv_id": "1905.11946"
      },
      {
        "name": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control",
        "type": "Thesis",
        "arxiv_id": "2512.05115"
      },
      {
        "name": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World",
        "type": "Thesis",
        "arxiv_id": "2512.23421"
      },
      {
        "name": "DVGT: Driving Visual Geometry Transformer",
        "type": "Thesis",
        "arxiv_id": "2512.16919"
      },
      {
        "name": "Adding Conditional Control to Text-to-Image Diffusion Models",
        "type": "Thesis",
        "arxiv_id": "2302.05543"
      },
      {
        "name": "Scaling Instruction-Finetuned Language Models",
        "type": "Thesis",
        "arxiv_id": "2210.11416"
      },
      {
        "name": "High-Resolution Image Synthesis with Latent Diffusion Models",
        "type": "Thesis",
        "arxiv_id": "2112.10752"
      },
      {
        "name": "AUTO-ENCODING VARIATIONAL BAYES",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
        "type": "Thesis",
        "arxiv_id": "1801.03924"
      },
      {
        "name": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset",
        "type": "Thesis",
        "arxiv_id": "1912.04838"
      },
      {
        "name": "Histograms of oriented gradients for human detection",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Generative Adversarial Networks",
        "type": "Thesis",
        "arxiv_id": "2203.00667"
      },
      {
        "name": "Generalization of Diffusion Models Arises with a Balanced Representation Space",
        "type": "Thesis",
        "arxiv_id": "2512.20963"
      },
      {
        "name": "Stable Velocity: A Variance Perspective on Flow Matching",
        "type": "Thesis",
        "arxiv_id": "2602.05435"
      },
      {
        "name": "Laminating Representation Autoencoders for Efficient Diffusion",
        "type": "Thesis",
        "arxiv_id": "2602.04873"
      },
      {
        "name": "Adaptive 1D Video Diffusion Autoencoder",
        "type": "Thesis",
        "arxiv_id": "2602.04220"
      },
      {
        "name": "Test-Time Conditioning with Representation-Aligned Visual Features",
        "type": "Thesis",
        "arxiv_id": "2602.03753"
      },
      {
        "name": "Bio-inspired fine-tuning for selective transfer learning in image classification",
        "type": "Thesis",
        "arxiv_id": "2601.11235"
      },
      {
        "name": "VGG Induced Deep Hand Sign Language Detection",
        "type": "Thesis",
        "arxiv_id": "2601.08262"
      },
      {
        "name": "MediaPipe: A Framework for Building Perception Pipelines",
        "type": "Thesis",
        "arxiv_id": "1906.08172"
      },
      {
        "name": "Hand Pose Estimation and Hand Shape Classification Using Multi-layered Randomized Decision Forests",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "A robust static hand gesture recognition system using geometry based normalizations and Krawtchouk moments",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Hand signal classification system for sign language communication in Virtual Reality",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Real-Time Static Hand Sign Recognition System using MediaPipe and Fully Connected Neural Network",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Real-Time Gesture Recognition to Aid Communication in Children with Motor Impairments",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Visualizing Data using t-SNE",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Learning Multiple Layers of Features from Tiny Images",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "LLM Social Simulations Are a Promising Research Method",
        "type": "Thesis",
        "arxiv_id": "2504.02234"
      },
      {
        "name": "Federated Contrastive Learning With Feature-Based Distillation for Human Activity Recognition",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Diffuse and Disperse: Image Generation with Representation Regularization",
        "type": "Thesis",
        "arxiv_id": "2506.09027"
      },
      {
        "name": "Deep Autoencoder Neural Networks: A Comprehensive Review and New Perspectives",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems",
        "type": "Thesis",
        "arxiv_id": "2502.07350"
      },
      {
        "name": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
        "type": "Thesis",
        "arxiv_id": "1505.04597"
      },
      {
        "name": "Improved Distribution Matching Distillation for Fast Image Synthesis",
        "type": "Thesis",
        "arxiv_id": "2405.14867"
      },
      {
        "name": "Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation",
        "type": "Thesis",
        "arxiv_id": "2403.12015"
      },
      {
        "name": "Evolutionary optimization of model merging recipes",
        "type": "Thesis",
        "arxiv_id": "2403.13187"
      },
      {
        "name": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
        "type": "Thesis",
        "arxiv_id": "2412.07772"
      },
      {
        "name": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment",
        "type": "Thesis",
        "arxiv_id": "2404.16022"
      },
      {
        "name": "Denoising Diffusion Probabilistic Models",
        "type": "Thesis",
        "arxiv_id": "2006.11239"
      },
      {
        "name": "HunyuanVideo: A Systematic Framework For Large Video Generative Models",
        "type": "Thesis",
        "arxiv_id": "2412.03603"
      },
      {
        "name": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency",
        "type": "Thesis",
        "arxiv_id": "2409.02634"
      },
      {
        "name": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models",
        "type": "Thesis",
        "arxiv_id": "2502.01061"
      },
      {
        "name": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation",
        "type": "Thesis",
        "arxiv_id": "2410.07718"
      },
      {
        "name": "EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation",
        "type": "Thesis",
        "arxiv_id": "2411.10061"
      },
      {
        "name": "Feature Pyramid Networks for Object Detection",
        "type": "Thesis",
        "arxiv_id": "1612.03144"
      },
      {
        "name": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation",
        "type": "Thesis",
        "arxiv_id": "1406.1078"
      },
      {
        "name": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving",
        "type": "Thesis",
        "arxiv_id": "2312.09245"
      },
      {
        "name": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability",
        "type": "Thesis",
        "arxiv_id": "2405.17398"
      },
      {
        "name": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving",
        "type": "Thesis",
        "arxiv_id": "2411.15139"
      },
      {
        "name": "How Far is Video Generation from World Model: A Physical Law Perspective",
        "type": "Thesis",
        "arxiv_id": "2411.02385"
      },
      {
        "name": "EMMA: End-to-End Multimodal Model for Autonomous Driving",
        "type": "Thesis",
        "arxiv_id": "2410.23262"
      },
      {
        "name": "Language Models are Few-Shot Learners",
        "type": "Thesis",
        "arxiv_id": "2005.14165"
      },
      {
        "name": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling",
        "type": "Thesis",
        "arxiv_id": "2412.05271"
      },
      {
        "name": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models",
        "type": "Thesis",
        "arxiv_id": "2504.10479"
      },
      {
        "name": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models",
        "type": "Thesis",
        "arxiv_id": "2407.11691"
      },
      {
        "name": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step",
        "type": "Thesis",
        "arxiv_id": "2411.10440"
      },
      {
        "name": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency",
        "type": "Thesis",
        "arxiv_id": "2508.18265"
      },
      {
        "name": "Proximal Policy Optimization Algorithms",
        "type": "Thesis",
        "arxiv_id": "1707.06347"
      },
      {
        "name": "Flow-GRPO: Training Flow Matching Models via Online RL",
        "type": "Thesis",
        "arxiv_id": "2505.05470"
      },
      {
        "name": "DanceGRPO: Unleashing GRPO on Visual Generation",
        "type": "Thesis",
        "arxiv_id": "2505.07818"
      },
      {
        "name": "Seedance 1.0: Exploring the Boundaries of Video Generation Models",
        "type": "Thesis",
        "arxiv_id": "2506.09113"
      },
      {
        "name": "SkyReels-V2: Infinite-length Film Generative Model",
        "type": "Thesis",
        "arxiv_id": "2504.13074"
      },
      {
        "name": "Unified Reward Model for Multimodal Understanding and Generation",
        "type": "Thesis",
        "arxiv_id": "2503.05236"
      },
      {
        "name": "5分で分かる!? 有名論文ナナメ読み：Jacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
        "type": "Thesis",
        "arxiv_id": "1606.05250"
      },
      {
        "name": "Neural Machine Translation of Rare Words with Subword Units",
        "type": "Thesis",
        "arxiv_id": "1508.07909"
      },
      {
        "name": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing",
        "type": "Thesis",
        "arxiv_id": "1808.06226"
      },
      {
        "name": "GPT-4 Technical Report",
        "type": "Thesis",
        "arxiv_id": "2303.08774"
      },
      {
        "name": "LLaMA: Open and Efficient Foundation Language Models",
        "type": "Thesis",
        "arxiv_id": "2302.13971"
      },
      {
        "name": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
        "type": "Thesis",
        "arxiv_id": "2201.11903"
      },
      {
        "name": "Training Verifiers to Solve Math Word Problems",
        "type": "Thesis",
        "arxiv_id": "2110.14168"
      },
      {
        "name": "Crystal structure of the nucleosome core particle at 2.8 Å resolution",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
        "type": "Thesis",
        "arxiv_id": "2312.00752"
      },
      {
        "name": "A catalogue of splice junction sequences.",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Origin of the Genetic Code",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "SGDR: Stochastic Gradient Descent with Warm Restarts",
        "type": "Thesis",
        "arxiv_id": "1608.03983"
      },
      {
        "name": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
        "type": "Thesis",
        "arxiv_id": "1701.06538"
      },
      {
        "name": "Compression of individual sequences via variable-rate coding",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Pointer Sentinel Mixture Models",
        "type": "Thesis",
        "arxiv_id": "1609.07843"
      },
      {
        "name": "Measuring Massive Multitask Language Understanding",
        "type": "Thesis",
        "arxiv_id": "2009.03300"
      },
      {
        "name": "Let's Verify Step by Step",
        "type": "Thesis",
        "arxiv_id": "2305.20050"
      },
      {
        "name": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark",
        "type": "Thesis",
        "arxiv_id": "2311.12022"
      },
      {
        "name": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
        "type": "Thesis",
        "arxiv_id": "2006.16668"
      },
      {
        "name": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation",
        "type": "Thesis",
        "arxiv_id": "1311.2524"
      },
      {
        "name": "MuMu-LLaMA: Multi-modal music understanding and generation via large language models",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy",
        "type": "Thesis",
        "arxiv_id": "2601.03305"
      },
      {
        "name": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism",
        "type": "Thesis",
        "arxiv_id": "2512.21452"
      },
      {
        "name": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
        "type": "Thesis",
        "arxiv_id": "2601.04720"
      },
      {
        "name": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Pattern Recognition and Machine Learning",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Bagging Predictors",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Automatic Image-Based Plant Disease Severity Estimation Using Deep Learning",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Plant identification using deep neural networks via optimization of transfer learning parameters",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "New perspectives on plant disease characterization based on deep learning",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Deep Learning for Plant Identification in Natural Environment",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Going deeper in the automated identification of Herbarium specimens",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Densely Connected Convolutional Networks",
        "type": "Thesis",
        "arxiv_id": "1608.06993"
      },
      {
        "name": "MobileNetV2: Inverted Residuals and Linear Bottlenecks",
        "type": "Thesis",
        "arxiv_id": "1801.04381"
      },
      {
        "name": "Communication-Efficient Learning of Deep Networks from Decentralized Data",
        "type": "Thesis",
        "arxiv_id": "1602.05629"
      },
      {
        "name": "A Comprehensive Survey on Transfer Learning",
        "type": "Thesis",
        "arxiv_id": "1911.02685"
      },
      {
        "name": "Privacy Preserved and Decentralized Smartphone Recommendation System",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "A Survey on Heterogeneity Taxonomy, Security and Privacy Preservation in the Integration of IoT, Wireless Sensor Networks and Federated Learning",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "A privacy-preserving federated learning with a secure collaborative for malware detection models using Internet of Things resources",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Graph-based deep learning techniques for remote sensing applications: Techniques, taxonomy, and applications - A comprehensive review",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Federated Learning Architectures: A Performance Evaluation With Crop Yield Prediction Application",
        "type": "Thesis",
        "arxiv_id": "2408.02998"
      },
      {
        "name": "Segment Anything",
        "type": "Thesis",
        "arxiv_id": "2304.02643"
      },
      {
        "name": "Visual Instruction Tuning",
        "type": "Thesis",
        "arxiv_id": "2304.08485"
      },
      {
        "name": "Improved Baselines with Visual Instruction Tuning",
        "type": "Thesis",
        "arxiv_id": "2310.03744"
      },
      {
        "name": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset",
        "type": "Thesis",
        "arxiv_id": "2405.18842"
      },
      {
        "name": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs",
        "type": "Thesis",
        "arxiv_id": "2505.15436"
      },
      {
        "name": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views",
        "type": "Thesis",
        "arxiv_id": "2510.18632"
      },
      {
        "name": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning",
        "type": "Thesis",
        "arxiv_id": "2510.13515"
      },
      {
        "name": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe",
        "type": "Thesis",
        "arxiv_id": "2511.16334"
      },
      {
        "name": "Vision-Language-Action (VLA) Models: Concepts, Progress, Applications and Challenges",
        "type": "Thesis",
        "arxiv_id": "2505.04769"
      },
      {
        "name": "Leave No Observation Behind: Real-time Correction for VLA Action Chunks",
        "type": "Thesis",
        "arxiv_id": "2509.23224"
      },
      {
        "name": "Latent Chain-of-Thought World Modeling for End-to-End Driving",
        "type": "Thesis",
        "arxiv_id": "2512.10226"
      },
      {
        "name": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks",
        "type": "Thesis",
        "arxiv_id": "2510.04898"
      },
      {
        "name": "Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models",
        "type": "Thesis",
        "arxiv_id": "2512.09927"
      },
      {
        "name": "TransSIL: A Silhouette Cue-Aware Image Classification Framework for Bird Ecological Monitoring Systems",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "CBRFormer: rendering technology-based transformer for refinement segmentation of bridge crack images",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "A Fast and Compact Threshold Switch-Based Cellular Nonlinear Network Cell",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Deep contrastive learning enables genome-wide virtual screening.",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "LLaVA-based semantic feature modulation diffusion model for underwater image enhancement",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "3DGS-Drag: Dragging Gaussians for Intuitive Point-Based 3D Editing",
        "type": "Thesis",
        "arxiv_id": "2601.07963"
      },
      {
        "name": "LILaC: Late Interacting in Layered Component Graph for Open-domain Multimodal Multihop Retrieval",
        "type": "Thesis",
        "arxiv_id": "2602.04263"
      },
      {
        "name": "Bidirectional Normalizing Flow: From Data to Noise and Back",
        "type": "Thesis",
        "arxiv_id": "2512.10953"
      },
      {
        "name": "One-step Latent-free Image Generation with Pixel Mean Flows",
        "type": "Thesis",
        "arxiv_id": "2601.22158"
      },
      {
        "name": "Meta Flow Maps enable scalable reward alignment",
        "type": "Thesis",
        "arxiv_id": "2601.14430"
      },
      {
        "name": "Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective",
        "type": "Thesis",
        "arxiv_id": "2602.05319"
      },
      {
        "name": "Generative Modeling via Drifting",
        "type": "Thesis",
        "arxiv_id": "2602.04770"
      },
      {
        "name": "A Simple Framework for Contrastive Learning of Visual Representations",
        "type": "Thesis",
        "arxiv_id": "2002.05709"
      },
      {
        "name": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning",
        "type": "Thesis",
        "arxiv_id": "2512.24146"
      },
      {
        "name": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints",
        "type": "Thesis",
        "arxiv_id": "2510.14847"
      },
      {
        "name": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders",
        "type": "Thesis",
        "arxiv_id": "2601.16208"
      },
      {
        "name": "Simulating the Visual World with Artificial Intelligence: A Roadmap",
        "type": "Thesis",
        "arxiv_id": "2511.08585"
      },
      {
        "name": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks",
        "type": "Thesis",
        "arxiv_id": "2602.01630"
      },
      {
        "name": "RecTok: Reconstruction Distillation along Rectified Flow",
        "type": "Thesis",
        "arxiv_id": "2512.13421"
      },
      {
        "name": "RePack then Refine: Efficient Diffusion Transformer with Vision Foundation Model",
        "type": "Thesis",
        "arxiv_id": "2512.12083"
      },
      {
        "name": "Diffusion Models Beat GANs on Image Synthesis",
        "type": "Thesis",
        "arxiv_id": "2105.05233"
      },
      {
        "name": "DINOv2: Learning Robust Visual Features without Supervision",
        "type": "Thesis",
        "arxiv_id": "2304.07193"
      },
      {
        "name": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss",
        "type": "Thesis",
        "arxiv_id": "2602.02493"
      },
      {
        "name": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
        "type": "Thesis",
        "arxiv_id": "2505.02567"
      },
      {
        "name": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery",
        "type": "Thesis",
        "arxiv_id": "2602.02465"
      },
      {
        "name": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation",
        "type": "Thesis",
        "arxiv_id": "2601.15369"
      },
      {
        "name": "Distributed Representations of Words and Phrases and their Compositionality",
        "type": "Thesis",
        "arxiv_id": "1310.4546"
      },
      {
        "name": "GloVe: Global Vectors for Word Representation",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Deep Contextualized Word Representations",
        "type": "Thesis",
        "arxiv_id": "1802.05365"
      },
      {
        "name": "Protein Language Models: Is Scaling Necessary?",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis",
        "type": "Thesis",
        "arxiv_id": "2601.17203"
      },
      {
        "name": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Generative Classifiers Avoid Shortcut Solutions",
        "type": "Thesis",
        "arxiv_id": "2512.25034"
      },
      {
        "name": "Edge Large AI Model Agent-Empowered Cognitive Multimodal Semantic Communication",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Language Models are Unsupervised Multitask Learners",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "The Llama 3 Herd of Models",
        "type": "Thesis",
        "arxiv_id": "2407.21783"
      },
      {
        "name": "A Survey on Diffusion Language Models",
        "type": "Thesis",
        "arxiv_id": "2508.10875"
      },
      {
        "name": "Training Optimal Large Diffusion Language Models",
        "type": "Thesis",
        "arxiv_id": "2510.03280"
      },
      {
        "name": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models",
        "type": "Thesis",
        "arxiv_id": "2509.10396"
      },
      {
        "name": "Encoder-Decoder Diffusion Language Models for Efficient Training and Inference",
        "type": "Thesis",
        "arxiv_id": "2510.22852"
      },
      {
        "name": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model",
        "type": "Thesis",
        "arxiv_id": "2601.15892"
      },
      {
        "name": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications",
        "type": "Thesis",
        "arxiv_id": "2602.05665"
      },
      {
        "name": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search",
        "type": "Thesis",
        "arxiv_id": "2602.04248"
      },
      {
        "name": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking",
        "type": "Thesis",
        "arxiv_id": "2602.03224"
      },
      {
        "name": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents",
        "type": "Thesis",
        "arxiv_id": "2602.01869"
      },
      {
        "name": "Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting",
        "type": "Thesis",
        "arxiv_id": "2602.01776"
      },
      {
        "name": "Training language models to follow instructions with human feedback",
        "type": "Thesis",
        "arxiv_id": "2203.02155"
      },
      {
        "name": "PaLM: Scaling Language Modeling with Pathways",
        "type": "Thesis",
        "arxiv_id": "2204.02311"
      },
      {
        "name": "ReAct: Synergizing Reasoning and Acting in Language Models",
        "type": "Thesis",
        "arxiv_id": "2210.03629"
      },
      {
        "name": "From System 1 to System 2: A Survey of Reasoning Large Language Models",
        "type": "Thesis",
        "arxiv_id": "2502.17419"
      },
      {
        "name": "Deep Research Agents: A Systematic Examination And Roadmap",
        "type": "Thesis",
        "arxiv_id": "2506.18096"
      },
      {
        "name": "Table-R1: Inference-Time Scaling for Table Reasoning",
        "type": "Thesis",
        "arxiv_id": "2505.23621"
      },
      {
        "name": "Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis",
        "type": "Thesis",
        "arxiv_id": "2601.04879"
      },
      {
        "name": "PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware Sequence-Level Policy Optimization",
        "type": "Thesis",
        "arxiv_id": "2601.10029"
      },
      {
        "name": "Gradient-Guided Learning Network for Infrared Small Target Detection",
        "type": "Thesis",
        "arxiv_id": "2512.09497"
      },
      {
        "name": "A Retrieval-Augmented Generation Approach to Extracting Algorithmic Logic from Neural Networks",
        "type": "Thesis",
        "arxiv_id": "2512.04329"
      },
      {
        "name": "CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation",
        "type": "Thesis",
        "arxiv_id": "2511.01243"
      },
      {
        "name": "IMobileTransformer: A fusion-based lightweight model for rice disease identification",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers",
        "type": "Thesis",
        "arxiv_id": "2511.09554"
      },
      {
        "name": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Deep learning models for efficient geotechnical predictions: reducing training effort and data requirements with transfer learning",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Image quality assessment: from error visibility to structural similarity",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction",
        "type": "Thesis",
        "arxiv_id": "2601.18993"
      },
      {
        "name": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos",
        "type": "Thesis",
        "arxiv_id": "2601.00393"
      },
      {
        "name": "Scalable Diffusion Models with Transformers",
        "type": "Thesis",
        "arxiv_id": "2212.09748"
      },
      {
        "name": "RoFormer: Enhanced Transformer with Rotary Position Embedding",
        "type": "Thesis",
        "arxiv_id": "2104.09864"
      },
      {
        "name": "The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey",
        "type": "Thesis",
        "arxiv_id": "2502.10498"
      },
      {
        "name": "Vision meets robotics: The KITTI dataset",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "Least-Squares Estimation of Transformation Parameters Between Two Point Patterns",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations",
        "type": "Thesis",
        "arxiv_id": "2601.22376"
      },
      {
        "name": "Deep Learning",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction",
        "type": "Thesis",
        "arxiv_id": "2509.13414"
      },
      {
        "name": "Seedream 4.0: Toward Next-generation Multimodal Image Generation",
        "type": "Thesis",
        "arxiv_id": "2509.20427"
      },
      {
        "name": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation",
        "type": "Thesis",
        "arxiv_id": "2508.09987"
      },
      {
        "name": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding",
        "type": "Thesis",
        "arxiv_id": "2510.06308"
      },
      {
        "name": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale",
        "type": "Thesis",
        "arxiv_id": "2508.10711"
      },
      {
        "name": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
        "type": "Thesis",
        "arxiv_id": "1907.11692"
      },
      {
        "name": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning",
        "type": "Thesis",
        "arxiv_id": "2501.12948"
      },
      {
        "name": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning",
        "type": "Thesis",
        "arxiv_id": "2503.09516"
      },
      {
        "name": "Toward expert-level medical question answering with large language models",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training",
        "type": "Thesis",
        "arxiv_id": "2501.17161"
      },
      {
        "name": "Equivariant Diffusion for Crystal Structure Prediction",
        "type": "Thesis",
        "arxiv_id": "2512.07289"
      },
      {
        "name": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling",
        "type": "Thesis",
        "arxiv_id": "2512.14614"
      },
      {
        "name": "Native and Compact Structured Latents for 3D Generation",
        "type": "Thesis",
        "arxiv_id": "2512.14692"
      },
      {
        "name": "and as an in",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?",
        "type": "Thesis",
        "arxiv_id": "2512.24497"
      },
      {
        "name": "Progressive Learned Image Transmission for Semantic Communication Using Hierarchical VAE",
        "type": "Thesis",
        "arxiv_id": ""
      },
      {
        "name": "STORM: Search-Guided Generative World Models for Robotic Manipulation",
        "type": "Thesis",
        "arxiv_id": "2512.18477"
      },
      {
        "name": "Evolutionary Optimization of Model Merging Recipes",
        "type": "Thesis",
        "arxiv_id": "2403.13187"
      },
      {
        "name": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
        "type": "Thesis",
        "arxiv_id": "1406.1078"
      },
      {
        "name": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving",
        "type": "Thesis",
        "arxiv_id": "2312.09245"
      },
      {
        "name": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models",
        "type": "Thesis",
        "arxiv_id": "2407.11691"
      },
      {
        "name": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
        "type": "Thesis",
        "arxiv_id": "2201.11903"
      },
      {
        "name": "Rich feature hierarchies for accurate object detection and semantic segmentation",
        "type": "Thesis",
        "arxiv_id": "1311.2524"
      },
      {
        "name": "Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism",
        "type": "Thesis",
        "arxiv_id": "2512.21452"
      },
      {
        "name": "Federated Learning Architectures: A Performance Evaluation with Crop Yield Prediction Application",
        "type": "Thesis",
        "arxiv_id": "2408.02998"
      },
      {
        "name": "Enhancing Descriptive Image Quality Assessment with A Large-scale Multi-modal Dataset",
        "type": "Thesis",
        "arxiv_id": "2405.18842"
      },
      {
        "name": "Deep contextualized word representations",
        "type": "Thesis",
        "arxiv_id": "1802.05365"
      },
      {
        "name": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
        "type": "Thesis",
        "arxiv_id": "2501.12948"
      },
      {
        "name": "Ashish Vaswani",
        "type": "Person"
      },
      {
        "name": "Noam Shazeer",
        "type": "Person"
      },
      {
        "name": "Niki Parmar",
        "type": "Person"
      },
      {
        "name": "Jakob Uszkoreit",
        "type": "Person"
      },
      {
        "name": "Llion Jones",
        "type": "Person"
      },
      {
        "name": "Aidan N. Gomez",
        "type": "Person"
      },
      {
        "name": "Lukasz Kaiser",
        "type": "Person"
      },
      {
        "name": "Illia Polosukhin",
        "type": "Person"
      },
      {
        "name": "Kaiming He",
        "type": "Person"
      },
      {
        "name": "Xiangyu Zhang",
        "type": "Person"
      },
      {
        "name": "Shaoqing Ren",
        "type": "Person"
      },
      {
        "name": "Jian Sun",
        "type": "Person"
      },
      {
        "name": "Diederik P. Kingma",
        "type": "Person"
      },
      {
        "name": "Jimmy Ba",
        "type": "Person"
      },
      {
        "name": "Sepp Hochreiter",
        "type": "Person"
      },
      {
        "name": "J. Schmidhuber",
        "type": "Person"
      },
      {
        "name": "Nitish Srivastava",
        "type": "Person"
      },
      {
        "name": "Geoffrey E. Hinton",
        "type": "Person"
      },
      {
        "name": "A. Krizhevsky",
        "type": "Person"
      },
      {
        "name": "I. Sutskever",
        "type": "Person"
      },
      {
        "name": "R. Salakhutdinov",
        "type": "Person"
      },
      {
        "name": "Christian Szegedy",
        "type": "Person"
      },
      {
        "name": "Vincent Vanhoucke",
        "type": "Person"
      },
      {
        "name": "Sergey Ioffe",
        "type": "Person"
      },
      {
        "name": "Jonathon Shlens",
        "type": "Person"
      },
      {
        "name": "Zbigniew Wojna",
        "type": "Person"
      },
      {
        "name": "Shaina Raza",
        "type": "Person"
      },
      {
        "name": "Mizanur Rahman",
        "type": "Person"
      },
      {
        "name": "Safiullah Kamawal",
        "type": "Person"
      },
      {
        "name": "Armin Toroghi",
        "type": "Person"
      },
      {
        "name": "Ananya Raval",
        "type": "Person"
      },
      {
        "name": "F. Navah",
        "type": "Person"
      },
      {
        "name": "Amirmohammad Kazemeini",
        "type": "Person"
      },
      {
        "name": "Zhuoran Yang",
        "type": "Person"
      },
      {
        "name": "Xi Guo",
        "type": "Person"
      },
      {
        "name": "Chenjing Ding",
        "type": "Person"
      },
      {
        "name": "Chiyu Wang",
        "type": "Person"
      },
      {
        "name": "Wei Wu",
        "type": "Person"
      },
      {
        "name": "Yanyong Zhang",
        "type": "Person"
      },
      {
        "name": "Zisheng Wang",
        "type": "Person"
      },
      {
        "name": "Junjie Chen",
        "type": "Person"
      },
      {
        "name": "Chisen Wang",
        "type": "Person"
      },
      {
        "name": "Cong Peng",
        "type": "Person"
      },
      {
        "name": "Jianping Xuan",
        "type": "Person"
      },
      {
        "name": "Tielin Shi",
        "type": "Person"
      },
      {
        "name": "Ming J. Zuo",
        "type": "Person"
      },
      {
        "name": "Abdullah Al Ahad Khan",
        "type": "Person"
      },
      {
        "name": "Md Habib Ullah",
        "type": "Person"
      },
      {
        "name": "Ruchira Tabassum",
        "type": "Person"
      },
      {
        "name": "Md Faisal Kabir",
        "type": "Person"
      },
      {
        "name": "Jinghuan Zhang",
        "type": "Person"
      },
      {
        "name": "Wang Chen",
        "type": "Person"
      },
      {
        "name": "Jian Zhang",
        "type": "Person"
      },
      {
        "name": "Karen Simonyan",
        "type": "Person"
      },
      {
        "name": "Andrew Zisserman",
        "type": "Person"
      },
      {
        "name": "P. Cochat",
        "type": "Person"
      },
      {
        "name": "L. Vaucoret",
        "type": "Person"
      },
      {
        "name": "J. Sarles",
        "type": "Person"
      },
      {
        "name": "Ross Girshick",
        "type": "Person"
      },
      {
        "name": "Yuqi Cheng",
        "type": "Person"
      },
      {
        "name": "Yunkang Cao",
        "type": "Person"
      },
      {
        "name": "Haiming Yao",
        "type": "Person"
      },
      {
        "name": "Wei Luo",
        "type": "Person"
      },
      {
        "name": "Cheng Jiang",
        "type": "Person"
      },
      {
        "name": "Hui Zhang",
        "type": "Person"
      },
      {
        "name": "Weiming Shen",
        "type": "Person"
      },
      {
        "name": "Naveen Kumar Srinivasa",
        "type": "Person"
      },
      {
        "name": "Ajeet Rao Chalamala",
        "type": "Person"
      },
      {
        "name": "Kumar Singh",
        "type": "Person"
      },
      {
        "name": "Ieee Krishna Mohan Senior Member",
        "type": "Person"
      },
      {
        "name": "K. Naveen",
        "type": "Person"
      },
      {
        "name": "Srinivasa Rao",
        "type": "Person"
      },
      {
        "name": "Ajeet Kumar Singh",
        "type": "Person"
      },
      {
        "name": "Manlin Zhang",
        "type": "Person"
      },
      {
        "name": "Jie Wu",
        "type": "Person"
      },
      {
        "name": "Yuxi Ren",
        "type": "Person"
      },
      {
        "name": "Jiahong Yang",
        "type": "Person"
      },
      {
        "name": "Ming Li",
        "type": "Person"
      },
      {
        "name": "Andy J. Ma",
        "type": "Person"
      },
      {
        "name": "Salma Haidar",
        "type": "Person"
      },
      {
        "name": "José Oramas",
        "type": "Person"
      },
      {
        "name": "Hongbo Jiang",
        "type": "Person"
      },
      {
        "name": "Lei Ye",
        "type": "Person"
      },
      {
        "name": "Jingyang Hu",
        "type": "Person"
      },
      {
        "name": "Xiaotian Chen",
        "type": "Person"
      },
      {
        "name": "Siyu Chen",
        "type": "Person"
      },
      {
        "name": "Wei Zhang",
        "type": "Person"
      },
      {
        "name": "Kehua Yang",
        "type": "Person"
      },
      {
        "name": "Diederik P Kingma",
        "type": "Person"
      },
      {
        "name": "Max Welling",
        "type": "Person"
      },
      {
        "name": "John C. Duchi",
        "type": "Person"
      },
      {
        "name": "Elad Hazan",
        "type": "Person"
      },
      {
        "name": "Y. Singer",
        "type": "Person"
      },
      {
        "name": "Alex Graves",
        "type": "Person"
      },
      {
        "name": "Abdel-rahman Mohamed",
        "type": "Person"
      },
      {
        "name": "Geoffrey Hinton",
        "type": "Person"
      },
      {
        "name": "Nian Wang",
        "type": "Person"
      },
      {
        "name": "Zhigao Cui",
        "type": "Person"
      },
      {
        "name": "Yanzhao Su",
        "type": "Person"
      },
      {
        "name": "Yunwei Lan",
        "type": "Person"
      },
      {
        "name": "Yuanliang Xue",
        "type": "Person"
      },
      {
        "name": "Cong Zhang",
        "type": "Person"
      },
      {
        "name": "Aihua Li",
        "type": "Person"
      },
      {
        "name": "Leong Kah Meng",
        "type": "Person"
      },
      {
        "name": "Ho Hooi Yi",
        "type": "Person"
      },
      {
        "name": "Ng Bo Wei",
        "type": "Person"
      },
      {
        "name": "Lim Jia Xin",
        "type": "Person"
      },
      {
        "name": "Zailan Arabee Abdul Salam",
        "type": "Person"
      },
      {
        "name": "Xin Cheng",
        "type": "Person"
      },
      {
        "name": "Wangding Zeng",
        "type": "Person"
      },
      {
        "name": "Damai Dai",
        "type": "Person"
      },
      {
        "name": "Qinyu Chen",
        "type": "Person"
      },
      {
        "name": "Bingxuan Wang",
        "type": "Person"
      },
      {
        "name": "Zhenda Xie",
        "type": "Person"
      },
      {
        "name": "Kezhao Huang",
        "type": "Person"
      },
      {
        "name": "Xingkai Yu",
        "type": "Person"
      },
      {
        "name": "Zhewen Hao",
        "type": "Person"
      },
      {
        "name": "Yukun Li",
        "type": "Person"
      },
      {
        "name": "Han Zhang",
        "type": "Person"
      },
      {
        "name": "Huishuai Zhang",
        "type": "Person"
      },
      {
        "name": "Dongyan Zhao",
        "type": "Person"
      },
      {
        "name": "Wenfeng Liang",
        "type": "Person"
      },
      {
        "name": "Mostafa Saberian",
        "type": "Person"
      },
      {
        "name": "Vidya Samadi",
        "type": "Person"
      },
      {
        "name": "Ioana Popescu",
        "type": "Person"
      },
      {
        "name": "Husheng Fang",
        "type": "Person"
      },
      {
        "name": "Shunlin Liang",
        "type": "Person"
      },
      {
        "name": "Wenyuan Li",
        "type": "Person"
      },
      {
        "name": "Yongzhe Chen",
        "type": "Person"
      },
      {
        "name": "Han Ma",
        "type": "Person"
      },
      {
        "name": "Jianglei Xu",
        "type": "Person"
      },
      {
        "name": "Yichuan Ma",
        "type": "Person"
      },
      {
        "name": "Tao He",
        "type": "Person"
      },
      {
        "name": "Feng Tian",
        "type": "Person"
      },
      {
        "name": "Fengjiao Zhang",
        "type": "Person"
      },
      {
        "name": "Hui Liang",
        "type": "Person"
      },
      {
        "name": "Wei Liu",
        "type": "Person"
      },
      {
        "name": "Yangqing Jia",
        "type": "Person"
      },
      {
        "name": "Pierre Sermanet",
        "type": "Person"
      },
      {
        "name": "Scott Reed",
        "type": "Person"
      },
      {
        "name": "Dragomir Anguelov",
        "type": "Person"
      },
      {
        "name": "Dumitru Erhan",
        "type": "Person"
      },
      {
        "name": "Andrew Rabinovich",
        "type": "Person"
      },
      {
        "name": "Olga Russakovsky",
        "type": "Person"
      },
      {
        "name": "Jia Deng",
        "type": "Person"
      },
      {
        "name": "Hao Su",
        "type": "Person"
      },
      {
        "name": "Jonathan Krause",
        "type": "Person"
      },
      {
        "name": "Sanjeev Satheesh",
        "type": "Person"
      },
      {
        "name": "Sean Ma",
        "type": "Person"
      },
      {
        "name": "Zhiheng Huang",
        "type": "Person"
      },
      {
        "name": "Andrej Karpathy",
        "type": "Person"
      },
      {
        "name": "Aditya Khosla",
        "type": "Person"
      },
      {
        "name": "Michael Bernstein",
        "type": "Person"
      },
      {
        "name": "Alexander C. Berg",
        "type": "Person"
      },
      {
        "name": "Li Fei-Fei",
        "type": "Person"
      },
      {
        "name": "Boyang Zheng",
        "type": "Person"
      },
      {
        "name": "Nanye Ma",
        "type": "Person"
      },
      {
        "name": "Shengbang Tong",
        "type": "Person"
      },
      {
        "name": "Saining Xie",
        "type": "Person"
      },
      {
        "name": "S. Rizvi",
        "type": "Person"
      },
      {
        "name": "Daniel Levine",
        "type": "Person"
      },
      {
        "name": "Aakash Patel",
        "type": "Person"
      },
      {
        "name": "Shiyang Zhang",
        "type": "Person"
      },
      {
        "name": "Eric Wang",
        "type": "Person"
      },
      {
        "name": "Curtis Jamison Perry",
        "type": "Person"
      },
      {
        "name": "Ivan Vrkic",
        "type": "Person"
      },
      {
        "name": "Nicole Mayerli Constante",
        "type": "Person"
      },
      {
        "name": "Zirui Fu",
        "type": "Person"
      },
      {
        "name": "Sizhuang He",
        "type": "Person"
      },
      {
        "name": "David Zhang",
        "type": "Person"
      },
      {
        "name": "Cerise Tang",
        "type": "Person"
      },
      {
        "name": "Zhuoyang Lyu",
        "type": "Person"
      },
      {
        "name": "Rayyan Y Darji",
        "type": "Person"
      },
      {
        "name": "Chang Li",
        "type": "Person"
      },
      {
        "name": "Emily Sun",
        "type": "Person"
      },
      {
        "name": "David Jeong",
        "type": "Person"
      },
      {
        "name": "Lawrence Zhao",
        "type": "Person"
      },
      {
        "name": "J. Kwan",
        "type": "Person"
      },
      {
        "name": "David Braun",
        "type": "Person"
      },
      {
        "name": "Brian Hafler",
        "type": "Person"
      },
      {
        "name": "Hattie Chung",
        "type": "Person"
      },
      {
        "name": "R. M. Dhodapkar",
        "type": "Person"
      },
      {
        "name": "Paul F. Jaeger",
        "type": "Person"
      },
      {
        "name": "Bryan Perozzi",
        "type": "Person"
      },
      {
        "name": "Jeffrey Ishizuka",
        "type": "Person"
      },
      {
        "name": "Shekoofeh Azizi",
        "type": "Person"
      },
      {
        "name": "D. van Dijk",
        "type": "Person"
      },
      {
        "name": "Şafak Kılıç",
        "type": "Person"
      },
      {
        "name": "Zhengyu Zhao",
        "type": "Person"
      },
      {
        "name": "Hanwei Zhang",
        "type": "Person"
      },
      {
        "name": "Renjue Li",
        "type": "Person"
      },
      {
        "name": "R. Sicre",
        "type": "Person"
      },
      {
        "name": "L. Amsaleg",
        "type": "Person"
      },
      {
        "name": "Michael Backes",
        "type": "Person"
      },
      {
        "name": "Qi Li",
        "type": "Person"
      },
      {
        "name": "Chao Shen",
        "type": "Person"
      },
      {
        "name": "Yifei Ge",
        "type": "Person"
      },
      {
        "name": "Zhuo Li",
        "type": "Person"
      },
      {
        "name": "Xuebin Yue",
        "type": "Person"
      },
      {
        "name": "Hengyi Li",
        "type": "Person"
      },
      {
        "name": "Lin Meng",
        "type": "Person"
      },
      {
        "name": "Colin Raffel",
        "type": "Person"
      },
      {
        "name": "Adam Roberts",
        "type": "Person"
      },
      {
        "name": "Katherine Lee",
        "type": "Person"
      },
      {
        "name": "Sharan Narang",
        "type": "Person"
      },
      {
        "name": "Michael Matena",
        "type": "Person"
      },
      {
        "name": "Yanqi Zhou",
        "type": "Person"
      },
      {
        "name": "Wei Li",
        "type": "Person"
      },
      {
        "name": "Peter J. Liu",
        "type": "Person"
      },
      {
        "name": "M. Heusel",
        "type": "Person"
      },
      {
        "name": "Hubert Ramsauer",
        "type": "Person"
      },
      {
        "name": "Thomas Unterthiner",
        "type": "Person"
      },
      {
        "name": "Bernhard Nessler",
        "type": "Person"
      },
      {
        "name": "Holger Caesar",
        "type": "Person"
      },
      {
        "name": "Varun Bankiti",
        "type": "Person"
      },
      {
        "name": "Alex H. Lang",
        "type": "Person"
      },
      {
        "name": "Sourabh Vora",
        "type": "Person"
      },
      {
        "name": "Venice Erin Liong",
        "type": "Person"
      },
      {
        "name": "Qiang Xu",
        "type": "Person"
      },
      {
        "name": "Anush Krishnan",
        "type": "Person"
      },
      {
        "name": "Yu Pan",
        "type": "Person"
      },
      {
        "name": "Giancarlo Baldan",
        "type": "Person"
      },
      {
        "name": "Oscar Beijbom",
        "type": "Person"
      },
      {
        "name": "Alexey Dosovitskiy",
        "type": "Person"
      },
      {
        "name": "German Ros",
        "type": "Person"
      },
      {
        "name": "Felipe Codevilla",
        "type": "Person"
      },
      {
        "name": "Antonio Lopez",
        "type": "Person"
      },
      {
        "name": "Vladlen Koltun",
        "type": "Person"
      },
      {
        "name": "Fachrina Dewi Puspitasari",
        "type": "Person"
      },
      {
        "name": "Chaoning Zhang",
        "type": "Person"
      },
      {
        "name": "Joseph Cho",
        "type": "Person"
      },
      {
        "name": "Adnan Haider",
        "type": "Person"
      },
      {
        "name": "Noor Ul Eman",
        "type": "Person"
      },
      {
        "name": "Omer Amin",
        "type": "Person"
      },
      {
        "name": "Alexis Mankowski",
        "type": "Person"
      },
      {
        "name": "Muhammad Umair",
        "type": "Person"
      },
      {
        "name": "Jingyao Zheng",
        "type": "Person"
      },
      {
        "name": "Sheng Zheng",
        "type": "Person"
      },
      {
        "name": "Lik-Hang Lee",
        "type": "Person"
      },
      {
        "name": "Caiyan Qin",
        "type": "Person"
      },
      {
        "name": "Tae-Ho Kim",
        "type": "Person"
      },
      {
        "name": "Choong Seon Hong",
        "type": "Person"
      },
      {
        "name": "Yang Yang",
        "type": "Person"
      },
      {
        "name": "Heng Tao Shen",
        "type": "Person"
      },
      {
        "name": "Bohan Li",
        "type": "Person"
      },
      {
        "name": "Zhuang Ma",
        "type": "Person"
      },
      {
        "name": "Dalong Du",
        "type": "Person"
      },
      {
        "name": "Baorui Peng",
        "type": "Person"
      },
      {
        "name": "Zhujin Liang",
        "type": "Person"
      },
      {
        "name": "Zhenqiang Liu",
        "type": "Person"
      },
      {
        "name": "Chao Ma",
        "type": "Person"
      },
      {
        "name": "Yueming Jin",
        "type": "Person"
      },
      {
        "name": "Hao Zhao",
        "type": "Person"
      },
      {
        "name": "Wenjun Zeng",
        "type": "Person"
      },
      {
        "name": "Xin Jin",
        "type": "Person"
      },
      {
        "name": "Guosheng Zhao",
        "type": "Person"
      },
      {
        "name": "Yaozeng Wang",
        "type": "Person"
      },
      {
        "name": "Xiaofeng Wang",
        "type": "Person"
      },
      {
        "name": "Zheng Zhu",
        "type": "Person"
      },
      {
        "name": "Tingdong Yu",
        "type": "Person"
      },
      {
        "name": "Guan Huang",
        "type": "Person"
      },
      {
        "name": "Yongchen Zai",
        "type": "Person"
      },
      {
        "name": "Ji Jiao",
        "type": "Person"
      },
      {
        "name": "Changliang Xue",
        "type": "Person"
      },
      {
        "name": "Xiaole Wang",
        "type": "Person"
      },
      {
        "name": "Zhen Yang",
        "type": "Person"
      },
      {
        "name": "Futang Zhu",
        "type": "Person"
      },
      {
        "name": "Xingang Wang",
        "type": "Person"
      },
      {
        "name": "Ahmad Rahimi",
        "type": "Person"
      },
      {
        "name": "Valentin Gerard",
        "type": "Person"
      },
      {
        "name": "Eloi Zablocki",
        "type": "Person"
      },
      {
        "name": "Matthieu Cord",
        "type": "Person"
      },
      {
        "name": "Alexandre Alahi",
        "type": "Person"
      },
      {
        "name": "W. Marsden",
        "type": "Person"
      },
      {
        "name": "Wei Dong",
        "type": "Person"
      },
      {
        "name": "R. Socher",
        "type": "Person"
      },
      {
        "name": "Li-Jia Li",
        "type": "Person"
      },
      {
        "name": "K. Li",
        "type": "Person"
      },
      {
        "name": "R. Stephenson",
        "type": "Person"
      },
      {
        "name": "Jaskirat Singh",
        "type": "Person"
      },
      {
        "name": "Xingjian Leng",
        "type": "Person"
      },
      {
        "name": "Zongze Wu",
        "type": "Person"
      },
      {
        "name": "Liang Zheng",
        "type": "Person"
      },
      {
        "name": "Richard Zhang",
        "type": "Person"
      },
      {
        "name": "Eli Shechtman",
        "type": "Person"
      },
      {
        "name": "Zhifeng Wang",
        "type": "Person"
      },
      {
        "name": "Minghui Wang",
        "type": "Person"
      },
      {
        "name": "Chunyan Zeng",
        "type": "Person"
      },
      {
        "name": "Longlong Li",
        "type": "Person"
      },
      {
        "name": "Ehsan Zakeri",
        "type": "Person"
      },
      {
        "name": "Amanda Spilkin",
        "type": "Person"
      },
      {
        "name": "Hanae Elmekki",
        "type": "Person"
      },
      {
        "name": "Antonela Zanuttini",
        "type": "Person"
      },
      {
        "name": "L. Kadem",
        "type": "Person"
      },
      {
        "name": "Jamal Bentahar",
        "type": "Person"
      },
      {
        "name": "Wen-Fang Xie",
        "type": "Person"
      },
      {
        "name": "Philippe Pibarot",
        "type": "Person"
      },
      {
        "name": "Ana Davila",
        "type": "Person"
      },
      {
        "name": "Jacinto Colan",
        "type": "Person"
      },
      {
        "name": "Yasuhisa Hasegawa",
        "type": "Person"
      },
      {
        "name": "Subham Sharma",
        "type": "Person"
      },
      {
        "name": "Sharmila Subudhi",
        "type": "Person"
      },
      {
        "name": "Yoshua Bengio",
        "type": "Person"
      },
      {
        "name": "Aaron Courville",
        "type": "Person"
      },
      {
        "name": "Pascal Vincent",
        "type": "Person"
      },
      {
        "name": "D. Touretzky",
        "type": "Person"
      },
      {
        "name": "M. C. Mozer",
        "type": "Person"
      },
      {
        "name": "M. E. Hasselmo",
        "type": "Person"
      },
      {
        "name": "RegressionChristopher",
        "type": "Person"
      },
      {
        "name": "I. K.",
        "type": "Person"
      },
      {
        "name": "WilliamsNeural",
        "type": "Person"
      },
      {
        "name": "GroupAston",
        "type": "Person"
      },
      {
        "name": "UniversityBirmingham",
        "type": "Person"
      },
      {
        "name": "Danilo Jimenez Rezende",
        "type": "Person"
      },
      {
        "name": "S. Mohamed",
        "type": "Person"
      },
      {
        "name": "Daan Wierstra",
        "type": "Person"
      },
      {
        "name": "Shanchuan Lin",
        "type": "Person"
      },
      {
        "name": "Anran Wang",
        "type": "Person"
      },
      {
        "name": "Xiao Yang",
        "type": "Person"
      },
      {
        "name": "Zhiyuan Chen",
        "type": "Person"
      },
      {
        "name": "Jiajiong Cao",
        "type": "Person"
      },
      {
        "name": "Zhiquan Chen",
        "type": "Person"
      },
      {
        "name": "Yuming Li",
        "type": "Person"
      },
      {
        "name": "Chenguang Ma",
        "type": "Person"
      },
      {
        "name": "Wenzhao Zheng",
        "type": "Person"
      },
      {
        "name": "Ruiqi Song",
        "type": "Person"
      },
      {
        "name": "Xianda Guo",
        "type": "Person"
      },
      {
        "name": "Chenming Zhang",
        "type": "Person"
      },
      {
        "name": "Long Chen",
        "type": "Person"
      },
      {
        "name": "Shiyin Lu",
        "type": "Person"
      },
      {
        "name": "Yang Li",
        "type": "Person"
      },
      {
        "name": "Qing-Guo Chen",
        "type": "Person"
      },
      {
        "name": "Zhao Xu",
        "type": "Person"
      },
      {
        "name": "Weihua Luo",
        "type": "Person"
      },
      {
        "name": "Kaifu Zhang",
        "type": "Person"
      },
      {
        "name": "Han-Jia Ye",
        "type": "Person"
      },
      {
        "name": "Jie Liu",
        "type": "Person"
      },
      {
        "name": "Gongye Liu",
        "type": "Person"
      },
      {
        "name": "Jiajun Liang",
        "type": "Person"
      },
      {
        "name": "Ziyang Yuan",
        "type": "Person"
      },
      {
        "name": "Xiaokun Liu",
        "type": "Person"
      },
      {
        "name": "Mingwu Zheng",
        "type": "Person"
      },
      {
        "name": "Xiele Wu",
        "type": "Person"
      },
      {
        "name": "Qiulin Wang",
        "type": "Person"
      },
      {
        "name": "Menghan Xia",
        "type": "Person"
      },
      {
        "name": "Xintao Wang",
        "type": "Person"
      },
      {
        "name": "Xiaohong Liu",
        "type": "Person"
      },
      {
        "name": "Fei Yang",
        "type": "Person"
      },
      {
        "name": "Pengfei Wan",
        "type": "Person"
      },
      {
        "name": "Di Zhang",
        "type": "Person"
      },
      {
        "name": "Kun Gai",
        "type": "Person"
      },
      {
        "name": "Yujiu Yang",
        "type": "Person"
      },
      {
        "name": "Wanli Ouyang",
        "type": "Person"
      },
      {
        "name": "D. Rumelhart",
        "type": "Person"
      },
      {
        "name": "Ronald J. Williams",
        "type": "Person"
      },
      {
        "name": "M. Schuster",
        "type": "Person"
      },
      {
        "name": "K. Paliwal",
        "type": "Person"
      },
      {
        "name": "Santiago Fern´andez",
        "type": "Person"
      },
      {
        "name": "Faustino J. Gomez",
        "type": "Person"
      },
      {
        "name": "J¨urgen Schmidhuber",
        "type": "Person"
      },
      {
        "name": "Tianming Sun",
        "type": "Person"
      },
      {
        "name": "Bin Feng",
        "type": "Person"
      },
      {
        "name": "Jinpeng Huo",
        "type": "Person"
      },
      {
        "name": "Yu Xiao",
        "type": "Person"
      },
      {
        "name": "Wengan Wang",
        "type": "Person"
      },
      {
        "name": "Jin Peng",
        "type": "Person"
      },
      {
        "name": "Zehua Li",
        "type": "Person"
      },
      {
        "name": "Chengjie Du",
        "type": "Person"
      },
      {
        "name": "Wenxian Wang",
        "type": "Person"
      },
      {
        "name": "G. Zou",
        "type": "Person"
      },
      {
        "name": "Lei Liu",
        "type": "Person"
      },
      {
        "name": "Francis R. Willett",
        "type": "Person"
      },
      {
        "name": "Erin M. Kunz",
        "type": "Person"
      },
      {
        "name": "Chaofei Fan",
        "type": "Person"
      },
      {
        "name": "Donald T. Avansino",
        "type": "Person"
      },
      {
        "name": "G. Wilson",
        "type": "Person"
      },
      {
        "name": "Eun Young Choi",
        "type": "Person"
      },
      {
        "name": "Foram B. Kamdar",
        "type": "Person"
      },
      {
        "name": "M. Glasser",
        "type": "Person"
      },
      {
        "name": "L. Hochberg",
        "type": "Person"
      },
      {
        "name": "S. Druckmann",
        "type": "Person"
      },
      {
        "name": "K. Shenoy",
        "type": "Person"
      },
      {
        "name": "J. Henderson",
        "type": "Person"
      },
      {
        "name": "Shibhansh Dohare",
        "type": "Person"
      },
      {
        "name": "J. F. Hernandez-Garcia",
        "type": "Person"
      },
      {
        "name": "Qingfeng Lan",
        "type": "Person"
      },
      {
        "name": "Parash Rahman",
        "type": "Person"
      },
      {
        "name": "A. Mahmood",
        "type": "Person"
      },
      {
        "name": "R. Sutton",
        "type": "Person"
      },
      {
        "name": "S. Ambrogio",
        "type": "Person"
      },
      {
        "name": "P. Narayanan",
        "type": "Person"
      },
      {
        "name": "A. Okazaki",
        "type": "Person"
      },
      {
        "name": "A. Fasoli",
        "type": "Person"
      },
      {
        "name": "C. Mackin",
        "type": "Person"
      },
      {
        "name": "K. Hosokawa",
        "type": "Person"
      },
      {
        "name": "A. Nomura",
        "type": "Person"
      },
      {
        "name": "Takeo Yasuda",
        "type": "Person"
      },
      {
        "name": "An Chen",
        "type": "Person"
      },
      {
        "name": "A. Friz",
        "type": "Person"
      },
      {
        "name": "M. Ishii",
        "type": "Person"
      },
      {
        "name": "J. Luquin",
        "type": "Person"
      },
      {
        "name": "Y. Kohda",
        "type": "Person"
      },
      {
        "name": "N. Saulnier",
        "type": "Person"
      },
      {
        "name": "K. Brew",
        "type": "Person"
      },
      {
        "name": "Samuel Choi",
        "type": "Person"
      },
      {
        "name": "I. Ok",
        "type": "Person"
      },
      {
        "name": "Timothy Philip",
        "type": "Person"
      },
      {
        "name": "Victor Chan",
        "type": "Person"
      },
      {
        "name": "M. Silvestre",
        "type": "Person"
      },
      {
        "name": "Ishtiaq Ahsan",
        "type": "Person"
      },
      {
        "name": "Vijay Narayanan",
        "type": "Person"
      },
      {
        "name": "H. Tsai",
        "type": "Person"
      },
      {
        "name": "Geoffrey W. Burr",
        "type": "Person"
      },
      {
        "name": "J. Shin",
        "type": "Person"
      },
      {
        "name": "Sang Joon Kim",
        "type": "Person"
      },
      {
        "name": "Yike Sun",
        "type": "Person"
      },
      {
        "name": "Haotong Yang",
        "type": "Person"
      },
      {
        "name": "Zhouchen Lin",
        "type": "Person"
      },
      {
        "name": "Muhan Zhang",
        "type": "Person"
      },
      {
        "name": "Ning Ding",
        "type": "Person"
      },
      {
        "name": "Fangcheng Liu",
        "type": "Person"
      },
      {
        "name": "Kyungrae Kim",
        "type": "Person"
      },
      {
        "name": "Linji Hao",
        "type": "Person"
      },
      {
        "name": "Kyeng-Hun Lee",
        "type": "Person"
      },
      {
        "name": "Hyeonmok Ko",
        "type": "Person"
      },
      {
        "name": "Yehui Tang",
        "type": "Person"
      },
      {
        "name": "Huinan Xu",
        "type": "Person"
      },
      {
        "name": "Xuyang Feng",
        "type": "Person"
      },
      {
        "name": "Junhong Chen",
        "type": "Person"
      },
      {
        "name": "Junchen Liu",
        "type": "Person"
      },
      {
        "name": "Kaiwen Deng",
        "type": "Person"
      },
      {
        "name": "Kai Ding",
        "type": "Person"
      },
      {
        "name": "Shengning Long",
        "type": "Person"
      },
      {
        "name": "Jiaxue Shuai",
        "type": "Person"
      },
      {
        "name": "Zhaorong Li",
        "type": "Person"
      },
      {
        "name": "Shiping Liu",
        "type": "Person"
      },
      {
        "name": "Guirong Xue",
        "type": "Person"
      },
      {
        "name": "Zhan Xiao",
        "type": "Person"
      },
      {
        "name": "Albert Tseng",
        "type": "Person"
      },
      {
        "name": "Christopher De Sa",
        "type": "Person"
      },
      {
        "name": "Hong Liu",
        "type": "Person"
      },
      {
        "name": "Jiaqi Zhang",
        "type": "Person"
      },
      {
        "name": "Chao Wang",
        "type": "Person"
      },
      {
        "name": "Xing Hu",
        "type": "Person"
      },
      {
        "name": "Linkun Lyu",
        "type": "Person"
      },
      {
        "name": "Jiaqi Sun",
        "type": "Person"
      },
      {
        "name": "Xurui Yang",
        "type": "Person"
      },
      {
        "name": "Bo Wang",
        "type": "Person"
      },
      {
        "name": "Fengcun Li",
        "type": "Person"
      },
      {
        "name": "Yulei Qian",
        "type": "Person"
      },
      {
        "name": "Lingtong Si",
        "type": "Person"
      },
      {
        "name": "Yerui Sun",
        "type": "Person"
      },
      {
        "name": "Rumei Li",
        "type": "Person"
      },
      {
        "name": "Peng Pei",
        "type": "Person"
      },
      {
        "name": "Yuchen Xie",
        "type": "Person"
      },
      {
        "name": "Xunliang Cai",
        "type": "Person"
      },
      {
        "name": "Yann LeCun",
        "type": "Person"
      },
      {
        "name": "L. Bottou",
        "type": "Person"
      },
      {
        "name": "P. Haffner",
        "type": "Person"
      },
      {
        "name": "R. Tibshirani",
        "type": "Person"
      },
      {
        "name": "Tsung-Yi Lin",
        "type": "Person"
      },
      {
        "name": "Michael Maire",
        "type": "Person"
      },
      {
        "name": "Serge Belongie",
        "type": "Person"
      },
      {
        "name": "Lubomir Bourdev",
        "type": "Person"
      },
      {
        "name": "James Hays",
        "type": "Person"
      },
      {
        "name": "Pietro Perona",
        "type": "Person"
      },
      {
        "name": "Deva Ramanan",
        "type": "Person"
      },
      {
        "name": "C. Lawrence Zitnick",
        "type": "Person"
      },
      {
        "name": "Piotr Dollár",
        "type": "Person"
      },
      {
        "name": "Herve Goeau",
        "type": "Person"
      },
      {
        "name": "Pierre Bonnet",
        "type": "Person"
      },
      {
        "name": "Alexis Joly",
        "type": "Person"
      },
      {
        "name": "Safa Ben Atitallah",
        "type": "Person"
      },
      {
        "name": "Maha Driss",
        "type": "Person"
      },
      {
        "name": "Henda Ben Ghezela",
        "type": "Person"
      },
      {
        "name": "Sareer Ul Amin",
        "type": "Person"
      },
      {
        "name": "Yonghoon Jung",
        "type": "Person"
      },
      {
        "name": "Muhammad Fayaz",
        "type": "Person"
      },
      {
        "name": "Bumsoo Kim",
        "type": "Person"
      },
      {
        "name": "Sanghyun Seo",
        "type": "Person"
      },
      {
        "name": "Ayşe Aybilge Murat",
        "type": "Person"
      },
      {
        "name": "M. S. Kıran",
        "type": "Person"
      },
      {
        "name": "Hongyan Zhu",
        "type": "Person"
      },
      {
        "name": "Shuai Qin",
        "type": "Person"
      },
      {
        "name": "Min Su",
        "type": "Person"
      },
      {
        "name": "Chengzhi Lin",
        "type": "Person"
      },
      {
        "name": "Anjie Li",
        "type": "Person"
      },
      {
        "name": "Junfeng Gao",
        "type": "Person"
      },
      {
        "name": "Abdul Rehman Khan",
        "type": "Person"
      },
      {
        "name": "Asifullah Khan",
        "type": "Person"
      },
      {
        "name": "D. E. Boukhari",
        "type": "Person"
      },
      {
        "name": "F. Dornaika",
        "type": "Person"
      },
      {
        "name": "A. Chemsa",
        "type": "Person"
      },
      {
        "name": "Abdelmalik Taleb-Ahmed",
        "type": "Person"
      },
      {
        "name": "Benjamin DeMeo",
        "type": "Person"
      },
      {
        "name": "Charlotte Nesbitt",
        "type": "Person"
      },
      {
        "name": "S. A. Miller",
        "type": "Person"
      },
      {
        "name": "Daniel B. Burkhardt",
        "type": "Person"
      },
      {
        "name": "Inna Lipchina",
        "type": "Person"
      },
      {
        "name": "Doris Fu",
        "type": "Person"
      },
      {
        "name": "Peter Holderreith",
        "type": "Person"
      },
      {
        "name": "David Kim",
        "type": "Person"
      },
      {
        "name": "Sergey Kolchenko",
        "type": "Person"
      },
      {
        "name": "Artur Szałata",
        "type": "Person"
      },
      {
        "name": "Ishan Gupta",
        "type": "Person"
      },
      {
        "name": "Christine Kerr",
        "type": "Person"
      },
      {
        "name": "Thomas Pfefer",
        "type": "Person"
      },
      {
        "name": "Raziel Rojas-Rodriguez",
        "type": "Person"
      },
      {
        "name": "Sunil Kuppassani",
        "type": "Person"
      },
      {
        "name": "Laurens Kruidenier",
        "type": "Person"
      },
      {
        "name": "Parul B Doshi",
        "type": "Person"
      },
      {
        "name": "Mahdi Zamanighomi",
        "type": "Person"
      },
      {
        "name": "James J. Collins",
        "type": "Person"
      },
      {
        "name": "A. Shalek",
        "type": "Person"
      },
      {
        "name": "F. Theis",
        "type": "Person"
      },
      {
        "name": "Mauricio Cortes",
        "type": "Person"
      },
      {
        "name": "D. Lowe",
        "type": "Person"
      },
      {
        "name": "Xiang An",
        "type": "Person"
      },
      {
        "name": "Yin Xie",
        "type": "Person"
      },
      {
        "name": "Kaicheng Yang",
        "type": "Person"
      },
      {
        "name": "Wenkang Zhang",
        "type": "Person"
      },
      {
        "name": "Xiuwei Zhao",
        "type": "Person"
      },
      {
        "name": "Zheng Cheng",
        "type": "Person"
      },
      {
        "name": "Yirui Wang",
        "type": "Person"
      },
      {
        "name": "Songcen Xu",
        "type": "Person"
      },
      {
        "name": "Changrui Chen",
        "type": "Person"
      },
      {
        "name": "Didi Zhu",
        "type": "Person"
      },
      {
        "name": "Chunsheng Wu",
        "type": "Person"
      },
      {
        "name": "Huajie Tan",
        "type": "Person"
      },
      {
        "name": "Chunyuan Li",
        "type": "Person"
      },
      {
        "name": "Jing Yang",
        "type": "Person"
      },
      {
        "name": "Jie Yu",
        "type": "Person"
      },
      {
        "name": "Xiyao Wang",
        "type": "Person"
      },
      {
        "name": "Bin Qin",
        "type": "Person"
      },
      {
        "name": "Yumeng Wang",
        "type": "Person"
      },
      {
        "name": "Zizhen Yan",
        "type": "Person"
      },
      {
        "name": "Ziyong Feng",
        "type": "Person"
      },
      {
        "name": "Ziwei Liu",
        "type": "Person"
      },
      {
        "name": "Bo Li",
        "type": "Person"
      },
      {
        "name": "Jiankang Deng",
        "type": "Person"
      },
      {
        "name": "Kento Kawaharazuka",
        "type": "Person"
      },
      {
        "name": "Jihoon Oh",
        "type": "Person"
      },
      {
        "name": "Jun Yamada",
        "type": "Person"
      },
      {
        "name": "Ingmar Posner",
        "type": "Person"
      },
      {
        "name": "Yuke Zhu",
        "type": "Person"
      },
      {
        "name": "Lukas Muttenthaler",
        "type": "Person"
      },
      {
        "name": "Klaus Greff",
        "type": "Person"
      },
      {
        "name": "Frieda Born",
        "type": "Person"
      },
      {
        "name": "Bernhard Spitzer",
        "type": "Person"
      },
      {
        "name": "Simon Kornblith",
        "type": "Person"
      },
      {
        "name": "Klaus-Robert Muller",
        "type": "Person"
      },
      {
        "name": "Andrew Kyle Lampinen",
        "type": "Person"
      },
      {
        "name": "Lucas Beyer",
        "type": "Person"
      },
      {
        "name": "Alexander Kolesnikov",
        "type": "Person"
      },
      {
        "name": "Dirk Weissenborn",
        "type": "Person"
      },
      {
        "name": "Xiaohua Zhai",
        "type": "Person"
      },
      {
        "name": "Mostafa Dehghani",
        "type": "Person"
      },
      {
        "name": "Matthias Minderer",
        "type": "Person"
      },
      {
        "name": "Georg Heigold",
        "type": "Person"
      },
      {
        "name": "Sylvain Gelly",
        "type": "Person"
      },
      {
        "name": "Neil Houlsby",
        "type": "Person"
      },
      {
        "name": "Alec Radford",
        "type": "Person"
      },
      {
        "name": "Jong Wook Kim",
        "type": "Person"
      },
      {
        "name": "Chris Hallacy",
        "type": "Person"
      },
      {
        "name": "Aditya Ramesh",
        "type": "Person"
      },
      {
        "name": "Gabriel Goh",
        "type": "Person"
      },
      {
        "name": "Sandhini Agarwal",
        "type": "Person"
      },
      {
        "name": "Girish Sastry",
        "type": "Person"
      },
      {
        "name": "Amanda Askell",
        "type": "Person"
      },
      {
        "name": "Pamela Mishkin",
        "type": "Person"
      },
      {
        "name": "Jack Clark",
        "type": "Person"
      },
      {
        "name": "Gretchen Krueger",
        "type": "Person"
      },
      {
        "name": "Ilya Sutskever",
        "type": "Person"
      },
      {
        "name": "Individualized Treat",
        "type": "Person"
      },
      {
        "name": "Jinsung Yoon",
        "type": "Person"
      },
      {
        "name": "Zhengyang Geng",
        "type": "Person"
      },
      {
        "name": "Yiyang Lu",
        "type": "Person"
      },
      {
        "name": "J. Zico Kolter",
        "type": "Person"
      },
      {
        "name": "Jiachen Lei",
        "type": "Person"
      },
      {
        "name": "Keli Liu",
        "type": "Person"
      },
      {
        "name": "Julius Berner",
        "type": "Person"
      },
      {
        "name": "Haiming Yu",
        "type": "Person"
      },
      {
        "name": "Hongkai Zheng",
        "type": "Person"
      },
      {
        "name": "Jiahong Wu",
        "type": "Person"
      },
      {
        "name": "Xiangxiang Chu",
        "type": "Person"
      },
      {
        "name": "Minglei Shi",
        "type": "Person"
      },
      {
        "name": "Haolin Wang",
        "type": "Person"
      },
      {
        "name": "Borui Zhang",
        "type": "Person"
      },
      {
        "name": "Bohan Zeng",
        "type": "Person"
      },
      {
        "name": "Xiaoshi Wu",
        "type": "Person"
      },
      {
        "name": "Yuanxing Zhang",
        "type": "Person"
      },
      {
        "name": "Huan Yang",
        "type": "Person"
      },
      {
        "name": "Jie Zhou",
        "type": "Person"
      },
      {
        "name": "Jiwen Lu",
        "type": "Person"
      },
      {
        "name": "Yongsheng Yu",
        "type": "Person"
      },
      {
        "name": "Wei Xiong",
        "type": "Person"
      },
      {
        "name": "Weili Nie",
        "type": "Person"
      },
      {
        "name": "Yichen Sheng",
        "type": "Person"
      },
      {
        "name": "Shiqiu Liu",
        "type": "Person"
      },
      {
        "name": "Jiebo Luo",
        "type": "Person"
      },
      {
        "name": "Zhiheng Liu",
        "type": "Person"
      },
      {
        "name": "Weiming Ren",
        "type": "Person"
      },
      {
        "name": "Haozhe Liu",
        "type": "Person"
      },
      {
        "name": "Zijian Zhou",
        "type": "Person"
      },
      {
        "name": "Shoufa Chen",
        "type": "Person"
      },
      {
        "name": "Haonan Qiu",
        "type": "Person"
      },
      {
        "name": "Xiaoke Huang",
        "type": "Person"
      },
      {
        "name": "Zhaochong An",
        "type": "Person"
      },
      {
        "name": "Fanny Yang",
        "type": "Person"
      },
      {
        "name": "Aditya Patel",
        "type": "Person"
      },
      {
        "name": "Viktar Atliha",
        "type": "Person"
      },
      {
        "name": "Tony Ng",
        "type": "Person"
      },
      {
        "name": "Xiao Han",
        "type": "Person"
      },
      {
        "name": "Chuyan Zhu",
        "type": "Person"
      },
      {
        "name": "Chenyang Zhang",
        "type": "Person"
      },
      {
        "name": "Ding Liu",
        "type": "Person"
      },
      {
        "name": "Juan-Manuel Perez-Rua",
        "type": "Person"
      },
      {
        "name": "Sen He",
        "type": "Person"
      },
      {
        "name": "Jürgen Schmidhuber",
        "type": "Person"
      },
      {
        "name": "Wenhu Chen",
        "type": "Person"
      },
      {
        "name": "Ping Luo",
        "type": "Person"
      },
      {
        "name": "Tao Xiang",
        "type": "Person"
      },
      {
        "name": "Jonas Schult",
        "type": "Person"
      },
      {
        "name": "Yuren Cong",
        "type": "Person"
      },
      {
        "name": "Jacob Devlin",
        "type": "Person"
      },
      {
        "name": "Ming-Wei Chang",
        "type": "Person"
      },
      {
        "name": "Kenton Lee",
        "type": "Person"
      },
      {
        "name": "Kristina Toutanova",
        "type": "Person"
      },
      {
        "name": "Vrushali Pagire",
        "type": "Person"
      },
      {
        "name": "M. Chavali",
        "type": "Person"
      },
      {
        "name": "Ashish Kale",
        "type": "Person"
      },
      {
        "name": "Jinjie Ni",
        "type": "Person"
      },
      {
        "name": "Qian Liu",
        "type": "Person"
      },
      {
        "name": "Longxu Dou",
        "type": "Person"
      },
      {
        "name": "Chao Du",
        "type": "Person"
      },
      {
        "name": "Zili Wang",
        "type": "Person"
      },
      {
        "name": "Hang Yan",
        "type": "Person"
      },
      {
        "name": "Tianyu Pang",
        "type": "Person"
      },
      {
        "name": "Michael Qizhe Shieh",
        "type": "Person"
      },
      {
        "name": "Zirui Wu",
        "type": "Person"
      },
      {
        "name": "Lin Zheng",
        "type": "Person"
      },
      {
        "name": "Zhihui Xie",
        "type": "Person"
      },
      {
        "name": "Jiacheng Ye",
        "type": "Person"
      },
      {
        "name": "Jiahui Gao",
        "type": "Person"
      },
      {
        "name": "Shansan Gong",
        "type": "Person"
      },
      {
        "name": "Yansong Feng",
        "type": "Person"
      },
      {
        "name": "Zhenguo Li",
        "type": "Person"
      },
      {
        "name": "Wei Bi",
        "type": "Person"
      },
      {
        "name": "Guorui Zhou",
        "type": "Person"
      },
      {
        "name": "Lingpeng Kong",
        "type": "Person"
      },
      {
        "name": "Zhicheng Cai",
        "type": "Person"
      },
      {
        "name": "Xinyuan Guo",
        "type": "Person"
      },
      {
        "name": "Yu Pei",
        "type": "Person"
      },
      {
        "name": "Jiangtao Feng",
        "type": "Person"
      },
      {
        "name": "Jinsong Su",
        "type": "Person"
      },
      {
        "name": "Jiangjie Chen",
        "type": "Person"
      },
      {
        "name": "Ya-Qin Zhang",
        "type": "Person"
      },
      {
        "name": "Wei-Ying Ma",
        "type": "Person"
      },
      {
        "name": "Mingxuan Wang",
        "type": "Person"
      },
      {
        "name": "Hao Zhou",
        "type": "Person"
      },
      {
        "name": "Mingyue Cheng",
        "type": "Person"
      },
      {
        "name": "Jie Ouyang",
        "type": "Person"
      },
      {
        "name": "Shuo Yu",
        "type": "Person"
      },
      {
        "name": "Ruiran Yan",
        "type": "Person"
      },
      {
        "name": "Yucong Luo",
        "type": "Person"
      },
      {
        "name": "Zirui Liu",
        "type": "Person"
      },
      {
        "name": "Daoyu Wang",
        "type": "Person"
      },
      {
        "name": "Qi Liu",
        "type": "Person"
      },
      {
        "name": "Enhong Chen",
        "type": "Person"
      },
      {
        "name": "Jie Hu",
        "type": "Person"
      },
      {
        "name": "Li Shen",
        "type": "Person"
      },
      {
        "name": "Samuel Albanie",
        "type": "Person"
      },
      {
        "name": "Gang Sun",
        "type": "Person"
      },
      {
        "name": "Enhua Wu",
        "type": "Person"
      },
      {
        "name": "I. Loshchilov",
        "type": "Person"
      },
      {
        "name": "F. Hutter",
        "type": "Person"
      },
      {
        "name": "Mingxing Tan",
        "type": "Person"
      },
      {
        "name": "Quoc V. Le",
        "type": "Person"
      },
      {
        "name": "Tianqi Liu",
        "type": "Person"
      },
      {
        "name": "Zhaoxi Chen",
        "type": "Person"
      },
      {
        "name": "Zihao Huang",
        "type": "Person"
      },
      {
        "name": "Shaocong Xu",
        "type": "Person"
      },
      {
        "name": "Saining Zhang",
        "type": "Person"
      },
      {
        "name": "Chongjie Ye",
        "type": "Person"
      },
      {
        "name": "Zhiguo Cao",
        "type": "Person"
      },
      {
        "name": "Tianze Xia",
        "type": "Person"
      },
      {
        "name": "Yongkang Li",
        "type": "Person"
      },
      {
        "name": "Lijun Zhou",
        "type": "Person"
      },
      {
        "name": "Jingfeng Yao",
        "type": "Person"
      },
      {
        "name": "Kaixin Xiong",
        "type": "Person"
      },
      {
        "name": "Haiyang Sun",
        "type": "Person"
      },
      {
        "name": "Bing Wang",
        "type": "Person"
      },
      {
        "name": "Kun Ma",
        "type": "Person"
      },
      {
        "name": "Guang Chen",
        "type": "Person"
      },
      {
        "name": "Hangjun Ye",
        "type": "Person"
      },
      {
        "name": "Wenyu Liu",
        "type": "Person"
      },
      {
        "name": "Xinggang Wang",
        "type": "Person"
      },
      {
        "name": "Sicheng Zuo",
        "type": "Person"
      },
      {
        "name": "Zixun Xie",
        "type": "Person"
      },
      {
        "name": "Shaoqing Xu",
        "type": "Person"
      },
      {
        "name": "Fang Li",
        "type": "Person"
      },
      {
        "name": "Shengyin Jiang",
        "type": "Person"
      },
      {
        "name": "Zhi-Xin Yang",
        "type": "Person"
      },
      {
        "name": "Lvmin Zhang",
        "type": "Person"
      },
      {
        "name": "Anyi Rao",
        "type": "Person"
      },
      {
        "name": "Maneesh Agrawala",
        "type": "Person"
      },
      {
        "name": "Hyung Won Chung",
        "type": "Person"
      },
      {
        "name": "Le Hou",
        "type": "Person"
      },
      {
        "name": "Shayne Longpre",
        "type": "Person"
      },
      {
        "name": "Barret Zoph",
        "type": "Person"
      },
      {
        "name": "Yi Tay",
        "type": "Person"
      },
      {
        "name": "William Fedus",
        "type": "Person"
      },
      {
        "name": "Yunxuan Li",
        "type": "Person"
      },
      {
        "name": "Xuezhi Wang",
        "type": "Person"
      },
      {
        "name": "Siddhartha Brahma",
        "type": "Person"
      },
      {
        "name": "Albert Webson",
        "type": "Person"
      },
      {
        "name": "Shixiang Shane Gu",
        "type": "Person"
      },
      {
        "name": "Zhuyun Dai",
        "type": "Person"
      },
      {
        "name": "Mirac Suzgun",
        "type": "Person"
      },
      {
        "name": "Xinyun Chen",
        "type": "Person"
      },
      {
        "name": "Aakanksha Chowdhery",
        "type": "Person"
      },
      {
        "name": "Alex Castro-Ros",
        "type": "Person"
      },
      {
        "name": "Marie Pellat",
        "type": "Person"
      },
      {
        "name": "Kevin Robinson",
        "type": "Person"
      },
      {
        "name": "Dasha Valter",
        "type": "Person"
      },
      {
        "name": "Gaurav Mishra",
        "type": "Person"
      },
      {
        "name": "Adams Yu",
        "type": "Person"
      },
      {
        "name": "Vincent Zhao",
        "type": "Person"
      },
      {
        "name": "Yanping Huang",
        "type": "Person"
      },
      {
        "name": "Andrew Dai",
        "type": "Person"
      },
      {
        "name": "Hongkun Yu",
        "type": "Person"
      },
      {
        "name": "Slav Petrov",
        "type": "Person"
      },
      {
        "name": "Ed H. Chi",
        "type": "Person"
      },
      {
        "name": "Jeff Dean",
        "type": "Person"
      },
      {
        "name": "Denny Zhou",
        "type": "Person"
      },
      {
        "name": "Jason Wei",
        "type": "Person"
      },
      {
        "name": "Robin Rombach",
        "type": "Person"
      },
      {
        "name": "Andreas Blattmann",
        "type": "Person"
      },
      {
        "name": "Dominik Lorenz",
        "type": "Person"
      },
      {
        "name": "Patrick Esser",
        "type": "Person"
      },
      {
        "name": "Björn Ommer",
        "type": "Person"
      },
      {
        "name": "Romain Lopez",
        "type": "Person"
      },
      {
        "name": "Pierre Boyeau",
        "type": "Person"
      },
      {
        "name": "N. Yosef",
        "type": "Person"
      },
      {
        "name": "Michael I. Jordan",
        "type": "Person"
      },
      {
        "name": "J. Regier",
        "type": "Person"
      },
      {
        "name": "Phillip Isola",
        "type": "Person"
      },
      {
        "name": "Alexei A. Efros",
        "type": "Person"
      },
      {
        "name": "Oliver Wang",
        "type": "Person"
      },
      {
        "name": "Pei Sun",
        "type": "Person"
      },
      {
        "name": "Henrik Kretzschmar",
        "type": "Person"
      },
      {
        "name": "Xerxes Dotiwalla",
        "type": "Person"
      },
      {
        "name": "Aurelien Chouard",
        "type": "Person"
      },
      {
        "name": "Vijaysai Patnaik",
        "type": "Person"
      },
      {
        "name": "Paul Tsui",
        "type": "Person"
      },
      {
        "name": "James Guo",
        "type": "Person"
      },
      {
        "name": "Yin Zhou",
        "type": "Person"
      },
      {
        "name": "Yuning Chai",
        "type": "Person"
      },
      {
        "name": "Benjamin Caine",
        "type": "Person"
      },
      {
        "name": "Vijay Vasudevan",
        "type": "Person"
      },
      {
        "name": "Wei Han",
        "type": "Person"
      },
      {
        "name": "Jiquan Ngiam",
        "type": "Person"
      },
      {
        "name": "Hang Zhao",
        "type": "Person"
      },
      {
        "name": "Aleksei Timofeev",
        "type": "Person"
      },
      {
        "name": "Scott Ettinger",
        "type": "Person"
      },
      {
        "name": "Maxim Krivokon",
        "type": "Person"
      },
      {
        "name": "Amy Gao",
        "type": "Person"
      },
      {
        "name": "Aditya Joshi",
        "type": "Person"
      },
      {
        "name": "Sheng Zhao",
        "type": "Person"
      },
      {
        "name": "Shuyang Cheng",
        "type": "Person"
      },
      {
        "name": "Yu Zhang",
        "type": "Person"
      },
      {
        "name": "Zhifeng Chen",
        "type": "Person"
      },
      {
        "name": "Navneet Dalal",
        "type": "Person"
      },
      {
        "name": "B. Triggs",
        "type": "Person"
      },
      {
        "name": "Gilad Cohen",
        "type": "Person"
      },
      {
        "name": "Raja Giryes",
        "type": "Person"
      },
      {
        "name": "Zekai Zhang",
        "type": "Person"
      },
      {
        "name": "Xiao Li",
        "type": "Person"
      },
      {
        "name": "Xiang Li",
        "type": "Person"
      },
      {
        "name": "Lianghe Shi",
        "type": "Person"
      },
      {
        "name": "Meng Wu",
        "type": "Person"
      },
      {
        "name": "Molei Tao",
        "type": "Person"
      },
      {
        "name": "Qing Qu",
        "type": "Person"
      },
      {
        "name": "Donglin Yang",
        "type": "Person"
      },
      {
        "name": "Yongxing Zhang",
        "type": "Person"
      },
      {
        "name": "Xin Yu",
        "type": "Person"
      },
      {
        "name": "Liang Hou",
        "type": "Person"
      },
      {
        "name": "Xin Tao",
        "type": "Person"
      },
      {
        "name": "Xiaojuan Qi",
        "type": "Person"
      },
      {
        "name": "Renjie Liao",
        "type": "Person"
      },
      {
        "name": "Ramón Calvo-González",
        "type": "Person"
      },
      {
        "name": "François Fleuret",
        "type": "Person"
      },
      {
        "name": "Yao Teng",
        "type": "Person"
      },
      {
        "name": "Minxuan Lin",
        "type": "Person"
      },
      {
        "name": "Xian Liu",
        "type": "Person"
      },
      {
        "name": "Shuai Wang",
        "type": "Person"
      },
      {
        "name": "Xihui Liu",
        "type": "Person"
      },
      {
        "name": "Nicolas Sereyjol-Garros",
        "type": "Person"
      },
      {
        "name": "Ellington Kirby",
        "type": "Person"
      },
      {
        "name": "Victor Letzelter",
        "type": "Person"
      },
      {
        "name": "Victor Besnier",
        "type": "Person"
      },
      {
        "name": "Nermin Samet",
        "type": "Person"
      },
      {
        "name": "Camillo Lugaresi",
        "type": "Person"
      },
      {
        "name": "Jiuqiang Tang",
        "type": "Person"
      },
      {
        "name": "Hadon Nash",
        "type": "Person"
      },
      {
        "name": "Chris McClanahan",
        "type": "Person"
      },
      {
        "name": "Esha Uboweja",
        "type": "Person"
      },
      {
        "name": "Michael Hays",
        "type": "Person"
      },
      {
        "name": "Fan Zhang",
        "type": "Person"
      },
      {
        "name": "Chuo-Ling Chang",
        "type": "Person"
      },
      {
        "name": "Ming Guang Yong",
        "type": "Person"
      },
      {
        "name": "Juhyun Lee",
        "type": "Person"
      },
      {
        "name": "Wan-Teh Chang",
        "type": "Person"
      },
      {
        "name": "Wei Hua",
        "type": "Person"
      },
      {
        "name": "Manfred Georg",
        "type": "Person"
      },
      {
        "name": "Matthias Grundmann",
        "type": "Person"
      },
      {
        "name": "Cem Keskin",
        "type": "Person"
      },
      {
        "name": "Mustafa Furkan Kıraç",
        "type": "Person"
      },
      {
        "name": "Yunus Emre Kara",
        "type": "Person"
      },
      {
        "name": "L. Akarun",
        "type": "Person"
      },
      {
        "name": "S. P. Priyal",
        "type": "Person"
      },
      {
        "name": "P. Bora",
        "type": "Person"
      },
      {
        "name": "Octavian Dudas",
        "type": "Person"
      },
      {
        "name": "C. Nandra",
        "type": "Person"
      },
      {
        "name": "C. Mocan",
        "type": "Person"
      },
      {
        "name": "D. Gorgan",
        "type": "Person"
      },
      {
        "name": "Avinash Dhiran",
        "type": "Person"
      },
      {
        "name": "Anurag Kumbhare",
        "type": "Person"
      },
      {
        "name": "Achal Patil",
        "type": "Person"
      },
      {
        "name": "Mrugank Vichare",
        "type": "Person"
      },
      {
        "name": "Dhananjay Patel",
        "type": "Person"
      },
      {
        "name": "Saransh Mishra",
        "type": "Person"
      },
      {
        "name": "Pavan Nair",
        "type": "Person"
      },
      {
        "name": "Pushpalatha M",
        "type": "Person"
      },
      {
        "name": "Poornima S",
        "type": "Person"
      },
      {
        "name": "A. Dempster",
        "type": "Person"
      },
      {
        "name": "N. Laird",
        "type": "Person"
      },
      {
        "name": "D. Rubin",
        "type": "Person"
      },
      {
        "name": "L. Maaten",
        "type": "Person"
      },
      {
        "name": "Jacy Reese Anthis",
        "type": "Person"
      },
      {
        "name": "Ryan Liu",
        "type": "Person"
      },
      {
        "name": "Sean M. Richardson",
        "type": "Person"
      },
      {
        "name": "Austin C. Kozlowski",
        "type": "Person"
      },
      {
        "name": "Bernard Koch",
        "type": "Person"
      },
      {
        "name": "James Evans",
        "type": "Person"
      },
      {
        "name": "Erik Brynjolfsson",
        "type": "Person"
      },
      {
        "name": "Zhiwen Xiao",
        "type": "Person"
      },
      {
        "name": "Huagang Tong",
        "type": "Person"
      },
      {
        "name": "Runqian Wang",
        "type": "Person"
      },
      {
        "name": "Ibomoiye Domor Mienye",
        "type": "Person"
      },
      {
        "name": "Theo G. Swart",
        "type": "Person"
      },
      {
        "name": "Jusheng Zhang",
        "type": "Person"
      },
      {
        "name": "Zimeng Huang",
        "type": "Person"
      },
      {
        "name": "Yijia Fan",
        "type": "Person"
      },
      {
        "name": "Ningyuan Liu",
        "type": "Person"
      },
      {
        "name": "Mingyan Li",
        "type": "Person"
      },
      {
        "name": "Zhuojie Yang",
        "type": "Person"
      },
      {
        "name": "Jiawei Yao",
        "type": "Person"
      },
      {
        "name": "Jian Wang",
        "type": "Person"
      },
      {
        "name": "Keze Wang",
        "type": "Person"
      },
      {
        "name": "Olaf Ronneberger",
        "type": "Person"
      },
      {
        "name": "Philipp Fischer",
        "type": "Person"
      },
      {
        "name": "Thomas Brox",
        "type": "Person"
      },
      {
        "name": "Tianwei Yin",
        "type": "Person"
      },
      {
        "name": "Michaël Gharbi",
        "type": "Person"
      },
      {
        "name": "Taesung Park",
        "type": "Person"
      },
      {
        "name": "Fredo Durand",
        "type": "Person"
      },
      {
        "name": "William T. Freeman",
        "type": "Person"
      },
      {
        "name": "Axel Sauer",
        "type": "Person"
      },
      {
        "name": "Frederic Boesel",
        "type": "Person"
      },
      {
        "name": "Tim Dockhorn",
        "type": "Person"
      },
      {
        "name": "Takuya Akiba",
        "type": "Person"
      },
      {
        "name": "Makoto Shing",
        "type": "Person"
      },
      {
        "name": "Yujin Tang",
        "type": "Person"
      },
      {
        "name": "Qi Sun",
        "type": "Person"
      },
      {
        "name": "David Ha",
        "type": "Person"
      },
      {
        "name": "Qiang Zhang",
        "type": "Person"
      },
      {
        "name": "Xun Huang",
        "type": "Person"
      },
      {
        "name": "Zinan Guo",
        "type": "Person"
      },
      {
        "name": "Yanze Wu",
        "type": "Person"
      },
      {
        "name": "Zhuowei Chen",
        "type": "Person"
      },
      {
        "name": "Lang Chen",
        "type": "Person"
      },
      {
        "name": "Peng Zhang",
        "type": "Person"
      },
      {
        "name": "Qian He",
        "type": "Person"
      },
      {
        "name": "Jonathan Ho",
        "type": "Person"
      },
      {
        "name": "Ajay Jain",
        "type": "Person"
      },
      {
        "name": "Pieter Abbeel",
        "type": "Person"
      },
      {
        "name": "Weijie Kong",
        "type": "Person"
      },
      {
        "name": "Qi Tian",
        "type": "Person"
      },
      {
        "name": "Zijian Zhang",
        "type": "Person"
      },
      {
        "name": "Rox Min",
        "type": "Person"
      },
      {
        "name": "Zuozhuo Dai",
        "type": "Person"
      },
      {
        "name": "Jin Zhou",
        "type": "Person"
      },
      {
        "name": "Jiangfeng Xiong",
        "type": "Person"
      },
      {
        "name": "Xin Li",
        "type": "Person"
      },
      {
        "name": "Bo Wu",
        "type": "Person"
      },
      {
        "name": "Jianwei Zhang",
        "type": "Person"
      },
      {
        "name": "Kathrina Wu",
        "type": "Person"
      },
      {
        "name": "Qin Lin",
        "type": "Person"
      },
      {
        "name": "Junkun Yuan",
        "type": "Person"
      },
      {
        "name": "Yanxin Long",
        "type": "Person"
      },
      {
        "name": "Aladdin Wang",
        "type": "Person"
      },
      {
        "name": "Andong Wang",
        "type": "Person"
      },
      {
        "name": "Changlin Li",
        "type": "Person"
      },
      {
        "name": "Duojun Huang",
        "type": "Person"
      },
      {
        "name": "Fang Yang",
        "type": "Person"
      },
      {
        "name": "Hao Tan",
        "type": "Person"
      },
      {
        "name": "Hongmei Wang",
        "type": "Person"
      },
      {
        "name": "Jacob Song",
        "type": "Person"
      },
      {
        "name": "Jiawang Bai",
        "type": "Person"
      },
      {
        "name": "Jianbing Wu",
        "type": "Person"
      },
      {
        "name": "Jinbao Xue",
        "type": "Person"
      },
      {
        "name": "Joey Wang",
        "type": "Person"
      },
      {
        "name": "Kai Wang",
        "type": "Person"
      },
      {
        "name": "Mengyang Liu",
        "type": "Person"
      },
      {
        "name": "Pengyu Li",
        "type": "Person"
      },
      {
        "name": "Shuai Li",
        "type": "Person"
      },
      {
        "name": "Weiyan Wang",
        "type": "Person"
      },
      {
        "name": "Wenqing Yu",
        "type": "Person"
      },
      {
        "name": "Xinchi Deng",
        "type": "Person"
      },
      {
        "name": "Yi Chen",
        "type": "Person"
      },
      {
        "name": "Yutao Cui",
        "type": "Person"
      },
      {
        "name": "Yuanbo Peng",
        "type": "Person"
      },
      {
        "name": "Zhentao Yu",
        "type": "Person"
      },
      {
        "name": "Zhiyu He",
        "type": "Person"
      },
      {
        "name": "Zhiyong Xu",
        "type": "Person"
      },
      {
        "name": "Zixiang Zhou",
        "type": "Person"
      },
      {
        "name": "Zunnan Xu",
        "type": "Person"
      },
      {
        "name": "Yangyu Tao",
        "type": "Person"
      },
      {
        "name": "Qinglin Lu",
        "type": "Person"
      },
      {
        "name": "Songtao Liu",
        "type": "Person"
      },
      {
        "name": "Dax Zhou",
        "type": "Person"
      },
      {
        "name": "Hongfa Wang",
        "type": "Person"
      },
      {
        "name": "Yong Yang",
        "type": "Person"
      },
      {
        "name": "Di Wang",
        "type": "Person"
      },
      {
        "name": "Yuhong Liu",
        "type": "Person"
      },
      {
        "name": "Jie Jiang",
        "type": "Person"
      },
      {
        "name": "Caesar Zhong",
        "type": "Person"
      },
      {
        "name": "Jianwen Jiang",
        "type": "Person"
      },
      {
        "name": "Chao Liang",
        "type": "Person"
      },
      {
        "name": "Jiaqi Yang",
        "type": "Person"
      },
      {
        "name": "Gaojie Lin",
        "type": "Person"
      },
      {
        "name": "Tianyun Zhong",
        "type": "Person"
      },
      {
        "name": "Yanbo Zheng",
        "type": "Person"
      },
      {
        "name": "Zerong Zheng",
        "type": "Person"
      },
      {
        "name": "Jiahao Cui",
        "type": "Person"
      },
      {
        "name": "Hui Li",
        "type": "Person"
      },
      {
        "name": "Yao Yao",
        "type": "Person"
      },
      {
        "name": "Hao Zhu",
        "type": "Person"
      },
      {
        "name": "Hanlin Shang",
        "type": "Person"
      },
      {
        "name": "Kaihui Cheng",
        "type": "Person"
      },
      {
        "name": "Hang Zhou",
        "type": "Person"
      },
      {
        "name": "Siyu Zhu",
        "type": "Person"
      },
      {
        "name": "Jingdong Wang",
        "type": "Person"
      },
      {
        "name": "Rang Meng",
        "type": "Person"
      },
      {
        "name": "Xingyu Zhang",
        "type": "Person"
      },
      {
        "name": "Bharath Hariharan",
        "type": "Person"
      },
      {
        "name": "Kyunghyun Cho",
        "type": "Person"
      },
      {
        "name": "Bart van Merrienboer",
        "type": "Person"
      },
      {
        "name": "Caglar Gulcehre",
        "type": "Person"
      },
      {
        "name": "Dzmitry Bahdanau",
        "type": "Person"
      },
      {
        "name": "Fethi Bougares",
        "type": "Person"
      },
      {
        "name": "Holger Schwenk",
        "type": "Person"
      },
      {
        "name": "Erfei Cui",
        "type": "Person"
      },
      {
        "name": "Wenhai Wang",
        "type": "Person"
      },
      {
        "name": "Zhiqi Li",
        "type": "Person"
      },
      {
        "name": "Jiangwei Xie",
        "type": "Person"
      },
      {
        "name": "Haoming Zou",
        "type": "Person"
      },
      {
        "name": "Hanming Deng",
        "type": "Person"
      },
      {
        "name": "Gen Luo",
        "type": "Person"
      },
      {
        "name": "Lewei Lu",
        "type": "Person"
      },
      {
        "name": "Xizhou Zhu",
        "type": "Person"
      },
      {
        "name": "Jifeng Dai",
        "type": "Person"
      },
      {
        "name": "Shenyuan Gao",
        "type": "Person"
      },
      {
        "name": "Jiazhi Yang",
        "type": "Person"
      },
      {
        "name": "Li Chen",
        "type": "Person"
      },
      {
        "name": "Kashyap Chitta",
        "type": "Person"
      },
      {
        "name": "Yihang Qiu",
        "type": "Person"
      },
      {
        "name": "Andreas Geiger",
        "type": "Person"
      },
      {
        "name": "Jun Zhang",
        "type": "Person"
      },
      {
        "name": "Hongyang Li",
        "type": "Person"
      },
      {
        "name": "Bencheng Liao",
        "type": "Person"
      },
      {
        "name": "Shaoyu Chen",
        "type": "Person"
      },
      {
        "name": "Haoran Yin",
        "type": "Person"
      },
      {
        "name": "Bo Jiang",
        "type": "Person"
      },
      {
        "name": "Cheng Wang",
        "type": "Person"
      },
      {
        "name": "Sixu Yan",
        "type": "Person"
      },
      {
        "name": "Xinbang Zhang",
        "type": "Person"
      },
      {
        "name": "Xiangyu Li",
        "type": "Person"
      },
      {
        "name": "Ying Zhang",
        "type": "Person"
      },
      {
        "name": "Qian Zhang",
        "type": "Person"
      },
      {
        "name": "Bingyi Kang",
        "type": "Person"
      },
      {
        "name": "Yang Yue",
        "type": "Person"
      },
      {
        "name": "Rui Lu",
        "type": "Person"
      },
      {
        "name": "Zhijie Lin",
        "type": "Person"
      },
      {
        "name": "Yang Zhao",
        "type": "Person"
      },
      {
        "name": "Kaixin Wang",
        "type": "Person"
      },
      {
        "name": "Gao Huang",
        "type": "Person"
      },
      {
        "name": "Jiashi Feng",
        "type": "Person"
      },
      {
        "name": "Jyh-Jing Hwang",
        "type": "Person"
      },
      {
        "name": "Runsheng Xu",
        "type": "Person"
      },
      {
        "name": "Hubert Lin",
        "type": "Person"
      },
      {
        "name": "Wei-Chih Hung",
        "type": "Person"
      },
      {
        "name": "Jingwei Ji",
        "type": "Person"
      },
      {
        "name": "Kristy Choi",
        "type": "Person"
      },
      {
        "name": "Di Huang",
        "type": "Person"
      },
      {
        "name": "Tong He",
        "type": "Person"
      },
      {
        "name": "Paul Covington",
        "type": "Person"
      },
      {
        "name": "Benjamin Sapp",
        "type": "Person"
      },
      {
        "name": "Tom B. Brown",
        "type": "Person"
      },
      {
        "name": "Benjamin Mann",
        "type": "Person"
      },
      {
        "name": "Nick Ryder",
        "type": "Person"
      },
      {
        "name": "Melanie Subbiah",
        "type": "Person"
      },
      {
        "name": "Jared Kaplan",
        "type": "Person"
      },
      {
        "name": "Prafulla Dhariwal",
        "type": "Person"
      },
      {
        "name": "Arvind Neelakantan",
        "type": "Person"
      },
      {
        "name": "Pranav Shyam",
        "type": "Person"
      },
      {
        "name": "Ariel Herbert-Voss",
        "type": "Person"
      },
      {
        "name": "Tom Henighan",
        "type": "Person"
      },
      {
        "name": "Rewon Child",
        "type": "Person"
      },
      {
        "name": "Daniel M. Ziegler",
        "type": "Person"
      },
      {
        "name": "Jeffrey Wu",
        "type": "Person"
      },
      {
        "name": "Clemens Winter",
        "type": "Person"
      },
      {
        "name": "Christopher Hesse",
        "type": "Person"
      },
      {
        "name": "Mark Chen",
        "type": "Person"
      },
      {
        "name": "Eric Sigler",
        "type": "Person"
      },
      {
        "name": "Mateusz Litwin",
        "type": "Person"
      },
      {
        "name": "Scott Gray",
        "type": "Person"
      },
      {
        "name": "Benjamin Chess",
        "type": "Person"
      },
      {
        "name": "Christopher Berner",
        "type": "Person"
      },
      {
        "name": "Sam McCandlish",
        "type": "Person"
      },
      {
        "name": "Dario Amodei",
        "type": "Person"
      },
      {
        "name": "Zhe Chen",
        "type": "Person"
      },
      {
        "name": "Weiyun Wang",
        "type": "Person"
      },
      {
        "name": "Yue Cao",
        "type": "Person"
      },
      {
        "name": "Yangzhou Liu",
        "type": "Person"
      },
      {
        "name": "Zhangwei Gao",
        "type": "Person"
      },
      {
        "name": "Jinguo Zhu",
        "type": "Person"
      },
      {
        "name": "Shenglong Ye",
        "type": "Person"
      },
      {
        "name": "Hao Tian",
        "type": "Person"
      },
      {
        "name": "Zhaoyang Liu",
        "type": "Person"
      },
      {
        "name": "Lixin Gu",
        "type": "Person"
      },
      {
        "name": "Xuehui Wang",
        "type": "Person"
      },
      {
        "name": "Qingyun Li",
        "type": "Person"
      },
      {
        "name": "Yiming Ren",
        "type": "Person"
      },
      {
        "name": "Zixuan Chen",
        "type": "Person"
      },
      {
        "name": "Jiapeng Luo",
        "type": "Person"
      },
      {
        "name": "Jiahao Wang",
        "type": "Person"
      },
      {
        "name": "Tan Jiang",
        "type": "Person"
      },
      {
        "name": "Conghui He",
        "type": "Person"
      },
      {
        "name": "Botian Shi",
        "type": "Person"
      },
      {
        "name": "Xingcheng Zhang",
        "type": "Person"
      },
      {
        "name": "Han Lv",
        "type": "Person"
      },
      {
        "name": "Yi Wang",
        "type": "Person"
      },
      {
        "name": "Wenqi Shao",
        "type": "Person"
      },
      {
        "name": "Pei Chu",
        "type": "Person"
      },
      {
        "name": "Zhongying Tu",
        "type": "Person"
      },
      {
        "name": "Zhiyong Wu",
        "type": "Person"
      },
      {
        "name": "Huipeng Deng",
        "type": "Person"
      },
      {
        "name": "Jiaye Ge",
        "type": "Person"
      },
      {
        "name": "Kai Chen",
        "type": "Person"
      },
      {
        "name": "Kaipeng Zhang",
        "type": "Person"
      },
      {
        "name": "Limin Wang",
        "type": "Person"
      },
      {
        "name": "Min Dou",
        "type": "Person"
      },
      {
        "name": "Tong Lu",
        "type": "Person"
      },
      {
        "name": "Dahua Lin",
        "type": "Person"
      },
      {
        "name": "Yu Qiao",
        "type": "Person"
      },
      {
        "name": "Yuchen Duan",
        "type": "Person"
      },
      {
        "name": "Weijie Su",
        "type": "Person"
      },
      {
        "name": "Jie Shao",
        "type": "Person"
      },
      {
        "name": "Xingguang Wei",
        "type": "Person"
      },
      {
        "name": "Hongjie Zhang",
        "type": "Person"
      },
      {
        "name": "Haomin Wang",
        "type": "Person"
      },
      {
        "name": "Weiye Xu",
        "type": "Person"
      },
      {
        "name": "Hao Li",
        "type": "Person"
      },
      {
        "name": "Nianchen Deng",
        "type": "Person"
      },
      {
        "name": "Songze Li",
        "type": "Person"
      },
      {
        "name": "Yinan He",
        "type": "Person"
      },
      {
        "name": "Junjun He",
        "type": "Person"
      },
      {
        "name": "Yingtong Xiong",
        "type": "Person"
      },
      {
        "name": "Wenwen Qu",
        "type": "Person"
      },
      {
        "name": "Peng Sun",
        "type": "Person"
      },
      {
        "name": "Penglong Jiao",
        "type": "Person"
      },
      {
        "name": "Lijun Wu",
        "type": "Person"
      },
      {
        "name": "Haodong Duan",
        "type": "Person"
      },
      {
        "name": "Xinyu Fang",
        "type": "Person"
      },
      {
        "name": "Junming Yang",
        "type": "Person"
      },
      {
        "name": "Xiangyu Zhao",
        "type": "Person"
      },
      {
        "name": "Yuxuan Qiao",
        "type": "Person"
      },
      {
        "name": "Mo Li",
        "type": "Person"
      },
      {
        "name": "Amit Agarwal",
        "type": "Person"
      },
      {
        "name": "Lin Chen",
        "type": "Person"
      },
      {
        "name": "Yuan Liu",
        "type": "Person"
      },
      {
        "name": "Yubo Ma",
        "type": "Person"
      },
      {
        "name": "Hailong Sun",
        "type": "Person"
      },
      {
        "name": "Yifan Zhang",
        "type": "Person"
      },
      {
        "name": "Tack Hwa Wong",
        "type": "Person"
      },
      {
        "name": "Peiheng Zhou",
        "type": "Person"
      },
      {
        "name": "Xiaozhe Li",
        "type": "Person"
      },
      {
        "name": "Chaoyou Fu",
        "type": "Person"
      },
      {
        "name": "Junbo Cui",
        "type": "Person"
      },
      {
        "name": "Jixuan Chen",
        "type": "Person"
      },
      {
        "name": "Enxin Song",
        "type": "Person"
      },
      {
        "name": "Song Mao",
        "type": "Person"
      },
      {
        "name": "Shengyuan Ding",
        "type": "Person"
      },
      {
        "name": "Tianhao Liang",
        "type": "Person"
      },
      {
        "name": "Zicheng Zhang",
        "type": "Person"
      },
      {
        "name": "Xiaoyi Dong",
        "type": "Person"
      },
      {
        "name": "Yuhang Zang",
        "type": "Person"
      },
      {
        "name": "Pan Zhang",
        "type": "Person"
      },
      {
        "name": "Jiaqi Wang",
        "type": "Person"
      },
      {
        "name": "Guowei Xu",
        "type": "Person"
      },
      {
        "name": "Peng Jin",
        "type": "Person"
      },
      {
        "name": "Ziang Wu",
        "type": "Person"
      },
      {
        "name": "Yibing Song",
        "type": "Person"
      },
      {
        "name": "Lichao Sun",
        "type": "Person"
      },
      {
        "name": "Li Yuan",
        "type": "Person"
      },
      {
        "name": "Hengjun Pu",
        "type": "Person"
      },
      {
        "name": "Long Cui",
        "type": "Person"
      },
      {
        "name": "Linglin Jing",
        "type": "Person"
      },
      {
        "name": "Zhaokai Wang",
        "type": "Person"
      },
      {
        "name": "Ganlin Yang",
        "type": "Person"
      },
      {
        "name": "Qi Wei",
        "type": "Person"
      },
      {
        "name": "Jinhui Yin",
        "type": "Person"
      },
      {
        "name": "Wenhao Li",
        "type": "Person"
      },
      {
        "name": "Guanzhou Chen",
        "type": "Person"
      },
      {
        "name": "Zichen Ding",
        "type": "Person"
      },
      {
        "name": "Changyao Tian",
        "type": "Person"
      },
      {
        "name": "Zhenyu Wu",
        "type": "Person"
      },
      {
        "name": "Jingjing Xie",
        "type": "Person"
      },
      {
        "name": "Zehao Li",
        "type": "Person"
      },
      {
        "name": "Bowen Yang",
        "type": "Person"
      },
      {
        "name": "Zhi Hou",
        "type": "Person"
      },
      {
        "name": "Haoran Hao",
        "type": "Person"
      },
      {
        "name": "Tianyi Zhang",
        "type": "Person"
      },
      {
        "name": "Bin Fu",
        "type": "Person"
      },
      {
        "name": "Biqing Qi",
        "type": "Person"
      },
      {
        "name": "Qipeng Guo",
        "type": "Person"
      },
      {
        "name": "Wenwei Zhang",
        "type": "Person"
      },
      {
        "name": "Songyang Zhang",
        "type": "Person"
      },
      {
        "name": "Maosong Cao",
        "type": "Person"
      },
      {
        "name": "Junyao Lin",
        "type": "Person"
      },
      {
        "name": "Kexian Tang",
        "type": "Person"
      },
      {
        "name": "Jianfei Gao",
        "type": "Person"
      },
      {
        "name": "Haian Huang",
        "type": "Person"
      },
      {
        "name": "Yuzhe Gu",
        "type": "Person"
      },
      {
        "name": "Chengqi Lyu",
        "type": "Person"
      },
      {
        "name": "Huanze Tang",
        "type": "Person"
      },
      {
        "name": "Rui Wang",
        "type": "Person"
      },
      {
        "name": "Haijun Lv",
        "type": "Person"
      },
      {
        "name": "Bowen Zhou",
        "type": "Person"
      },
      {
        "name": "John Schulman",
        "type": "Person"
      },
      {
        "name": "Filip Wolski",
        "type": "Person"
      },
      {
        "name": "Oleg Klimov",
        "type": "Person"
      },
      {
        "name": "Yangguang Li",
        "type": "Person"
      },
      {
        "name": "Jiaheng Liu",
        "type": "Person"
      },
      {
        "name": "Zeyue Xue",
        "type": "Person"
      },
      {
        "name": "Yu Gao",
        "type": "Person"
      },
      {
        "name": "Fangyuan Kong",
        "type": "Person"
      },
      {
        "name": "Lingting Zhu",
        "type": "Person"
      },
      {
        "name": "Mengzhao Chen",
        "type": "Person"
      },
      {
        "name": "Qiushan Guo",
        "type": "Person"
      },
      {
        "name": "Weilin Huang",
        "type": "Person"
      },
      {
        "name": "Haoyuan Guo",
        "type": "Person"
      },
      {
        "name": "Tuyen Hoang",
        "type": "Person"
      },
      {
        "name": "Lu Jiang",
        "type": "Person"
      },
      {
        "name": "Huixia Li",
        "type": "Person"
      },
      {
        "name": "Jiashi Li",
        "type": "Person"
      },
      {
        "name": "Liang Li",
        "type": "Person"
      },
      {
        "name": "Xiaojie Li",
        "type": "Person"
      },
      {
        "name": "Xunsong Li",
        "type": "Person"
      },
      {
        "name": "Yifu Li",
        "type": "Person"
      },
      {
        "name": "Jiawei Liu",
        "type": "Person"
      },
      {
        "name": "Shu Liu",
        "type": "Person"
      },
      {
        "name": "Xiaonan Nie",
        "type": "Person"
      },
      {
        "name": "Zhiwu Qing",
        "type": "Person"
      },
      {
        "name": "Li Sun",
        "type": "Person"
      },
      {
        "name": "Zhi Tian",
        "type": "Person"
      },
      {
        "name": "Sen Wang",
        "type": "Person"
      },
      {
        "name": "Guoqiang Wei",
        "type": "Person"
      },
      {
        "name": "Guohong Wu",
        "type": "Person"
      },
      {
        "name": "Ruiqi Xia",
        "type": "Person"
      },
      {
        "name": "Fei Xiao",
        "type": "Person"
      },
      {
        "name": "Xuefeng Xiao",
        "type": "Person"
      },
      {
        "name": "Jiangqiao Yan",
        "type": "Person"
      },
      {
        "name": "Ceyuan Yang",
        "type": "Person"
      },
      {
        "name": "Jianchao Yang",
        "type": "Person"
      },
      {
        "name": "Runkai Yang",
        "type": "Person"
      },
      {
        "name": "Tao Yang",
        "type": "Person"
      },
      {
        "name": "Yihang Yang",
        "type": "Person"
      },
      {
        "name": "Zilyu Ye",
        "type": "Person"
      },
      {
        "name": "Xuejiao Zeng",
        "type": "Person"
      },
      {
        "name": "Yan Zeng",
        "type": "Person"
      },
      {
        "name": "Heng Zhang",
        "type": "Person"
      },
      {
        "name": "Xiaozheng Zheng",
        "type": "Person"
      },
      {
        "name": "Peihao Zhu",
        "type": "Person"
      },
      {
        "name": "Jiaxin Zou",
        "type": "Person"
      },
      {
        "name": "Feilong Zuo",
        "type": "Person"
      },
      {
        "name": "Guibin Chen",
        "type": "Person"
      },
      {
        "name": "Dixuan Lin",
        "type": "Person"
      },
      {
        "name": "Jiangping Yang",
        "type": "Person"
      },
      {
        "name": "Chunze Lin",
        "type": "Person"
      },
      {
        "name": "Junchen Zhu",
        "type": "Person"
      },
      {
        "name": "Mingyuan Fan",
        "type": "Person"
      },
      {
        "name": "Hao Zhang",
        "type": "Person"
      },
      {
        "name": "Sheng Chen",
        "type": "Person"
      },
      {
        "name": "Zheng Chen",
        "type": "Person"
      },
      {
        "name": "Chengcheng Ma",
        "type": "Person"
      },
      {
        "name": "Weiming Xiong",
        "type": "Person"
      },
      {
        "name": "Wei Wang",
        "type": "Person"
      },
      {
        "name": "Nuo Pang",
        "type": "Person"
      },
      {
        "name": "Kang Kang",
        "type": "Person"
      },
      {
        "name": "Zhiheng Xu",
        "type": "Person"
      },
      {
        "name": "Yuzhe Jin",
        "type": "Person"
      },
      {
        "name": "Yupeng Liang",
        "type": "Person"
      },
      {
        "name": "Yubing Song",
        "type": "Person"
      },
      {
        "name": "Peng Zhao",
        "type": "Person"
      },
      {
        "name": "Boyuan Xu",
        "type": "Person"
      },
      {
        "name": "Di Qiu",
        "type": "Person"
      },
      {
        "name": "Debang Li",
        "type": "Person"
      },
      {
        "name": "Zhengcong Fei",
        "type": "Person"
      },
      {
        "name": "Yahui Zhou",
        "type": "Person"
      },
      {
        "name": "Yibin Wang",
        "type": "Person"
      },
      {
        "name": "Cheng Jin",
        "type": "Person"
      },
      {
        "name": "知秀 柴田",
        "type": "Person"
      },
      {
        "name": "Pranav Rajpurkar",
        "type": "Person"
      },
      {
        "name": "Konstantin Lopyrev",
        "type": "Person"
      },
      {
        "name": "Percy Liang",
        "type": "Person"
      },
      {
        "name": "Rico Sennrich",
        "type": "Person"
      },
      {
        "name": "Barry Haddow",
        "type": "Person"
      },
      {
        "name": "Alexandra Birch",
        "type": "Person"
      },
      {
        "name": "Taku Kudo",
        "type": "Person"
      },
      {
        "name": "John Richardson",
        "type": "Person"
      },
      {
        "name": "OpenAI",
        "type": "Person"
      },
      {
        "name": "Josh Achiam",
        "type": "Person"
      },
      {
        "name": "Steven Adler",
        "type": "Person"
      },
      {
        "name": "Lama Ahmad",
        "type": "Person"
      },
      {
        "name": "Ilge Akkaya",
        "type": "Person"
      },
      {
        "name": "Florencia Leoni Aleman",
        "type": "Person"
      },
      {
        "name": "Diogo Almeida",
        "type": "Person"
      },
      {
        "name": "Janko Altenschmidt",
        "type": "Person"
      },
      {
        "name": "Sam Altman",
        "type": "Person"
      },
      {
        "name": "Shyamal Anadkat",
        "type": "Person"
      },
      {
        "name": "Red Avila",
        "type": "Person"
      },
      {
        "name": "Igor Babuschkin",
        "type": "Person"
      },
      {
        "name": "Suchir Balaji",
        "type": "Person"
      },
      {
        "name": "Valerie Balcom",
        "type": "Person"
      },
      {
        "name": "Paul Baltescu",
        "type": "Person"
      },
      {
        "name": "Haiming Bao",
        "type": "Person"
      },
      {
        "name": "Mohammad Bavarian",
        "type": "Person"
      },
      {
        "name": "Jeff Belgum",
        "type": "Person"
      },
      {
        "name": "Irwan Bello",
        "type": "Person"
      },
      {
        "name": "Jake Berdine",
        "type": "Person"
      },
      {
        "name": "Gabriel Bernadett-Shapiro",
        "type": "Person"
      },
      {
        "name": "Lenny Bogdonoff",
        "type": "Person"
      },
      {
        "name": "Oleg Boiko",
        "type": "Person"
      },
      {
        "name": "Madelaine Boyd",
        "type": "Person"
      },
      {
        "name": "Anna-Luisa Brakman",
        "type": "Person"
      },
      {
        "name": "Greg Brockman",
        "type": "Person"
      },
      {
        "name": "Tim Brooks",
        "type": "Person"
      },
      {
        "name": "Miles Brundage",
        "type": "Person"
      },
      {
        "name": "Kevin Button",
        "type": "Person"
      },
      {
        "name": "Trevor Cai",
        "type": "Person"
      },
      {
        "name": "Rosie Campbell",
        "type": "Person"
      },
      {
        "name": "Andrew Cann",
        "type": "Person"
      },
      {
        "name": "Brittany Carey",
        "type": "Person"
      },
      {
        "name": "Chelsea Carlson",
        "type": "Person"
      },
      {
        "name": "Rory Carmichael",
        "type": "Person"
      },
      {
        "name": "Brooke Chan",
        "type": "Person"
      },
      {
        "name": "Che Chang",
        "type": "Person"
      },
      {
        "name": "Fotis Chantzis",
        "type": "Person"
      },
      {
        "name": "Derek Chen",
        "type": "Person"
      },
      {
        "name": "Sully Chen",
        "type": "Person"
      },
      {
        "name": "Ruby Chen",
        "type": "Person"
      },
      {
        "name": "Jason Chen",
        "type": "Person"
      },
      {
        "name": "Ben Chess",
        "type": "Person"
      },
      {
        "name": "Chester Cho",
        "type": "Person"
      },
      {
        "name": "Casey Chu",
        "type": "Person"
      },
      {
        "name": "Dave Cummings",
        "type": "Person"
      },
      {
        "name": "Jeremiah Currier",
        "type": "Person"
      },
      {
        "name": "Yunxing Dai",
        "type": "Person"
      },
      {
        "name": "Cory Decareaux",
        "type": "Person"
      },
      {
        "name": "Thomas Degry",
        "type": "Person"
      },
      {
        "name": "Noah Deutsch",
        "type": "Person"
      },
      {
        "name": "Damien Deville",
        "type": "Person"
      },
      {
        "name": "Arka Dhar",
        "type": "Person"
      },
      {
        "name": "David Dohan",
        "type": "Person"
      },
      {
        "name": "Steve Dowling",
        "type": "Person"
      },
      {
        "name": "Sheila Dunning",
        "type": "Person"
      },
      {
        "name": "Adrien Ecoffet",
        "type": "Person"
      },
      {
        "name": "Atty Eleti",
        "type": "Person"
      },
      {
        "name": "Tyna Eloundou",
        "type": "Person"
      },
      {
        "name": "David Farhi",
        "type": "Person"
      },
      {
        "name": "Liam Fedus",
        "type": "Person"
      },
      {
        "name": "Niko Felix",
        "type": "Person"
      },
      {
        "name": "Simón Posada Fishman",
        "type": "Person"
      },
      {
        "name": "Juston Forte",
        "type": "Person"
      },
      {
        "name": "Isabella Fulford",
        "type": "Person"
      },
      {
        "name": "Leo Gao",
        "type": "Person"
      },
      {
        "name": "Elie Georges",
        "type": "Person"
      },
      {
        "name": "Christian Gibson",
        "type": "Person"
      },
      {
        "name": "Vik Goel",
        "type": "Person"
      },
      {
        "name": "Tarun Gogineni",
        "type": "Person"
      },
      {
        "name": "Rapha Gontijo-Lopes",
        "type": "Person"
      },
      {
        "name": "Jonathan Gordon",
        "type": "Person"
      },
      {
        "name": "Morgan Grafstein",
        "type": "Person"
      },
      {
        "name": "Ryan Greene",
        "type": "Person"
      },
      {
        "name": "Joshua Gross",
        "type": "Person"
      },
      {
        "name": "Yufei Guo",
        "type": "Person"
      },
      {
        "name": "Jesse Han",
        "type": "Person"
      },
      {
        "name": "Jeff Harris",
        "type": "Person"
      },
      {
        "name": "Yuchen He",
        "type": "Person"
      },
      {
        "name": "Mike Heaton",
        "type": "Person"
      },
      {
        "name": "Johannes Heidecke",
        "type": "Person"
      },
      {
        "name": "Chris Hesse",
        "type": "Person"
      },
      {
        "name": "Alan Hickey",
        "type": "Person"
      },
      {
        "name": "Wade Hickey",
        "type": "Person"
      },
      {
        "name": "Peter Hoeschele",
        "type": "Person"
      },
      {
        "name": "Brandon Houghton",
        "type": "Person"
      },
      {
        "name": "Kenny Hsu",
        "type": "Person"
      },
      {
        "name": "Shengli Hu",
        "type": "Person"
      },
      {
        "name": "Xin Hu",
        "type": "Person"
      },
      {
        "name": "Joost Huizinga",
        "type": "Person"
      },
      {
        "name": "Shantanu Jain",
        "type": "Person"
      },
      {
        "name": "Shawn Jain",
        "type": "Person"
      },
      {
        "name": "Joanne Jang",
        "type": "Person"
      },
      {
        "name": "Angela Jiang",
        "type": "Person"
      },
      {
        "name": "Roger Jiang",
        "type": "Person"
      },
      {
        "name": "Haozhun Jin",
        "type": "Person"
      },
      {
        "name": "Denny Jin",
        "type": "Person"
      },
      {
        "name": "Shino Jomoto",
        "type": "Person"
      },
      {
        "name": "Billie Jonn",
        "type": "Person"
      },
      {
        "name": "Heewoo Jun",
        "type": "Person"
      },
      {
        "name": "Tomer Kaftan",
        "type": "Person"
      },
      {
        "name": "Łukasz Kaiser",
        "type": "Person"
      },
      {
        "name": "Ali Kamali",
        "type": "Person"
      },
      {
        "name": "Ingmar Kanitscheider",
        "type": "Person"
      },
      {
        "name": "Nitish Shirish Keskar",
        "type": "Person"
      },
      {
        "name": "Tabarak Khan",
        "type": "Person"
      },
      {
        "name": "Logan Kilpatrick",
        "type": "Person"
      },
      {
        "name": "Christina Kim",
        "type": "Person"
      },
      {
        "name": "Yongjik Kim",
        "type": "Person"
      },
      {
        "name": "Jan Hendrik Kirchner",
        "type": "Person"
      },
      {
        "name": "Jamie Kiros",
        "type": "Person"
      },
      {
        "name": "Matt Knight",
        "type": "Person"
      },
      {
        "name": "Daniel Kokotajlo",
        "type": "Person"
      },
      {
        "name": "Łukasz Kondraciuk",
        "type": "Person"
      },
      {
        "name": "Andrew Kondrich",
        "type": "Person"
      },
      {
        "name": "Aris Konstantinidis",
        "type": "Person"
      },
      {
        "name": "Kyle Kosic",
        "type": "Person"
      },
      {
        "name": "Vishal Kuo",
        "type": "Person"
      },
      {
        "name": "Michael Lampe",
        "type": "Person"
      },
      {
        "name": "Ikai Lan",
        "type": "Person"
      },
      {
        "name": "Teddy Lee",
        "type": "Person"
      },
      {
        "name": "Jan Leike",
        "type": "Person"
      },
      {
        "name": "Jade Leung",
        "type": "Person"
      },
      {
        "name": "Daniel Levy",
        "type": "Person"
      },
      {
        "name": "Chak Ming Li",
        "type": "Person"
      },
      {
        "name": "Rachel Lim",
        "type": "Person"
      },
      {
        "name": "Molly Lin",
        "type": "Person"
      },
      {
        "name": "Stephanie Lin",
        "type": "Person"
      },
      {
        "name": "Theresa Lopez",
        "type": "Person"
      },
      {
        "name": "Ryan Lowe",
        "type": "Person"
      },
      {
        "name": "Patricia Lue",
        "type": "Person"
      },
      {
        "name": "Anna Makanju",
        "type": "Person"
      },
      {
        "name": "Kim Malfacini",
        "type": "Person"
      },
      {
        "name": "Sam Manning",
        "type": "Person"
      },
      {
        "name": "Todor Markov",
        "type": "Person"
      },
      {
        "name": "Yaniv Markovski",
        "type": "Person"
      },
      {
        "name": "Bianca Martin",
        "type": "Person"
      },
      {
        "name": "Katie Mayer",
        "type": "Person"
      },
      {
        "name": "Andrew Mayne",
        "type": "Person"
      },
      {
        "name": "Bob McGrew",
        "type": "Person"
      },
      {
        "name": "Scott Mayer McKinney",
        "type": "Person"
      },
      {
        "name": "Christine McLeavey",
        "type": "Person"
      },
      {
        "name": "Paul McMillan",
        "type": "Person"
      },
      {
        "name": "Jake McNeil",
        "type": "Person"
      },
      {
        "name": "David Medina",
        "type": "Person"
      },
      {
        "name": "Aalok Mehta",
        "type": "Person"
      },
      {
        "name": "Jacob Menick",
        "type": "Person"
      },
      {
        "name": "Luke Metz",
        "type": "Person"
      },
      {
        "name": "Andrey Mishchenko",
        "type": "Person"
      },
      {
        "name": "Vinnie Monaco",
        "type": "Person"
      },
      {
        "name": "Evan Morikawa",
        "type": "Person"
      },
      {
        "name": "Daniel Mossing",
        "type": "Person"
      },
      {
        "name": "Tong Mu",
        "type": "Person"
      },
      {
        "name": "Mira Murati",
        "type": "Person"
      },
      {
        "name": "Oleg Murk",
        "type": "Person"
      },
      {
        "name": "David Mély",
        "type": "Person"
      },
      {
        "name": "Ashvin Nair",
        "type": "Person"
      },
      {
        "name": "Reiichiro Nakano",
        "type": "Person"
      },
      {
        "name": "Rajeev Nayak",
        "type": "Person"
      },
      {
        "name": "Richard Ngo",
        "type": "Person"
      },
      {
        "name": "Hyeonwoo Noh",
        "type": "Person"
      },
      {
        "name": "Long Ouyang",
        "type": "Person"
      },
      {
        "name": "Cullen O'Keefe",
        "type": "Person"
      },
      {
        "name": "Jakub Pachocki",
        "type": "Person"
      },
      {
        "name": "Alex Paino",
        "type": "Person"
      },
      {
        "name": "Joe Palermo",
        "type": "Person"
      },
      {
        "name": "Ashley Pantuliano",
        "type": "Person"
      },
      {
        "name": "Giambattista Parascandolo",
        "type": "Person"
      },
      {
        "name": "Joel Parish",
        "type": "Person"
      },
      {
        "name": "Emy Parparita",
        "type": "Person"
      },
      {
        "name": "Alex Passos",
        "type": "Person"
      },
      {
        "name": "Mikhail Pavlov",
        "type": "Person"
      },
      {
        "name": "Andrew Peng",
        "type": "Person"
      },
      {
        "name": "Adam Perelman",
        "type": "Person"
      },
      {
        "name": "Filipe de Avila Belbute Peres",
        "type": "Person"
      },
      {
        "name": "Michael Petrov",
        "type": "Person"
      },
      {
        "name": "Henrique Ponde de Oliveira Pinto",
        "type": "Person"
      },
      {
        "name": "Michael",
        "type": "Person"
      },
      {
        "name": "Pokorny",
        "type": "Person"
      },
      {
        "name": "Michelle Pokrass",
        "type": "Person"
      },
      {
        "name": "Vitchyr H. Pong",
        "type": "Person"
      },
      {
        "name": "Tolly Powell",
        "type": "Person"
      },
      {
        "name": "Alethea Power",
        "type": "Person"
      },
      {
        "name": "Boris Power",
        "type": "Person"
      },
      {
        "name": "Elizabeth Proehl",
        "type": "Person"
      },
      {
        "name": "Raul Puri",
        "type": "Person"
      },
      {
        "name": "Jack Rae",
        "type": "Person"
      },
      {
        "name": "Cameron Raymond",
        "type": "Person"
      },
      {
        "name": "Francis Real",
        "type": "Person"
      },
      {
        "name": "Kendra Rimbach",
        "type": "Person"
      },
      {
        "name": "Carl Ross",
        "type": "Person"
      },
      {
        "name": "Bob Rotsted",
        "type": "Person"
      },
      {
        "name": "Henri Roussez",
        "type": "Person"
      },
      {
        "name": "Mario Saltarelli",
        "type": "Person"
      },
      {
        "name": "Ted Sanders",
        "type": "Person"
      },
      {
        "name": "Shibani Santurkar",
        "type": "Person"
      },
      {
        "name": "Heather Schmidt",
        "type": "Person"
      },
      {
        "name": "David Schnurr",
        "type": "Person"
      },
      {
        "name": "Daniel Selsam",
        "type": "Person"
      },
      {
        "name": "Kyla Sheppard",
        "type": "Person"
      },
      {
        "name": "Toki Sherbakov",
        "type": "Person"
      },
      {
        "name": "Jessica Shieh",
        "type": "Person"
      },
      {
        "name": "Sarah Shoker",
        "type": "Person"
      },
      {
        "name": "Szymon Sidor",
        "type": "Person"
      },
      {
        "name": "Maddie Simens",
        "type": "Person"
      },
      {
        "name": "Jordan Sitkin",
        "type": "Person"
      },
      {
        "name": "Katarina Slama",
        "type": "Person"
      },
      {
        "name": "Ian Sohl",
        "type": "Person"
      },
      {
        "name": "Benjamin Sokolowsky",
        "type": "Person"
      },
      {
        "name": "Yang Song",
        "type": "Person"
      },
      {
        "name": "Natalie Staudacher",
        "type": "Person"
      },
      {
        "name": "Felipe Petroski Such",
        "type": "Person"
      },
      {
        "name": "Natalie Summers",
        "type": "Person"
      },
      {
        "name": "Jie Tang",
        "type": "Person"
      },
      {
        "name": "Nikolas Tezak",
        "type": "Person"
      },
      {
        "name": "Madeleine B. Thompson",
        "type": "Person"
      },
      {
        "name": "Phil Tillet",
        "type": "Person"
      },
      {
        "name": "Amin Tootoonchian",
        "type": "Person"
      },
      {
        "name": "Elizabeth Tseng",
        "type": "Person"
      },
      {
        "name": "Preston Tuggle",
        "type": "Person"
      },
      {
        "name": "Nick Turley",
        "type": "Person"
      },
      {
        "name": "Jerry Tworek",
        "type": "Person"
      },
      {
        "name": "Juan Felipe Cerón Uribe",
        "type": "Person"
      },
      {
        "name": "Andrea Vallone",
        "type": "Person"
      },
      {
        "name": "Arun Vijayvergiya",
        "type": "Person"
      },
      {
        "name": "Chelsea Voss",
        "type": "Person"
      },
      {
        "name": "Carroll Wainwright",
        "type": "Person"
      },
      {
        "name": "Justin Jay Wang",
        "type": "Person"
      },
      {
        "name": "Alvin Wang",
        "type": "Person"
      },
      {
        "name": "Ben Wang",
        "type": "Person"
      },
      {
        "name": "Jonathan Ward",
        "type": "Person"
      },
      {
        "name": "CJ Weinmann",
        "type": "Person"
      },
      {
        "name": "Akila Welihinda",
        "type": "Person"
      },
      {
        "name": "Peter Welinder",
        "type": "Person"
      },
      {
        "name": "Jiayi Weng",
        "type": "Person"
      },
      {
        "name": "Lilian Weng",
        "type": "Person"
      },
      {
        "name": "Matt Wiethoff",
        "type": "Person"
      },
      {
        "name": "Dave Willner",
        "type": "Person"
      },
      {
        "name": "Samuel Wolrich",
        "type": "Person"
      },
      {
        "name": "Hannah Wong",
        "type": "Person"
      },
      {
        "name": "Lauren Workman",
        "type": "Person"
      },
      {
        "name": "Sherwin Wu",
        "type": "Person"
      },
      {
        "name": "Jeff Wu",
        "type": "Person"
      },
      {
        "name": "Michael Wu",
        "type": "Person"
      },
      {
        "name": "Kai Xiao",
        "type": "Person"
      },
      {
        "name": "Tao Xu",
        "type": "Person"
      },
      {
        "name": "Sarah Yoo",
        "type": "Person"
      },
      {
        "name": "Kevin Yu",
        "type": "Person"
      },
      {
        "name": "Qiming Yuan",
        "type": "Person"
      },
      {
        "name": "Wojciech Zaremba",
        "type": "Person"
      },
      {
        "name": "Rowan Zellers",
        "type": "Person"
      },
      {
        "name": "Chong Zhang",
        "type": "Person"
      },
      {
        "name": "Marvin Zhang",
        "type": "Person"
      },
      {
        "name": "Shengjia Zhao",
        "type": "Person"
      },
      {
        "name": "Tianhao Zheng",
        "type": "Person"
      },
      {
        "name": "Juntang Zhuang",
        "type": "Person"
      },
      {
        "name": "William Zhuk",
        "type": "Person"
      },
      {
        "name": "Hugo Touvron",
        "type": "Person"
      },
      {
        "name": "Thibaut Lavril",
        "type": "Person"
      },
      {
        "name": "Gautier Izacard",
        "type": "Person"
      },
      {
        "name": "Xavier Martinet",
        "type": "Person"
      },
      {
        "name": "Marie-Anne Lachaux",
        "type": "Person"
      },
      {
        "name": "Timothée Lacroix",
        "type": "Person"
      },
      {
        "name": "Baptiste Rozière",
        "type": "Person"
      },
      {
        "name": "Naman Goyal",
        "type": "Person"
      },
      {
        "name": "Eric Hambro",
        "type": "Person"
      },
      {
        "name": "Faisal Azhar",
        "type": "Person"
      },
      {
        "name": "Aurelien Rodriguez",
        "type": "Person"
      },
      {
        "name": "Armand Joulin",
        "type": "Person"
      },
      {
        "name": "Edouard Grave",
        "type": "Person"
      },
      {
        "name": "Guillaume Lample",
        "type": "Person"
      },
      {
        "name": "Dale Schuurmans",
        "type": "Person"
      },
      {
        "name": "Maarten Bosma",
        "type": "Person"
      },
      {
        "name": "Brian Ichter",
        "type": "Person"
      },
      {
        "name": "Fei Xia",
        "type": "Person"
      },
      {
        "name": "Ed Chi",
        "type": "Person"
      },
      {
        "name": "Quoc Le",
        "type": "Person"
      },
      {
        "name": "Karl Cobbe",
        "type": "Person"
      },
      {
        "name": "Vineet Kosaraju",
        "type": "Person"
      },
      {
        "name": "Matthias Plappert",
        "type": "Person"
      },
      {
        "name": "Jacob Hilton",
        "type": "Person"
      },
      {
        "name": "Albert Gu",
        "type": "Person"
      },
      {
        "name": "Tri Dao",
        "type": "Person"
      },
      {
        "name": "Stephen M. Mount",
        "type": "Person"
      },
      {
        "name": "F. Crick",
        "type": "Person"
      },
      {
        "name": "Ilya Loshchilov",
        "type": "Person"
      },
      {
        "name": "Frank Hutter",
        "type": "Person"
      },
      {
        "name": "Azalia Mirhoseini",
        "type": "Person"
      },
      {
        "name": "Krzysztof Maziarz",
        "type": "Person"
      },
      {
        "name": "Andy Davis",
        "type": "Person"
      },
      {
        "name": "J. Ziv",
        "type": "Person"
      },
      {
        "name": "A. Lempel",
        "type": "Person"
      },
      {
        "name": "Stephen Merity",
        "type": "Person"
      },
      {
        "name": "Caiming Xiong",
        "type": "Person"
      },
      {
        "name": "James Bradbury",
        "type": "Person"
      },
      {
        "name": "Richard Socher",
        "type": "Person"
      },
      {
        "name": "Dan Hendrycks",
        "type": "Person"
      },
      {
        "name": "Collin Burns",
        "type": "Person"
      },
      {
        "name": "Steven Basart",
        "type": "Person"
      },
      {
        "name": "Andy Zou",
        "type": "Person"
      },
      {
        "name": "Mantas Mazeika",
        "type": "Person"
      },
      {
        "name": "Dawn Song",
        "type": "Person"
      },
      {
        "name": "Jacob Steinhardt",
        "type": "Person"
      },
      {
        "name": "Hunter Lightman",
        "type": "Person"
      },
      {
        "name": "Yura Burda",
        "type": "Person"
      },
      {
        "name": "Harri Edwards",
        "type": "Person"
      },
      {
        "name": "Bowen Baker",
        "type": "Person"
      },
      {
        "name": "David Rein",
        "type": "Person"
      },
      {
        "name": "Betty Li Hou",
        "type": "Person"
      },
      {
        "name": "Asa Cooper Stickland",
        "type": "Person"
      },
      {
        "name": "Jackson Petty",
        "type": "Person"
      },
      {
        "name": "Richard Yuanzhe Pang",
        "type": "Person"
      },
      {
        "name": "Julien Dirani",
        "type": "Person"
      },
      {
        "name": "Julian Michael",
        "type": "Person"
      },
      {
        "name": "Samuel R. Bowman",
        "type": "Person"
      },
      {
        "name": "Dmitry Lepikhin",
        "type": "Person"
      },
      {
        "name": "HyoukJoong Lee",
        "type": "Person"
      },
      {
        "name": "Yuanzhong Xu",
        "type": "Person"
      },
      {
        "name": "Dehao Chen",
        "type": "Person"
      },
      {
        "name": "Orhan Firat",
        "type": "Person"
      },
      {
        "name": "Maxim Krikun",
        "type": "Person"
      },
      {
        "name": "Jeff Donahue",
        "type": "Person"
      },
      {
        "name": "Trevor Darrell",
        "type": "Person"
      },
      {
        "name": "Jitendra Malik",
        "type": "Person"
      },
      {
        "name": "Shansong Liu",
        "type": "Person"
      },
      {
        "name": "Atin Sakkeer Hussain",
        "type": "Person"
      },
      {
        "name": "Qilong Wu",
        "type": "Person"
      },
      {
        "name": "Chenshuo Sun",
        "type": "Person"
      },
      {
        "name": "Ying Shan",
        "type": "Person"
      },
      {
        "name": "Jiahang Tu",
        "type": "Person"
      },
      {
        "name": "Ye Li",
        "type": "Person"
      },
      {
        "name": "Yiming Wu",
        "type": "Person"
      },
      {
        "name": "Hanbin Zhao",
        "type": "Person"
      },
      {
        "name": "Chao Zhang",
        "type": "Person"
      },
      {
        "name": "Hui Qian",
        "type": "Person"
      },
      {
        "name": "Haotian Lv",
        "type": "Person"
      },
      {
        "name": "Yuhui Zhang",
        "type": "Person"
      },
      {
        "name": "Jiangbo Dai",
        "type": "Person"
      },
      {
        "name": "Hanli Wu",
        "type": "Person"
      },
      {
        "name": "Jiaji Wang",
        "type": "Person"
      },
      {
        "name": "Dawei Wang",
        "type": "Person"
      },
      {
        "name": "Mingxin Li",
        "type": "Person"
      },
      {
        "name": "Yanzhao Zhang",
        "type": "Person"
      },
      {
        "name": "Dingkun Long",
        "type": "Person"
      },
      {
        "name": "Keqin Chen",
        "type": "Person"
      },
      {
        "name": "Sibo Song",
        "type": "Person"
      },
      {
        "name": "Shuai Bai",
        "type": "Person"
      },
      {
        "name": "Zhibo Yang",
        "type": "Person"
      },
      {
        "name": "Pengjun Xie",
        "type": "Person"
      },
      {
        "name": "An Yang",
        "type": "Person"
      },
      {
        "name": "Dayiheng Liu",
        "type": "Person"
      },
      {
        "name": "Jingren Zhou",
        "type": "Person"
      },
      {
        "name": "Junyang Lin",
        "type": "Person"
      },
      {
        "name": "Mingyue Chen",
        "type": "Person"
      },
      {
        "name": "Xin Liao",
        "type": "Person"
      },
      {
        "name": "Han Fang",
        "type": "Person"
      },
      {
        "name": "Jinlin Guo",
        "type": "Person"
      },
      {
        "name": "Yanxiang Chen",
        "type": "Person"
      },
      {
        "name": "Xiaoshuai Wu",
        "type": "Person"
      },
      {
        "name": "Radford M. Neal",
        "type": "Person"
      },
      {
        "name": "L. Breiman",
        "type": "Person"
      },
      {
        "name": "Guan Wang",
        "type": "Person"
      },
      {
        "name": "Yu Sun",
        "type": "Person"
      },
      {
        "name": "Jianxin Wang",
        "type": "Person"
      },
      {
        "name": "Mostafa Mehdipour-Ghazi",
        "type": "Person"
      },
      {
        "name": "B. Yanikoglu",
        "type": "Person"
      },
      {
        "name": "E. Aptoula",
        "type": "Person"
      },
      {
        "name": "Sue Han Lee",
        "type": "Person"
      },
      {
        "name": "H. Goëau",
        "type": "Person"
      },
      {
        "name": "P. Bonnet",
        "type": "Person"
      },
      {
        "name": "A. Joly",
        "type": "Person"
      },
      {
        "name": "Haiyan Zhang",
        "type": "Person"
      },
      {
        "name": "Jose Carranza-Rojas",
        "type": "Person"
      },
      {
        "name": "Hervé Goeau",
        "type": "Person"
      },
      {
        "name": "Erick Mata-Montero",
        "type": "Person"
      },
      {
        "name": "Zhuang Liu",
        "type": "Person"
      },
      {
        "name": "Laurens van der Maaten",
        "type": "Person"
      },
      {
        "name": "Kilian Q. Weinberger",
        "type": "Person"
      },
      {
        "name": "Mark Sandler",
        "type": "Person"
      },
      {
        "name": "Andrew Howard",
        "type": "Person"
      },
      {
        "name": "Menglong Zhu",
        "type": "Person"
      },
      {
        "name": "Andrey Zhmoginov",
        "type": "Person"
      },
      {
        "name": "Liang-Chieh Chen",
        "type": "Person"
      },
      {
        "name": "H. Brendan McMahan",
        "type": "Person"
      },
      {
        "name": "Eider Moore",
        "type": "Person"
      },
      {
        "name": "Daniel Ramage",
        "type": "Person"
      },
      {
        "name": "Seth Hampson",
        "type": "Person"
      },
      {
        "name": "Blaise Agüera y Arcas",
        "type": "Person"
      },
      {
        "name": "Fuzhen Zhuang",
        "type": "Person"
      },
      {
        "name": "Zhiyuan Qi",
        "type": "Person"
      },
      {
        "name": "Keyu Duan",
        "type": "Person"
      },
      {
        "name": "Dongbo Xi",
        "type": "Person"
      },
      {
        "name": "Yongchun Zhu",
        "type": "Person"
      },
      {
        "name": "Hengshu Zhu",
        "type": "Person"
      },
      {
        "name": "Hui Xiong",
        "type": "Person"
      },
      {
        "name": "Qing He",
        "type": "Person"
      },
      {
        "name": "A. Khan",
        "type": "Person"
      },
      {
        "name": "Wadii Boulila",
        "type": "Person"
      },
      {
        "name": "G. A. Sampedro",
        "type": "Person"
      },
      {
        "name": "Sidra Abbas",
        "type": "Person"
      },
      {
        "name": "Chitapong Wechtaisong",
        "type": "Person"
      },
      {
        "name": "Tesfahunegn Minwuyelet Mengistu",
        "type": "Person"
      },
      {
        "name": "Taewoon Kim",
        "type": "Person"
      },
      {
        "name": "Jenn-Wei Lin",
        "type": "Person"
      },
      {
        "name": "A. Alamer",
        "type": "Person"
      },
      {
        "name": "Manel Khazri Khlifi",
        "type": "Person"
      },
      {
        "name": "I. Farah",
        "type": "Person"
      },
      {
        "name": "Anwesha Mukherjee",
        "type": "Person"
      },
      {
        "name": "Rajkumar Buyya",
        "type": "Person"
      },
      {
        "name": "Alexander Kirillov",
        "type": "Person"
      },
      {
        "name": "Eric Mintun",
        "type": "Person"
      },
      {
        "name": "Nikhila Ravi",
        "type": "Person"
      },
      {
        "name": "Hanzi Mao",
        "type": "Person"
      },
      {
        "name": "Chloe Rolland",
        "type": "Person"
      },
      {
        "name": "Laura Gustafson",
        "type": "Person"
      },
      {
        "name": "Tete Xiao",
        "type": "Person"
      },
      {
        "name": "Spencer Whitehead",
        "type": "Person"
      },
      {
        "name": "Wan-Yen Lo",
        "type": "Person"
      },
      {
        "name": "Haotian Liu",
        "type": "Person"
      },
      {
        "name": "Qingyang Wu",
        "type": "Person"
      },
      {
        "name": "Yong Jae Lee",
        "type": "Person"
      },
      {
        "name": "Yuheng Li",
        "type": "Person"
      },
      {
        "name": "Zhiyuan You",
        "type": "Person"
      },
      {
        "name": "Jinjin Gu",
        "type": "Person"
      },
      {
        "name": "Xin Cai",
        "type": "Person"
      },
      {
        "name": "Zheyuan Li",
        "type": "Person"
      },
      {
        "name": "Kaiwen Zhu",
        "type": "Person"
      },
      {
        "name": "Chao Dong",
        "type": "Person"
      },
      {
        "name": "Tianfan Xue",
        "type": "Person"
      },
      {
        "name": "Xintong Zhang",
        "type": "Person"
      },
      {
        "name": "Zhi Gao",
        "type": "Person"
      },
      {
        "name": "Bofei Zhang",
        "type": "Person"
      },
      {
        "name": "Pengxiang Li",
        "type": "Person"
      },
      {
        "name": "Xiaowen Zhang",
        "type": "Person"
      },
      {
        "name": "Yang Liu",
        "type": "Person"
      },
      {
        "name": "Tao Yuan",
        "type": "Person"
      },
      {
        "name": "Yuwei Wu",
        "type": "Person"
      },
      {
        "name": "Yunde Jia",
        "type": "Person"
      },
      {
        "name": "Song-Chun Zhu",
        "type": "Person"
      },
      {
        "name": "Qing Li",
        "type": "Person"
      },
      {
        "name": "Zhangquan Chen",
        "type": "Person"
      },
      {
        "name": "Manyuan Zhang",
        "type": "Person"
      },
      {
        "name": "Xinlei Yu",
        "type": "Person"
      },
      {
        "name": "Xufang Luo",
        "type": "Person"
      },
      {
        "name": "Mingze Sun",
        "type": "Person"
      },
      {
        "name": "Zihao Pan",
        "type": "Person"
      },
      {
        "name": "Yan Feng",
        "type": "Person"
      },
      {
        "name": "Ruqi Huang",
        "type": "Person"
      },
      {
        "name": "Tiancheng Gu",
        "type": "Person"
      },
      {
        "name": "Kaichen Zhang",
        "type": "Person"
      },
      {
        "name": "Yueyi Zhang",
        "type": "Person"
      },
      {
        "name": "Weidong Cai",
        "type": "Person"
      },
      {
        "name": "Lidong Bing",
        "type": "Person"
      },
      {
        "name": "Keming Wu",
        "type": "Person"
      },
      {
        "name": "Zuhao Yang",
        "type": "Person"
      },
      {
        "name": "Kairui Hu",
        "type": "Person"
      },
      {
        "name": "Bin Wang",
        "type": "Person"
      },
      {
        "name": "Xingxuan Li",
        "type": "Person"
      },
      {
        "name": "Ranjan Sapkota",
        "type": "Person"
      },
      {
        "name": "Yang Cao",
        "type": "Person"
      },
      {
        "name": "Konstantinos I. Roumeliotis",
        "type": "Person"
      },
      {
        "name": "Manoj Karkee",
        "type": "Person"
      },
      {
        "name": "Kohei Sendai",
        "type": "Person"
      },
      {
        "name": "Maxime Alvarez",
        "type": "Person"
      },
      {
        "name": "Tatsuya Matsushima",
        "type": "Person"
      },
      {
        "name": "Yutaka Matsuo",
        "type": "Person"
      },
      {
        "name": "Yusuke Iwasawa",
        "type": "Person"
      },
      {
        "name": "Shuhan Tan",
        "type": "Person"
      },
      {
        "name": "Yuxiao Chen",
        "type": "Person"
      },
      {
        "name": "Ran Tian",
        "type": "Person"
      },
      {
        "name": "Yurong You",
        "type": "Person"
      },
      {
        "name": "Yan Wang",
        "type": "Person"
      },
      {
        "name": "Wenjie Luo",
        "type": "Person"
      },
      {
        "name": "Yulong Cao",
        "type": "Person"
      },
      {
        "name": "Philipp Krahenbuhl",
        "type": "Person"
      },
      {
        "name": "Marco Pavone",
        "type": "Person"
      },
      {
        "name": "Boris Ivanovic",
        "type": "Person"
      },
      {
        "name": "Zheng Xiong",
        "type": "Person"
      },
      {
        "name": "Kang Li",
        "type": "Person"
      },
      {
        "name": "Zilin Wang",
        "type": "Person"
      },
      {
        "name": "Matthew Jackson",
        "type": "Person"
      },
      {
        "name": "Jakob Foerster",
        "type": "Person"
      },
      {
        "name": "Shimon Whiteson",
        "type": "Person"
      },
      {
        "name": "Yifan Ye",
        "type": "Person"
      },
      {
        "name": "Jiaqi Ma",
        "type": "Person"
      },
      {
        "name": "Jun Cen",
        "type": "Person"
      },
      {
        "name": "Zhihe Lu",
        "type": "Person"
      },
      {
        "name": "Hai Liu",
        "type": "Person"
      },
      {
        "name": "Yu Song",
        "type": "Person"
      },
      {
        "name": "Tingting Liu",
        "type": "Person"
      },
      {
        "name": "Zhaoli Zhang",
        "type": "Person"
      },
      {
        "name": "Xiaolan Yang",
        "type": "Person"
      },
      {
        "name": "Neal N. Xiong",
        "type": "Person"
      },
      {
        "name": "Honghu Chu",
        "type": "Person"
      },
      {
        "name": "Jiahao Gai",
        "type": "Person"
      },
      {
        "name": "Weiwei Chen",
        "type": "Person"
      },
      {
        "name": "Jun Ma",
        "type": "Person"
      },
      {
        "name": "A. S. Demirkol",
        "type": "Person"
      },
      {
        "name": "A. Ascoli",
        "type": "Person"
      },
      {
        "name": "I. Messaris",
        "type": "Person"
      },
      {
        "name": "V. Ntinas",
        "type": "Person"
      },
      {
        "name": "D. Prousalis",
        "type": "Person"
      },
      {
        "name": "R. Tetzlaff",
        "type": "Person"
      },
      {
        "name": "Yinjun Jia",
        "type": "Person"
      },
      {
        "name": "Bowen Gao",
        "type": "Person"
      },
      {
        "name": "Jiaxin Tan",
        "type": "Person"
      },
      {
        "name": "Jiqing Zheng",
        "type": "Person"
      },
      {
        "name": "Xin Hong",
        "type": "Person"
      },
      {
        "name": "Wenyu Zhu",
        "type": "Person"
      },
      {
        "name": "Haichuan Tan",
        "type": "Person"
      },
      {
        "name": "Yuan Xiao",
        "type": "Person"
      },
      {
        "name": "Liping Tan",
        "type": "Person"
      },
      {
        "name": "Hongyi Cai",
        "type": "Person"
      },
      {
        "name": "Yanwen Huang",
        "type": "Person"
      },
      {
        "name": "Zhiheng Deng",
        "type": "Person"
      },
      {
        "name": "Xiangwei Wu",
        "type": "Person"
      },
      {
        "name": "Yue Jin",
        "type": "Person"
      },
      {
        "name": "Yafei Yuan",
        "type": "Person"
      },
      {
        "name": "Jiekang Tian",
        "type": "Person"
      },
      {
        "name": "Wei He",
        "type": "Person"
      },
      {
        "name": "Weiying Ma",
        "type": "Person"
      },
      {
        "name": "Chuangye Yan",
        "type": "Person"
      },
      {
        "name": "Yanyan Lan",
        "type": "Person"
      },
      {
        "name": "Guodong Fan",
        "type": "Person"
      },
      {
        "name": "Shengning Zhou",
        "type": "Person"
      },
      {
        "name": "Zhen Hua",
        "type": "Person"
      },
      {
        "name": "Jinjiang Li",
        "type": "Person"
      },
      {
        "name": "Jingchun Zhou",
        "type": "Person"
      },
      {
        "name": "Jiahua Dong",
        "type": "Person"
      },
      {
        "name": "Yu-Xiong Wang",
        "type": "Person"
      },
      {
        "name": "Joohyung Yun",
        "type": "Person"
      },
      {
        "name": "Doyup Lee",
        "type": "Person"
      },
      {
        "name": "Wook-Shin Han",
        "type": "Person"
      },
      {
        "name": "Qiao Sun",
        "type": "Person"
      },
      {
        "name": "Xianbang Wang",
        "type": "Person"
      },
      {
        "name": "Zhicheng Jiang",
        "type": "Person"
      },
      {
        "name": "Hanhong Zhao",
        "type": "Person"
      },
      {
        "name": "Susie Lu",
        "type": "Person"
      },
      {
        "name": "Tianhong Li",
        "type": "Person"
      },
      {
        "name": "Peter Potaptchik",
        "type": "Person"
      },
      {
        "name": "Adhi Saravanan",
        "type": "Person"
      },
      {
        "name": "Abbas Mammadov",
        "type": "Person"
      },
      {
        "name": "Alvaro Prat",
        "type": "Person"
      },
      {
        "name": "Michael S. Albergo",
        "type": "Person"
      },
      {
        "name": "Yee Whye Teh",
        "type": "Person"
      },
      {
        "name": "Yinan Huang",
        "type": "Person"
      },
      {
        "name": "Hans Hao-Hsun Hsu",
        "type": "Person"
      },
      {
        "name": "Junran Wang",
        "type": "Person"
      },
      {
        "name": "Bo Dai",
        "type": "Person"
      },
      {
        "name": "Pan Li",
        "type": "Person"
      },
      {
        "name": "Mingyang Deng",
        "type": "Person"
      },
      {
        "name": "He Li",
        "type": "Person"
      },
      {
        "name": "Yilun Du",
        "type": "Person"
      },
      {
        "name": "Ting Chen",
        "type": "Person"
      },
      {
        "name": "Mohammad Norouzi",
        "type": "Person"
      },
      {
        "name": "Chubin Chen",
        "type": "Person"
      },
      {
        "name": "Sujie Hu",
        "type": "Person"
      },
      {
        "name": "Jiashu Zhu",
        "type": "Person"
      },
      {
        "name": "Meiqi Wu",
        "type": "Person"
      },
      {
        "name": "Jintao Chen",
        "type": "Person"
      },
      {
        "name": "Yanxun Li",
        "type": "Person"
      },
      {
        "name": "Nisha Huang",
        "type": "Person"
      },
      {
        "name": "Chengyu Fang",
        "type": "Person"
      },
      {
        "name": "Xiu Li",
        "type": "Person"
      },
      {
        "name": "Xiaokun Feng",
        "type": "Person"
      },
      {
        "name": "Chen Zhu",
        "type": "Person"
      },
      {
        "name": "Bingze Song",
        "type": "Person"
      },
      {
        "name": "Fangyuan Mao",
        "type": "Person"
      },
      {
        "name": "Kaiqi Huang",
        "type": "Person"
      },
      {
        "name": "Ziteng Wang",
        "type": "Person"
      },
      {
        "name": "Bingda Tang",
        "type": "Person"
      },
      {
        "name": "Ellis Brown",
        "type": "Person"
      },
      {
        "name": "Jihan Yang",
        "type": "Person"
      },
      {
        "name": "Rob Fergus",
        "type": "Person"
      },
      {
        "name": "Jingtong Yue",
        "type": "Person"
      },
      {
        "name": "Ziqi Huang",
        "type": "Person"
      },
      {
        "name": "Kaixin Zhu",
        "type": "Person"
      },
      {
        "name": "Daili Hua",
        "type": "Person"
      },
      {
        "name": "Bozhou Li",
        "type": "Person"
      },
      {
        "name": "Chengzhuo Tong",
        "type": "Person"
      },
      {
        "name": "Yuran Wang",
        "type": "Person"
      },
      {
        "name": "Xinyi Huang",
        "type": "Person"
      },
      {
        "name": "Yifan Dai",
        "type": "Person"
      },
      {
        "name": "Zixiang Zhang",
        "type": "Person"
      },
      {
        "name": "Yifan Yang",
        "type": "Person"
      },
      {
        "name": "Zhou Liu",
        "type": "Person"
      },
      {
        "name": "Hao Liang",
        "type": "Person"
      },
      {
        "name": "Xiaochen Ma",
        "type": "Person"
      },
      {
        "name": "Ruichuan An",
        "type": "Person"
      },
      {
        "name": "Tianyi Bai",
        "type": "Person"
      },
      {
        "name": "Hongcheng Gao",
        "type": "Person"
      },
      {
        "name": "Junbo Niu",
        "type": "Person"
      },
      {
        "name": "Yang Shi",
        "type": "Person"
      },
      {
        "name": "Xinlong Chen",
        "type": "Person"
      },
      {
        "name": "Yue Ding",
        "type": "Person"
      },
      {
        "name": "Kai Zeng",
        "type": "Person"
      },
      {
        "name": "Yiwen Tang",
        "type": "Person"
      },
      {
        "name": "Wentao Zhang",
        "type": "Person"
      },
      {
        "name": "Qingyu Shi",
        "type": "Person"
      },
      {
        "name": "Size Wu",
        "type": "Person"
      },
      {
        "name": "Jinbin Bai",
        "type": "Person"
      },
      {
        "name": "Kaidong Yu",
        "type": "Person"
      },
      {
        "name": "Yujing Wang",
        "type": "Person"
      },
      {
        "name": "Yunhai Tong",
        "type": "Person"
      },
      {
        "name": "Xiangtai Li",
        "type": "Person"
      },
      {
        "name": "Xuelong Li",
        "type": "Person"
      },
      {
        "name": "Guanfang Dong",
        "type": "Person"
      },
      {
        "name": "Luke Schultz",
        "type": "Person"
      },
      {
        "name": "Negar Hassanpour",
        "type": "Person"
      },
      {
        "name": "Chao Gao",
        "type": "Person"
      },
      {
        "name": "Alex Nichol",
        "type": "Person"
      },
      {
        "name": "Maxime Oquab",
        "type": "Person"
      },
      {
        "name": "Timothée Darcet",
        "type": "Person"
      },
      {
        "name": "Théo Moutakanni",
        "type": "Person"
      },
      {
        "name": "Huy Vo",
        "type": "Person"
      },
      {
        "name": "Marc Szafraniec",
        "type": "Person"
      },
      {
        "name": "Vasil Khalidov",
        "type": "Person"
      },
      {
        "name": "Pierre Fernandez",
        "type": "Person"
      },
      {
        "name": "Daniel Haziza",
        "type": "Person"
      },
      {
        "name": "Francisco Massa",
        "type": "Person"
      },
      {
        "name": "Alaaeldin El-Nouby",
        "type": "Person"
      },
      {
        "name": "Mahmoud Assran",
        "type": "Person"
      },
      {
        "name": "Nicolas Ballas",
        "type": "Person"
      },
      {
        "name": "Wojciech Galuba",
        "type": "Person"
      },
      {
        "name": "Russell Howes",
        "type": "Person"
      },
      {
        "name": "Po-Yao Huang",
        "type": "Person"
      },
      {
        "name": "Shang-Wen Li",
        "type": "Person"
      },
      {
        "name": "Ishan Misra",
        "type": "Person"
      },
      {
        "name": "Michael Rabbat",
        "type": "Person"
      },
      {
        "name": "Vasu Sharma",
        "type": "Person"
      },
      {
        "name": "Gabriel Synnaeve",
        "type": "Person"
      },
      {
        "name": "Hu Xu",
        "type": "Person"
      },
      {
        "name": "Hervé Jegou",
        "type": "Person"
      },
      {
        "name": "Julien Mairal",
        "type": "Person"
      },
      {
        "name": "Patrick Labatut",
        "type": "Person"
      },
      {
        "name": "Piotr Bojanowski",
        "type": "Person"
      },
      {
        "name": "Zehong Ma",
        "type": "Person"
      },
      {
        "name": "Ruihan Xu",
        "type": "Person"
      },
      {
        "name": "Shiliang Zhang",
        "type": "Person"
      },
      {
        "name": "Shanshan Zhao",
        "type": "Person"
      },
      {
        "name": "Xinjie Zhang",
        "type": "Person"
      },
      {
        "name": "Jintao Guo",
        "type": "Person"
      },
      {
        "name": "Jiakui Hu",
        "type": "Person"
      },
      {
        "name": "Lunhao Duan",
        "type": "Person"
      },
      {
        "name": "Minghao Fu",
        "type": "Person"
      },
      {
        "name": "Yong Xien Chng",
        "type": "Person"
      },
      {
        "name": "Guo-Hua Wang",
        "type": "Person"
      },
      {
        "name": "Jana Zeller",
        "type": "Person"
      },
      {
        "name": "Thaddäus Wiedemer",
        "type": "Person"
      },
      {
        "name": "Fanfei Li",
        "type": "Person"
      },
      {
        "name": "Thomas Klein",
        "type": "Person"
      },
      {
        "name": "Prasanna Mayilvahanan",
        "type": "Person"
      },
      {
        "name": "Matthias Bethge",
        "type": "Person"
      },
      {
        "name": "Felix Wichmann",
        "type": "Person"
      },
      {
        "name": "Ryan Cotterell",
        "type": "Person"
      },
      {
        "name": "Wieland Brendel",
        "type": "Person"
      },
      {
        "name": "Letian Zhang",
        "type": "Person"
      },
      {
        "name": "Sucheng Ren",
        "type": "Person"
      },
      {
        "name": "Yanqing Liu",
        "type": "Person"
      },
      {
        "name": "Xianhang Li",
        "type": "Person"
      },
      {
        "name": "Zeyu Wang",
        "type": "Person"
      },
      {
        "name": "Yuyin Zhou",
        "type": "Person"
      },
      {
        "name": "Huaxiu Yao",
        "type": "Person"
      },
      {
        "name": "Zeyu Zheng",
        "type": "Person"
      },
      {
        "name": "Guilin Liu",
        "type": "Person"
      },
      {
        "name": "Zhiding Yu",
        "type": "Person"
      },
      {
        "name": "Cihang Xie",
        "type": "Person"
      },
      {
        "name": "Tomas Mikolov",
        "type": "Person"
      },
      {
        "name": "Greg Corrado",
        "type": "Person"
      },
      {
        "name": "Jeffrey Dean",
        "type": "Person"
      },
      {
        "name": "Jeffrey Pennington",
        "type": "Person"
      },
      {
        "name": "Christopher D. Manning",
        "type": "Person"
      },
      {
        "name": "Matthew E. Peters",
        "type": "Person"
      },
      {
        "name": "Mark Neumann",
        "type": "Person"
      },
      {
        "name": "Mohit Iyyer",
        "type": "Person"
      },
      {
        "name": "Matt Gardner",
        "type": "Person"
      },
      {
        "name": "Christopher Clark",
        "type": "Person"
      },
      {
        "name": "Luke Zettlemoyer",
        "type": "Person"
      },
      {
        "name": "Quentin Fournier",
        "type": "Person"
      },
      {
        "name": "Robert M. Vernon",
        "type": "Person"
      },
      {
        "name": "Almer M. van der Sloot",
        "type": "Person"
      },
      {
        "name": "Benjamin Schulz",
        "type": "Person"
      },
      {
        "name": "Sarath Chandar",
        "type": "Person"
      },
      {
        "name": "C. Langmead",
        "type": "Person"
      },
      {
        "name": "Scott Friedman",
        "type": "Person"
      },
      {
        "name": "Sonja Schmer-Galunder",
        "type": "Person"
      },
      {
        "name": "Anthony Chen",
        "type": "Person"
      },
      {
        "name": "Jeffrey Rye",
        "type": "Person"
      },
      {
        "name": "Zhaohu Xing",
        "type": "Person"
      },
      {
        "name": "Tian Ye",
        "type": "Person"
      },
      {
        "name": "Yijun Yang",
        "type": "Person"
      },
      {
        "name": "D. Cai",
        "type": "Person"
      },
      {
        "name": "Baowen Gai",
        "type": "Person"
      },
      {
        "name": "Xiao-Jian Wu",
        "type": "Person"
      },
      {
        "name": "Feng Gao",
        "type": "Person"
      },
      {
        "name": "Lei Zhu",
        "type": "Person"
      },
      {
        "name": "Alexander C. Li",
        "type": "Person"
      },
      {
        "name": "Ananya Kumar",
        "type": "Person"
      },
      {
        "name": "Deepak Pathak",
        "type": "Person"
      },
      {
        "name": "Y. Sun",
        "type": "Person"
      },
      {
        "name": "Yinqiu Liu",
        "type": "Person"
      },
      {
        "name": "Shaoyong Guo",
        "type": "Person"
      },
      {
        "name": "Xuesong Qiu",
        "type": "Person"
      },
      {
        "name": "Jiewei Chen",
        "type": "Person"
      },
      {
        "name": "Jiakai Hao",
        "type": "Person"
      },
      {
        "name": "Dusist Niyato",
        "type": "Person"
      },
      {
        "name": "R. Child",
        "type": "Person"
      },
      {
        "name": "D. Luan",
        "type": "Person"
      },
      {
        "name": "Aaron Grattafiori",
        "type": "Person"
      },
      {
        "name": "Abhimanyu Dubey",
        "type": "Person"
      },
      {
        "name": "Abhinav Jauhri",
        "type": "Person"
      },
      {
        "name": "Abhinav Pandey",
        "type": "Person"
      },
      {
        "name": "Abhishek Kadian",
        "type": "Person"
      },
      {
        "name": "Ahmad Al-Dahle",
        "type": "Person"
      },
      {
        "name": "Aiesha Letman",
        "type": "Person"
      },
      {
        "name": "Akhil Mathur",
        "type": "Person"
      },
      {
        "name": "Alan Schelten",
        "type": "Person"
      },
      {
        "name": "Alex Vaughan",
        "type": "Person"
      },
      {
        "name": "Amy Yang",
        "type": "Person"
      },
      {
        "name": "Angela Fan",
        "type": "Person"
      },
      {
        "name": "Anirudh Goyal",
        "type": "Person"
      },
      {
        "name": "Anthony Hartshorn",
        "type": "Person"
      },
      {
        "name": "Aobo Yang",
        "type": "Person"
      },
      {
        "name": "Archi Mitra",
        "type": "Person"
      },
      {
        "name": "Archie Sravankumar",
        "type": "Person"
      },
      {
        "name": "Artem Korenev",
        "type": "Person"
      },
      {
        "name": "Arthur Hinsvark",
        "type": "Person"
      },
      {
        "name": "Arun Rao",
        "type": "Person"
      },
      {
        "name": "Aston Zhang",
        "type": "Person"
      },
      {
        "name": "Austen Gregerson",
        "type": "Person"
      },
      {
        "name": "Ava Spataru",
        "type": "Person"
      },
      {
        "name": "Baptiste Roziere",
        "type": "Person"
      },
      {
        "name": "Bethany Biron",
        "type": "Person"
      },
      {
        "name": "Binh Tang",
        "type": "Person"
      },
      {
        "name": "Bobbie Chern",
        "type": "Person"
      },
      {
        "name": "Charlotte Caucheteux",
        "type": "Person"
      },
      {
        "name": "Chaya Nayak",
        "type": "Person"
      },
      {
        "name": "Chloe Bi",
        "type": "Person"
      },
      {
        "name": "Chris Marra",
        "type": "Person"
      },
      {
        "name": "Chris McConnell",
        "type": "Person"
      },
      {
        "name": "Christian Keller",
        "type": "Person"
      },
      {
        "name": "Christophe Touret",
        "type": "Person"
      },
      {
        "name": "Chunyang Wu",
        "type": "Person"
      },
      {
        "name": "Corinne Wong",
        "type": "Person"
      },
      {
        "name": "Cristian Canton Ferrer",
        "type": "Person"
      },
      {
        "name": "Cyrus Nikolaidis",
        "type": "Person"
      },
      {
        "name": "Damien Allonsius",
        "type": "Person"
      },
      {
        "name": "Daniel Song",
        "type": "Person"
      },
      {
        "name": "Danielle Pintz",
        "type": "Person"
      },
      {
        "name": "Danny Livshits",
        "type": "Person"
      },
      {
        "name": "Danny Wyatt",
        "type": "Person"
      },
      {
        "name": "David Esiobu",
        "type": "Person"
      },
      {
        "name": "Dhruv Choudhary",
        "type": "Person"
      },
      {
        "name": "Dhruv Mahajan",
        "type": "Person"
      },
      {
        "name": "Diego Garcia-Olano",
        "type": "Person"
      },
      {
        "name": "Diego Perino",
        "type": "Person"
      },
      {
        "name": "Dieuwke Hupkes",
        "type": "Person"
      },
      {
        "name": "Egor Lakomkin",
        "type": "Person"
      },
      {
        "name": "Ehab AlBadawy",
        "type": "Person"
      },
      {
        "name": "Elina Lobanova",
        "type": "Person"
      },
      {
        "name": "Emily Dinan",
        "type": "Person"
      },
      {
        "name": "Eric Michael Smith",
        "type": "Person"
      },
      {
        "name": "Filip Radenovic",
        "type": "Person"
      },
      {
        "name": "Francisco Guzmán",
        "type": "Person"
      },
      {
        "name": "Frank Zhang",
        "type": "Person"
      },
      {
        "name": "Gabrielle Lee",
        "type": "Person"
      },
      {
        "name": "Georgia Lewis Anderson",
        "type": "Person"
      },
      {
        "name": "Govind Thattai",
        "type": "Person"
      },
      {
        "name": "Graeme Nail",
        "type": "Person"
      },
      {
        "name": "Gregoire Mialon",
        "type": "Person"
      },
      {
        "name": "Guan Pang",
        "type": "Person"
      },
      {
        "name": "Guillem Cucurell",
        "type": "Person"
      },
      {
        "name": "Hailey Nguyen",
        "type": "Person"
      },
      {
        "name": "Hannah Korevaar",
        "type": "Person"
      },
      {
        "name": "Iliyan Zarov",
        "type": "Person"
      },
      {
        "name": "Imanol Arrieta Ibarra",
        "type": "Person"
      },
      {
        "name": "Isabel Kloumann",
        "type": "Person"
      },
      {
        "name": "Ivan Evtimov",
        "type": "Person"
      },
      {
        "name": "Jack Zhang",
        "type": "Person"
      },
      {
        "name": "Jade Copet",
        "type": "Person"
      },
      {
        "name": "Jaewon Lee",
        "type": "Person"
      },
      {
        "name": "Jan Geffert",
        "type": "Person"
      },
      {
        "name": "Jana Vranes",
        "type": "Person"
      },
      {
        "name": "Jason Park",
        "type": "Person"
      },
      {
        "name": "Jay Mahadeokar",
        "type": "Person"
      },
      {
        "name": "Jeet Shah",
        "type": "Person"
      },
      {
        "name": "Jelmer van der Linde",
        "type": "Person"
      },
      {
        "name": "Jennifer Billock",
        "type": "Person"
      },
      {
        "name": "Jenny Hong",
        "type": "Person"
      },
      {
        "name": "Jenya Lee",
        "type": "Person"
      },
      {
        "name": "Jeremy Fu",
        "type": "Person"
      },
      {
        "name": "Jianfeng Chi",
        "type": "Person"
      },
      {
        "name": "Jianyu Huang",
        "type": "Person"
      },
      {
        "name": "Jiawen Liu",
        "type": "Person"
      },
      {
        "name": "Jie Wang",
        "type": "Person"
      },
      {
        "name": "Jiecao Yu",
        "type": "Person"
      },
      {
        "name": "Joanna Bitton",
        "type": "Person"
      },
      {
        "name": "Joe Spisak",
        "type": "Person"
      },
      {
        "name": "Jongsoo Park",
        "type": "Person"
      },
      {
        "name": "Joseph Rocca",
        "type": "Person"
      },
      {
        "name": "Joshua Johnstun",
        "type": "Person"
      },
      {
        "name": "Joshua Saxe",
        "type": "Person"
      },
      {
        "name": "Junteng Jia",
        "type": "Person"
      },
      {
        "name": "Kalyan Vasuden Alwala",
        "type": "Person"
      },
      {
        "name": "Karthik Prasad",
        "type": "Person"
      },
      {
        "name": "Kartikeya Upasani",
        "type": "Person"
      },
      {
        "name": "Kate Plawiak",
        "type": "Person"
      },
      {
        "name": "Ke Li",
        "type": "Person"
      },
      {
        "name": "Kenneth Heafield",
        "type": "Person"
      },
      {
        "name": "Kevin Stone",
        "type": "Person"
      },
      {
        "name": "Khalid El-Arini",
        "type": "Person"
      },
      {
        "name": "Krithika Iyer",
        "type": "Person"
      },
      {
        "name": "Kshitiz Malik",
        "type": "Person"
      },
      {
        "name": "Kuenley Chiu",
        "type": "Person"
      },
      {
        "name": "Kunal Bhalla",
        "type": "Person"
      },
      {
        "name": "Kushal Lakhotia",
        "type": "Person"
      },
      {
        "name": "Lauren Rantala-Yeary",
        "type": "Person"
      },
      {
        "name": "Lawrence Chen",
        "type": "Person"
      },
      {
        "name": "Liang Tan",
        "type": "Person"
      },
      {
        "name": "Liz Jenkins",
        "type": "Person"
      },
      {
        "name": "Louis Martin",
        "type": "Person"
      },
      {
        "name": "Lovish Madaan",
        "type": "Person"
      },
      {
        "name": "Lubo Malo",
        "type": "Person"
      },
      {
        "name": "Lukas Blecher",
        "type": "Person"
      },
      {
        "name": "Lukas Landzaat",
        "type": "Person"
      },
      {
        "name": "Luke de Oliveira",
        "type": "Person"
      },
      {
        "name": "Madeline Muzzi",
        "type": "Person"
      },
      {
        "name": "Mahesh Pasupuleti",
        "type": "Person"
      },
      {
        "name": "Mannat Singh",
        "type": "Person"
      },
      {
        "name": "Manohar Paluri",
        "type": "Person"
      },
      {
        "name": "Marcin Kardas",
        "type": "Person"
      },
      {
        "name": "Maria Tsimpoukelli",
        "type": "Person"
      },
      {
        "name": "Mathew Oldham",
        "type": "Person"
      },
      {
        "name": "Mathieu Rita",
        "type": "Person"
      },
      {
        "name": "Maya Pavlova",
        "type": "Person"
      },
      {
        "name": "Melanie Kambadur",
        "type": "Person"
      },
      {
        "name": "Mike Lewis",
        "type": "Person"
      },
      {
        "name": "Min Si",
        "type": "Person"
      },
      {
        "name": "Mitesh Kumar Singh",
        "type": "Person"
      },
      {
        "name": "Mona Hassan",
        "type": "Person"
      },
      {
        "name": "Narjes Torabi",
        "type": "Person"
      },
      {
        "name": "Nikolay Bashlykov",
        "type": "Person"
      },
      {
        "name": "Nikolay Bogoychev",
        "type": "Person"
      },
      {
        "name": "Niladri Chatterji",
        "type": "Person"
      },
      {
        "name": "Ning Zhang",
        "type": "Person"
      },
      {
        "name": "Olivier Duchenne",
        "type": "Person"
      },
      {
        "name": "Onur Çelebi",
        "type": "Person"
      },
      {
        "name": "Patrick Alrassy",
        "type": "Person"
      },
      {
        "name": "Pengchuan Zhang",
        "type": "Person"
      },
      {
        "name": "Pengwei Li",
        "type": "Person"
      },
      {
        "name": "Petar Vasic",
        "type": "Person"
      },
      {
        "name": "Peter Weng",
        "type": "Person"
      },
      {
        "name": "Prajjwal Bhargava",
        "type": "Person"
      },
      {
        "name": "Pratik Dubal",
        "type": "Person"
      },
      {
        "name": "Praveen Krishnan",
        "type": "Person"
      },
      {
        "name": "Punit Singh Koura",
        "type": "Person"
      },
      {
        "name": "Puxin Xu",
        "type": "Person"
      },
      {
        "name": "Qingxiao Dong",
        "type": "Person"
      },
      {
        "name": "Ragavan Srinivasan",
        "type": "Person"
      },
      {
        "name": "Raj Ganapathy",
        "type": "Person"
      },
      {
        "name": "Ramon Calderer",
        "type": "Person"
      },
      {
        "name": "Ricardo Silveira Cabral",
        "type": "Person"
      },
      {
        "name": "Robert Stojnic",
        "type": "Person"
      },
      {
        "name": "Roberta Raileanu",
        "type": "Person"
      },
      {
        "name": "Rohan Maheswari",
        "type": "Person"
      },
      {
        "name": "Rohit Girdhar",
        "type": "Person"
      },
      {
        "name": "Rohit Patel",
        "type": "Person"
      },
      {
        "name": "Romain Sauvestre",
        "type": "Person"
      },
      {
        "name": "Ronnie Polidoro",
        "type": "Person"
      },
      {
        "name": "Roshan Sumbaly",
        "type": "Person"
      },
      {
        "name": "Ross Taylor",
        "type": "Person"
      },
      {
        "name": "Ruan Silva",
        "type": "Person"
      },
      {
        "name": "Rui Hou",
        "type": "Person"
      },
      {
        "name": "Saghar Hosseini",
        "type": "Person"
      },
      {
        "name": "Sahana Chennabasappa",
        "type": "Person"
      },
      {
        "name": "Sanjay Singh",
        "type": "Person"
      },
      {
        "name": "Sean Bell",
        "type": "Person"
      },
      {
        "name": "Seohyun Sonia Kim",
        "type": "Person"
      },
      {
        "name": "Sergey Edunov",
        "type": "Person"
      },
      {
        "name": "Shaoliang Nie",
        "type": "Person"
      },
      {
        "name": "Sharath Raparthy",
        "type": "Person"
      },
      {
        "name": "Sheng Shen",
        "type": "Person"
      },
      {
        "name": "Shengye Wan",
        "type": "Person"
      },
      {
        "name": "Shruti Bhosale",
        "type": "Person"
      },
      {
        "name": "Shun Zhang",
        "type": "Person"
      },
      {
        "name": "Simon Vandenhende",
        "type": "Person"
      },
      {
        "name": "Soumya Batra",
        "type": "Person"
      },
      {
        "name": "Spencer Whitman",
        "type": "Person"
      },
      {
        "name": "Sten Sootla",
        "type": "Person"
      },
      {
        "name": "Stephane Collot",
        "type": "Person"
      },
      {
        "name": "Suchin Gururangan",
        "type": "Person"
      },
      {
        "name": "Sydney Borodinsky",
        "type": "Person"
      },
      {
        "name": "Tamar Herman",
        "type": "Person"
      },
      {
        "name": "Tara Fowler",
        "type": "Person"
      },
      {
        "name": "Tarek Sheasha",
        "type": "Person"
      },
      {
        "name": "Thomas Georgiou",
        "type": "Person"
      },
      {
        "name": "Thomas Scialom",
        "type": "Person"
      },
      {
        "name": "Tobias Speckbacher",
        "type": "Person"
      },
      {
        "name": "Todor Mihaylov",
        "type": "Person"
      },
      {
        "name": "Tong Xiao",
        "type": "Person"
      },
      {
        "name": "Ujjwal Karn",
        "type": "Person"
      },
      {
        "name": "Vedanuj Goswami",
        "type": "Person"
      },
      {
        "name": "Vibhor Gupta",
        "type": "Person"
      },
      {
        "name": "Vignesh Ramanathan",
        "type": "Person"
      },
      {
        "name": "Viktor Kerkez",
        "type": "Person"
      },
      {
        "name": "Vincent Gonguet",
        "type": "Person"
      },
      {
        "name": "Virginie Do",
        "type": "Person"
      },
      {
        "name": "Vish Vogeti",
        "type": "Person"
      },
      {
        "name": "Vítor Albiero",
        "type": "Person"
      },
      {
        "name": "Vladan Petrovic",
        "type": "Person"
      },
      {
        "name": "Weiwei Chu",
        "type": "Person"
      },
      {
        "name": "Wenhan Xiong",
        "type": "Person"
      },
      {
        "name": "Wenyin Fu",
        "type": "Person"
      },
      {
        "name": "Whitney Meers",
        "type": "Person"
      },
      {
        "name": "Xiaodong Wang",
        "type": "Person"
      },
      {
        "name": "Xiaofang Wang",
        "type": "Person"
      },
      {
        "name": "Xiaoqing Ellen Tan",
        "type": "Person"
      },
      {
        "name": "Xide Xia",
        "type": "Person"
      },
      {
        "name": "Xinfeng Xie",
        "type": "Person"
      },
      {
        "name": "Xuchao Jia",
        "type": "Person"
      },
      {
        "name": "Xuewei Wang",
        "type": "Person"
      },
      {
        "name": "Yaelle Goldschlag",
        "type": "Person"
      },
      {
        "name": "Yashesh Gaur",
        "type": "Person"
      },
      {
        "name": "Yasmine Babaei",
        "type": "Person"
      },
      {
        "name": "Yi Wen",
        "type": "Person"
      },
      {
        "name": "Yiwen Song",
        "type": "Person"
      },
      {
        "name": "Yuchen Zhang",
        "type": "Person"
      },
      {
        "name": "Yue Li",
        "type": "Person"
      },
      {
        "name": "Yuning Mao",
        "type": "Person"
      },
      {
        "name": "Zacharie Delpierre Coudert",
        "type": "Person"
      },
      {
        "name": "Zheng Yan",
        "type": "Person"
      },
      {
        "name": "Zhengxing Chen",
        "type": "Person"
      },
      {
        "name": "Zoe Papakipos",
        "type": "Person"
      },
      {
        "name": "Aaditya Singh",
        "type": "Person"
      },
      {
        "name": "Aayushi Srivastava",
        "type": "Person"
      },
      {
        "name": "Abha Jain",
        "type": "Person"
      },
      {
        "name": "Adam Kelsey",
        "type": "Person"
      },
      {
        "name": "Adam Shajnfeld",
        "type": "Person"
      },
      {
        "name": "Adithya Gangidi",
        "type": "Person"
      },
      {
        "name": "Adolfo Victoria",
        "type": "Person"
      },
      {
        "name": "Ahuva Goldstand",
        "type": "Person"
      },
      {
        "name": "Ajay Menon",
        "type": "Person"
      },
      {
        "name": "Ajay Sharma",
        "type": "Person"
      },
      {
        "name": "Alex Boesenberg",
        "type": "Person"
      },
      {
        "name": "Alexei Baevski",
        "type": "Person"
      },
      {
        "name": "Allie Feinstein",
        "type": "Person"
      },
      {
        "name": "Amanda Kallet",
        "type": "Person"
      },
      {
        "name": "Amit Sangani",
        "type": "Person"
      },
      {
        "name": "Amos Teo",
        "type": "Person"
      },
      {
        "name": "Anam Yunus",
        "type": "Person"
      },
      {
        "name": "Andrei Lupu",
        "type": "Person"
      },
      {
        "name": "Andres Alvarado",
        "type": "Person"
      },
      {
        "name": "Andrew Caples",
        "type": "Person"
      },
      {
        "name": "Andrew Gu",
        "type": "Person"
      },
      {
        "name": "Andrew Ho",
        "type": "Person"
      },
      {
        "name": "Andrew Poulton",
        "type": "Person"
      },
      {
        "name": "Andrew Ryan",
        "type": "Person"
      },
      {
        "name": "Ankit Ramchandani",
        "type": "Person"
      },
      {
        "name": "Annie Dong",
        "type": "Person"
      },
      {
        "name": "Annie Franco",
        "type": "Person"
      },
      {
        "name": "Anuj Goyal",
        "type": "Person"
      },
      {
        "name": "Aparajita Saraf",
        "type": "Person"
      },
      {
        "name": "Arkabandhu Chowdhury",
        "type": "Person"
      },
      {
        "name": "Ashley Gabriel",
        "type": "Person"
      },
      {
        "name": "Ashwin Bharambe",
        "type": "Person"
      },
      {
        "name": "Assaf Eisenman",
        "type": "Person"
      },
      {
        "name": "Azadeh Yazdan",
        "type": "Person"
      },
      {
        "name": "Beau James",
        "type": "Person"
      },
      {
        "name": "Ben Maurer",
        "type": "Person"
      },
      {
        "name": "Benjamin Leonhardi",
        "type": "Person"
      },
      {
        "name": "Bernie Huang",
        "type": "Person"
      },
      {
        "name": "Beth Loyd",
        "type": "Person"
      },
      {
        "name": "Beto De Paola",
        "type": "Person"
      },
      {
        "name": "Bhargavi Paranjape",
        "type": "Person"
      },
      {
        "name": "Bing Liu",
        "type": "Person"
      },
      {
        "name": "Boyu Ni",
        "type": "Person"
      },
      {
        "name": "Braden Hancock",
        "type": "Person"
      },
      {
        "name": "Bram Wasti",
        "type": "Person"
      },
      {
        "name": "Brandon Spence",
        "type": "Person"
      },
      {
        "name": "Brani Stojkovic",
        "type": "Person"
      },
      {
        "name": "Brian Gamido",
        "type": "Person"
      },
      {
        "name": "Britt Montalvo",
        "type": "Person"
      },
      {
        "name": "Carl Parker",
        "type": "Person"
      },
      {
        "name": "Carly Burton",
        "type": "Person"
      },
      {
        "name": "Catalina Mejia",
        "type": "Person"
      },
      {
        "name": "Ce Liu",
        "type": "Person"
      },
      {
        "name": "Changhan Wang",
        "type": "Person"
      },
      {
        "name": "Changkyu Kim",
        "type": "Person"
      },
      {
        "name": "Chao Zhou",
        "type": "Person"
      },
      {
        "name": "Chester Hu",
        "type": "Person"
      },
      {
        "name": "Ching-Hsiang Chu",
        "type": "Person"
      },
      {
        "name": "Chris Cai",
        "type": "Person"
      },
      {
        "name": "Chris Tindal",
        "type": "Person"
      },
      {
        "name": "Christoph Feichtenhofer",
        "type": "Person"
      },
      {
        "name": "Cynthia Gao",
        "type": "Person"
      },
      {
        "name": "Damon Civin",
        "type": "Person"
      },
      {
        "name": "Dana Beaty",
        "type": "Person"
      },
      {
        "name": "Daniel Kreymer",
        "type": "Person"
      },
      {
        "name": "Daniel Li",
        "type": "Person"
      },
      {
        "name": "David Adkins",
        "type": "Person"
      },
      {
        "name": "David Xu",
        "type": "Person"
      },
      {
        "name": "Davide Testuggine",
        "type": "Person"
      },
      {
        "name": "Delia David",
        "type": "Person"
      },
      {
        "name": "Devi Parikh",
        "type": "Person"
      },
      {
        "name": "Diana Liskovich",
        "type": "Person"
      },
      {
        "name": "Didem Foss",
        "type": "Person"
      },
      {
        "name": "Dingkang Wang",
        "type": "Person"
      },
      {
        "name": "Duc Le",
        "type": "Person"
      },
      {
        "name": "Dustin Holland",
        "type": "Person"
      },
      {
        "name": "Edward Dowling",
        "type": "Person"
      },
      {
        "name": "Eissa Jamil",
        "type": "Person"
      },
      {
        "name": "Elaine Montgomery",
        "type": "Person"
      },
      {
        "name": "Eleonora Presani",
        "type": "Person"
      },
      {
        "name": "Emily Hahn",
        "type": "Person"
      },
      {
        "name": "Emily Wood",
        "type": "Person"
      },
      {
        "name": "Eric-Tuan Le",
        "type": "Person"
      },
      {
        "name": "Erik Brinkman",
        "type": "Person"
      },
      {
        "name": "Esteban Arcaute",
        "type": "Person"
      },
      {
        "name": "Evan Dunbar",
        "type": "Person"
      },
      {
        "name": "Evan Smothers",
        "type": "Person"
      },
      {
        "name": "Fei Sun",
        "type": "Person"
      },
      {
        "name": "Felix Kreuk",
        "type": "Person"
      },
      {
        "name": "Filippos Kokkinos",
        "type": "Person"
      },
      {
        "name": "Firat Ozgenel",
        "type": "Person"
      },
      {
        "name": "Francesco Caggioni",
        "type": "Person"
      },
      {
        "name": "Frank Kanayet",
        "type": "Person"
      },
      {
        "name": "Frank Seide",
        "type": "Person"
      },
      {
        "name": "Gabriela Medina Florez",
        "type": "Person"
      },
      {
        "name": "Gabriella Schwarz",
        "type": "Person"
      },
      {
        "name": "Gada Badeer",
        "type": "Person"
      },
      {
        "name": "Georgia Swee",
        "type": "Person"
      },
      {
        "name": "Gil Halpern",
        "type": "Person"
      },
      {
        "name": "Grant Herman",
        "type": "Person"
      },
      {
        "name": "Grigory Sizov",
        "type": "Person"
      },
      {
        "name": "Guangyi",
        "type": "Person"
      },
      {
        "name": "Zhang",
        "type": "Person"
      },
      {
        "name": "Guna Lakshminarayanan",
        "type": "Person"
      },
      {
        "name": "Hakan Inan",
        "type": "Person"
      },
      {
        "name": "Hamid Shojanazeri",
        "type": "Person"
      },
      {
        "name": "Han Zou",
        "type": "Person"
      },
      {
        "name": "Hannah Wang",
        "type": "Person"
      },
      {
        "name": "Hanwen Zha",
        "type": "Person"
      },
      {
        "name": "Haroun Habeeb",
        "type": "Person"
      },
      {
        "name": "Harrison Rudolph",
        "type": "Person"
      },
      {
        "name": "Helen Suk",
        "type": "Person"
      },
      {
        "name": "Henry Aspegren",
        "type": "Person"
      },
      {
        "name": "Hunter Goldman",
        "type": "Person"
      },
      {
        "name": "Hongyuan Zhan",
        "type": "Person"
      },
      {
        "name": "Ibrahim Damlaj",
        "type": "Person"
      },
      {
        "name": "Igor Molybog",
        "type": "Person"
      },
      {
        "name": "Igor Tufanov",
        "type": "Person"
      },
      {
        "name": "Ilias Leontiadis",
        "type": "Person"
      },
      {
        "name": "Irina-Elena Veliche",
        "type": "Person"
      },
      {
        "name": "Itai Gat",
        "type": "Person"
      },
      {
        "name": "Jake Weissman",
        "type": "Person"
      },
      {
        "name": "James Geboski",
        "type": "Person"
      },
      {
        "name": "James Kohli",
        "type": "Person"
      },
      {
        "name": "Janice Lam",
        "type": "Person"
      },
      {
        "name": "Japhet Asher",
        "type": "Person"
      },
      {
        "name": "Jean-Baptiste Gaya",
        "type": "Person"
      },
      {
        "name": "Jeff Marcus",
        "type": "Person"
      },
      {
        "name": "Jeff Tang",
        "type": "Person"
      },
      {
        "name": "Jennifer Chan",
        "type": "Person"
      },
      {
        "name": "Jenny Zhen",
        "type": "Person"
      },
      {
        "name": "Jeremy Reizenstein",
        "type": "Person"
      },
      {
        "name": "Jeremy Teboul",
        "type": "Person"
      },
      {
        "name": "Jessica Zhong",
        "type": "Person"
      },
      {
        "name": "Jian Jin",
        "type": "Person"
      },
      {
        "name": "Jingyi Yang",
        "type": "Person"
      },
      {
        "name": "Joe Cummings",
        "type": "Person"
      },
      {
        "name": "Jon Carvill",
        "type": "Person"
      },
      {
        "name": "Jon Shepard",
        "type": "Person"
      },
      {
        "name": "Jonathan McPhie",
        "type": "Person"
      },
      {
        "name": "Jonathan Torres",
        "type": "Person"
      },
      {
        "name": "Josh Ginsburg",
        "type": "Person"
      },
      {
        "name": "Junjie Wang",
        "type": "Person"
      },
      {
        "name": "Kai Wu",
        "type": "Person"
      },
      {
        "name": "Kam Hou U",
        "type": "Person"
      },
      {
        "name": "Karan Saxena",
        "type": "Person"
      },
      {
        "name": "Kartikay Khandelwal",
        "type": "Person"
      },
      {
        "name": "Katayoun Zand",
        "type": "Person"
      },
      {
        "name": "Kathy Matosich",
        "type": "Person"
      },
      {
        "name": "Kaushik Veeraraghavan",
        "type": "Person"
      },
      {
        "name": "Kelly Michelena",
        "type": "Person"
      },
      {
        "name": "Keqian Li",
        "type": "Person"
      },
      {
        "name": "Kiran Jagadeesh",
        "type": "Person"
      },
      {
        "name": "Kun Huang",
        "type": "Person"
      },
      {
        "name": "Kunal Chawla",
        "type": "Person"
      },
      {
        "name": "Kyle Huang",
        "type": "Person"
      },
      {
        "name": "Lailin Chen",
        "type": "Person"
      },
      {
        "name": "Lakshya Garg",
        "type": "Person"
      },
      {
        "name": "Lavender A",
        "type": "Person"
      },
      {
        "name": "Leandro Silva",
        "type": "Person"
      },
      {
        "name": "Lee Bell",
        "type": "Person"
      },
      {
        "name": "Lei Zhang",
        "type": "Person"
      },
      {
        "name": "Liangpeng Guo",
        "type": "Person"
      },
      {
        "name": "Licheng Yu",
        "type": "Person"
      },
      {
        "name": "Liron Moshkovich",
        "type": "Person"
      },
      {
        "name": "Luca Wehrstedt",
        "type": "Person"
      },
      {
        "name": "Madian Khabsa",
        "type": "Person"
      },
      {
        "name": "Manav Avalani",
        "type": "Person"
      },
      {
        "name": "Manish Bhatt",
        "type": "Person"
      },
      {
        "name": "Martynas Mankus",
        "type": "Person"
      },
      {
        "name": "Matan Hasson",
        "type": "Person"
      },
      {
        "name": "Matthew Lennie",
        "type": "Person"
      },
      {
        "name": "Matthias Reso",
        "type": "Person"
      },
      {
        "name": "Maxim Groshev",
        "type": "Person"
      },
      {
        "name": "Maxim Naumov",
        "type": "Person"
      },
      {
        "name": "Maya Lathi",
        "type": "Person"
      },
      {
        "name": "Meghan Keneally",
        "type": "Person"
      },
      {
        "name": "Miao Liu",
        "type": "Person"
      },
      {
        "name": "Michael L. Seltzer",
        "type": "Person"
      },
      {
        "name": "Michal Valko",
        "type": "Person"
      },
      {
        "name": "Michelle Restrepo",
        "type": "Person"
      },
      {
        "name": "Mihir Patel",
        "type": "Person"
      },
      {
        "name": "Mik Vyatskov",
        "type": "Person"
      },
      {
        "name": "Mikayel Samvelyan",
        "type": "Person"
      },
      {
        "name": "Mike Clark",
        "type": "Person"
      },
      {
        "name": "Mike Macey",
        "type": "Person"
      },
      {
        "name": "Mike Wang",
        "type": "Person"
      },
      {
        "name": "Miquel Jubert Hermoso",
        "type": "Person"
      },
      {
        "name": "Mo Metanat",
        "type": "Person"
      },
      {
        "name": "Mohammad Rastegari",
        "type": "Person"
      },
      {
        "name": "Munish Bansal",
        "type": "Person"
      },
      {
        "name": "Nandhini Santhanam",
        "type": "Person"
      },
      {
        "name": "Natascha Parks",
        "type": "Person"
      },
      {
        "name": "Natasha White",
        "type": "Person"
      },
      {
        "name": "Navyata Bawa",
        "type": "Person"
      },
      {
        "name": "Nayan Singhal",
        "type": "Person"
      },
      {
        "name": "Nick Egebo",
        "type": "Person"
      },
      {
        "name": "Nicolas Usunier",
        "type": "Person"
      },
      {
        "name": "Nikhil Mehta",
        "type": "Person"
      },
      {
        "name": "Nikolay Pavlovich Laptev",
        "type": "Person"
      },
      {
        "name": "Ning Dong",
        "type": "Person"
      },
      {
        "name": "Norman Cheng",
        "type": "Person"
      },
      {
        "name": "Oleg Chernoguz",
        "type": "Person"
      },
      {
        "name": "Olivia Hart",
        "type": "Person"
      },
      {
        "name": "Omkar Salpekar",
        "type": "Person"
      },
      {
        "name": "Ozlem Kalinli",
        "type": "Person"
      },
      {
        "name": "Parkin Kent",
        "type": "Person"
      },
      {
        "name": "Parth Parekh",
        "type": "Person"
      },
      {
        "name": "Paul Saab",
        "type": "Person"
      },
      {
        "name": "Pavan Balaji",
        "type": "Person"
      },
      {
        "name": "Pedro Rittner",
        "type": "Person"
      },
      {
        "name": "Philip Bontrager",
        "type": "Person"
      },
      {
        "name": "Pierre Roux",
        "type": "Person"
      },
      {
        "name": "Piotr Dollar",
        "type": "Person"
      },
      {
        "name": "Polina Zvyagina",
        "type": "Person"
      },
      {
        "name": "Prashant Ratanchandani",
        "type": "Person"
      },
      {
        "name": "Pritish Yuvraj",
        "type": "Person"
      },
      {
        "name": "Qian Liang",
        "type": "Person"
      },
      {
        "name": "Rachad Alao",
        "type": "Person"
      },
      {
        "name": "Rachel Rodriguez",
        "type": "Person"
      },
      {
        "name": "Rafi Ayub",
        "type": "Person"
      },
      {
        "name": "Raghotham Murthy",
        "type": "Person"
      },
      {
        "name": "Raghu Nayani",
        "type": "Person"
      },
      {
        "name": "Rahul Mitra",
        "type": "Person"
      },
      {
        "name": "Rangaprabhu Parthasarathy",
        "type": "Person"
      },
      {
        "name": "Raymond Li",
        "type": "Person"
      },
      {
        "name": "Rebekkah Hogan",
        "type": "Person"
      },
      {
        "name": "Robin Battey",
        "type": "Person"
      },
      {
        "name": "Rocky Wang",
        "type": "Person"
      },
      {
        "name": "Russ Howes",
        "type": "Person"
      },
      {
        "name": "Ruty Rinott",
        "type": "Person"
      },
      {
        "name": "Sachin Mehta",
        "type": "Person"
      },
      {
        "name": "Sachin Siby",
        "type": "Person"
      },
      {
        "name": "Sai Jayesh Bondu",
        "type": "Person"
      },
      {
        "name": "Samyak Datta",
        "type": "Person"
      },
      {
        "name": "Sara Chugh",
        "type": "Person"
      },
      {
        "name": "Sara Hunt",
        "type": "Person"
      },
      {
        "name": "Sargun Dhillon",
        "type": "Person"
      },
      {
        "name": "Sasha Sidorov",
        "type": "Person"
      },
      {
        "name": "Satadru Pan",
        "type": "Person"
      },
      {
        "name": "Saurabh Mahajan",
        "type": "Person"
      },
      {
        "name": "Saurabh Verma",
        "type": "Person"
      },
      {
        "name": "Seiji Yamamoto",
        "type": "Person"
      },
      {
        "name": "Sharadh Ramaswamy",
        "type": "Person"
      },
      {
        "name": "Shaun Lindsay",
        "type": "Person"
      },
      {
        "name": "Sheng Feng",
        "type": "Person"
      },
      {
        "name": "Shenghao Lin",
        "type": "Person"
      },
      {
        "name": "Shengxin Cindy Zha",
        "type": "Person"
      },
      {
        "name": "Shishir Patil",
        "type": "Person"
      },
      {
        "name": "Shiva Shankar",
        "type": "Person"
      },
      {
        "name": "Shuqiang Zhang",
        "type": "Person"
      },
      {
        "name": "Sinong Wang",
        "type": "Person"
      },
      {
        "name": "Sneha Agarwal",
        "type": "Person"
      },
      {
        "name": "Soji Sajuyigbe",
        "type": "Person"
      },
      {
        "name": "Soumith Chintala",
        "type": "Person"
      },
      {
        "name": "Stephanie Max",
        "type": "Person"
      },
      {
        "name": "Stephen Chen",
        "type": "Person"
      },
      {
        "name": "Steve Kehoe",
        "type": "Person"
      },
      {
        "name": "Steve Satterfield",
        "type": "Person"
      },
      {
        "name": "Sudarshan Govindaprasad",
        "type": "Person"
      },
      {
        "name": "Sumit Gupta",
        "type": "Person"
      },
      {
        "name": "Summer Deng",
        "type": "Person"
      },
      {
        "name": "Sungmin Cho",
        "type": "Person"
      },
      {
        "name": "Sunny Virk",
        "type": "Person"
      },
      {
        "name": "Suraj Subramanian",
        "type": "Person"
      },
      {
        "name": "Sy Choudhury",
        "type": "Person"
      },
      {
        "name": "Sydney Goldman",
        "type": "Person"
      },
      {
        "name": "Tal Remez",
        "type": "Person"
      },
      {
        "name": "Tamar Glaser",
        "type": "Person"
      },
      {
        "name": "Tamara Best",
        "type": "Person"
      },
      {
        "name": "Thilo Koehler",
        "type": "Person"
      },
      {
        "name": "Thomas Robinson",
        "type": "Person"
      },
      {
        "name": "Tianhe Li",
        "type": "Person"
      },
      {
        "name": "Tianjun Zhang",
        "type": "Person"
      },
      {
        "name": "Tim Matthews",
        "type": "Person"
      },
      {
        "name": "Timothy Chou",
        "type": "Person"
      },
      {
        "name": "Tzook Shaked",
        "type": "Person"
      },
      {
        "name": "Varun Vontimitta",
        "type": "Person"
      },
      {
        "name": "Victoria Ajayi",
        "type": "Person"
      },
      {
        "name": "Victoria Montanez",
        "type": "Person"
      },
      {
        "name": "Vijai Mohan",
        "type": "Person"
      },
      {
        "name": "Vinay Satish Kumar",
        "type": "Person"
      },
      {
        "name": "Vishal Mangla",
        "type": "Person"
      },
      {
        "name": "Vlad Ionescu",
        "type": "Person"
      },
      {
        "name": "Vlad Poenaru",
        "type": "Person"
      },
      {
        "name": "Vlad Tiberiu Mihailescu",
        "type": "Person"
      },
      {
        "name": "Vladimir Ivanov",
        "type": "Person"
      },
      {
        "name": "Wenchen Wang",
        "type": "Person"
      },
      {
        "name": "Wenwen Jiang",
        "type": "Person"
      },
      {
        "name": "Wes Bouaziz",
        "type": "Person"
      },
      {
        "name": "Will Constable",
        "type": "Person"
      },
      {
        "name": "Xiaocheng Tang",
        "type": "Person"
      },
      {
        "name": "Xiaojian Wu",
        "type": "Person"
      },
      {
        "name": "Xiaolan Wang",
        "type": "Person"
      },
      {
        "name": "Xilun Wu",
        "type": "Person"
      },
      {
        "name": "Xinbo Gao",
        "type": "Person"
      },
      {
        "name": "Yaniv Kleinman",
        "type": "Person"
      },
      {
        "name": "Yanjun Chen",
        "type": "Person"
      },
      {
        "name": "Ye Hu",
        "type": "Person"
      },
      {
        "name": "Ye Jia",
        "type": "Person"
      },
      {
        "name": "Ye Qi",
        "type": "Person"
      },
      {
        "name": "Yenda Li",
        "type": "Person"
      },
      {
        "name": "Yilin Zhang",
        "type": "Person"
      },
      {
        "name": "Yossi Adi",
        "type": "Person"
      },
      {
        "name": "Youngjin Nam",
        "type": "Person"
      },
      {
        "name": "Yu",
        "type": "Person"
      },
      {
        "name": "Wang",
        "type": "Person"
      },
      {
        "name": "Yu Zhao",
        "type": "Person"
      },
      {
        "name": "Yuchen Hao",
        "type": "Person"
      },
      {
        "name": "Yundi Qian",
        "type": "Person"
      },
      {
        "name": "Yunlu Li",
        "type": "Person"
      },
      {
        "name": "Yuzi He",
        "type": "Person"
      },
      {
        "name": "Zach Rait",
        "type": "Person"
      },
      {
        "name": "Zachary DeVito",
        "type": "Person"
      },
      {
        "name": "Zef Rosnbrick",
        "type": "Person"
      },
      {
        "name": "Zhaoduo Wen",
        "type": "Person"
      },
      {
        "name": "Zhenyu Yang",
        "type": "Person"
      },
      {
        "name": "Zhiwei Zhao",
        "type": "Person"
      },
      {
        "name": "Zhiyu Ma",
        "type": "Person"
      },
      {
        "name": "Tianyi Li",
        "type": "Person"
      },
      {
        "name": "Mingda Chen",
        "type": "Person"
      },
      {
        "name": "Bowei Guo",
        "type": "Person"
      },
      {
        "name": "Zhiqiang Shen",
        "type": "Person"
      },
      {
        "name": "Siyan Zhao",
        "type": "Person"
      },
      {
        "name": "Mengchen Liu",
        "type": "Person"
      },
      {
        "name": "Jing Huang",
        "type": "Person"
      },
      {
        "name": "Chenyu Wang",
        "type": "Person"
      },
      {
        "name": "Bo Liu",
        "type": "Person"
      },
      {
        "name": "Yuandong Tian",
        "type": "Person"
      },
      {
        "name": "Aditya Grover",
        "type": "Person"
      },
      {
        "name": "Feiyu Chen",
        "type": "Person"
      },
      {
        "name": "Marianne Arriola",
        "type": "Person"
      },
      {
        "name": "Yair Schiff",
        "type": "Person"
      },
      {
        "name": "Hao Phung",
        "type": "Person"
      },
      {
        "name": "Aaron Gokaslan",
        "type": "Person"
      },
      {
        "name": "Volodymyr Kuleshov",
        "type": "Person"
      },
      {
        "name": "Chenghao Fan",
        "type": "Person"
      },
      {
        "name": "Wen Heng",
        "type": "Person"
      },
      {
        "name": "Sichen Liu",
        "type": "Person"
      },
      {
        "name": "Yuxuan Song",
        "type": "Person"
      },
      {
        "name": "Jing Su",
        "type": "Person"
      },
      {
        "name": "Xiaoye Qu",
        "type": "Person"
      },
      {
        "name": "Kai Shen",
        "type": "Person"
      },
      {
        "name": "Wei Wei",
        "type": "Person"
      },
      {
        "name": "Chang Yang",
        "type": "Person"
      },
      {
        "name": "Chuang Zhou",
        "type": "Person"
      },
      {
        "name": "Yilin Xiao",
        "type": "Person"
      },
      {
        "name": "Su Dong",
        "type": "Person"
      },
      {
        "name": "Luyao Zhuang",
        "type": "Person"
      },
      {
        "name": "Yujing Zhang",
        "type": "Person"
      },
      {
        "name": "Zhu Wang",
        "type": "Person"
      },
      {
        "name": "Zijin Hong",
        "type": "Person"
      },
      {
        "name": "Zheng Yuan",
        "type": "Person"
      },
      {
        "name": "Zhishang Xiang",
        "type": "Person"
      },
      {
        "name": "Shengyuan Chen",
        "type": "Person"
      },
      {
        "name": "Huachi Zhou",
        "type": "Person"
      },
      {
        "name": "Qinggang Zhang",
        "type": "Person"
      },
      {
        "name": "Ninghao Liu",
        "type": "Person"
      },
      {
        "name": "Xinrun Wang",
        "type": "Person"
      },
      {
        "name": "Yi Chang",
        "type": "Person"
      },
      {
        "name": "Xiao Huang",
        "type": "Person"
      },
      {
        "name": "Hao Lu",
        "type": "Person"
      },
      {
        "name": "Haoyuan Huang",
        "type": "Person"
      },
      {
        "name": "Yulin Zhou",
        "type": "Person"
      },
      {
        "name": "Chen Li",
        "type": "Person"
      },
      {
        "name": "Ningxin Zhu",
        "type": "Person"
      },
      {
        "name": "Yu Cheng",
        "type": "Person"
      },
      {
        "name": "Jiuan Zhou",
        "type": "Person"
      },
      {
        "name": "Yongkang Hu",
        "type": "Person"
      },
      {
        "name": "Yihang Chen",
        "type": "Person"
      },
      {
        "name": "Huichi Zhou",
        "type": "Person"
      },
      {
        "name": "Mingang Chen",
        "type": "Person"
      },
      {
        "name": "Zhizhong Zhang",
        "type": "Person"
      },
      {
        "name": "Kun Shao",
        "type": "Person"
      },
      {
        "name": "Yuan Xie",
        "type": "Person"
      },
      {
        "name": "Zhaoxia Yin",
        "type": "Person"
      },
      {
        "name": "Qirui Mi",
        "type": "Person"
      },
      {
        "name": "Zhijian Ma",
        "type": "Person"
      },
      {
        "name": "Mengyue Yang",
        "type": "Person"
      },
      {
        "name": "Haoxuan Li",
        "type": "Person"
      },
      {
        "name": "Yisen Wang",
        "type": "Person"
      },
      {
        "name": "Haifeng Zhang",
        "type": "Person"
      },
      {
        "name": "Jun Wang",
        "type": "Person"
      },
      {
        "name": "Xiaoyu Tao",
        "type": "Person"
      },
      {
        "name": "Ze Guo",
        "type": "Person"
      },
      {
        "name": "Xu Jiang",
        "type": "Person"
      },
      {
        "name": "Carroll L. Wainwright",
        "type": "Person"
      },
      {
        "name": "Alex Ray",
        "type": "Person"
      },
      {
        "name": "Fraser Kelton",
        "type": "Person"
      },
      {
        "name": "Luke Miller",
        "type": "Person"
      },
      {
        "name": "Paul Christiano",
        "type": "Person"
      },
      {
        "name": "Paul Barham",
        "type": "Person"
      },
      {
        "name": "Charles Sutton",
        "type": "Person"
      },
      {
        "name": "Sebastian Gehrmann",
        "type": "Person"
      },
      {
        "name": "Parker Schuh",
        "type": "Person"
      },
      {
        "name": "Kensen Shi",
        "type": "Person"
      },
      {
        "name": "Sasha Tsvyashchenko",
        "type": "Person"
      },
      {
        "name": "Joshua Maynez",
        "type": "Person"
      },
      {
        "name": "Abhishek Rao",
        "type": "Person"
      },
      {
        "name": "Parker Barnes",
        "type": "Person"
      },
      {
        "name": "Vinodkumar Prabhakaran",
        "type": "Person"
      },
      {
        "name": "Emily Reif",
        "type": "Person"
      },
      {
        "name": "Nan Du",
        "type": "Person"
      },
      {
        "name": "Ben Hutchinson",
        "type": "Person"
      },
      {
        "name": "Reiner Pope",
        "type": "Person"
      },
      {
        "name": "Jacob Austin",
        "type": "Person"
      },
      {
        "name": "Michael Isard",
        "type": "Person"
      },
      {
        "name": "Guy Gur-Ari",
        "type": "Person"
      },
      {
        "name": "Pengcheng Yin",
        "type": "Person"
      },
      {
        "name": "Toju Duke",
        "type": "Person"
      },
      {
        "name": "Anselm Levskaya",
        "type": "Person"
      },
      {
        "name": "Sanjay Ghemawat",
        "type": "Person"
      },
      {
        "name": "Sunipa Dev",
        "type": "Person"
      },
      {
        "name": "Henryk Michalewski",
        "type": "Person"
      },
      {
        "name": "Xavier Garcia",
        "type": "Person"
      },
      {
        "name": "Vedant Misra",
        "type": "Person"
      },
      {
        "name": "Daphne Ippolito",
        "type": "Person"
      },
      {
        "name": "David Luan",
        "type": "Person"
      },
      {
        "name": "Hyeontaek Lim",
        "type": "Person"
      },
      {
        "name": "Alexander Spiridonov",
        "type": "Person"
      },
      {
        "name": "Ryan Sepassi",
        "type": "Person"
      },
      {
        "name": "Shivani Agrawal",
        "type": "Person"
      },
      {
        "name": "Mark Omernick",
        "type": "Person"
      },
      {
        "name": "Andrew M. Dai",
        "type": "Person"
      },
      {
        "name": "Thanumalayan Sankaranarayana Pillai",
        "type": "Person"
      },
      {
        "name": "Aitor Lewkowycz",
        "type": "Person"
      },
      {
        "name": "Erica Moreira",
        "type": "Person"
      },
      {
        "name": "Oleksandr Polozov",
        "type": "Person"
      },
      {
        "name": "Zongwei Zhou",
        "type": "Person"
      },
      {
        "name": "Brennan Saeta",
        "type": "Person"
      },
      {
        "name": "Mark Diaz",
        "type": "Person"
      },
      {
        "name": "Michele Catasta",
        "type": "Person"
      },
      {
        "name": "Kathy Meier-Hellstern",
        "type": "Person"
      },
      {
        "name": "Douglas Eck",
        "type": "Person"
      },
      {
        "name": "Noah Fiedel",
        "type": "Person"
      },
      {
        "name": "Shunyu Yao",
        "type": "Person"
      },
      {
        "name": "Jeffrey Zhao",
        "type": "Person"
      },
      {
        "name": "Dian Yu",
        "type": "Person"
      },
      {
        "name": "Izhak Shafran",
        "type": "Person"
      },
      {
        "name": "Karthik Narasimhan",
        "type": "Person"
      },
      {
        "name": "Yuan Cao",
        "type": "Person"
      },
      {
        "name": "Zhong-Zhi Li",
        "type": "Person"
      },
      {
        "name": "Duzhen Zhang",
        "type": "Person"
      },
      {
        "name": "Ming-Liang Zhang",
        "type": "Person"
      },
      {
        "name": "Jiaxin Zhang",
        "type": "Person"
      },
      {
        "name": "Zengyan Liu",
        "type": "Person"
      },
      {
        "name": "Yuxuan Yao",
        "type": "Person"
      },
      {
        "name": "Haotian Xu",
        "type": "Person"
      },
      {
        "name": "Junhao Zheng",
        "type": "Person"
      },
      {
        "name": "Pei-Jie Wang",
        "type": "Person"
      },
      {
        "name": "Xiuyi Chen",
        "type": "Person"
      },
      {
        "name": "Yingying Zhang",
        "type": "Person"
      },
      {
        "name": "Fei Yin",
        "type": "Person"
      },
      {
        "name": "Zhiwei Li",
        "type": "Person"
      },
      {
        "name": "Bao-Long Bi",
        "type": "Person"
      },
      {
        "name": "Ling-Rui Mei",
        "type": "Person"
      },
      {
        "name": "Junfeng Fang",
        "type": "Person"
      },
      {
        "name": "Xiao Liang",
        "type": "Person"
      },
      {
        "name": "Zhijiang Guo",
        "type": "Person"
      },
      {
        "name": "Le Song",
        "type": "Person"
      },
      {
        "name": "Cheng-Lin Liu",
        "type": "Person"
      },
      {
        "name": "Yuxuan Huang",
        "type": "Person"
      },
      {
        "name": "Haozheng Zhang",
        "type": "Person"
      },
      {
        "name": "Meng Fang",
        "type": "Person"
      },
      {
        "name": "Linyi Yang",
        "type": "Person"
      },
      {
        "name": "Xiaoguang Li",
        "type": "Person"
      },
      {
        "name": "Lifeng Shang",
        "type": "Person"
      },
      {
        "name": "Jianye Hao",
        "type": "Person"
      },
      {
        "name": "Zheyuan Yang",
        "type": "Person"
      },
      {
        "name": "Lyuhao Chen",
        "type": "Person"
      },
      {
        "name": "Arman Cohan",
        "type": "Person"
      },
      {
        "name": "Yilun Zhao",
        "type": "Person"
      },
      {
        "name": "Yuqian Wang",
        "type": "Person"
      },
      {
        "name": "Chengzhong Chu",
        "type": "Person"
      },
      {
        "name": "Yu Duan",
        "type": "Person"
      },
      {
        "name": "Mingkang Long",
        "type": "Person"
      },
      {
        "name": "Tingyue Pan",
        "type": "Person"
      },
      {
        "name": "Qingchuan Li",
        "type": "Person"
      },
      {
        "name": "Mingfan Pan",
        "type": "Person"
      },
      {
        "name": "Jinmiao Zhao",
        "type": "Person"
      },
      {
        "name": "Chuang Yu",
        "type": "Person"
      },
      {
        "name": "Zelin Shi",
        "type": "Person"
      },
      {
        "name": "Yunpeng Liu",
        "type": "Person"
      },
      {
        "name": "Yingdi Zhang",
        "type": "Person"
      },
      {
        "name": "Waleed Khalid",
        "type": "Person"
      },
      {
        "name": "Dmitry Ignatov",
        "type": "Person"
      },
      {
        "name": "Radu Timofte",
        "type": "Person"
      },
      {
        "name": "Yu Tian",
        "type": "Person"
      },
      {
        "name": "Zhongheng Yang",
        "type": "Person"
      },
      {
        "name": "Chenshi Liu",
        "type": "Person"
      },
      {
        "name": "Yiyun Su",
        "type": "Person"
      },
      {
        "name": "Ziwei Hong",
        "type": "Person"
      },
      {
        "name": "Zexi Gong",
        "type": "Person"
      },
      {
        "name": "Jingyuan Xu",
        "type": "Person"
      },
      {
        "name": "Yang Lu",
        "type": "Person"
      },
      {
        "name": "Haoyang Zhou",
        "type": "Person"
      },
      {
        "name": "Peng Wang",
        "type": "Person"
      },
      {
        "name": "Erzhi Wang",
        "type": "Person"
      },
      {
        "name": "Gongfa Li",
        "type": "Person"
      },
      {
        "name": "Tongjian Yu",
        "type": "Person"
      },
      {
        "name": "Isaac Robinson",
        "type": "Person"
      },
      {
        "name": "Peter Robicheaux",
        "type": "Person"
      },
      {
        "name": "Matvei Popov",
        "type": "Person"
      },
      {
        "name": "Neehar Peri",
        "type": "Person"
      },
      {
        "name": "Jinhui Yi",
        "type": "Person"
      },
      {
        "name": "Gina Lopez",
        "type": "Person"
      },
      {
        "name": "S. Hadir",
        "type": "Person"
      },
      {
        "name": "Jan Weyler",
        "type": "Person"
      },
      {
        "name": "Lasse Klingbeil",
        "type": "Person"
      },
      {
        "name": "Marion Deichmann",
        "type": "Person"
      },
      {
        "name": "Juergen Gall",
        "type": "Person"
      },
      {
        "name": "S. J. Seidel",
        "type": "Person"
      },
      {
        "name": "Haoding Xu",
        "type": "Person"
      },
      {
        "name": "Xuzhen He",
        "type": "Person"
      },
      {
        "name": "Shaoheng Dai",
        "type": "Person"
      },
      {
        "name": "Caihui Zhu",
        "type": "Person"
      },
      {
        "name": "F. Shan",
        "type": "Person"
      },
      {
        "name": "Qin Zhao",
        "type": "Person"
      },
      {
        "name": "Faning Dang",
        "type": "Person"
      },
      {
        "name": "Daichao Sheng",
        "type": "Person"
      },
      {
        "name": "Zhou Wang",
        "type": "Person"
      },
      {
        "name": "A. Bovik",
        "type": "Person"
      },
      {
        "name": "H. Sheikh",
        "type": "Person"
      },
      {
        "name": "Eero P. Simoncelli",
        "type": "Person"
      },
      {
        "name": "Wei Cao",
        "type": "Person"
      },
      {
        "name": "Fengrui Tian",
        "type": "Person"
      },
      {
        "name": "Yulun Wu",
        "type": "Person"
      },
      {
        "name": "Yingying Li",
        "type": "Person"
      },
      {
        "name": "Shenlong Wang",
        "type": "Person"
      },
      {
        "name": "Ning Yu",
        "type": "Person"
      },
      {
        "name": "Yaoyao Liu",
        "type": "Person"
      },
      {
        "name": "Yuxue Yang",
        "type": "Person"
      },
      {
        "name": "Lue Fan",
        "type": "Person"
      },
      {
        "name": "Ziqi Shi",
        "type": "Person"
      },
      {
        "name": "Junran Peng",
        "type": "Person"
      },
      {
        "name": "Feng Wang",
        "type": "Person"
      },
      {
        "name": "Zhaoxiang Zhang",
        "type": "Person"
      },
      {
        "name": "William Peebles",
        "type": "Person"
      },
      {
        "name": "Jianlin Su",
        "type": "Person"
      },
      {
        "name": "Yu Lu",
        "type": "Person"
      },
      {
        "name": "Shengfeng Pan",
        "type": "Person"
      },
      {
        "name": "Ahmed Murtadha",
        "type": "Person"
      },
      {
        "name": "Bo Wen",
        "type": "Person"
      },
      {
        "name": "Yunfeng Liu",
        "type": "Person"
      },
      {
        "name": "Sifan Tu",
        "type": "Person"
      },
      {
        "name": "Xin Zhou",
        "type": "Person"
      },
      {
        "name": "Dingkang Liang",
        "type": "Person"
      },
      {
        "name": "Xingyu Jiang",
        "type": "Person"
      },
      {
        "name": "Yumeng Zhang",
        "type": "Person"
      },
      {
        "name": "Xiaofan Li",
        "type": "Person"
      },
      {
        "name": "Xiang Bai",
        "type": "Person"
      },
      {
        "name": "Philip Lenz",
        "type": "Person"
      },
      {
        "name": "C. Stiller",
        "type": "Person"
      },
      {
        "name": "R. Urtasun",
        "type": "Person"
      },
      {
        "name": "S. Umeyama",
        "type": "Person"
      },
      {
        "name": "Run Wang",
        "type": "Person"
      },
      {
        "name": "Chaoyi Zhou",
        "type": "Person"
      },
      {
        "name": "Amir Salarpour",
        "type": "Person"
      },
      {
        "name": "Xi Liu",
        "type": "Person"
      },
      {
        "name": "Zhi-Qi Cheng",
        "type": "Person"
      },
      {
        "name": "Feng Luo",
        "type": "Person"
      },
      {
        "name": "Mert D. Pesé",
        "type": "Person"
      },
      {
        "name": "Siyu Huang",
        "type": "Person"
      },
      {
        "name": "Xingbang Hao",
        "type": "Person"
      },
      {
        "name": "Guigang Zhang",
        "type": "Person"
      },
      {
        "name": "Shang Ma",
        "type": "Person"
      },
      {
        "name": "Nikhil Keetha",
        "type": "Person"
      },
      {
        "name": "Norman Müller",
        "type": "Person"
      },
      {
        "name": "Johannes Schönberger",
        "type": "Person"
      },
      {
        "name": "Lorenzo Porzi",
        "type": "Person"
      },
      {
        "name": "Tobias Fischer",
        "type": "Person"
      },
      {
        "name": "Arno Knapitsch",
        "type": "Person"
      },
      {
        "name": "Duncan Zauss",
        "type": "Person"
      },
      {
        "name": "Ethan Weber",
        "type": "Person"
      },
      {
        "name": "Nelson Antunes",
        "type": "Person"
      },
      {
        "name": "Jonathon Luiten",
        "type": "Person"
      },
      {
        "name": "Manuel Lopez-Antequera",
        "type": "Person"
      },
      {
        "name": "Samuel Rota Bulò",
        "type": "Person"
      },
      {
        "name": "Christian Richardt",
        "type": "Person"
      },
      {
        "name": "Sebastian Scherer",
        "type": "Person"
      },
      {
        "name": "Peter Kontschieder",
        "type": "Person"
      },
      {
        "name": "Team Seedream",
        "type": "Person"
      },
      {
        "name": ":",
        "type": "Person"
      },
      {
        "name": "Yunpeng Chen",
        "type": "Person"
      },
      {
        "name": "Lixue Gong",
        "type": "Person"
      },
      {
        "name": "Meng Guo",
        "type": "Person"
      },
      {
        "name": "Zhiyao Guo",
        "type": "Person"
      },
      {
        "name": "Xiaoxia Hou",
        "type": "Person"
      },
      {
        "name": "Yixuan Huang",
        "type": "Person"
      },
      {
        "name": "Xiaowen Jian",
        "type": "Person"
      },
      {
        "name": "Huafeng Kuang",
        "type": "Person"
      },
      {
        "name": "Zhichao Lai",
        "type": "Person"
      },
      {
        "name": "Fanshi Li",
        "type": "Person"
      },
      {
        "name": "Xiaochen Lian",
        "type": "Person"
      },
      {
        "name": "Chao Liao",
        "type": "Person"
      },
      {
        "name": "Liyang Liu",
        "type": "Person"
      },
      {
        "name": "Yanzuo Lu",
        "type": "Person"
      },
      {
        "name": "Zhengxiong Luo",
        "type": "Person"
      },
      {
        "name": "Tongtong Ou",
        "type": "Person"
      },
      {
        "name": "Guang Shi",
        "type": "Person"
      },
      {
        "name": "Yichun Shi",
        "type": "Person"
      },
      {
        "name": "Shiqi Sun",
        "type": "Person"
      },
      {
        "name": "Xun Wang",
        "type": "Person"
      },
      {
        "name": "Ye Wang",
        "type": "Person"
      },
      {
        "name": "Guofeng Wu",
        "type": "Person"
      },
      {
        "name": "Wenxu Wu",
        "type": "Person"
      },
      {
        "name": "Yonghui Wu",
        "type": "Person"
      },
      {
        "name": "Xin Xia",
        "type": "Person"
      },
      {
        "name": "Shuang Xu",
        "type": "Person"
      },
      {
        "name": "Xin Yan",
        "type": "Person"
      },
      {
        "name": "Zhonghua Zhai",
        "type": "Person"
      },
      {
        "name": "Chenlin Zhang",
        "type": "Person"
      },
      {
        "name": "Qi Zhang",
        "type": "Person"
      },
      {
        "name": "Xinyu Zhang",
        "type": "Person"
      },
      {
        "name": "Yuwei Zhang",
        "type": "Person"
      },
      {
        "name": "Shijia Zhao",
        "type": "Person"
      },
      {
        "name": "Wenliang Zhao",
        "type": "Person"
      },
      {
        "name": "Wenjia Zhu",
        "type": "Person"
      },
      {
        "name": "Junyan Ye",
        "type": "Person"
      },
      {
        "name": "Dongzhi Jiang",
        "type": "Person"
      },
      {
        "name": "Zihao Wang",
        "type": "Person"
      },
      {
        "name": "Leqi Zhu",
        "type": "Person"
      },
      {
        "name": "Zhenghao Hu",
        "type": "Person"
      },
      {
        "name": "Zilong Huang",
        "type": "Person"
      },
      {
        "name": "Jun He",
        "type": "Person"
      },
      {
        "name": "Zhiyuan Yan",
        "type": "Person"
      },
      {
        "name": "Jinghua Yu",
        "type": "Person"
      },
      {
        "name": "Hongsheng Li",
        "type": "Person"
      },
      {
        "name": "Weijia Li",
        "type": "Person"
      },
      {
        "name": "Yi Xin",
        "type": "Person"
      },
      {
        "name": "Qi Qin",
        "type": "Person"
      },
      {
        "name": "Siqi Luo",
        "type": "Person"
      },
      {
        "name": "Juncheng Yan",
        "type": "Person"
      },
      {
        "name": "Yan Tai",
        "type": "Person"
      },
      {
        "name": "Jiayi Lei",
        "type": "Person"
      },
      {
        "name": "Yuewen Cao",
        "type": "Person"
      },
      {
        "name": "Keqi Wang",
        "type": "Person"
      },
      {
        "name": "Qian Yu",
        "type": "Person"
      },
      {
        "name": "Dengyang Jiang",
        "type": "Person"
      },
      {
        "name": "Yuandong Pu",
        "type": "Person"
      },
      {
        "name": "Haoxing Chen",
        "type": "Person"
      },
      {
        "name": "Le Zhuo",
        "type": "Person"
      },
      {
        "name": "Tianbin Li",
        "type": "Person"
      },
      {
        "name": "Ming Hu",
        "type": "Person"
      },
      {
        "name": "Jin Ye",
        "type": "Person"
      },
      {
        "name": "Bo Zhang",
        "type": "Person"
      },
      {
        "name": "Chang Xu",
        "type": "Person"
      },
      {
        "name": "Guangtao Zhai",
        "type": "Person"
      },
      {
        "name": "Yihao Liu",
        "type": "Person"
      },
      {
        "name": "NextStep Team",
        "type": "Person"
      },
      {
        "name": "Chunrui Han",
        "type": "Person"
      },
      {
        "name": "Guopeng Li",
        "type": "Person"
      },
      {
        "name": "Jingwei Wu",
        "type": "Person"
      },
      {
        "name": "Quan Sun",
        "type": "Person"
      },
      {
        "name": "Yan Cai",
        "type": "Person"
      },
      {
        "name": "Yuang Peng",
        "type": "Person"
      },
      {
        "name": "Zheng Ge",
        "type": "Person"
      },
      {
        "name": "Deyu Zhou",
        "type": "Person"
      },
      {
        "name": "Haomiao Tang",
        "type": "Person"
      },
      {
        "name": "Hongyu Zhou",
        "type": "Person"
      },
      {
        "name": "Kenkun Liu",
        "type": "Person"
      },
      {
        "name": "Ailin Huang",
        "type": "Person"
      },
      {
        "name": "Changxin Miao",
        "type": "Person"
      },
      {
        "name": "Deshan Sun",
        "type": "Person"
      },
      {
        "name": "En Yu",
        "type": "Person"
      },
      {
        "name": "Fukun Yin",
        "type": "Person"
      },
      {
        "name": "Gang Yu",
        "type": "Person"
      },
      {
        "name": "Hao Nie",
        "type": "Person"
      },
      {
        "name": "Haoran Lv",
        "type": "Person"
      },
      {
        "name": "Hanpeng Hu",
        "type": "Person"
      },
      {
        "name": "Jia Wang",
        "type": "Person"
      },
      {
        "name": "Jian Zhou",
        "type": "Person"
      },
      {
        "name": "Jianjian Sun",
        "type": "Person"
      },
      {
        "name": "Kaijun Tan",
        "type": "Person"
      },
      {
        "name": "Kang An",
        "type": "Person"
      },
      {
        "name": "Kangheng Lin",
        "type": "Person"
      },
      {
        "name": "Liang Zhao",
        "type": "Person"
      },
      {
        "name": "Mei Chen",
        "type": "Person"
      },
      {
        "name": "Peng Xing",
        "type": "Person"
      },
      {
        "name": "Shiyu Liu",
        "type": "Person"
      },
      {
        "name": "Shutao Xia",
        "type": "Person"
      },
      {
        "name": "Tianhao You",
        "type": "Person"
      },
      {
        "name": "Wei Ji",
        "type": "Person"
      },
      {
        "name": "Xianfang Zeng",
        "type": "Person"
      },
      {
        "name": "Xin Han",
        "type": "Person"
      },
      {
        "name": "Xuelin Zhang",
        "type": "Person"
      },
      {
        "name": "Yana Wei",
        "type": "Person"
      },
      {
        "name": "Yanming Xu",
        "type": "Person"
      },
      {
        "name": "Yimin Jiang",
        "type": "Person"
      },
      {
        "name": "Yingming Wang",
        "type": "Person"
      },
      {
        "name": "Yu Zhou",
        "type": "Person"
      },
      {
        "name": "Yucheng Han",
        "type": "Person"
      },
      {
        "name": "Ziyang Meng",
        "type": "Person"
      },
      {
        "name": "Binxing Jiao",
        "type": "Person"
      },
      {
        "name": "Daxin Jiang",
        "type": "Person"
      },
      {
        "name": "Yibo Zhu",
        "type": "Person"
      },
      {
        "name": "Yinhan Liu",
        "type": "Person"
      },
      {
        "name": "Myle Ott",
        "type": "Person"
      },
      {
        "name": "Jingfei Du",
        "type": "Person"
      },
      {
        "name": "Mandar Joshi",
        "type": "Person"
      },
      {
        "name": "Danqi Chen",
        "type": "Person"
      },
      {
        "name": "Omer Levy",
        "type": "Person"
      },
      {
        "name": "Veselin Stoyanov",
        "type": "Person"
      },
      {
        "name": "DeepSeek-AI",
        "type": "Person"
      },
      {
        "name": "Daya Guo",
        "type": "Person"
      },
      {
        "name": "Dejian Yang",
        "type": "Person"
      },
      {
        "name": "Haowei Zhang",
        "type": "Person"
      },
      {
        "name": "Junxiao Song",
        "type": "Person"
      },
      {
        "name": "Peiyi Wang",
        "type": "Person"
      },
      {
        "name": "Qihao Zhu",
        "type": "Person"
      },
      {
        "name": "Runxin Xu",
        "type": "Person"
      },
      {
        "name": "Ruoyu Zhang",
        "type": "Person"
      },
      {
        "name": "Shirong Ma",
        "type": "Person"
      },
      {
        "name": "Xiao Bi",
        "type": "Person"
      },
      {
        "name": "Xiaokang Zhang",
        "type": "Person"
      },
      {
        "name": "Yu Wu",
        "type": "Person"
      },
      {
        "name": "Z. F. Wu",
        "type": "Person"
      },
      {
        "name": "Zhibin Gou",
        "type": "Person"
      },
      {
        "name": "Zhihong Shao",
        "type": "Person"
      },
      {
        "name": "Zhuoshu Li",
        "type": "Person"
      },
      {
        "name": "Ziyi Gao",
        "type": "Person"
      },
      {
        "name": "Aixin Liu",
        "type": "Person"
      },
      {
        "name": "Bing Xue",
        "type": "Person"
      },
      {
        "name": "Bochao Wu",
        "type": "Person"
      },
      {
        "name": "Bei Feng",
        "type": "Person"
      },
      {
        "name": "Chengda Lu",
        "type": "Person"
      },
      {
        "name": "Chenggang Zhao",
        "type": "Person"
      },
      {
        "name": "Chengqi Deng",
        "type": "Person"
      },
      {
        "name": "Chenyu Zhang",
        "type": "Person"
      },
      {
        "name": "Chong Ruan",
        "type": "Person"
      },
      {
        "name": "Deli Chen",
        "type": "Person"
      },
      {
        "name": "Dongjie Ji",
        "type": "Person"
      },
      {
        "name": "Erhang Li",
        "type": "Person"
      },
      {
        "name": "Fangyun Lin",
        "type": "Person"
      },
      {
        "name": "Fucong Dai",
        "type": "Person"
      },
      {
        "name": "Fuli Luo",
        "type": "Person"
      },
      {
        "name": "Guangbo Hao",
        "type": "Person"
      },
      {
        "name": "Guanting Chen",
        "type": "Person"
      },
      {
        "name": "Guowei Li",
        "type": "Person"
      },
      {
        "name": "H. Zhang",
        "type": "Person"
      },
      {
        "name": "Han Bao",
        "type": "Person"
      },
      {
        "name": "Hanwei Xu",
        "type": "Person"
      },
      {
        "name": "Haocheng Wang",
        "type": "Person"
      },
      {
        "name": "Honghui Ding",
        "type": "Person"
      },
      {
        "name": "Huajian Xin",
        "type": "Person"
      },
      {
        "name": "Huazuo Gao",
        "type": "Person"
      },
      {
        "name": "Hui Qu",
        "type": "Person"
      },
      {
        "name": "Jianzhong Guo",
        "type": "Person"
      },
      {
        "name": "Jiawei Wang",
        "type": "Person"
      },
      {
        "name": "Jingchang Chen",
        "type": "Person"
      },
      {
        "name": "Jingyang Yuan",
        "type": "Person"
      },
      {
        "name": "Junjie Qiu",
        "type": "Person"
      },
      {
        "name": "Junlong Li",
        "type": "Person"
      },
      {
        "name": "J. L. Cai",
        "type": "Person"
      },
      {
        "name": "Jiaqi Ni",
        "type": "Person"
      },
      {
        "name": "Jian Liang",
        "type": "Person"
      },
      {
        "name": "Jin Chen",
        "type": "Person"
      },
      {
        "name": "Kai Dong",
        "type": "Person"
      },
      {
        "name": "Kai Hu",
        "type": "Person"
      },
      {
        "name": "Kaige Gao",
        "type": "Person"
      },
      {
        "name": "Kang Guan",
        "type": "Person"
      },
      {
        "name": "Kexin Huang",
        "type": "Person"
      },
      {
        "name": "Kuai Yu",
        "type": "Person"
      },
      {
        "name": "Lean Wang",
        "type": "Person"
      },
      {
        "name": "Lecong Zhang",
        "type": "Person"
      },
      {
        "name": "Litong Wang",
        "type": "Person"
      },
      {
        "name": "Liyue Zhang",
        "type": "Person"
      },
      {
        "name": "Lei Xu",
        "type": "Person"
      },
      {
        "name": "Leyi Xia",
        "type": "Person"
      },
      {
        "name": "Mingchuan Zhang",
        "type": "Person"
      },
      {
        "name": "Minghua Zhang",
        "type": "Person"
      },
      {
        "name": "Minghui Tang",
        "type": "Person"
      },
      {
        "name": "Meng Li",
        "type": "Person"
      },
      {
        "name": "Miaojun Wang",
        "type": "Person"
      },
      {
        "name": "Mingming Li",
        "type": "Person"
      },
      {
        "name": "Ning Tian",
        "type": "Person"
      },
      {
        "name": "Panpan Huang",
        "type": "Person"
      },
      {
        "name": "Qiancheng Wang",
        "type": "Person"
      },
      {
        "name": "Qiushi Du",
        "type": "Person"
      },
      {
        "name": "Ruiqi Ge",
        "type": "Person"
      },
      {
        "name": "Ruisong Zhang",
        "type": "Person"
      },
      {
        "name": "Ruizhe Pan",
        "type": "Person"
      },
      {
        "name": "Runji Wang",
        "type": "Person"
      },
      {
        "name": "R. J. Chen",
        "type": "Person"
      },
      {
        "name": "R. L. Jin",
        "type": "Person"
      },
      {
        "name": "Ruyi Chen",
        "type": "Person"
      },
      {
        "name": "Shanghao Lu",
        "type": "Person"
      },
      {
        "name": "Shangyan Zhou",
        "type": "Person"
      },
      {
        "name": "Shanhuang Chen",
        "type": "Person"
      },
      {
        "name": "Shengfeng Ye",
        "type": "Person"
      },
      {
        "name": "Shiyu Wang",
        "type": "Person"
      },
      {
        "name": "Shuiping Yu",
        "type": "Person"
      },
      {
        "name": "Shunfeng Zhou",
        "type": "Person"
      },
      {
        "name": "Shuting Pan",
        "type": "Person"
      },
      {
        "name": "S. S. Li",
        "type": "Person"
      },
      {
        "name": "Shuang Zhou",
        "type": "Person"
      },
      {
        "name": "Shaoqing Wu",
        "type": "Person"
      },
      {
        "name": "Tao Yun",
        "type": "Person"
      },
      {
        "name": "Tian Pei",
        "type": "Person"
      },
      {
        "name": "Tianyu Sun",
        "type": "Person"
      },
      {
        "name": "T. Wang",
        "type": "Person"
      },
      {
        "name": "Wanjia Zhao",
        "type": "Person"
      },
      {
        "name": "Wen Liu",
        "type": "Person"
      },
      {
        "name": "Wenjun Gao",
        "type": "Person"
      },
      {
        "name": "Wenqin Yu",
        "type": "Person"
      },
      {
        "name": "W. L. Xiao",
        "type": "Person"
      },
      {
        "name": "Wei An",
        "type": "Person"
      },
      {
        "name": "Xiaodong Liu",
        "type": "Person"
      },
      {
        "name": "Xiaohan Wang",
        "type": "Person"
      },
      {
        "name": "Xiaokang Chen",
        "type": "Person"
      },
      {
        "name": "Xiaotao Nie",
        "type": "Person"
      },
      {
        "name": "Xin Liu",
        "type": "Person"
      },
      {
        "name": "Xin Xie",
        "type": "Person"
      },
      {
        "name": "Xingchao Liu",
        "type": "Person"
      },
      {
        "name": "Xinyu Yang",
        "type": "Person"
      },
      {
        "name": "Xinyuan Li",
        "type": "Person"
      },
      {
        "name": "Xuecheng Su",
        "type": "Person"
      },
      {
        "name": "Xuheng Lin",
        "type": "Person"
      },
      {
        "name": "X. Q. Li",
        "type": "Person"
      },
      {
        "name": "Xiangyue Jin",
        "type": "Person"
      },
      {
        "name": "Xiaojin Shen",
        "type": "Person"
      },
      {
        "name": "Xiaosha Chen",
        "type": "Person"
      },
      {
        "name": "Xiaowen Sun",
        "type": "Person"
      },
      {
        "name": "Xiaoxiang Wang",
        "type": "Person"
      },
      {
        "name": "Xinnan Song",
        "type": "Person"
      },
      {
        "name": "Xinyi Zhou",
        "type": "Person"
      },
      {
        "name": "Xianzu Wang",
        "type": "Person"
      },
      {
        "name": "Xinxia Shan",
        "type": "Person"
      },
      {
        "name": "Y. K. Li",
        "type": "Person"
      },
      {
        "name": "Y. Q. Wang",
        "type": "Person"
      },
      {
        "name": "Y. X. Wei",
        "type": "Person"
      },
      {
        "name": "Yang Zhang",
        "type": "Person"
      },
      {
        "name": "Yanhong Xu",
        "type": "Person"
      },
      {
        "name": "Yao Li",
        "type": "Person"
      },
      {
        "name": "Yao Zhao",
        "type": "Person"
      },
      {
        "name": "Yaofeng Sun",
        "type": "Person"
      },
      {
        "name": "Yaohui Wang",
        "type": "Person"
      },
      {
        "name": "Yi Yu",
        "type": "Person"
      },
      {
        "name": "Yichao Zhang",
        "type": "Person"
      },
      {
        "name": "Yifan Shi",
        "type": "Person"
      },
      {
        "name": "Yiliang Xiong",
        "type": "Person"
      },
      {
        "name": "Ying He",
        "type": "Person"
      },
      {
        "name": "Yishi Piao",
        "type": "Person"
      },
      {
        "name": "Yisong Wang",
        "type": "Person"
      },
      {
        "name": "Yixuan Tan",
        "type": "Person"
      },
      {
        "name": "Yiyang Ma",
        "type": "Person"
      },
      {
        "name": "Yiyuan Liu",
        "type": "Person"
      },
      {
        "name": "Yongqiang Guo",
        "type": "Person"
      },
      {
        "name": "Yuan Ou",
        "type": "Person"
      },
      {
        "name": "Yuduan Wang",
        "type": "Person"
      },
      {
        "name": "Yue Gong",
        "type": "Person"
      },
      {
        "name": "Yuheng Zou",
        "type": "Person"
      },
      {
        "name": "Yujia He",
        "type": "Person"
      },
      {
        "name": "Yunfan Xiong",
        "type": "Person"
      },
      {
        "name": "Yuxiang Luo",
        "type": "Person"
      },
      {
        "name": "Yuxiang You",
        "type": "Person"
      },
      {
        "name": "Yuxuan Liu",
        "type": "Person"
      },
      {
        "name": "Yuyang Zhou",
        "type": "Person"
      },
      {
        "name": "Y. X. Zhu",
        "type": "Person"
      },
      {
        "name": "Yaohui Li",
        "type": "Person"
      },
      {
        "name": "Yi Zheng",
        "type": "Person"
      },
      {
        "name": "Yuchen Zhu",
        "type": "Person"
      },
      {
        "name": "Yunxian Ma",
        "type": "Person"
      },
      {
        "name": "Ying Tang",
        "type": "Person"
      },
      {
        "name": "Yukun Zha",
        "type": "Person"
      },
      {
        "name": "Yuting Yan",
        "type": "Person"
      },
      {
        "name": "Z. Z. Ren",
        "type": "Person"
      },
      {
        "name": "Zehui Ren",
        "type": "Person"
      },
      {
        "name": "Zhangli Sha",
        "type": "Person"
      },
      {
        "name": "Zhe Fu",
        "type": "Person"
      },
      {
        "name": "Zhean Xu",
        "type": "Person"
      },
      {
        "name": "Zhengyan Zhang",
        "type": "Person"
      },
      {
        "name": "Zhicheng Ma",
        "type": "Person"
      },
      {
        "name": "Zhigang Yan",
        "type": "Person"
      },
      {
        "name": "Zhiyu Wu",
        "type": "Person"
      },
      {
        "name": "Zihui Gu",
        "type": "Person"
      },
      {
        "name": "Zijia Zhu",
        "type": "Person"
      },
      {
        "name": "Zijun Liu",
        "type": "Person"
      },
      {
        "name": "Zilin Li",
        "type": "Person"
      },
      {
        "name": "Ziwei Xie",
        "type": "Person"
      },
      {
        "name": "Ziyang Song",
        "type": "Person"
      },
      {
        "name": "Zizheng Pan",
        "type": "Person"
      },
      {
        "name": "Zhen Huang",
        "type": "Person"
      },
      {
        "name": "Zhipeng Xu",
        "type": "Person"
      },
      {
        "name": "Zhongyu Zhang",
        "type": "Person"
      },
      {
        "name": "Zhen Zhang",
        "type": "Person"
      },
      {
        "name": "Bowen Jin",
        "type": "Person"
      },
      {
        "name": "Hansi Zeng",
        "type": "Person"
      },
      {
        "name": "Zhenrui Yue",
        "type": "Person"
      },
      {
        "name": "Sercan Arik",
        "type": "Person"
      },
      {
        "name": "Dong Wang",
        "type": "Person"
      },
      {
        "name": "Hamed Zamani",
        "type": "Person"
      },
      {
        "name": "Jiawei Han",
        "type": "Person"
      },
      {
        "name": "Karan Singhal",
        "type": "Person"
      },
      {
        "name": "Tao Tu",
        "type": "Person"
      },
      {
        "name": "Juraj Gottweis",
        "type": "Person"
      },
      {
        "name": "R. Sayres",
        "type": "Person"
      },
      {
        "name": "Ellery Wulczyn",
        "type": "Person"
      },
      {
        "name": "Mohamed Amin",
        "type": "Person"
      },
      {
        "name": "Kevin Clark",
        "type": "Person"
      },
      {
        "name": "Stephen R. Pfohl",
        "type": "Person"
      },
      {
        "name": "Heather Cole-Lewis",
        "type": "Person"
      },
      {
        "name": "Darlene Neal",
        "type": "Person"
      },
      {
        "name": "Q. Rashid",
        "type": "Person"
      },
      {
        "name": "Mike Schaekermann",
        "type": "Person"
      },
      {
        "name": "Amy Wang",
        "type": "Person"
      },
      {
        "name": "Dev Dash",
        "type": "Person"
      },
      {
        "name": "Jonathan H. Chen",
        "type": "Person"
      },
      {
        "name": "Nigam H. Shah",
        "type": "Person"
      },
      {
        "name": "Sami Lachgar",
        "type": "Person"
      },
      {
        "name": "P. Mansfield",
        "type": "Person"
      },
      {
        "name": "Sushant Prakash",
        "type": "Person"
      },
      {
        "name": "Bradley Green",
        "type": "Person"
      },
      {
        "name": "Ewa Dominowska",
        "type": "Person"
      },
      {
        "name": "Nenad Tomašev",
        "type": "Person"
      },
      {
        "name": "Yun Liu",
        "type": "Person"
      },
      {
        "name": "Renee Wong",
        "type": "Person"
      },
      {
        "name": "Christopher Semturs",
        "type": "Person"
      },
      {
        "name": "S. Mahdavi",
        "type": "Person"
      },
      {
        "name": "Joelle K. Barral",
        "type": "Person"
      },
      {
        "name": "Dale R. Webster",
        "type": "Person"
      },
      {
        "name": "G. Corrado",
        "type": "Person"
      },
      {
        "name": "Yossi Matias",
        "type": "Person"
      },
      {
        "name": "A. Karthikesalingam",
        "type": "Person"
      },
      {
        "name": "Vivek Natarajan",
        "type": "Person"
      },
      {
        "name": "Tianzhe Chu",
        "type": "Person"
      },
      {
        "name": "Yuexiang Zhai",
        "type": "Person"
      },
      {
        "name": "Sergey Levine",
        "type": "Person"
      },
      {
        "name": "Yi Ma",
        "type": "Person"
      },
      {
        "name": "Peijia Lin",
        "type": "Person"
      },
      {
        "name": "Pin Chen",
        "type": "Person"
      },
      {
        "name": "Rui Jiao",
        "type": "Person"
      },
      {
        "name": "Qing Mo",
        "type": "Person"
      },
      {
        "name": "Jianhuan Cen",
        "type": "Person"
      },
      {
        "name": "Wenbing Huang",
        "type": "Person"
      },
      {
        "name": "Dan Huang",
        "type": "Person"
      },
      {
        "name": "Yutong Lu",
        "type": "Person"
      },
      {
        "name": "Wenqiang Sun",
        "type": "Person"
      },
      {
        "name": "Haiyu Zhang",
        "type": "Person"
      },
      {
        "name": "Haoyuan Wang",
        "type": "Person"
      },
      {
        "name": "Junta Wu",
        "type": "Person"
      },
      {
        "name": "Zehan Wang",
        "type": "Person"
      },
      {
        "name": "Zhenwei Wang",
        "type": "Person"
      },
      {
        "name": "Yunhong Wang",
        "type": "Person"
      },
      {
        "name": "Tengfei Wang",
        "type": "Person"
      },
      {
        "name": "Chunchao Guo",
        "type": "Person"
      },
      {
        "name": "Jianfeng Xiang",
        "type": "Person"
      },
      {
        "name": "Xiaoxue Chen",
        "type": "Person"
      },
      {
        "name": "Sicheng Xu",
        "type": "Person"
      },
      {
        "name": "Ruicheng Wang",
        "type": "Person"
      },
      {
        "name": "Zelong Lv",
        "type": "Person"
      },
      {
        "name": "Yu Deng",
        "type": "Person"
      },
      {
        "name": "Hongyuan Zhu",
        "type": "Person"
      },
      {
        "name": "Yue Dong",
        "type": "Person"
      },
      {
        "name": "Nicholas Jing Yuan",
        "type": "Person"
      },
      {
        "name": "Jiaolong Yang",
        "type": "Person"
      },
      {
        "name": "Basile Terver",
        "type": "Person"
      },
      {
        "name": "Tsung-Yen Yang",
        "type": "Person"
      },
      {
        "name": "Jean Ponce",
        "type": "Person"
      },
      {
        "name": "Adrien Bardes",
        "type": "Person"
      },
      {
        "name": "Guangyi Zhang",
        "type": "Person"
      },
      {
        "name": "Hanlei Li",
        "type": "Person"
      },
      {
        "name": "Yunlong Cai",
        "type": "Person"
      },
      {
        "name": "Qiyu Hu",
        "type": "Person"
      },
      {
        "name": "Guanding Yu",
        "type": "Person"
      },
      {
        "name": "Zhijing Qin",
        "type": "Person"
      },
      {
        "name": "Wenjun Lin",
        "type": "Person"
      },
      {
        "name": "Jensen Zhang",
        "type": "Person"
      },
      {
        "name": "Kaitong Cai",
        "type": "Person"
      },
      {
        "name": "Transformer",
        "type": "TechArticle"
      },
      {
        "name": "WMT 2014 English-to-German translation task",
        "type": "Dataset"
      },
      {
        "name": "WMT 2014 English-to-French translation task",
        "type": "Dataset"
      },
      {
        "name": "BLEU",
        "type": "SoftwareApplication"
      },
      {
        "name": "English constituency parsing",
        "type": "Dataset"
      },
      {
        "name": "residual learning framework",
        "type": "CreativeWork"
      },
      {
        "name": "residual networks",
        "type": "CreativeWork"
      },
      {
        "name": "VGG nets",
        "type": "CreativeWork"
      },
      {
        "name": "ImageNet dataset",
        "type": "Dataset"
      },
      {
        "name": "ILSVRC 2015 classification task",
        "type": "Dataset"
      },
      {
        "name": "CIFAR-10",
        "type": "Dataset"
      },
      {
        "name": "COCO object detection dataset",
        "type": "Dataset"
      },
      {
        "name": "ILSVRC & COCO 2015 competitions",
        "type": "Dataset"
      },
      {
        "name": "ImageNet detection",
        "type": "Dataset"
      },
      {
        "name": "ImageNet localization",
        "type": "Dataset"
      },
      {
        "name": "COCO detection",
        "type": "Dataset"
      },
      {
        "name": "COCO segmentation",
        "type": "Dataset"
      },
      {
        "name": "Adam",
        "type": "SoftwareApplication"
      },
      {
        "name": "AdaMax",
        "type": "SoftwareApplication"
      },
      {
        "name": "other stochastic optimization methods",
        "type": "SoftwareApplication"
      },
      {
        "name": "Dropout",
        "type": "SoftwareApplication"
      },
      {
        "name": "neural networks",
        "type": "SoftwareApplication"
      },
      {
        "name": "ILSVRC 2012 classification challenge validation set",
        "type": "Dataset"
      },
      {
        "name": "top-1 error",
        "type": "Dataset"
      },
      {
        "name": "top-5 error",
        "type": "Dataset"
      },
      {
        "name": "convolutional networks",
        "type": "TechArticle"
      },
      {
        "name": "Inception Architecture",
        "type": "TechArticle"
      },
      {
        "name": "InstaDrive",
        "type": "SoftwareApplication"
      },
      {
        "name": "Instance Flow Guider",
        "type": "SoftwareApplication"
      },
      {
        "name": "Spatial Geometric Aligner",
        "type": "SoftwareApplication"
      },
      {
        "name": "nuScenes dataset",
        "type": "Dataset"
      },
      {
        "name": "CARLA",
        "type": "SoftwareApplication"
      },
      {
        "name": "CNC-VLM",
        "type": "SoftwareApplication"
      },
      {
        "name": "RLHF",
        "type": "SoftwareApplication"
      },
      {
        "name": "multimodal learning",
        "type": "SoftwareApplication"
      },
      {
        "name": "CNC fault detection",
        "type": "Dataset"
      },
      {
        "name": "Bidirectional Long Short-Term Memory (BiLSTM)",
        "type": "SoftwareApplication"
      },
      {
        "name": "New York Independent System Operator",
        "type": "Dataset"
      },
      {
        "name": "MAE",
        "type": "SoftwareApplication"
      },
      {
        "name": "RMSE",
        "type": "SoftwareApplication"
      },
      {
        "name": "sMAPE",
        "type": "SoftwareApplication"
      },
      {
        "name": "MAPE",
        "type": "SoftwareApplication"
      },
      {
        "name": "R²",
        "type": "SoftwareApplication"
      },
      {
        "name": "four-point laser metric calibration",
        "type": "Dataset"
      },
      {
        "name": "mamba segmentation",
        "type": "SoftwareApplication"
      },
      {
        "name": "Alex Krizhevsky",
        "type": "Person"
      },
      {
        "name": "ImageNet",
        "type": "Dataset"
      },
      {
        "name": "deep convolutional neural network",
        "type": "CreativeWork"
      },
      {
        "name": "ImageNet Challenge 2014",
        "type": "Dataset"
      },
      {
        "name": "ConvNet models",
        "type": "SoftwareApplication"
      },
      {
        "name": "Faster R-CNN",
        "type": "SoftwareApplication"
      },
      {
        "name": "Region Proposal Network (RPN)",
        "type": "SoftwareApplication"
      },
      {
        "name": "SPPnet",
        "type": "SoftwareApplication"
      },
      {
        "name": "Fast R-CNN",
        "type": "SoftwareApplication"
      },
      {
        "name": "VGG-16",
        "type": "SoftwareApplication"
      },
      {
        "name": "PASCAL VOC 2007",
        "type": "Dataset"
      },
      {
        "name": "PASCAL VOC 2012",
        "type": "Dataset"
      },
      {
        "name": "MS COCO",
        "type": "Dataset"
      },
      {
        "name": "ILSVRC",
        "type": "Dataset"
      },
      {
        "name": "COCO 2015",
        "type": "Dataset"
      },
      {
        "name": "Federated Learning Optimal Transport (FLOT)",
        "type": "SoftwareApplication"
      },
      {
        "name": "GTSRB",
        "type": "Dataset"
      },
      {
        "name": "KBTS",
        "type": "Dataset"
      },
      {
        "name": "CIFAR10",
        "type": "Dataset"
      },
      {
        "name": "EMNIST",
        "type": "Dataset"
      },
      {
        "name": "DiffusionEngine",
        "type": "SoftwareApplication"
      },
      {
        "name": "object detection",
        "type": "Dataset"
      },
      {
        "name": "contrastive learning",
        "type": "SoftwareApplication"
      },
      {
        "name": "hyperspectral image prediction",
        "type": "Dataset"
      },
      {
        "name": "WarmGait",
        "type": "SoftwareApplication"
      },
      {
        "name": "Person re-identification (Re-ID)",
        "type": "TechArticle"
      },
      {
        "name": "thermal array sensors",
        "type": "TechArticle"
      },
      {
        "name": "Taylor Finite Difference (TFD)",
        "type": "TechArticle"
      },
      {
        "name": "gait recognition",
        "type": "TechArticle"
      },
      {
        "name": "edge module",
        "type": "TechArticle"
      },
      {
        "name": "gait profiles",
        "type": "TechArticle"
      },
      {
        "name": "average recognition accuracy of 87.3%",
        "type": "Dataset"
      },
      {
        "name": "stochastic variational inference and learning algorithm",
        "type": "SoftwareApplication"
      },
      {
        "name": "variational lower bound",
        "type": "Dataset"
      },
      {
        "name": "lower bound estimator",
        "type": "SoftwareApplication"
      },
      {
        "name": "standard stochastic gradient methods",
        "type": "SoftwareApplication"
      },
      {
        "name": "i.i.d. datasets",
        "type": "Dataset"
      },
      {
        "name": "approximate inference model",
        "type": "SoftwareApplication"
      },
      {
        "name": "recognition model",
        "type": "SoftwareApplication"
      },
      {
        "name": "Reducing the Dimensionality of Data with Neural Networks",
        "type": "Article"
      },
      {
        "name": "Online Learning",
        "type": "Dataset"
      },
      {
        "name": "Stochastic Optimization",
        "type": "Dataset"
      },
      {
        "name": "Recurrent neural networks",
        "type": "TechArticle"
      },
      {
        "name": "Connectionist Temporal Classification",
        "type": "TechArticle"
      },
      {
        "name": "Long Short-term Memory",
        "type": "TechArticle"
      },
      {
        "name": "deep recurrent neural networks",
        "type": "TechArticle"
      },
      {
        "name": "deep Long Short-term Memory RNNs",
        "type": "TechArticle"
      },
      {
        "name": "TIMIT phoneme recognition benchmark",
        "type": "Dataset"
      },
      {
        "name": "GAN",
        "type": "SoftwareApplication"
      },
      {
        "name": "PBD",
        "type": "SoftwareApplication"
      },
      {
        "name": "DWD",
        "type": "SoftwareApplication"
      },
      {
        "name": "seven benchmarks",
        "type": "Dataset"
      },
      {
        "name": "NianWang-HJJGCDX",
        "type": "Person"
      },
      {
        "name": "AdamW optimizer",
        "type": "SoftwareApplication"
      },
      {
        "name": "Adam optimizer",
        "type": "SoftwareApplication"
      },
      {
        "name": "face mask detection model",
        "type": "SoftwareApplication"
      },
      {
        "name": "Adam with L2-regularization",
        "type": "SoftwareApplication"
      },
      {
        "name": "Mixture-of-Experts (MoE)",
        "type": "SoftwareApplication"
      },
      {
        "name": "Transformers",
        "type": "SoftwareApplication"
      },
      {
        "name": "Engram",
        "type": "SoftwareApplication"
      },
      {
        "name": "MMLU",
        "type": "Dataset"
      },
      {
        "name": "CMMLU",
        "type": "Dataset"
      },
      {
        "name": "BBH",
        "type": "Dataset"
      },
      {
        "name": "ARC-Challenge",
        "type": "Dataset"
      },
      {
        "name": "HumanEval",
        "type": "Dataset"
      },
      {
        "name": "MATH",
        "type": "Dataset"
      },
      {
        "name": "Multi-Query NIAH",
        "type": "Dataset"
      },
      {
        "name": "Neural Hierarchical Interpolation for Time Series Forecasting (N-HiTS)",
        "type": "SoftwareApplication"
      },
      {
        "name": "Network-Based Expansion Analysis for Interpretable Time Series Forecasting (N-BEATS)",
        "type": "SoftwareApplication"
      },
      {
        "name": "long short-term memory (LSTM)",
        "type": "SoftwareApplication"
      },
      {
        "name": "Multi-Quantile Loss",
        "type": "SoftwareApplication"
      },
      {
        "name": "flood prediction",
        "type": "Dataset"
      },
      {
        "name": "headwater streams in Georgia and North Carolina, USA",
        "type": "Dataset"
      },
      {
        "name": "95th percentile prediction uncertainty (95 PPU)",
        "type": "Dataset"
      },
      {
        "name": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model",
        "type": "Report"
      },
      {
        "name": "harmonized Landsat and Sentinel-2 data",
        "type": "Dataset"
      },
      {
        "name": "NASA-IBM geospatial foundation model",
        "type": "SoftwareApplication"
      },
      {
        "name": "Inception",
        "type": "SoftwareApplication"
      },
      {
        "name": "GoogLeNet",
        "type": "SoftwareApplication"
      },
      {
        "name": "ImageNet Large-Scale Visual Recognition Challenge 2014",
        "type": "Dataset"
      },
      {
        "name": "ILSVRC 2014",
        "type": "Dataset"
      },
      {
        "name": "Batch Normalization",
        "type": "SoftwareApplication"
      },
      {
        "name": "state-of-the-art image classification model",
        "type": "SoftwareApplication"
      },
      {
        "name": "This paper",
        "type": "Article"
      },
      {
        "name": "Diffusion Transformers (DiT)",
        "type": "TechArticle"
      },
      {
        "name": "VAE encoder",
        "type": "TechArticle"
      },
      {
        "name": "Representation Autoencoders (RAEs)",
        "type": "TechArticle"
      },
      {
        "name": "DINO",
        "type": "TechArticle"
      },
      {
        "name": "SigLIP",
        "type": "TechArticle"
      },
      {
        "name": "DDT head",
        "type": "TechArticle"
      },
      {
        "name": "FID",
        "type": "Dataset"
      },
      {
        "name": "Cell2Sentence (C2S) framework",
        "type": "SoftwareApplication"
      },
      {
        "name": "Large Language Models (LLMs)",
        "type": "SoftwareApplication"
      },
      {
        "name": "C2S-Scale",
        "type": "SoftwareApplication"
      },
      {
        "name": "silmitasertib (CX-4945)",
        "type": "Dataset"
      },
      {
        "name": "single-cell RNA sequencing",
        "type": "Dataset"
      },
      {
        "name": "transcriptomic data",
        "type": "Dataset"
      },
      {
        "name": "biological text",
        "type": "Dataset"
      },
      {
        "name": "metadata",
        "type": "Dataset"
      },
      {
        "name": "human cell models",
        "type": "Dataset"
      },
      {
        "name": "HybridVisionNet",
        "type": "SoftwareApplication"
      },
      {
        "name": "fundus imaging",
        "type": "Dataset"
      },
      {
        "name": "DI",
        "type": "SoftwareApplication"
      },
      {
        "name": "DiffPure",
        "type": "SoftwareApplication"
      },
      {
        "name": "Google Cloud Vision",
        "type": "SoftwareApplication"
      },
      {
        "name": "transfer attacks",
        "type": "SoftwareApplication"
      },
      {
        "name": "defenses",
        "type": "SoftwareApplication"
      },
      {
        "name": "computer vision systems",
        "type": "SoftwareApplication"
      },
      {
        "name": "adversarial images",
        "type": "Dataset"
      },
      {
        "name": "imperceptibility metrics",
        "type": "Dataset"
      },
      {
        "name": "user study",
        "type": "Dataset"
      },
      {
        "name": "Lp constraint",
        "type": "Dataset"
      },
      {
        "name": "authors",
        "type": "Person"
      },
      {
        "name": "OGNet",
        "type": "SoftwareApplication"
      },
      {
        "name": "YOLO-OG",
        "type": "SoftwareApplication"
      },
      {
        "name": "Empty-dish Recycling Robot",
        "type": "CreativeWork"
      },
      {
        "name": "Dish-10",
        "type": "Dataset"
      },
      {
        "name": "Dish-20",
        "type": "Dataset"
      },
      {
        "name": "mean Average Precision (mAP)",
        "type": "TechArticle"
      },
      {
        "name": "Colossal Clean Crawled Corpus",
        "type": "Dataset"
      },
      {
        "name": "pre-trained models",
        "type": "SoftwareApplication"
      },
      {
        "name": "code",
        "type": "SoftwareApplication"
      },
      {
        "name": "summarization",
        "type": "Dataset"
      },
      {
        "name": "question answering",
        "type": "Dataset"
      },
      {
        "name": "text classification",
        "type": "Dataset"
      },
      {
        "name": "Two Time-Scale Update Rule",
        "type": "TechArticle"
      },
      {
        "name": "Local Nash Equilibrium",
        "type": "TechArticle"
      },
      {
        "name": "nuScenes",
        "type": "Dataset"
      },
      {
        "name": "KITTI dataset",
        "type": "Dataset"
      },
      {
        "name": "nuTonomy scenes",
        "type": "Dataset"
      },
      {
        "name": "Robust detection and tracking of objects",
        "type": "Report"
      },
      {
        "name": "3D detection and tracking metrics",
        "type": "Report"
      },
      {
        "name": "baselines for lidar and image based detection and tracking",
        "type": "Report"
      },
      {
        "name": "Image based benchmark datasets",
        "type": "Dataset"
      },
      {
        "name": "autonomous vehicle technology",
        "type": "TechArticle"
      },
      {
        "name": "computer vision tasks",
        "type": "TechArticle"
      },
      {
        "name": "machine learning based methods for detection and tracking",
        "type": "TechArticle"
      },
      {
        "name": "autonomous driving research",
        "type": "Report"
      },
      {
        "name": "classic modular pipeline",
        "type": "SoftwareApplication"
      },
      {
        "name": "end-to-end model trained via imitation learning",
        "type": "SoftwareApplication"
      },
      {
        "name": "end-to-end model trained via reinforcement learning",
        "type": "SoftwareApplication"
      },
      {
        "name": "autonomous urban driving systems",
        "type": "SoftwareApplication"
      },
      {
        "name": "Sora",
        "type": "SoftwareApplication"
      },
      {
        "name": "MNIST",
        "type": "Dataset"
      },
      {
        "name": "text-to-video generation",
        "type": "SoftwareApplication"
      },
      {
        "name": "world modeling",
        "type": "SoftwareApplication"
      },
      {
        "name": "OmniNWM",
        "type": "SoftwareApplication"
      },
      {
        "name": "autonomous driving world models",
        "type": "SoftwareApplication"
      },
      {
        "name": "panoramic videos",
        "type": "Dataset"
      },
      {
        "name": "3D occupancy",
        "type": "Dataset"
      },
      {
        "name": "normalized panoramic Plucker ray-map representation",
        "type": "SoftwareApplication"
      },
      {
        "name": "occupancy-grounded rewards",
        "type": "SoftwareApplication"
      },
      {
        "name": "ConsisDrive",
        "type": "SoftwareApplication"
      },
      {
        "name": "Instance-Masked Attention",
        "type": "SoftwareApplication"
      },
      {
        "name": "Instance-Masked Loss",
        "type": "SoftwareApplication"
      },
      {
        "name": "UniDriveDreamer",
        "type": "SoftwareApplication"
      },
      {
        "name": "LiDAR-specific variational autoencoder (VAE)",
        "type": "SoftwareApplication"
      },
      {
        "name": "video VAE",
        "type": "SoftwareApplication"
      },
      {
        "name": "Unified Latent Anchoring (ULA)",
        "type": "SoftwareApplication"
      },
      {
        "name": "diffusion transformer",
        "type": "SoftwareApplication"
      },
      {
        "name": "multi-camera video",
        "type": "Dataset"
      },
      {
        "name": "LiDAR sequence",
        "type": "Dataset"
      },
      {
        "name": "autonomous driving",
        "type": "TechArticle"
      },
      {
        "name": "SVD",
        "type": "SoftwareApplication"
      },
      {
        "name": "LTX",
        "type": "SoftwareApplication"
      },
      {
        "name": "MAD-LTX",
        "type": "SoftwareApplication"
      },
      {
        "name": "video diffusion models",
        "type": "SoftwareApplication"
      },
      {
        "name": "driving world models",
        "type": "SoftwareApplication"
      },
      {
        "name": "REPA",
        "type": "SoftwareApplication"
      },
      {
        "name": "iREPA",
        "type": "SoftwareApplication"
      },
      {
        "name": "ImageNet-1K",
        "type": "Dataset"
      },
      {
        "name": "REPA-E",
        "type": "SoftwareApplication"
      },
      {
        "name": "Meanflow",
        "type": "SoftwareApplication"
      },
      {
        "name": "JiT",
        "type": "SoftwareApplication"
      },
      {
        "name": "SCB-DETR",
        "type": "SoftwareApplication"
      },
      {
        "name": "SCBehavior dataset",
        "type": "Dataset"
      },
      {
        "name": "baseline model",
        "type": "SoftwareApplication"
      },
      {
        "name": "AP50",
        "type": "TechArticle"
      },
      {
        "name": "We",
        "type": "Person"
      },
      {
        "name": "ultrasound-cardiac-feature-net (UCF-Net)",
        "type": "SoftwareApplication"
      },
      {
        "name": "filtered integral quasi-super-twisting algorithm (FIQSTA)",
        "type": "SoftwareApplication"
      },
      {
        "name": "proportional (P) controller",
        "type": "SoftwareApplication"
      },
      {
        "name": "sliding mode controller",
        "type": "SoftwareApplication"
      },
      {
        "name": "super-twisting algorithm (STA)",
        "type": "SoftwareApplication"
      },
      {
        "name": "integral quasi-STA",
        "type": "SoftwareApplication"
      },
      {
        "name": "deep ultrasound image features",
        "type": "Dataset"
      },
      {
        "name": "cardiac phantom",
        "type": "Dataset"
      },
      {
        "name": "BioTune",
        "type": "SoftwareApplication"
      },
      {
        "name": "AutoRGN",
        "type": "SoftwareApplication"
      },
      {
        "name": "LoRA",
        "type": "SoftwareApplication"
      },
      {
        "name": "nine image classification datasets",
        "type": "Dataset"
      },
      {
        "name": "medical imaging",
        "type": "Dataset"
      },
      {
        "name": "four different CNN architectures",
        "type": "Dataset"
      },
      {
        "name": "VGG-16 net",
        "type": "SoftwareApplication"
      },
      {
        "name": "NUS dataset",
        "type": "Dataset"
      },
      {
        "name": "Google's open source Application Programming Interface (API)",
        "type": "SoftwareApplication"
      },
      {
        "name": "convolutional neural network",
        "type": "SoftwareApplication"
      },
      {
        "name": "Python",
        "type": "SoftwareApplication"
      },
      {
        "name": "Keras",
        "type": "SoftwareApplication"
      },
      {
        "name": "machine learning algorithms",
        "type": "SoftwareApplication"
      },
      {
        "name": "unsupervised feature learning",
        "type": "SoftwareApplication"
      },
      {
        "name": "deep learning",
        "type": "SoftwareApplication"
      },
      {
        "name": "probabilistic models",
        "type": "SoftwareApplication"
      },
      {
        "name": "auto-encoders",
        "type": "SoftwareApplication"
      },
      {
        "name": "manifold learning",
        "type": "SoftwareApplication"
      },
      {
        "name": "deep networks",
        "type": "SoftwareApplication"
      },
      {
        "name": "representation learning",
        "type": "SoftwareApplication"
      },
      {
        "name": "density estimation",
        "type": "SoftwareApplication"
      },
      {
        "name": "Stacked Denoising Autoencoders",
        "type": "SoftwareApplication"
      },
      {
        "name": "Stochastic Backpropagation",
        "type": "SoftwareApplication"
      },
      {
        "name": "Deep Generative Models",
        "type": "SoftwareApplication"
      },
      {
        "name": "SDXL",
        "type": "SoftwareApplication"
      },
      {
        "name": "SDXL-Lightning models",
        "type": "SoftwareApplication"
      },
      {
        "name": "UNet",
        "type": "SoftwareApplication"
      },
      {
        "name": "EchoMimic",
        "type": "SoftwareApplication"
      },
      {
        "name": "portrait image animation",
        "type": "CreativeWork"
      },
      {
        "name": "audios",
        "type": "Dataset"
      },
      {
        "name": "facial landmarks",
        "type": "Dataset"
      },
      {
        "name": "public datasets",
        "type": "Dataset"
      },
      {
        "name": "collected dataset",
        "type": "Dataset"
      },
      {
        "name": "GenAD",
        "type": "SoftwareApplication"
      },
      {
        "name": "variational autoencoder",
        "type": "SoftwareApplication"
      },
      {
        "name": "temporal model",
        "type": "SoftwareApplication"
      },
      {
        "name": "instance-centric scene tokenizer",
        "type": "SoftwareApplication"
      },
      {
        "name": "end-to-end autonomous driving methods",
        "type": "SoftwareApplication"
      },
      {
        "name": "Ovis",
        "type": "SoftwareApplication"
      },
      {
        "name": "Multimodal Large Language Models (MLLMs)",
        "type": "SoftwareApplication"
      },
      {
        "name": "LLM",
        "type": "SoftwareApplication"
      },
      {
        "name": "vision transformer",
        "type": "SoftwareApplication"
      },
      {
        "name": "MLP",
        "type": "SoftwareApplication"
      },
      {
        "name": "Qwen-VL-Plus",
        "type": "SoftwareApplication"
      },
      {
        "name": "various multimodal benchmarks",
        "type": "Dataset"
      },
      {
        "name": "VideoReward",
        "type": "SoftwareApplication"
      },
      {
        "name": "Flow-DPO",
        "type": "SoftwareApplication"
      },
      {
        "name": "Flow-RWR",
        "type": "SoftwareApplication"
      },
      {
        "name": "Flow-NRG",
        "type": "SoftwareApplication"
      },
      {
        "name": "rectified flow techniques",
        "type": "SoftwareApplication"
      },
      {
        "name": "large-scale human preference dataset",
        "type": "Dataset"
      },
      {
        "name": "modern video generation models",
        "type": "SoftwareApplication"
      },
      {
        "name": "supervised fine-tuning methods",
        "type": "SoftwareApplication"
      },
      {
        "name": "Long Short-term Memory RNN",
        "type": "TechArticle"
      },
      {
        "name": "Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks",
        "type": "Article"
      },
      {
        "name": "Recurrent Neural Networks",
        "type": "SoftwareApplication"
      },
      {
        "name": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures",
        "type": "Article"
      },
      {
        "name": "bidirectional LSTM",
        "type": "SoftwareApplication"
      },
      {
        "name": "neural network architectures",
        "type": "SoftwareApplication"
      },
      {
        "name": "machine learning",
        "type": "SoftwareApplication"
      },
      {
        "name": "artificial synapses",
        "type": "SoftwareApplication"
      },
      {
        "name": "flexible sensors",
        "type": "Dataset"
      },
      {
        "name": "flexible sensory systems",
        "type": "Dataset"
      },
      {
        "name": "soft/humanoid robotics",
        "type": "Dataset"
      },
      {
        "name": "human activity monitoring",
        "type": "Dataset"
      },
      {
        "name": "speech-to-text BCI",
        "type": "TechArticle"
      },
      {
        "name": "speech brain–computer interfaces (BCIs)",
        "type": "TechArticle"
      },
      {
        "name": "intracortical microelectrode arrays",
        "type": "Dataset"
      },
      {
        "name": "study participant",
        "type": "Person"
      },
      {
        "name": "word error rate",
        "type": "Dataset"
      },
      {
        "name": "50-word vocabulary",
        "type": "Dataset"
      },
      {
        "name": "125,000-word vocabulary",
        "type": "Dataset"
      },
      {
        "name": "neural code for speech",
        "type": "Dataset"
      },
      {
        "name": "deep-learning methods",
        "type": "TechArticle"
      },
      {
        "name": "backpropagation algorithm",
        "type": "TechArticle"
      },
      {
        "name": "continual backpropagation algorithm",
        "type": "TechArticle"
      },
      {
        "name": "artificial neural networks",
        "type": "TechArticle"
      },
      {
        "name": "gradient descent",
        "type": "TechArticle"
      },
      {
        "name": "analog-AI chip",
        "type": "TechArticle"
      },
      {
        "name": "speech-recognition tasks",
        "type": "Dataset"
      },
      {
        "name": "keyword-spotting network",
        "type": "SoftwareApplication"
      },
      {
        "name": "MLPerf",
        "type": "Dataset"
      },
      {
        "name": "recurrent neural-network transducer (RNNT)",
        "type": "SoftwareApplication"
      },
      {
        "name": "analog in-memory computing (analog-AI)",
        "type": "SoftwareApplication"
      },
      {
        "name": "graphics processing units",
        "type": "SoftwareApplication"
      },
      {
        "name": "central processing units",
        "type": "SoftwareApplication"
      },
      {
        "name": "BPE tokenizers",
        "type": "SoftwareApplication"
      },
      {
        "name": "LiteToken",
        "type": "SoftwareApplication"
      },
      {
        "name": "intermediate merge residues",
        "type": "Dataset"
      },
      {
        "name": "token fragmentation",
        "type": "Dataset"
      },
      {
        "name": "noisy or misspelled inputs",
        "type": "Dataset"
      },
      {
        "name": "MeKi",
        "type": "SoftwareApplication"
      },
      {
        "name": "Large Language Models",
        "type": "SoftwareApplication"
      },
      {
        "name": "dense LLM baselines",
        "type": "SoftwareApplication"
      },
      {
        "name": "Gengram",
        "type": "SoftwareApplication"
      },
      {
        "name": "genomic foundation models",
        "type": "SoftwareApplication"
      },
      {
        "name": "functional genomics tasks",
        "type": "Dataset"
      },
      {
        "name": "Large Lookup Layer (L$^3$)",
        "type": "CreativeWork"
      },
      {
        "name": "tokenizer embedding table",
        "type": "CreativeWork"
      },
      {
        "name": "transformers",
        "type": "CreativeWork"
      },
      {
        "name": "language modeling",
        "type": "Dataset"
      },
      {
        "name": "downstream tasks",
        "type": "Dataset"
      },
      {
        "name": "dense models",
        "type": "CreativeWork"
      },
      {
        "name": "iso-sparse MoEs",
        "type": "CreativeWork"
      },
      {
        "name": "Mixture-of-Experts (MoE) architectures",
        "type": "TechArticle"
      },
      {
        "name": "embedding scaling",
        "type": "TechArticle"
      },
      {
        "name": "expert scaling",
        "type": "TechArticle"
      },
      {
        "name": "LongCat-Flash-Lite",
        "type": "SoftwareApplication"
      },
      {
        "name": "parameter-equivalent MoE baselines",
        "type": "Dataset"
      },
      {
        "name": "existing models of comparable scale",
        "type": "Dataset"
      },
      {
        "name": "LeNet",
        "type": "SoftwareApplication"
      },
      {
        "name": "Lasso",
        "type": "SoftwareApplication"
      },
      {
        "name": "PASCAL",
        "type": "Dataset"
      },
      {
        "name": "SUN",
        "type": "Dataset"
      },
      {
        "name": "Deformable Parts Model",
        "type": "SoftwareApplication"
      },
      {
        "name": "LifeCLEF plant identification challenge",
        "type": "Report"
      },
      {
        "name": "dataset",
        "type": "Dataset"
      },
      {
        "name": "participatory sensing plateform",
        "type": "SoftwareApplication"
      },
      {
        "name": "FedMicro-IDA",
        "type": "SoftwareApplication"
      },
      {
        "name": "MaleVis",
        "type": "Dataset"
      },
      {
        "name": "IoT-malware detection and classification use case",
        "type": "Report"
      },
      {
        "name": "federated learning",
        "type": "SoftwareApplication"
      },
      {
        "name": "microservices-based architecture",
        "type": "SoftwareApplication"
      },
      {
        "name": "existing state-of-the-art methods",
        "type": "SoftwareApplication"
      },
      {
        "name": "YOLO",
        "type": "SoftwareApplication"
      },
      {
        "name": "large language models",
        "type": "SoftwareApplication"
      },
      {
        "name": "large vision models",
        "type": "SoftwareApplication"
      },
      {
        "name": "multimodal large language models",
        "type": "SoftwareApplication"
      },
      {
        "name": "Web of Science",
        "type": "Dataset"
      },
      {
        "name": "arXiv",
        "type": "Dataset"
      },
      {
        "name": "agricultural question-answering",
        "type": "SoftwareApplication"
      },
      {
        "name": "robotic automation",
        "type": "SoftwareApplication"
      },
      {
        "name": "advanced image analysis",
        "type": "SoftwareApplication"
      },
      {
        "name": "remote sensing",
        "type": "Dataset"
      },
      {
        "name": "spectral data",
        "type": "Dataset"
      },
      {
        "name": "pragmatic framework",
        "type": "Report"
      },
      {
        "name": "Multi-axis vision transformer",
        "type": "SoftwareApplication"
      },
      {
        "name": "medical image segmentation",
        "type": "Dataset"
      },
      {
        "name": "Active learning framework leveraging transcriptomics identifies modulators of disease phenotypes",
        "type": "Article"
      },
      {
        "name": "active deep-learning framework",
        "type": "SoftwareApplication"
      },
      {
        "name": "generalizable algorithm",
        "type": "SoftwareApplication"
      },
      {
        "name": "state-of-the-art models",
        "type": "SoftwareApplication"
      },
      {
        "name": "omics",
        "type": "Dataset"
      },
      {
        "name": "classical recall",
        "type": "Dataset"
      },
      {
        "name": "hematological discovery campaigns",
        "type": "Dataset"
      },
      {
        "name": "lab-in-the-loop signature refinement step",
        "type": "SoftwareApplication"
      },
      {
        "name": "LLaVA-OneVision-1.5",
        "type": "SoftwareApplication"
      },
      {
        "name": "LLaVA-OneVision-1.5-Mid-Traning",
        "type": "Dataset"
      },
      {
        "name": "LLaVA-OneVision-1.5-Instruct",
        "type": "Dataset"
      },
      {
        "name": "Qwen2.5-VL-7B",
        "type": "SoftwareApplication"
      },
      {
        "name": "Qwen2.5-VL-3B",
        "type": "SoftwareApplication"
      },
      {
        "name": "Vision-Language-Action (VLA) models",
        "type": "SoftwareApplication"
      },
      {
        "name": "large language models (LLMs)",
        "type": "SoftwareApplication"
      },
      {
        "name": "vision-language models (VLMs)",
        "type": "SoftwareApplication"
      },
      {
        "name": "robotics community",
        "type": "Person"
      },
      {
        "name": "publicly available datasets",
        "type": "Dataset"
      },
      {
        "name": "evaluation benchmarks",
        "type": "Dataset"
      },
      {
        "name": "teacher model",
        "type": "SoftwareApplication"
      },
      {
        "name": "human-aligned models",
        "type": "SoftwareApplication"
      },
      {
        "name": "pretrained state-of-the-art vision foundation models",
        "type": "SoftwareApplication"
      },
      {
        "name": "dataset of human judgements spanning multiple levels of semantic abstractions",
        "type": "Dataset"
      },
      {
        "name": "human judgements",
        "type": "Dataset"
      },
      {
        "name": "Vision Transformer (ViT)",
        "type": "SoftwareApplication"
      },
      {
        "name": "Transformer architecture",
        "type": "SoftwareApplication"
      },
      {
        "name": "CIFAR-100",
        "type": "Dataset"
      },
      {
        "name": "VTAB",
        "type": "Dataset"
      },
      {
        "name": "ResNet-50",
        "type": "SoftwareApplication"
      },
      {
        "name": "CLIP",
        "type": "SoftwareApplication"
      },
      {
        "name": "MeanFlow",
        "type": "SoftwareApplication"
      },
      {
        "name": "improved MeanFlow",
        "type": "SoftwareApplication"
      },
      {
        "name": "iMF",
        "type": "SoftwareApplication"
      },
      {
        "name": "ImageNet 256×256",
        "type": "Dataset"
      },
      {
        "name": "ImageNet-256",
        "type": "Dataset"
      },
      {
        "name": "ImageNet-512",
        "type": "Dataset"
      },
      {
        "name": "DiT",
        "type": "SoftwareApplication"
      },
      {
        "name": "diffusion model",
        "type": "SoftwareApplication"
      },
      {
        "name": "consistency model",
        "type": "SoftwareApplication"
      },
      {
        "name": "VAE",
        "type": "SoftwareApplication"
      },
      {
        "name": "pixel-space diffusion",
        "type": "SoftwareApplication"
      },
      {
        "name": "pixel-space consistency models",
        "type": "SoftwareApplication"
      },
      {
        "name": "two-stage training framework",
        "type": "SoftwareApplication"
      },
      {
        "name": "SVG (Self-supervised representations for Visual Generation)",
        "type": "CreativeWork"
      },
      {
        "name": "Visual Foundation Model (VFM)",
        "type": "CreativeWork"
      },
      {
        "name": "GenEval",
        "type": "Dataset"
      },
      {
        "name": "DPG-Bench",
        "type": "Dataset"
      },
      {
        "name": "autoencoder",
        "type": "SoftwareApplication"
      },
      {
        "name": "generation model",
        "type": "SoftwareApplication"
      },
      {
        "name": "PixelDiT",
        "type": "SoftwareApplication"
      },
      {
        "name": "Diffusion Transformers (DiTs)",
        "type": "SoftwareApplication"
      },
      {
        "name": "ImageNet 256x256",
        "type": "Dataset"
      },
      {
        "name": "DPG-bench",
        "type": "Dataset"
      },
      {
        "name": "latent diffusion models",
        "type": "SoftwareApplication"
      },
      {
        "name": "TUNA",
        "type": "SoftwareApplication"
      },
      {
        "name": "Unified multimodal models (UMMs)",
        "type": "SoftwareApplication"
      },
      {
        "name": "representation encoder",
        "type": "SoftwareApplication"
      },
      {
        "name": "multimodal understanding and generation benchmarks",
        "type": "Dataset"
      },
      {
        "name": "image and video understanding",
        "type": "Dataset"
      },
      {
        "name": "image and video generation",
        "type": "Dataset"
      },
      {
        "name": "image editing",
        "type": "Dataset"
      },
      {
        "name": "BERT",
        "type": "SoftwareApplication"
      },
      {
        "name": "GLUE",
        "type": "Dataset"
      },
      {
        "name": "MultiNLI",
        "type": "Dataset"
      },
      {
        "name": "SQuAD v1.1",
        "type": "Dataset"
      },
      {
        "name": "SQuAD v2.0",
        "type": "Dataset"
      },
      {
        "name": "traditional methods",
        "type": "CreativeWork"
      },
      {
        "name": "deep learning methods",
        "type": "CreativeWork"
      },
      {
        "name": "diffusion language models (DLMs)",
        "type": "SoftwareApplication"
      },
      {
        "name": "autoregressive (AR) models",
        "type": "SoftwareApplication"
      },
      {
        "name": "AR coder",
        "type": "SoftwareApplication"
      },
      {
        "name": "HellaSwag",
        "type": "Dataset"
      },
      {
        "name": "Python tokens",
        "type": "Dataset"
      },
      {
        "name": "DreamOn",
        "type": "SoftwareApplication"
      },
      {
        "name": "Diffusion Language Models (DLMs)",
        "type": "SoftwareApplication"
      },
      {
        "name": "Dream-Coder-7B",
        "type": "SoftwareApplication"
      },
      {
        "name": "DiffuCoder-7B",
        "type": "SoftwareApplication"
      },
      {
        "name": "HumanEval-Infilling",
        "type": "Dataset"
      },
      {
        "name": "SantaCoder-FIM",
        "type": "Dataset"
      },
      {
        "name": "Forward Learning with EXperience (FLEX)",
        "type": "SoftwareApplication"
      },
      {
        "name": "AIME25",
        "type": "Dataset"
      },
      {
        "name": "USPTO50k",
        "type": "Dataset"
      },
      {
        "name": "ProteinGym",
        "type": "Dataset"
      },
      {
        "name": "Agent-R1",
        "type": "SoftwareApplication"
      },
      {
        "name": "Reinforcement Learning (RL)",
        "type": "SoftwareApplication"
      },
      {
        "name": "Markov Decision Process (MDP)",
        "type": "SoftwareApplication"
      },
      {
        "name": "Multihop QA benchmark tasks",
        "type": "Dataset"
      },
      {
        "name": "Squeeze-and-Excitation block",
        "type": "TechArticle"
      },
      {
        "name": "SENet",
        "type": "SoftwareApplication"
      },
      {
        "name": "ILSVRC 2017 classification submission",
        "type": "Dataset"
      },
      {
        "name": "convolutional neural networks",
        "type": "TechArticle"
      },
      {
        "name": "prior research",
        "type": "Report"
      },
      {
        "name": "MobileNets",
        "type": "SoftwareApplication"
      },
      {
        "name": "ResNet",
        "type": "SoftwareApplication"
      },
      {
        "name": "EfficientNets",
        "type": "SoftwareApplication"
      },
      {
        "name": "EfficientNet-B7",
        "type": "SoftwareApplication"
      },
      {
        "name": "Flowers",
        "type": "Dataset"
      },
      {
        "name": "Light-X",
        "type": "SoftwareApplication"
      },
      {
        "name": "Light-Syn",
        "type": "SoftwareApplication"
      },
      {
        "name": "DriveLaW",
        "type": "SoftwareApplication"
      },
      {
        "name": "DriveLaW-Video",
        "type": "SoftwareApplication"
      },
      {
        "name": "DriveLaW-Act",
        "type": "SoftwareApplication"
      },
      {
        "name": "NAVSIM",
        "type": "Dataset"
      },
      {
        "name": "FVD",
        "type": "Dataset"
      },
      {
        "name": "OpenScene",
        "type": "Dataset"
      },
      {
        "name": "Waymo",
        "type": "Dataset"
      },
      {
        "name": "KITTI",
        "type": "Dataset"
      },
      {
        "name": "DDAD",
        "type": "Dataset"
      },
      {
        "name": "ControlNet",
        "type": "SoftwareApplication"
      },
      {
        "name": "Stable Diffusion",
        "type": "SoftwareApplication"
      },
      {
        "name": "PaLM",
        "type": "SoftwareApplication"
      },
      {
        "name": "T5",
        "type": "SoftwareApplication"
      },
      {
        "name": "U-PaLM",
        "type": "SoftwareApplication"
      },
      {
        "name": "Flan-PaLM 540B",
        "type": "SoftwareApplication"
      },
      {
        "name": "PALM 540B",
        "type": "SoftwareApplication"
      },
      {
        "name": "Flan-T5",
        "type": "SoftwareApplication"
      },
      {
        "name": "PaLM 62B",
        "type": "SoftwareApplication"
      },
      {
        "name": "TyDiQA",
        "type": "Dataset"
      },
      {
        "name": "MGSM",
        "type": "Dataset"
      },
      {
        "name": "diffusion models",
        "type": "SoftwareApplication"
      },
      {
        "name": "image inpainting",
        "type": "Dataset"
      },
      {
        "name": "unconditional image generation",
        "type": "Dataset"
      },
      {
        "name": "semantic scene synthesis",
        "type": "Dataset"
      },
      {
        "name": "super-resolution",
        "type": "Dataset"
      },
      {
        "name": "CompVis/latent-diffusion",
        "type": "SoftwareApplication"
      },
      {
        "name": "Variational Auto-Encoder",
        "type": "SoftwareApplication"
      },
      {
        "name": "PSNR",
        "type": "Dataset"
      },
      {
        "name": "SSIM",
        "type": "Dataset"
      },
      {
        "name": "VGG network",
        "type": "SoftwareApplication"
      },
      {
        "name": "new dataset of human perceptual similarity judgments",
        "type": "Dataset"
      },
      {
        "name": "Waymo Open Dataset",
        "type": "Dataset"
      },
      {
        "name": "research community",
        "type": "Person"
      },
      {
        "name": "we",
        "type": "Person"
      },
      {
        "name": "2D detection and tracking tasks",
        "type": "Report"
      },
      {
        "name": "3D detection and tracking tasks",
        "type": "Report"
      },
      {
        "name": "3D detection methods",
        "type": "Report"
      },
      {
        "name": "diversity metric",
        "type": "Report"
      },
      {
        "name": "human detection",
        "type": "Dataset"
      },
      {
        "name": "Generative Adversarial Networks (GANs)",
        "type": "SoftwareApplication"
      },
      {
        "name": "GANs",
        "type": "SoftwareApplication"
      },
      {
        "name": "Data augmentation",
        "type": "Dataset"
      },
      {
        "name": "face images generation",
        "type": "Dataset"
      },
      {
        "name": "two-layer ReLU denoising autoencoder (DAE)",
        "type": "TechArticle"
      },
      {
        "name": "real-world unconditional and text-to-image diffusion models",
        "type": "TechArticle"
      },
      {
        "name": "representation-based method for detecting memorization",
        "type": "TechArticle"
      },
      {
        "name": "training-free editing technique",
        "type": "TechArticle"
      },
      {
        "name": "memorization",
        "type": "TechArticle"
      },
      {
        "name": "generalization",
        "type": "TechArticle"
      },
      {
        "name": "balanced representations",
        "type": "TechArticle"
      },
      {
        "name": "localized spiky representations",
        "type": "TechArticle"
      },
      {
        "name": "local data statistics",
        "type": "TechArticle"
      },
      {
        "name": "deep generative models",
        "type": "TechArticle"
      },
      {
        "name": "generative modeling",
        "type": "TechArticle"
      },
      {
        "name": "Stable Velocity",
        "type": "SoftwareApplication"
      },
      {
        "name": "flow matching",
        "type": "SoftwareApplication"
      },
      {
        "name": "Stable Velocity Matching (StableVM)",
        "type": "SoftwareApplication"
      },
      {
        "name": "Variance-Aware Representation Alignment (VA-REPA)",
        "type": "SoftwareApplication"
      },
      {
        "name": "Stable Velocity Sampling (StableVS)",
        "type": "SoftwareApplication"
      },
      {
        "name": "SD3.5",
        "type": "SoftwareApplication"
      },
      {
        "name": "Flux",
        "type": "SoftwareApplication"
      },
      {
        "name": "Qwen-Image",
        "type": "SoftwareApplication"
      },
      {
        "name": "Wan2.2",
        "type": "SoftwareApplication"
      },
      {
        "name": "FlatDINO",
        "type": "SoftwareApplication"
      },
      {
        "name": "DINOv2",
        "type": "SoftwareApplication"
      },
      {
        "name": "DiT-XL",
        "type": "SoftwareApplication"
      },
      {
        "name": "gFID",
        "type": "Dataset"
      },
      {
        "name": "One-Dimensional Diffusion Video Autoencoder (One-DVA)",
        "type": "Article"
      },
      {
        "name": "3D-CNN VAEs",
        "type": "Article"
      },
      {
        "name": "video autoencoders",
        "type": "Article"
      },
      {
        "name": "video generation models",
        "type": "Article"
      },
      {
        "name": "transformer-based framework",
        "type": "Article"
      },
      {
        "name": "query-based vision transformers",
        "type": "Article"
      },
      {
        "name": "pixel-space diffusion transformer",
        "type": "Article"
      },
      {
        "name": "two-stage training strategy",
        "type": "Article"
      },
      {
        "name": "variable-length dropout mechanism",
        "type": "Article"
      },
      {
        "name": "reconstruction metrics",
        "type": "Dataset"
      },
      {
        "name": "latent distribution",
        "type": "Dataset"
      },
      {
        "name": "decoder",
        "type": "SoftwareApplication"
      },
      {
        "name": "encoder",
        "type": "SoftwareApplication"
      },
      {
        "name": "REPA-G",
        "type": "SoftwareApplication"
      },
      {
        "name": "COCO",
        "type": "Dataset"
      },
      {
        "name": "evolutionary optimization",
        "type": "TechArticle"
      },
      {
        "name": "MediaPipe",
        "type": "SoftwareApplication"
      },
      {
        "name": "Multi-layered Randomized Decision Forests",
        "type": "SoftwareApplication"
      },
      {
        "name": "geometry based normalizations",
        "type": "TechArticle"
      },
      {
        "name": "Krawtchouk moments",
        "type": "TechArticle"
      },
      {
        "name": "machine learning model",
        "type": "SoftwareApplication"
      },
      {
        "name": "virtual reality headset",
        "type": "SoftwareApplication"
      },
      {
        "name": "system meant to facilitate communication with hearing impaired individuals",
        "type": "SoftwareApplication"
      },
      {
        "name": "fully connected neural network (FCNN)",
        "type": "SoftwareApplication"
      },
      {
        "name": "American Sign Language (ASL)",
        "type": "Dataset"
      },
      {
        "name": "World Health Organization",
        "type": "Report"
      },
      {
        "name": "Sign Language Recognition (SLR)",
        "type": "CreativeWork"
      },
      {
        "name": "EM algorithm",
        "type": "SoftwareApplication"
      },
      {
        "name": "t-SNE",
        "type": "SoftwareApplication"
      },
      {
        "name": "LLM social simulations",
        "type": "CreativeWork"
      },
      {
        "name": "human research subjects",
        "type": "Dataset"
      },
      {
        "name": "LLMs",
        "type": "SoftwareApplication"
      },
      {
        "name": "social science datasets",
        "type": "Dataset"
      },
      {
        "name": "conceptual models",
        "type": "CreativeWork"
      },
      {
        "name": "iterative evaluations",
        "type": "CreativeWork"
      },
      {
        "name": "FCLFD",
        "type": "SoftwareApplication"
      },
      {
        "name": "CST",
        "type": "SoftwareApplication"
      },
      {
        "name": "AWS",
        "type": "SoftwareApplication"
      },
      {
        "name": "WISDM",
        "type": "Dataset"
      },
      {
        "name": "PAMAP2",
        "type": "Dataset"
      },
      {
        "name": "Dispersive Loss",
        "type": "SoftwareApplication"
      },
      {
        "name": "representation alignment (REPA)",
        "type": "SoftwareApplication"
      },
      {
        "name": "diffusion-based generative models",
        "type": "SoftwareApplication"
      },
      {
        "name": "Knowledge-Aware Bayesian Bandits (KABB)",
        "type": "SoftwareApplication"
      },
      {
        "name": "multi-agent systems",
        "type": "SoftwareApplication"
      },
      {
        "name": "three-dimensional knowledge distance model",
        "type": "SoftwareApplication"
      },
      {
        "name": "dual-adaptation mechanism",
        "type": "SoftwareApplication"
      },
      {
        "name": "knowledge-aware Thompson Sampling strategy",
        "type": "SoftwareApplication"
      },
      {
        "name": "sliding-window convolutional network",
        "type": "SoftwareApplication"
      },
      {
        "name": "ISBI challenge for segmentation of neuronal structures in electron microscopic stacks",
        "type": "Dataset"
      },
      {
        "name": "ISBI cell tracking challenge 2015",
        "type": "Dataset"
      },
      {
        "name": "Caffe",
        "type": "SoftwareApplication"
      },
      {
        "name": "Distribution Matching Distillation (DMD)",
        "type": "SoftwareApplication"
      },
      {
        "name": "DMD2",
        "type": "SoftwareApplication"
      },
      {
        "name": "ImageNet-64x64",
        "type": "Dataset"
      },
      {
        "name": "COCO 2014",
        "type": "Dataset"
      },
      {
        "name": "Latent Adversarial Diffusion Distillation (LADD)",
        "type": "Thesis"
      },
      {
        "name": "adversarial diffusion distillation (ADD)",
        "type": "Thesis"
      },
      {
        "name": "Stable Diffusion 3 (8B)",
        "type": "SoftwareApplication"
      },
      {
        "name": "SD3-Turbo",
        "type": "SoftwareApplication"
      },
      {
        "name": "Large language models (LLMs)",
        "type": "CreativeWork"
      },
      {
        "name": "Japanese LLM with Math reasoning capabilities",
        "type": "CreativeWork"
      },
      {
        "name": "Japanese Math LLM",
        "type": "CreativeWork"
      },
      {
        "name": "culturally-aware Japanese VLM",
        "type": "CreativeWork"
      },
      {
        "name": "Japanese LLM benchmarks",
        "type": "Dataset"
      },
      {
        "name": "Japanese culture-specific content",
        "type": "Dataset"
      },
      {
        "name": "evolutionary approach",
        "type": "CreativeWork"
      },
      {
        "name": "model merging",
        "type": "CreativeWork"
      },
      {
        "name": "open-source models",
        "type": "CreativeWork"
      },
      {
        "name": "previous Japanese VLMs",
        "type": "CreativeWork"
      },
      {
        "name": "bidirectional diffusion transformer",
        "type": "SoftwareApplication"
      },
      {
        "name": "autoregressive transformer",
        "type": "SoftwareApplication"
      },
      {
        "name": "distribution matching distillation (DMD)",
        "type": "SoftwareApplication"
      },
      {
        "name": "generator",
        "type": "SoftwareApplication"
      },
      {
        "name": "student initialization scheme",
        "type": "SoftwareApplication"
      },
      {
        "name": "asymmetric distillation strategy",
        "type": "SoftwareApplication"
      },
      {
        "name": "causal student model",
        "type": "SoftwareApplication"
      },
      {
        "name": "bidirectional teacher",
        "type": "SoftwareApplication"
      },
      {
        "name": "VBench-Long benchmark",
        "type": "Dataset"
      },
      {
        "name": "KV caching",
        "type": "SoftwareApplication"
      },
      {
        "name": "streaming video-to-video translation",
        "type": "SoftwareApplication"
      },
      {
        "name": "image-to-video",
        "type": "SoftwareApplication"
      },
      {
        "name": "dynamic prompting",
        "type": "SoftwareApplication"
      },
      {
        "name": "PuLID",
        "type": "SoftwareApplication"
      },
      {
        "name": "text-to-image generation",
        "type": "TechArticle"
      },
      {
        "name": "Lightning T2I branch",
        "type": "SoftwareApplication"
      },
      {
        "name": "standard diffusion",
        "type": "SoftwareApplication"
      },
      {
        "name": "contrastive alignment loss",
        "type": "Dataset"
      },
      {
        "name": "accurate ID loss",
        "type": "Dataset"
      },
      {
        "name": "ID fidelity",
        "type": "Dataset"
      },
      {
        "name": "editability",
        "type": "Dataset"
      },
      {
        "name": "image elements",
        "type": "Dataset"
      },
      {
        "name": "background",
        "type": "Dataset"
      },
      {
        "name": "lighting",
        "type": "Dataset"
      },
      {
        "name": "composition",
        "type": "Dataset"
      },
      {
        "name": "style",
        "type": "Dataset"
      },
      {
        "name": "ToTheBeginning",
        "type": "Person"
      },
      {
        "name": "diffusion probabilistic models",
        "type": "CreativeWork"
      },
      {
        "name": "LSUN",
        "type": "Dataset"
      },
      {
        "name": "ProgressiveGAN",
        "type": "CreativeWork"
      },
      {
        "name": "Inception score",
        "type": "CreativeWork"
      },
      {
        "name": "FID score",
        "type": "CreativeWork"
      },
      {
        "name": "HunyuanVideo",
        "type": "SoftwareApplication"
      },
      {
        "name": "Runway Gen-3",
        "type": "SoftwareApplication"
      },
      {
        "name": "Luma 1.6",
        "type": "SoftwareApplication"
      },
      {
        "name": "Tencent",
        "type": "Person"
      },
      {
        "name": "Loopy",
        "type": "SoftwareApplication"
      },
      {
        "name": "audio-conditioned human video generation",
        "type": "TechArticle"
      },
      {
        "name": "diffusion-based video generation techniques",
        "type": "TechArticle"
      },
      {
        "name": "audio-driven portrait diffusion models",
        "type": "TechArticle"
      },
      {
        "name": "audio-only conditioned video diffusion model",
        "type": "SoftwareApplication"
      },
      {
        "name": "inter- and intra-clip temporal module",
        "type": "SoftwareApplication"
      },
      {
        "name": "audio-to-latents module",
        "type": "SoftwareApplication"
      },
      {
        "name": "long-term motion information",
        "type": "Dataset"
      },
      {
        "name": "audio-portrait movement correlation",
        "type": "Dataset"
      },
      {
        "name": "spatial motion templates",
        "type": "Dataset"
      },
      {
        "name": "various scenarios",
        "type": "Dataset"
      },
      {
        "name": "OmniHuman",
        "type": "SoftwareApplication"
      },
      {
        "name": "Diffusion Transformer-based framework",
        "type": "SoftwareApplication"
      },
      {
        "name": "existing end-to-end audio-driven methods",
        "type": "SoftwareApplication"
      },
      {
        "name": "large general video generation models",
        "type": "SoftwareApplication"
      },
      {
        "name": "Hallo",
        "type": "Article"
      },
      {
        "name": "HDTF",
        "type": "Dataset"
      },
      {
        "name": "CelebV",
        "type": "Dataset"
      },
      {
        "name": "Wild",
        "type": "Dataset"
      },
      {
        "name": "EchoMimicV2",
        "type": "SoftwareApplication"
      },
      {
        "name": "Audio-Pose Dynamic Harmonization strategy",
        "type": "SoftwareApplication"
      },
      {
        "name": "Pose Sampling",
        "type": "SoftwareApplication"
      },
      {
        "name": "Audio Diffusion",
        "type": "SoftwareApplication"
      },
      {
        "name": "Head Partial Attention",
        "type": "SoftwareApplication"
      },
      {
        "name": "Phase-specific Denoising Loss",
        "type": "SoftwareApplication"
      },
      {
        "name": "novel benchmark for evaluating the effectiveness of half-body human animation",
        "type": "Dataset"
      },
      {
        "name": "Recent work on human animation",
        "type": "Article"
      },
      {
        "name": "Towards Striking, Simplified, and Semi-Body Human Animation",
        "type": "Article"
      },
      {
        "name": "Feature Pyramid Network (FPN)",
        "type": "TechArticle"
      },
      {
        "name": "COCO detection benchmark",
        "type": "Dataset"
      },
      {
        "name": "COCO 2016 challenge winners",
        "type": "Report"
      },
      {
        "name": "RNN Encoder-Decoder",
        "type": "SoftwareApplication"
      },
      {
        "name": "statistical machine translation system",
        "type": "SoftwareApplication"
      },
      {
        "name": "log-linear model",
        "type": "SoftwareApplication"
      },
      {
        "name": "DriveMLM",
        "type": "SoftwareApplication"
      },
      {
        "name": "Autonomous driving (AD)",
        "type": "TechArticle"
      },
      {
        "name": "Autopilot",
        "type": "SoftwareApplication"
      },
      {
        "name": "Apollo",
        "type": "SoftwareApplication"
      },
      {
        "name": "CARLA Town05 Long",
        "type": "Dataset"
      },
      {
        "name": "multimodal LLM (MLLM)",
        "type": "SoftwareApplication"
      },
      {
        "name": "Vista",
        "type": "SoftwareApplication"
      },
      {
        "name": "general-purpose video generator",
        "type": "SoftwareApplication"
      },
      {
        "name": "driving world model",
        "type": "SoftwareApplication"
      },
      {
        "name": "multiple datasets",
        "type": "Dataset"
      },
      {
        "name": "DiffusionDrive",
        "type": "SoftwareApplication"
      },
      {
        "name": "vanilla diffusion policy",
        "type": "SoftwareApplication"
      },
      {
        "name": "NAVSIM dataset",
        "type": "Dataset"
      },
      {
        "name": "ResNet-34",
        "type": "SoftwareApplication"
      },
      {
        "name": "2D simulation testbed for object movement and collisions",
        "type": "Dataset"
      },
      {
        "name": "diffusion-based video generation models",
        "type": "SoftwareApplication"
      },
      {
        "name": "Gemini",
        "type": "SoftwareApplication"
      },
      {
        "name": "Waymo Open Motion Dataset (WOMD)",
        "type": "Dataset"
      },
      {
        "name": "Waymo Open Dataset (WOD)",
        "type": "Dataset"
      },
      {
        "name": "GPT-3",
        "type": "SoftwareApplication"
      },
      {
        "name": "NLP tasks and benchmarks",
        "type": "Dataset"
      },
      {
        "name": "translation, question-answering, and cloze tasks",
        "type": "Dataset"
      },
      {
        "name": "unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic",
        "type": "Dataset"
      },
      {
        "name": "news articles",
        "type": "Dataset"
      },
      {
        "name": "InternVL 2.5",
        "type": "SoftwareApplication"
      },
      {
        "name": "InternVL 2.0",
        "type": "SoftwareApplication"
      },
      {
        "name": "GPT-4o",
        "type": "SoftwareApplication"
      },
      {
        "name": "Claude-3.5-Sonnet",
        "type": "SoftwareApplication"
      },
      {
        "name": "MMMU benchmark",
        "type": "Dataset"
      },
      {
        "name": "InternVL3",
        "type": "SoftwareApplication"
      },
      {
        "name": "InternVL3-78B",
        "type": "SoftwareApplication"
      },
      {
        "name": "ChatGPT-4o",
        "type": "SoftwareApplication"
      },
      {
        "name": "Claude 3.5 Sonnet",
        "type": "SoftwareApplication"
      },
      {
        "name": "Gemini 2.5 Pro",
        "type": "SoftwareApplication"
      },
      {
        "name": "PyTorch",
        "type": "SoftwareApplication"
      },
      {
        "name": "OpenVLM Leaderboard",
        "type": "Dataset"
      },
      {
        "name": "large multi-modality models",
        "type": "SoftwareApplication"
      },
      {
        "name": "multi-modal benchmarks",
        "type": "Dataset"
      },
      {
        "name": "LLaVA-CoT",
        "type": "SoftwareApplication"
      },
      {
        "name": "LLaVA-CoT-100k",
        "type": "Dataset"
      },
      {
        "name": "Gemini-1.5-pro",
        "type": "SoftwareApplication"
      },
      {
        "name": "GPT-4o-mini",
        "type": "SoftwareApplication"
      },
      {
        "name": "Llama-3.2-90B-Vision-Instruct",
        "type": "SoftwareApplication"
      },
      {
        "name": "InternVL 3.5",
        "type": "SoftwareApplication"
      },
      {
        "name": "Cascade Reinforcement Learning (Cascade RL) framework",
        "type": "SoftwareApplication"
      },
      {
        "name": "Visual Resolution Router (ViR)",
        "type": "SoftwareApplication"
      },
      {
        "name": "Decoupled Vision-Language Deployment (DvD) strategy",
        "type": "SoftwareApplication"
      },
      {
        "name": "InternVL3.5-241B-A28B",
        "type": "SoftwareApplication"
      },
      {
        "name": "GPT-5",
        "type": "SoftwareApplication"
      },
      {
        "name": "MMMU",
        "type": "Dataset"
      },
      {
        "name": "MathVista",
        "type": "Dataset"
      },
      {
        "name": "InternVL series",
        "type": "SoftwareApplication"
      },
      {
        "name": "proximal policy optimization (PPO)",
        "type": "SoftwareApplication"
      },
      {
        "name": "trust region policy optimization (TRPO)",
        "type": "SoftwareApplication"
      },
      {
        "name": "simulated robotic locomotion",
        "type": "Dataset"
      },
      {
        "name": "Atari game playing",
        "type": "Dataset"
      },
      {
        "name": "Flow-GRPO",
        "type": "SoftwareApplication"
      },
      {
        "name": "ODE-to-SDE conversion",
        "type": "SoftwareApplication"
      },
      {
        "name": "Denoising Reduction strategy",
        "type": "SoftwareApplication"
      },
      {
        "name": "SD3.5-M",
        "type": "SoftwareApplication"
      },
      {
        "name": "online policy gradient reinforcement learning (RL)",
        "type": "SoftwareApplication"
      },
      {
        "name": "flow matching models",
        "type": "SoftwareApplication"
      },
      {
        "name": "Ordinary Differential Equation (ODE)",
        "type": "SoftwareApplication"
      },
      {
        "name": "Stochastic Differential Equation (SDE)",
        "type": "SoftwareApplication"
      },
      {
        "name": "text-to-image tasks",
        "type": "Dataset"
      },
      {
        "name": "compositional generation",
        "type": "Dataset"
      },
      {
        "name": "visual text rendering",
        "type": "Dataset"
      },
      {
        "name": "human preference alignment",
        "type": "Dataset"
      },
      {
        "name": "DanceGRPO",
        "type": "SoftwareApplication"
      },
      {
        "name": "Group Relative Policy Optimization (GRPO)",
        "type": "SoftwareApplication"
      },
      {
        "name": "DDPO",
        "type": "SoftwareApplication"
      },
      {
        "name": "DPOK",
        "type": "SoftwareApplication"
      },
      {
        "name": "HPS-v2.1",
        "type": "Dataset"
      },
      {
        "name": "CLIP Score",
        "type": "Dataset"
      },
      {
        "name": "VideoAlign",
        "type": "Dataset"
      },
      {
        "name": "Reinforcement Learning from Human Feedback (RLHF)",
        "type": "SoftwareApplication"
      },
      {
        "name": "Seedance 1.0",
        "type": "Report"
      },
      {
        "name": "diffusion modeling",
        "type": "TechArticle"
      },
      {
        "name": "multi-source data curation",
        "type": "Dataset"
      },
      {
        "name": "precision and meaningful video captioning",
        "type": "Dataset"
      },
      {
        "name": "efficient architecture design",
        "type": "TechArticle"
      },
      {
        "name": "proposed training paradigm",
        "type": "TechArticle"
      },
      {
        "name": "multi-shot generation",
        "type": "TechArticle"
      },
      {
        "name": "text-to-video",
        "type": "TechArticle"
      },
      {
        "name": "fine-grained supervised fine-tuning",
        "type": "TechArticle"
      },
      {
        "name": "video-specific RLHF",
        "type": "TechArticle"
      },
      {
        "name": "multi-dimensional reward mechanisms",
        "type": "TechArticle"
      },
      {
        "name": "multi-stage distillation strategies",
        "type": "TechArticle"
      },
      {
        "name": "system-level optimizations",
        "type": "TechArticle"
      },
      {
        "name": "NVIDIA-L20",
        "type": "SoftwareApplication"
      },
      {
        "name": "SkyReels-V2",
        "type": "SoftwareApplication"
      },
      {
        "name": "SkyCaptioner-V1",
        "type": "SoftwareApplication"
      },
      {
        "name": "Multi-modal Large Language Model (MLLM)",
        "type": "SoftwareApplication"
      },
      {
        "name": "Multi-stage Pretraining",
        "type": "SoftwareApplication"
      },
      {
        "name": "Reinforcement Learning",
        "type": "SoftwareApplication"
      },
      {
        "name": "Diffusion Forcing Framework",
        "type": "SoftwareApplication"
      },
      {
        "name": "Supervised Fine-Tuning (SFT)",
        "type": "SoftwareApplication"
      },
      {
        "name": "Motion-specific Reinforcement Learning (RL) training",
        "type": "SoftwareApplication"
      },
      {
        "name": "diffusion forcing framework with non-decreasing noise schedules",
        "type": "SoftwareApplication"
      },
      {
        "name": "UnifiedReward",
        "type": "SoftwareApplication"
      },
      {
        "name": "Direct Preference Optimization (DPO)",
        "type": "SoftwareApplication"
      },
      {
        "name": "BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "type": "Article"
      },
      {
        "name": "Stanford Question Answering Dataset (SQuAD)",
        "type": "Dataset"
      },
      {
        "name": "logistic regression model",
        "type": "SoftwareApplication"
      },
      {
        "name": "simple baseline",
        "type": "SoftwareApplication"
      },
      {
        "name": "Neural machine translation (NMT) models",
        "type": "TechArticle"
      },
      {
        "name": "subword models",
        "type": "TechArticle"
      },
      {
        "name": "back-off dictionary baseline",
        "type": "TechArticle"
      },
      {
        "name": "WMT 15 translation tasks English-German and English-Russian",
        "type": "Dataset"
      },
      {
        "name": "SentencePiece",
        "type": "SoftwareApplication"
      },
      {
        "name": "Neural Machine Translation",
        "type": "TechArticle"
      },
      {
        "name": "English-Japanese machine translation",
        "type": "Dataset"
      },
      {
        "name": "GPT-4",
        "type": "SoftwareApplication"
      },
      {
        "name": "Transformer-based model",
        "type": "SoftwareApplication"
      },
      {
        "name": "bar exam",
        "type": "Dataset"
      },
      {
        "name": "professional and academic benchmarks",
        "type": "Dataset"
      },
      {
        "name": "LLaMA",
        "type": "SoftwareApplication"
      },
      {
        "name": "Chinchilla-70B",
        "type": "SoftwareApplication"
      },
      {
        "name": "PaLM-540B",
        "type": "SoftwareApplication"
      },
      {
        "name": "chain of thought",
        "type": "TechArticle"
      },
      {
        "name": "GSM8K benchmark",
        "type": "Dataset"
      },
      {
        "name": "GSM8K",
        "type": "Dataset"
      },
      {
        "name": "verifiers",
        "type": "SoftwareApplication"
      },
      {
        "name": "transformer models",
        "type": "SoftwareApplication"
      },
      {
        "name": "finetuning baseline",
        "type": "SoftwareApplication"
      },
      {
        "name": "Crystal structure of the nucleosome core particle at 2.8 Å resolution",
        "type": "Article"
      },
      {
        "name": "Mamba",
        "type": "SoftwareApplication"
      },
      {
        "name": "Mamba-3B",
        "type": "SoftwareApplication"
      },
      {
        "name": "structured state space models (SSMs)",
        "type": "SoftwareApplication"
      },
      {
        "name": "linear attention",
        "type": "SoftwareApplication"
      },
      {
        "name": "gated convolution",
        "type": "SoftwareApplication"
      },
      {
        "name": "recurrent models",
        "type": "SoftwareApplication"
      },
      {
        "name": "language",
        "type": "Dataset"
      },
      {
        "name": "audio",
        "type": "Dataset"
      },
      {
        "name": "genomics",
        "type": "Dataset"
      },
      {
        "name": "A catalogue of splice junction sequences",
        "type": "Report"
      },
      {
        "name": "stochastic gradient descent",
        "type": "SoftwareApplication"
      },
      {
        "name": "EEG recordings",
        "type": "Dataset"
      },
      {
        "name": "loshchil",
        "type": "Person"
      },
      {
        "name": "Sparsely-Gated Mixture-of-Experts layer (MoE)",
        "type": "SoftwareApplication"
      },
      {
        "name": "machine translation",
        "type": "Dataset"
      },
      {
        "name": "LSTM layers",
        "type": "SoftwareApplication"
      },
      {
        "name": "large language modeling benchmarks",
        "type": "Dataset"
      },
      {
        "name": "machine translation benchmarks",
        "type": "Dataset"
      },
      {
        "name": "pointer sentinel mixture architecture",
        "type": "TechArticle"
      },
      {
        "name": "pointer sentinel-LSTM model",
        "type": "TechArticle"
      },
      {
        "name": "Penn Treebank",
        "type": "Dataset"
      },
      {
        "name": "WikiText corpus",
        "type": "Dataset"
      },
      {
        "name": "MATH dataset",
        "type": "Dataset"
      },
      {
        "name": "PRM800K",
        "type": "Dataset"
      },
      {
        "name": "process-supervised model",
        "type": "SoftwareApplication"
      },
      {
        "name": "reward model",
        "type": "SoftwareApplication"
      },
      {
        "name": "GPQA",
        "type": "Dataset"
      },
      {
        "name": "GShard",
        "type": "SoftwareApplication"
      },
      {
        "name": "XLA compiler",
        "type": "SoftwareApplication"
      },
      {
        "name": "multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts",
        "type": "SoftwareApplication"
      },
      {
        "name": "TPU v3 accelerators",
        "type": "SoftwareApplication"
      },
      {
        "name": "prior art",
        "type": "Article"
      },
      {
        "name": "PASCAL VOC dataset",
        "type": "Dataset"
      },
      {
        "name": "R-CNN",
        "type": "SoftwareApplication"
      },
      {
        "name": "OverFeat",
        "type": "SoftwareApplication"
      },
      {
        "name": "ILSVRC2013 detection dataset",
        "type": "Dataset"
      },
      {
        "name": "mean average precision (mAP)",
        "type": "Dataset"
      },
      {
        "name": "Supertype-Preserving Low-Rank Adaptation (SuPLoRA)",
        "type": "TechArticle"
      },
      {
        "name": "supertype-subtype concept hierarchy",
        "type": "TechArticle"
      },
      {
        "name": "group-wise suppression method",
        "type": "TechArticle"
      },
      {
        "name": "standard diffusion regularization",
        "type": "TechArticle"
      },
      {
        "name": "concept erasure approaches",
        "type": "TechArticle"
      },
      {
        "name": "benchmark",
        "type": "Dataset"
      },
      {
        "name": "Multi-modal Chain and Global Attention Network (MCGA-Net)",
        "type": "SoftwareApplication"
      },
      {
        "name": "DCGAN-based data augmentation strategy",
        "type": "SoftwareApplication"
      },
      {
        "name": "Multi-modal Chain Feature Fusion (MCFF)",
        "type": "SoftwareApplication"
      },
      {
        "name": "Global Attention Mechanism (GAM)",
        "type": "SoftwareApplication"
      },
      {
        "name": "GPR images",
        "type": "Dataset"
      },
      {
        "name": "Precision",
        "type": "SoftwareApplication"
      },
      {
        "name": "Recall",
        "type": "SoftwareApplication"
      },
      {
        "name": "mAP@50",
        "type": "SoftwareApplication"
      },
      {
        "name": "Qwen3-VL-Embedding",
        "type": "SoftwareApplication"
      },
      {
        "name": "Qwen3-VL-Reranker",
        "type": "SoftwareApplication"
      },
      {
        "name": "Qwen3-VL",
        "type": "SoftwareApplication"
      },
      {
        "name": "MMEB-V2",
        "type": "Dataset"
      },
      {
        "name": "FPSMark",
        "type": "SoftwareApplication"
      },
      {
        "name": "intrinsic signal localization network",
        "type": "SoftwareApplication"
      },
      {
        "name": "hybrid loss",
        "type": "SoftwareApplication"
      },
      {
        "name": "existing methods",
        "type": "SoftwareApplication"
      },
      {
        "name": "Probability Distributions",
        "type": "Article"
      },
      {
        "name": "Linear Models for Regression",
        "type": "Article"
      },
      {
        "name": "Linear Models for Classification",
        "type": "Article"
      },
      {
        "name": "Neural Networks",
        "type": "Article"
      },
      {
        "name": "Kernel Methods",
        "type": "Article"
      },
      {
        "name": "Sparse Kernel Machines",
        "type": "Article"
      },
      {
        "name": "Graphical Models",
        "type": "Article"
      },
      {
        "name": "Mixture Models and EM",
        "type": "Article"
      },
      {
        "name": "Approximate Inference",
        "type": "Article"
      },
      {
        "name": "Sampling Methods",
        "type": "Article"
      },
      {
        "name": "Continuous Latent Variables",
        "type": "Article"
      },
      {
        "name": "Sequential Data",
        "type": "Article"
      },
      {
        "name": "Combining Models",
        "type": "Article"
      },
      {
        "name": "PlantVillage dataset",
        "type": "Dataset"
      },
      {
        "name": "deep VGG16 model",
        "type": "SoftwareApplication"
      },
      {
        "name": "deep convolutional neural networks",
        "type": "SoftwareApplication"
      },
      {
        "name": "botanists",
        "type": "Person"
      },
      {
        "name": "deep neural networks",
        "type": "SoftwareApplication"
      },
      {
        "name": "transfer learning parameters",
        "type": "Dataset"
      },
      {
        "name": "A 26-layer deep learning model consisting of 8 residual building blocks",
        "type": "Thesis"
      },
      {
        "name": "BJFU100 dataset",
        "type": "Dataset"
      },
      {
        "name": "Plant image identification",
        "type": "CreativeWork"
      },
      {
        "name": "herbarium images",
        "type": "Dataset"
      },
      {
        "name": "photos of plants in the field",
        "type": "Dataset"
      },
      {
        "name": "big dataset with thousands of species from herbaria",
        "type": "Dataset"
      },
      {
        "name": "different datasets from different herbaria",
        "type": "Dataset"
      },
      {
        "name": "automated system",
        "type": "SoftwareApplication"
      },
      {
        "name": "Dense Convolutional Network (DenseNet)",
        "type": "SoftwareApplication"
      },
      {
        "name": "SVHN",
        "type": "Dataset"
      },
      {
        "name": "MobileNetV2",
        "type": "SoftwareApplication"
      },
      {
        "name": "SSDLite",
        "type": "SoftwareApplication"
      },
      {
        "name": "DeepLabv3",
        "type": "SoftwareApplication"
      },
      {
        "name": "Mobile DeepLabv3",
        "type": "SoftwareApplication"
      },
      {
        "name": "Imagenet",
        "type": "Dataset"
      },
      {
        "name": "VOC",
        "type": "Dataset"
      },
      {
        "name": "multiply-adds (MAdd)",
        "type": "Dataset"
      },
      {
        "name": "Federated Learning",
        "type": "TechArticle"
      },
      {
        "name": "synchronized stochastic gradient descent",
        "type": "TechArticle"
      },
      {
        "name": "five different model architectures",
        "type": "Dataset"
      },
      {
        "name": "four datasets",
        "type": "Dataset"
      },
      {
        "name": "Amazon Reviews",
        "type": "Dataset"
      },
      {
        "name": "Reuters-21578",
        "type": "Dataset"
      },
      {
        "name": "Office-31",
        "type": "Dataset"
      },
      {
        "name": "federated deep neural network (FDNN)",
        "type": "SoftwareApplication"
      },
      {
        "name": "Flipkart dataset",
        "type": "Dataset"
      },
      {
        "name": "Term Frequency-Inverse Document Frequency (TF-IDF)",
        "type": "SoftwareApplication"
      },
      {
        "name": "Federated learning",
        "type": "SoftwareApplication"
      },
      {
        "name": "Internet of Things",
        "type": "SoftwareApplication"
      },
      {
        "name": "Wireless Sensor Networks",
        "type": "SoftwareApplication"
      },
      {
        "name": "privacy-preserving federated learning",
        "type": "SoftwareApplication"
      },
      {
        "name": "malware detection models",
        "type": "SoftwareApplication"
      },
      {
        "name": "Internet of Things resources",
        "type": "Dataset"
      },
      {
        "name": "Long Short-Term Memory Network",
        "type": "SoftwareApplication"
      },
      {
        "name": "centralized federated learning framework",
        "type": "SoftwareApplication"
      },
      {
        "name": "decentralized federated learning framework",
        "type": "SoftwareApplication"
      },
      {
        "name": "crop yield prediction",
        "type": "Dataset"
      },
      {
        "name": "prediction accuracy",
        "type": "Dataset"
      },
      {
        "name": "precision",
        "type": "Dataset"
      },
      {
        "name": "recall",
        "type": "Dataset"
      },
      {
        "name": "F1-Score",
        "type": "Dataset"
      },
      {
        "name": "training time",
        "type": "Dataset"
      },
      {
        "name": "cloud-only framework",
        "type": "SoftwareApplication"
      },
      {
        "name": "Segment Anything (SA) project",
        "type": "CreativeWork"
      },
      {
        "name": "Segment Anything Model (SAM)",
        "type": "SoftwareApplication"
      },
      {
        "name": "SA-1B",
        "type": "Dataset"
      },
      {
        "name": "LLaVA: Large Language and Vision Assistant",
        "type": "SoftwareApplication"
      },
      {
        "name": "Science QA",
        "type": "Dataset"
      },
      {
        "name": "GPT-4 generated visual instruction tuning data",
        "type": "Dataset"
      },
      {
        "name": "code base",
        "type": "SoftwareApplication"
      },
      {
        "name": "LLaVA",
        "type": "SoftwareApplication"
      },
      {
        "name": "CLIP-ViT-L-336px",
        "type": "SoftwareApplication"
      },
      {
        "name": "MLP projection",
        "type": "SoftwareApplication"
      },
      {
        "name": "academic-task-oriented VQA data",
        "type": "Dataset"
      },
      {
        "name": "Large multimodal models (LMM)",
        "type": "SoftwareApplication"
      },
      {
        "name": "visual instruction tuning",
        "type": "SoftwareApplication"
      },
      {
        "name": "DepictQA-Wild",
        "type": "SoftwareApplication"
      },
      {
        "name": "DQ-495K",
        "type": "Dataset"
      },
      {
        "name": "Vision Language Models (VLMs)",
        "type": "SoftwareApplication"
      },
      {
        "name": "VLM-based Image Quality Assessment (IQA)",
        "type": "SoftwareApplication"
      },
      {
        "name": "GPT-4V",
        "type": "SoftwareApplication"
      },
      {
        "name": "traditional score-based methods",
        "type": "SoftwareApplication"
      },
      {
        "name": "prior VLM-based IQA models",
        "type": "SoftwareApplication"
      },
      {
        "name": "Chain-of-Focus (CoF) method",
        "type": "TechArticle"
      },
      {
        "name": "MM-CoF dataset",
        "type": "Dataset"
      },
      {
        "name": "Qwen2.5-VL model",
        "type": "SoftwareApplication"
      },
      {
        "name": "V* benchmark",
        "type": "Dataset"
      },
      {
        "name": "Vision language models (VLMs)",
        "type": "SoftwareApplication"
      },
      {
        "name": "3DThinker",
        "type": "SoftwareApplication"
      },
      {
        "name": "topological cognitive maps",
        "type": "SoftwareApplication"
      },
      {
        "name": "VGGT",
        "type": "SoftwareApplication"
      },
      {
        "name": "zhangquanchen",
        "type": "Person"
      },
      {
        "name": "UniME-V2",
        "type": "SoftwareApplication"
      },
      {
        "name": "UniME-V2-Reranker",
        "type": "SoftwareApplication"
      },
      {
        "name": "MMEB benchmark",
        "type": "Dataset"
      },
      {
        "name": "Universal Multimodal Embedding (UniME-V2) model",
        "type": "SoftwareApplication"
      },
      {
        "name": "MLLM-as-a-Judge mechanism",
        "type": "SoftwareApplication"
      },
      {
        "name": "OpenMMReasoner",
        "type": "SoftwareApplication"
      },
      {
        "name": "Qwen2.5-VL-7B-Instruct",
        "type": "SoftwareApplication"
      },
      {
        "name": "874K-sample cold-start dataset",
        "type": "Dataset"
      },
      {
        "name": "74K-sample dataset",
        "type": "Dataset"
      },
      {
        "name": "nine multimodal reasoning benchmarks",
        "type": "Dataset"
      },
      {
        "name": "agentic AI",
        "type": "SoftwareApplication"
      },
      {
        "name": "cross-modal learning architectures",
        "type": "SoftwareApplication"
      },
      {
        "name": "generalist agents",
        "type": "SoftwareApplication"
      },
      {
        "name": "action planners",
        "type": "SoftwareApplication"
      },
      {
        "name": "hierarchical controllers",
        "type": "SoftwareApplication"
      },
      {
        "name": "over 80 VLA models",
        "type": "SoftwareApplication"
      },
      {
        "name": "architectural innovations",
        "type": "CreativeWork"
      },
      {
        "name": "efficient training strategies",
        "type": "CreativeWork"
      },
      {
        "name": "real-time inference accelerations",
        "type": "CreativeWork"
      },
      {
        "name": "autonomous vehicles",
        "type": "Dataset"
      },
      {
        "name": "medical and industrial robotics",
        "type": "Dataset"
      },
      {
        "name": "precision agriculture",
        "type": "Dataset"
      },
      {
        "name": "humanoid robotics",
        "type": "Dataset"
      },
      {
        "name": "augmented reality",
        "type": "Dataset"
      },
      {
        "name": "agentic adaptation",
        "type": "CreativeWork"
      },
      {
        "name": "cross-embodiment planning",
        "type": "CreativeWork"
      },
      {
        "name": "socially aligned, adaptive, and general-purpose embodied agents",
        "type": "SoftwareApplication"
      },
      {
        "name": "intelligent, real-world robotics",
        "type": "Dataset"
      },
      {
        "name": "artificial general intelligence",
        "type": "Dataset"
      },
      {
        "name": "Applied-AI-Research-Lab",
        "type": "Person"
      },
      {
        "name": "Asynchronous Action Chunk Correction (A2C2)",
        "type": "SoftwareApplication"
      },
      {
        "name": "Real Time Chunking (RTC)",
        "type": "SoftwareApplication"
      },
      {
        "name": "dynamic Kinetix task suite",
        "type": "Dataset"
      },
      {
        "name": "LIBERO Spatial",
        "type": "Dataset"
      },
      {
        "name": "Latent-CoT-Drive (LCDrive)",
        "type": "SoftwareApplication"
      },
      {
        "name": "chain-of-thought (CoT) reasoning",
        "type": "TechArticle"
      },
      {
        "name": "large-scale end-to-end driving benchmark",
        "type": "Dataset"
      },
      {
        "name": "non-reasoning baselines",
        "type": "SoftwareApplication"
      },
      {
        "name": "text-reasoning baselines",
        "type": "SoftwareApplication"
      },
      {
        "name": "HyperVLA",
        "type": "SoftwareApplication"
      },
      {
        "name": "OpenVLA",
        "type": "SoftwareApplication"
      },
      {
        "name": "Token Expand-and-Merge-VLA (TEAM-VLA)",
        "type": "SoftwareApplication"
      },
      {
        "name": "LIBERO benchmark",
        "type": "Dataset"
      },
      {
        "name": "TransSIL",
        "type": "SoftwareApplication"
      },
      {
        "name": "CUB200-2011",
        "type": "Dataset"
      },
      {
        "name": "NABirds",
        "type": "Dataset"
      },
      {
        "name": "bird ecological intelligent detection system",
        "type": "SoftwareApplication"
      },
      {
        "name": "fine-grained bird image classification (FBIC)",
        "type": "SoftwareApplication"
      },
      {
        "name": "bridge crack images",
        "type": "Dataset"
      },
      {
        "name": "threshold switch (TS) model",
        "type": "TechArticle"
      },
      {
        "name": "standard isolated CNN cell",
        "type": "TechArticle"
      },
      {
        "name": "image processing tasks",
        "type": "Dataset"
      },
      {
        "name": "Deep contrastive learning enables genome-wide virtual screening",
        "type": "Article"
      },
      {
        "name": "DrugCLIP",
        "type": "SoftwareApplication"
      },
      {
        "name": "GenomeScreenDB",
        "type": "Dataset"
      },
      {
        "name": "AlphaFold2",
        "type": "SoftwareApplication"
      },
      {
        "name": "norepinephrine transporter",
        "type": "Dataset"
      },
      {
        "name": "thyroid hormone receptor interactor 12",
        "type": "Dataset"
      },
      {
        "name": "underwater image enhancement",
        "type": "Dataset"
      },
      {
        "name": "3DGS-Drag",
        "type": "SoftwareApplication"
      },
      {
        "name": "Dragging Gaussians for Intuitive Point-Based 3D Editing",
        "type": "Article"
      },
      {
        "name": "3D Gaussian Splatting",
        "type": "SoftwareApplication"
      },
      {
        "name": "RTX 4090 GPU",
        "type": "TechArticle"
      },
      {
        "name": "LILaC",
        "type": "SoftwareApplication"
      },
      {
        "name": "layered component graph",
        "type": "CreativeWork"
      },
      {
        "name": "late-interaction-based subgraph retrieval method",
        "type": "CreativeWork"
      },
      {
        "name": "five benchmarks",
        "type": "Dataset"
      },
      {
        "name": "Bidirectional Normalizing Flow (BiFlow)",
        "type": "SoftwareApplication"
      },
      {
        "name": "Normalizing Flows (NFs)",
        "type": "SoftwareApplication"
      },
      {
        "name": "TARFlow",
        "type": "SoftwareApplication"
      },
      {
        "name": "pixel MeanFlow",
        "type": "SoftwareApplication"
      },
      {
        "name": "Meta Flow Maps",
        "type": "SoftwareApplication"
      },
      {
        "name": "consistency models",
        "type": "SoftwareApplication"
      },
      {
        "name": "flow maps",
        "type": "SoftwareApplication"
      },
      {
        "name": "Best-of-1000",
        "type": "SoftwareApplication"
      },
      {
        "name": "Sequential Flow Matching",
        "type": "Thesis"
      },
      {
        "name": "Bayesian filtering",
        "type": "TechArticle"
      },
      {
        "name": "diffusion and flow-matching models",
        "type": "SoftwareApplication"
      },
      {
        "name": "full-step diffusion",
        "type": "SoftwareApplication"
      },
      {
        "name": "forecasting, decision-making and state estimation tasks",
        "type": "Dataset"
      },
      {
        "name": "Drifting Models",
        "type": "Thesis"
      },
      {
        "name": "flow-based models",
        "type": "Thesis"
      },
      {
        "name": "SimCLR",
        "type": "SoftwareApplication"
      },
      {
        "name": "AlexNet",
        "type": "SoftwareApplication"
      },
      {
        "name": "DivGenBench",
        "type": "Dataset"
      },
      {
        "name": "Directional Decoupling Alignment (D²-Align)",
        "type": "SoftwareApplication"
      },
      {
        "name": "Preference Mode Collapse (PMC)",
        "type": "Report"
      },
      {
        "name": "Reinforcement Learning from Human Feedback",
        "type": "Report"
      },
      {
        "name": "ImagerySearch",
        "type": "SoftwareApplication"
      },
      {
        "name": "LDT-Bench",
        "type": "Dataset"
      },
      {
        "name": "VBench",
        "type": "Dataset"
      },
      {
        "name": "SigLIP-2",
        "type": "SoftwareApplication"
      },
      {
        "name": "FLUX VAE",
        "type": "SoftwareApplication"
      },
      {
        "name": "VAEs",
        "type": "SoftwareApplication"
      },
      {
        "name": "video foundation models",
        "type": "CreativeWork"
      },
      {
        "name": "implicit world model",
        "type": "CreativeWork"
      },
      {
        "name": "video renderer",
        "type": "CreativeWork"
      },
      {
        "name": "world model",
        "type": "CreativeWork"
      },
      {
        "name": "video generation model",
        "type": "CreativeWork"
      },
      {
        "name": "robotics",
        "type": "CreativeWork"
      },
      {
        "name": "interactive gaming",
        "type": "CreativeWork"
      },
      {
        "name": "World models",
        "type": "TechArticle"
      },
      {
        "name": "visual prediction",
        "type": "Dataset"
      },
      {
        "name": "3D estimation",
        "type": "Dataset"
      },
      {
        "name": "symbol grounding",
        "type": "Dataset"
      },
      {
        "name": "unified design specification for world models",
        "type": "Report"
      },
      {
        "name": "interaction",
        "type": "Dataset"
      },
      {
        "name": "perception",
        "type": "Dataset"
      },
      {
        "name": "symbolic reasoning",
        "type": "Dataset"
      },
      {
        "name": "spatial representation",
        "type": "Dataset"
      },
      {
        "name": "RecTok",
        "type": "SoftwareApplication"
      },
      {
        "name": "visual tokenizers",
        "type": "TechArticle"
      },
      {
        "name": "vision foundation models",
        "type": "TechArticle"
      },
      {
        "name": "diffusion transformers",
        "type": "TechArticle"
      },
      {
        "name": "VFMs",
        "type": "TechArticle"
      },
      {
        "name": "gFID-50K",
        "type": "Dataset"
      },
      {
        "name": "Shi Qingyu",
        "type": "Person"
      },
      {
        "name": "RePack then Refine",
        "type": "Thesis"
      },
      {
        "name": "Vision Foundation Models",
        "type": "SoftwareApplication"
      },
      {
        "name": "Latent Diffusion Models",
        "type": "SoftwareApplication"
      },
      {
        "name": "Diffusion Transformers",
        "type": "SoftwareApplication"
      },
      {
        "name": "RePack-DiT-XL/1",
        "type": "SoftwareApplication"
      },
      {
        "name": "RePack module",
        "type": "SoftwareApplication"
      },
      {
        "name": "Latent-Guided Refiner",
        "type": "SoftwareApplication"
      },
      {
        "name": "Refiner module",
        "type": "SoftwareApplication"
      },
      {
        "name": "BigGAN-deep",
        "type": "TechArticle"
      },
      {
        "name": "ImageNet 128×128",
        "type": "Dataset"
      },
      {
        "name": "ImageNet 512×512",
        "type": "Dataset"
      },
      {
        "name": "classifier guidance",
        "type": "TechArticle"
      },
      {
        "name": "upsampling diffusion models",
        "type": "TechArticle"
      },
      {
        "name": "ViT model (Dosovitskiy et al., 2020)",
        "type": "SoftwareApplication"
      },
      {
        "name": "OpenCLIP (Ilharco et al., 2021)",
        "type": "SoftwareApplication"
      },
      {
        "name": "Dosovitskiy et al.",
        "type": "Person"
      },
      {
        "name": "Ilharco et al.",
        "type": "Person"
      },
      {
        "name": "PixelGen",
        "type": "SoftwareApplication"
      },
      {
        "name": "latent diffusion",
        "type": "SoftwareApplication"
      },
      {
        "name": "LPIPS",
        "type": "Dataset"
      },
      {
        "name": "Zehong-Ma",
        "type": "Person"
      },
      {
        "name": "multimodal understanding models",
        "type": "SoftwareApplication"
      },
      {
        "name": "image generation models",
        "type": "SoftwareApplication"
      },
      {
        "name": "autoregressive-based architectures",
        "type": "SoftwareApplication"
      },
      {
        "name": "diffusion-based models",
        "type": "SoftwareApplication"
      },
      {
        "name": "unified frameworks",
        "type": "SoftwareApplication"
      },
      {
        "name": "diffusion-based",
        "type": "SoftwareApplication"
      },
      {
        "name": "autoregressive-based",
        "type": "SoftwareApplication"
      },
      {
        "name": "hybrid approaches",
        "type": "SoftwareApplication"
      },
      {
        "name": "datasets and benchmarks",
        "type": "Dataset"
      },
      {
        "name": "GitHub",
        "type": "SoftwareApplication"
      },
      {
        "name": "MentisOculi",
        "type": "Dataset"
      },
      {
        "name": "Frontier models",
        "type": "SoftwareApplication"
      },
      {
        "name": "multimodal large language models (MLLMs)",
        "type": "SoftwareApplication"
      },
      {
        "name": "unified multimodal models (UMMs)",
        "type": "SoftwareApplication"
      },
      {
        "name": "OpenVision 3",
        "type": "SoftwareApplication"
      },
      {
        "name": "ViT",
        "type": "SoftwareApplication"
      },
      {
        "name": "ViT-VAE",
        "type": "SoftwareApplication"
      },
      {
        "name": "LLaVA-1.5",
        "type": "SoftwareApplication"
      },
      {
        "name": "RAE",
        "type": "SoftwareApplication"
      },
      {
        "name": "SeedBench",
        "type": "Dataset"
      },
      {
        "name": "POPE",
        "type": "Dataset"
      },
      {
        "name": "continuous Skip-gram model",
        "type": "SoftwareApplication"
      },
      {
        "name": "subsampling of the frequent words",
        "type": "SoftwareApplication"
      },
      {
        "name": "negative sampling",
        "type": "SoftwareApplication"
      },
      {
        "name": "hierarchical softmax",
        "type": "SoftwareApplication"
      },
      {
        "name": "simple method for finding phrases in text",
        "type": "SoftwareApplication"
      },
      {
        "name": "global logbilinear regression model",
        "type": "Thesis"
      },
      {
        "name": "global matrix factorization",
        "type": "CreativeWork"
      },
      {
        "name": "local context window methods",
        "type": "CreativeWork"
      },
      {
        "name": "word-word cooccurrence matrix",
        "type": "Dataset"
      },
      {
        "name": "word analogy task",
        "type": "Dataset"
      },
      {
        "name": "similarity tasks",
        "type": "Dataset"
      },
      {
        "name": "named entity recognition",
        "type": "Dataset"
      },
      {
        "name": "deep bidirectional language model",
        "type": "SoftwareApplication"
      },
      {
        "name": "textual entailment",
        "type": "Dataset"
      },
      {
        "name": "sentiment analysis",
        "type": "Dataset"
      },
      {
        "name": "Modern models for common NLP tasks",
        "type": "SoftwareApplication"
      },
      {
        "name": "2018 Twitter data spanning 51 U.S. regions and 99 countries",
        "type": "Dataset"
      },
      {
        "name": "18 international and 5 U.S.-based statistical gender gaps",
        "type": "Dataset"
      },
      {
        "name": "SegMamba-V2",
        "type": "SoftwareApplication"
      },
      {
        "name": "State Space Model (SSM)",
        "type": "SoftwareApplication"
      },
      {
        "name": "CRC-2000",
        "type": "Dataset"
      },
      {
        "name": "ge-xing",
        "type": "Person"
      },
      {
        "name": "diffusion-based generative classifiers",
        "type": "SoftwareApplication"
      },
      {
        "name": "autoregressive generative classifiers",
        "type": "SoftwareApplication"
      },
      {
        "name": "five standard image and text distribution shift benchmarks",
        "type": "Dataset"
      },
      {
        "name": "medical datasets",
        "type": "Dataset"
      },
      {
        "name": "satellite datasets",
        "type": "Dataset"
      },
      {
        "name": "Gaussian toy setting",
        "type": "Dataset"
      },
      {
        "name": "Edge Large AI Model Agent",
        "type": "SoftwareApplication"
      },
      {
        "name": "Cognitive Multimodal Semantic Communication",
        "type": "SoftwareApplication"
      },
      {
        "name": "GPT-2",
        "type": "SoftwareApplication"
      },
      {
        "name": "Llama 3",
        "type": "SoftwareApplication"
      },
      {
        "name": "Llama Guard 3",
        "type": "SoftwareApplication"
      },
      {
        "name": "Diffusion Language Models",
        "type": "TechArticle"
      },
      {
        "name": "autoregressive paradigm",
        "type": "TechArticle"
      },
      {
        "name": "masked language models",
        "type": "TechArticle"
      },
      {
        "name": "VILA-Lab",
        "type": "Person"
      },
      {
        "name": "Awesome-DLMs",
        "type": "SoftwareApplication"
      },
      {
        "name": "Quokka",
        "type": "Report"
      },
      {
        "name": "Chinchilla",
        "type": "Report"
      },
      {
        "name": "IGPO (Inpainting Guided Policy Optimization)",
        "type": "SoftwareApplication"
      },
      {
        "name": "masked diffusion large language models (dLLMs)",
        "type": "SoftwareApplication"
      },
      {
        "name": "autoregressive LLMs",
        "type": "SoftwareApplication"
      },
      {
        "name": "reinforcement learning",
        "type": "SoftwareApplication"
      },
      {
        "name": "GRPO",
        "type": "SoftwareApplication"
      },
      {
        "name": "Math500",
        "type": "Dataset"
      },
      {
        "name": "AMC",
        "type": "Dataset"
      },
      {
        "name": "supervised fine-tuning",
        "type": "SoftwareApplication"
      },
      {
        "name": "entropy-based filtering",
        "type": "SoftwareApplication"
      },
      {
        "name": "Efficient Encoder-Decoder Diffusion (E2D2)",
        "type": "SoftwareApplication"
      },
      {
        "name": "discrete diffusion models",
        "type": "CreativeWork"
      },
      {
        "name": "autoregressive approaches",
        "type": "CreativeWork"
      },
      {
        "name": "encoder-decoder architecture",
        "type": "CreativeWork"
      },
      {
        "name": "block diffusion models",
        "type": "CreativeWork"
      },
      {
        "name": "translation",
        "type": "Dataset"
      },
      {
        "name": "mathematical reasoning",
        "type": "Dataset"
      },
      {
        "name": "Stable-DiffCoder",
        "type": "SoftwareApplication"
      },
      {
        "name": "Seed-Coder",
        "type": "SoftwareApplication"
      },
      {
        "name": "block diffusion continual pretraining (CPT)",
        "type": "TechArticle"
      },
      {
        "name": "code benchmarks",
        "type": "Dataset"
      },
      {
        "name": "low-resource coding languages",
        "type": "Dataset"
      },
      {
        "name": "Large Language Model (LLM)-based agents",
        "type": "SoftwareApplication"
      },
      {
        "name": "graph-based agent memory",
        "type": "SoftwareApplication"
      },
      {
        "name": "self-evolving agent memory",
        "type": "SoftwareApplication"
      },
      {
        "name": "open-sourced libraries and benchmarks",
        "type": "Dataset"
      },
      {
        "name": "research papers",
        "type": "Article"
      },
      {
        "name": "open-source data",
        "type": "Dataset"
      },
      {
        "name": "projects",
        "type": "CreativeWork"
      },
      {
        "name": "https://github.com/DEEP-PolyU/Awesome-GraphMemory",
        "type": "Dataset"
      },
      {
        "name": "Empirical-MCTS",
        "type": "SoftwareApplication"
      },
      {
        "name": "Monte Carlo Tree Search (MCTS)",
        "type": "SoftwareApplication"
      },
      {
        "name": "Pairwise-Experience-Evolutionary Meta-Prompting (PE-EMP)",
        "type": "SoftwareApplication"
      },
      {
        "name": "Memory Optimization Agent",
        "type": "SoftwareApplication"
      },
      {
        "name": "ARC-AGI-2",
        "type": "Dataset"
      },
      {
        "name": "MathArena Apex",
        "type": "Dataset"
      },
      {
        "name": "Trust-Memevo benchmark",
        "type": "Dataset"
      },
      {
        "name": "TAME",
        "type": "SoftwareApplication"
      },
      {
        "name": "Agent Memory Misevolution",
        "type": "Report"
      },
      {
        "name": "ProcMEM",
        "type": "SoftwareApplication"
      },
      {
        "name": "Non-Parametric PPO",
        "type": "SoftwareApplication"
      },
      {
        "name": "Skill-MDP",
        "type": "Dataset"
      },
      {
        "name": "PPO Gate",
        "type": "SoftwareApplication"
      },
      {
        "name": "agentic time series forecasting (ATSF)",
        "type": "Thesis"
      },
      {
        "name": "workflow-based design",
        "type": "CreativeWork"
      },
      {
        "name": "agentic reinforcement learning",
        "type": "CreativeWork"
      },
      {
        "name": "hybrid agentic workflow paradigm",
        "type": "CreativeWork"
      },
      {
        "name": "InstructGPT",
        "type": "SoftwareApplication"
      },
      {
        "name": "OpenAI API",
        "type": "SoftwareApplication"
      },
      {
        "name": "labeler-written prompts",
        "type": "Dataset"
      },
      {
        "name": "prompts submitted through the OpenAI API",
        "type": "Dataset"
      },
      {
        "name": "dataset of labeler demonstrations",
        "type": "Dataset"
      },
      {
        "name": "dataset of rankings of model outputs",
        "type": "Dataset"
      },
      {
        "name": "public NLP datasets",
        "type": "Dataset"
      },
      {
        "name": "Pathways Language Model PaLM",
        "type": "SoftwareApplication"
      },
      {
        "name": "Pathways",
        "type": "SoftwareApplication"
      },
      {
        "name": "BIG-bench",
        "type": "Dataset"
      },
      {
        "name": "Transformer language model",
        "type": "SoftwareApplication"
      },
      {
        "name": "TPU v4 chips",
        "type": "SoftwareApplication"
      },
      {
        "name": "TPU Pods",
        "type": "SoftwareApplication"
      },
      {
        "name": "ReAct",
        "type": "SoftwareApplication"
      },
      {
        "name": "chain-of-thought prompting",
        "type": "SoftwareApplication"
      },
      {
        "name": "HotpotQA",
        "type": "Dataset"
      },
      {
        "name": "Fever",
        "type": "Dataset"
      },
      {
        "name": "Wikipedia API",
        "type": "SoftwareApplication"
      },
      {
        "name": "ALFWorld",
        "type": "Dataset"
      },
      {
        "name": "WebShop",
        "type": "Dataset"
      },
      {
        "name": "imitation and reinforcement learning methods",
        "type": "SoftwareApplication"
      },
      {
        "name": "OpenAI's o1/o3",
        "type": "SoftwareApplication"
      },
      {
        "name": "DeepSeek's R1",
        "type": "SoftwareApplication"
      },
      {
        "name": "zzli2022",
        "type": "Person"
      },
      {
        "name": "Awesome-Slow-Reason-System",
        "type": "Dataset"
      },
      {
        "name": "Deep Research (DR) agents",
        "type": "TechArticle"
      },
      {
        "name": "Model Context Protocols (MCPs)",
        "type": "TechArticle"
      },
      {
        "name": "taxonomy",
        "type": "Report"
      },
      {
        "name": "benchmarks",
        "type": "Dataset"
      },
      {
        "name": "repository of DR agent research",
        "type": "Dataset"
      },
      {
        "name": "DeepSeek-R1",
        "type": "SoftwareApplication"
      },
      {
        "name": "Table-R1-SFT",
        "type": "SoftwareApplication"
      },
      {
        "name": "Table-R1-Zero",
        "type": "SoftwareApplication"
      },
      {
        "name": "GPT-4.1",
        "type": "SoftwareApplication"
      },
      {
        "name": "GRPO algorithm",
        "type": "SoftwareApplication"
      },
      {
        "name": "large-scale dataset of reasoning traces",
        "type": "Dataset"
      },
      {
        "name": "Mind2Report",
        "type": "SoftwareApplication"
      },
      {
        "name": "QRC-Eval",
        "type": "Dataset"
      },
      {
        "name": "OpenAI deep research agents",
        "type": "SoftwareApplication"
      },
      {
        "name": "Gemini deep research agents",
        "type": "SoftwareApplication"
      },
      {
        "name": "general large language models (LLMs)",
        "type": "SoftwareApplication"
      },
      {
        "name": "PaperScout",
        "type": "SoftwareApplication"
      },
      {
        "name": "Proximal Sequence Policy Optimization (PSPO)",
        "type": "SoftwareApplication"
      },
      {
        "name": "synthetic and real-world benchmarks",
        "type": "Dataset"
      },
      {
        "name": "workflow-driven baselines",
        "type": "SoftwareApplication"
      },
      {
        "name": "RL baselines",
        "type": "SoftwareApplication"
      },
      {
        "name": "GGL-Net",
        "type": "SoftwareApplication"
      },
      {
        "name": "gradient supplementary module (GSM)",
        "type": "SoftwareApplication"
      },
      {
        "name": "two-way guidance fusion module (TGFM)",
        "type": "SoftwareApplication"
      },
      {
        "name": "NUAA-SIRST dataset",
        "type": "Dataset"
      },
      {
        "name": "NUDT-SIRST dataset",
        "type": "Dataset"
      },
      {
        "name": "YuChuang1205",
        "type": "Person"
      },
      {
        "name": "NN-RAG",
        "type": "SoftwareApplication"
      },
      {
        "name": "LEMUR dataset",
        "type": "Dataset"
      },
      {
        "name": "CenterMamba-SAM",
        "type": "SoftwareApplication"
      },
      {
        "name": "CenterMamba encoder",
        "type": "SoftwareApplication"
      },
      {
        "name": "memory-driven structural prompt generator",
        "type": "SoftwareApplication"
      },
      {
        "name": "memory-augmented multi-scale decoder",
        "type": "SoftwareApplication"
      },
      {
        "name": "brain lesion segmentation",
        "type": "Dataset"
      },
      {
        "name": "public benchmarks",
        "type": "Dataset"
      },
      {
        "name": "IMobileTransformer",
        "type": "SoftwareApplication"
      },
      {
        "name": "rice disease identification",
        "type": "Dataset"
      },
      {
        "name": "RF-DETR",
        "type": "SoftwareApplication"
      },
      {
        "name": "Roboflow100-VL",
        "type": "Dataset"
      },
      {
        "name": "D-FINE",
        "type": "SoftwareApplication"
      },
      {
        "name": "GroundingDINO",
        "type": "SoftwareApplication"
      },
      {
        "name": "vision-language model (VLM)",
        "type": "SoftwareApplication"
      },
      {
        "name": "DETRs",
        "type": "SoftwareApplication"
      },
      {
        "name": "UAV-based RGB images",
        "type": "Dataset"
      },
      {
        "name": "winter wheat",
        "type": "Dataset"
      },
      {
        "name": "winter rye",
        "type": "Dataset"
      },
      {
        "name": "transfer learning",
        "type": "TechArticle"
      },
      {
        "name": "error visibility",
        "type": "Dataset"
      },
      {
        "name": "structural similarity",
        "type": "Dataset"
      },
      {
        "name": "FreeOrbit4D",
        "type": "SoftwareApplication"
      },
      {
        "name": "monocular video",
        "type": "Dataset"
      },
      {
        "name": "geometry-complete 4D proxy",
        "type": "Dataset"
      },
      {
        "name": "object-centric multi-view diffusion model",
        "type": "SoftwareApplication"
      },
      {
        "name": "conditional video diffusion model",
        "type": "SoftwareApplication"
      },
      {
        "name": "diffusion-based methods",
        "type": "SoftwareApplication"
      },
      {
        "name": "edit propagation",
        "type": "CreativeWork"
      },
      {
        "name": "4D data generation",
        "type": "CreativeWork"
      },
      {
        "name": "NeoVerse",
        "type": "SoftwareApplication"
      },
      {
        "name": "4D world modeling methods",
        "type": "TechArticle"
      },
      {
        "name": "in-the-wild monocular videos",
        "type": "Dataset"
      },
      {
        "name": "standard reconstruction and generation benchmarks",
        "type": "Dataset"
      },
      {
        "name": "U-Net",
        "type": "CreativeWork"
      },
      {
        "name": "DiT-XL/2",
        "type": "CreativeWork"
      },
      {
        "name": "ImageNet 512x512",
        "type": "Dataset"
      },
      {
        "name": "Gflops",
        "type": "CreativeWork"
      },
      {
        "name": "RoPE",
        "type": "TechArticle"
      },
      {
        "name": "transformer",
        "type": "TechArticle"
      },
      {
        "name": "RoFormer",
        "type": "SoftwareApplication"
      },
      {
        "name": "Huggingface",
        "type": "SoftwareApplication"
      },
      {
        "name": "long text classification benchmark datasets",
        "type": "Dataset"
      },
      {
        "name": "Driving World Model (DWM)",
        "type": "TechArticle"
      },
      {
        "name": "autonomous driving (AD)",
        "type": "TechArticle"
      },
      {
        "name": "mainstream simulators",
        "type": "SoftwareApplication"
      },
      {
        "name": "high-impact datasets",
        "type": "Dataset"
      },
      {
        "name": "various metrics",
        "type": "Dataset"
      },
      {
        "name": "video",
        "type": "Dataset"
      },
      {
        "name": "point cloud",
        "type": "Dataset"
      },
      {
        "name": "occupancy",
        "type": "Dataset"
      },
      {
        "name": "latent feature",
        "type": "Dataset"
      },
      {
        "name": "traffic map",
        "type": "Dataset"
      },
      {
        "name": "AD research",
        "type": "TechArticle"
      },
      {
        "name": "representative approaches",
        "type": "TechArticle"
      },
      {
        "name": "generating tasks",
        "type": "TechArticle"
      },
      {
        "name": "driving tasks",
        "type": "TechArticle"
      },
      {
        "name": "current research",
        "type": "TechArticle"
      },
      {
        "name": "future directions",
        "type": "TechArticle"
      },
      {
        "name": "LMD0311",
        "type": "Person"
      },
      {
        "name": "FlexMap",
        "type": "SoftwareApplication"
      },
      {
        "name": "HD maps",
        "type": "Dataset"
      },
      {
        "name": "autonomous driving systems",
        "type": "SoftwareApplication"
      },
      {
        "name": "geometry-aware foundation model",
        "type": "SoftwareApplication"
      },
      {
        "name": "spatial-temporal enhancement module",
        "type": "SoftwareApplication"
      },
      {
        "name": "camera-aware decoder",
        "type": "SoftwareApplication"
      },
      {
        "name": "MapAnything",
        "type": "SoftwareApplication"
      },
      {
        "name": "Universal Feed-Forward Metric 3D Reconstruction",
        "type": "Article"
      },
      {
        "name": "Seedream 4.0",
        "type": "SoftwareApplication"
      },
      {
        "name": "Seedream 4.5",
        "type": "SoftwareApplication"
      },
      {
        "name": "Volcano Engine",
        "type": "SoftwareApplication"
      },
      {
        "name": "VLM model",
        "type": "SoftwareApplication"
      },
      {
        "name": "adversarial distillation",
        "type": "SoftwareApplication"
      },
      {
        "name": "distribution matching",
        "type": "SoftwareApplication"
      },
      {
        "name": "quantization",
        "type": "SoftwareApplication"
      },
      {
        "name": "speculative decoding",
        "type": "SoftwareApplication"
      },
      {
        "name": "text-to-image synthesis",
        "type": "Dataset"
      },
      {
        "name": "multi-image composition",
        "type": "Dataset"
      },
      {
        "name": "text-image pairs",
        "type": "Dataset"
      },
      {
        "name": "Echo-4o",
        "type": "SoftwareApplication"
      },
      {
        "name": "Echo-4o-Image",
        "type": "Dataset"
      },
      {
        "name": "Bagel",
        "type": "SoftwareApplication"
      },
      {
        "name": "GenEval++",
        "type": "Dataset"
      },
      {
        "name": "Imagine-Bench",
        "type": "Dataset"
      },
      {
        "name": "OmniGen2",
        "type": "SoftwareApplication"
      },
      {
        "name": "BLIP3-o",
        "type": "SoftwareApplication"
      },
      {
        "name": "Lumina-DiMOO",
        "type": "SoftwareApplication"
      },
      {
        "name": "autoregressive (AR) or hybrid AR-Diffusion paradigms",
        "type": "TechArticle"
      },
      {
        "name": "image-to-image generation",
        "type": "TechArticle"
      },
      {
        "name": "image understanding",
        "type": "TechArticle"
      },
      {
        "name": "multiple benchmarks",
        "type": "Dataset"
      },
      {
        "name": "code and checkpoints",
        "type": "Dataset"
      },
      {
        "name": "NextStep-1",
        "type": "SoftwareApplication"
      },
      {
        "name": "autoregressive models",
        "type": "SoftwareApplication"
      },
      {
        "name": "vector quantization (VQ)",
        "type": "SoftwareApplication"
      },
      {
        "name": "flow matching head",
        "type": "SoftwareApplication"
      },
      {
        "name": "text-to-image generation tasks",
        "type": "Dataset"
      },
      {
        "name": "Devlin et al., 2019",
        "type": "Person"
      },
      {
        "name": "RACE",
        "type": "Dataset"
      },
      {
        "name": "SQuAD",
        "type": "Dataset"
      },
      {
        "name": "reinforcement learning (RL)",
        "type": "SoftwareApplication"
      },
      {
        "name": "supervised learning",
        "type": "SoftwareApplication"
      },
      {
        "name": "mathematics",
        "type": "Dataset"
      },
      {
        "name": "coding competitions",
        "type": "Dataset"
      },
      {
        "name": "STEM fields",
        "type": "Dataset"
      },
      {
        "name": "human-labeled reasoning trajectories",
        "type": "Dataset"
      },
      {
        "name": "human-annotated demonstrations",
        "type": "Dataset"
      },
      {
        "name": "reasoning patterns",
        "type": "Dataset"
      },
      {
        "name": "self-reflection",
        "type": "Dataset"
      },
      {
        "name": "verification",
        "type": "Dataset"
      },
      {
        "name": "dynamic strategy adaptation",
        "type": "Dataset"
      },
      {
        "name": "smaller models",
        "type": "SoftwareApplication"
      },
      {
        "name": "Search-R1",
        "type": "SoftwareApplication"
      },
      {
        "name": "Qwen2.5-7B",
        "type": "SoftwareApplication"
      },
      {
        "name": "Qwen2.5-3B",
        "type": "SoftwareApplication"
      },
      {
        "name": "seven question-answering datasets",
        "type": "Dataset"
      },
      {
        "name": "PeterGriffinJin",
        "type": "Person"
      },
      {
        "name": "Med-PaLM",
        "type": "SoftwareApplication"
      },
      {
        "name": "Med-PaLM 2",
        "type": "SoftwareApplication"
      },
      {
        "name": "MedQA",
        "type": "Dataset"
      },
      {
        "name": "MedMCQA",
        "type": "Dataset"
      },
      {
        "name": "PubMedQA",
        "type": "Dataset"
      },
      {
        "name": "MMLU clinical topics",
        "type": "Dataset"
      },
      {
        "name": "Supervised fine-tuning (SFT)",
        "type": "SoftwareApplication"
      },
      {
        "name": "GeneralPoints",
        "type": "Dataset"
      },
      {
        "name": "V-IRL",
        "type": "Dataset"
      },
      {
        "name": "outcome-based reward",
        "type": "SoftwareApplication"
      },
      {
        "name": "EquiCSP",
        "type": "SoftwareApplication"
      },
      {
        "name": "Crystal Structure Prediction",
        "type": "Dataset"
      },
      {
        "name": "symmetry-aware deep learning models",
        "type": "SoftwareApplication"
      },
      {
        "name": "existing models",
        "type": "SoftwareApplication"
      },
      {
        "name": "WorldPlay",
        "type": "SoftwareApplication"
      },
      {
        "name": "Dual Action Representation",
        "type": "TechArticle"
      },
      {
        "name": "Reconstituted Context Memory",
        "type": "TechArticle"
      },
      {
        "name": "Context Forcing",
        "type": "TechArticle"
      },
      {
        "name": "streaming video diffusion model",
        "type": "SoftwareApplication"
      },
      {
        "name": "memory-aware model",
        "type": "SoftwareApplication"
      },
      {
        "name": "teacher",
        "type": "SoftwareApplication"
      },
      {
        "name": "student",
        "type": "SoftwareApplication"
      },
      {
        "name": "existing techniques",
        "type": "SoftwareApplication"
      },
      {
        "name": "O-Voxel",
        "type": "TechArticle"
      },
      {
        "name": "Sparse Compression VAE",
        "type": "TechArticle"
      },
      {
        "name": "flow-matching models",
        "type": "TechArticle"
      },
      {
        "name": "public 3D asset datasets",
        "type": "Dataset"
      },
      {
        "name": "JEPA-WMs",
        "type": "CreativeWork"
      },
      {
        "name": "DINO-WM",
        "type": "CreativeWork"
      },
      {
        "name": "V-JEPA-2-AC",
        "type": "CreativeWork"
      },
      {
        "name": "simulated environments",
        "type": "Dataset"
      },
      {
        "name": "real-world robotic data",
        "type": "Dataset"
      },
      {
        "name": "https://github.com/facebookresearch/jepa-wms",
        "type": "SoftwareApplication"
      },
      {
        "name": "PLIT",
        "type": "SoftwareApplication"
      },
      {
        "name": "hierarchical variational autoencoder",
        "type": "SoftwareApplication"
      },
      {
        "name": "rate attention mechanism",
        "type": "SoftwareApplication"
      },
      {
        "name": "spatial grouping strategy",
        "type": "SoftwareApplication"
      },
      {
        "name": "STORM",
        "type": "SoftwareApplication"
      },
      {
        "name": "Search-Guided Generative World Models",
        "type": "SoftwareApplication"
      },
      {
        "name": "CogACT",
        "type": "SoftwareApplication"
      },
      {
        "name": "SimplerEnv",
        "type": "Dataset"
      },
      {
        "name": "Frechet Video Distance",
        "type": "Dataset"
      },
      {
        "name": "Japanese VLMs",
        "type": "SoftwareApplication"
      },
      {
        "name": "chain of thought prompting",
        "type": "Thesis"
      },
      {
        "name": "R-CNN: Regions with CNN features",
        "type": "SoftwareApplication"
      },
      {
        "name": "Ground Penetrating Radar (GPR)",
        "type": "TechArticle"
      },
      {
        "name": "GPR road hidden defect images",
        "type": "Dataset"
      },
      {
        "name": "distortion identification",
        "type": "Dataset"
      },
      {
        "name": "instant rating",
        "type": "Dataset"
      },
      {
        "name": "reasoning tasks",
        "type": "Dataset"
      },
      {
        "name": "web-downloaded images",
        "type": "Dataset"
      },
      {
        "name": "model-processed images",
        "type": "Dataset"
      },
      {
        "name": "deep bidirectional language model (biLM)",
        "type": "SoftwareApplication"
      }
    ],
    "triples": [
      {
        "head": "Ashish Vaswani",
        "relation": "author_of",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "Noam Shazeer",
        "relation": "author_of",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "Niki Parmar",
        "relation": "author_of",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "Jakob Uszkoreit",
        "relation": "author_of",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "Llion Jones",
        "relation": "author_of",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "Aidan N. Gomez",
        "relation": "author_of",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "Lukasz Kaiser",
        "relation": "author_of",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "Illia Polosukhin",
        "relation": "author_of",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "Kaiming He",
        "relation": "author_of",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Xiangyu Zhang",
        "relation": "author_of",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Shaoqing Ren",
        "relation": "author_of",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Jian Sun",
        "relation": "author_of",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Diederik P. Kingma",
        "relation": "author_of",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "Jimmy Ba",
        "relation": "author_of",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "Sepp Hochreiter",
        "relation": "author_of",
        "tail": "Long Short-Term Memory"
      },
      {
        "head": "J. Schmidhuber",
        "relation": "author_of",
        "tail": "Long Short-Term Memory"
      },
      {
        "head": "Nitish Srivastava",
        "relation": "author_of",
        "tail": "Dropout: a simple way to prevent neural networks from overfitting"
      },
      {
        "head": "Geoffrey E. Hinton",
        "relation": "author_of",
        "tail": "Dropout: a simple way to prevent neural networks from overfitting"
      },
      {
        "head": "A. Krizhevsky",
        "relation": "author_of",
        "tail": "Dropout: a simple way to prevent neural networks from overfitting"
      },
      {
        "head": "I. Sutskever",
        "relation": "author_of",
        "tail": "Dropout: a simple way to prevent neural networks from overfitting"
      },
      {
        "head": "R. Salakhutdinov",
        "relation": "author_of",
        "tail": "Dropout: a simple way to prevent neural networks from overfitting"
      },
      {
        "head": "Christian Szegedy",
        "relation": "author_of",
        "tail": "Rethinking the Inception Architecture for Computer Vision"
      },
      {
        "head": "Vincent Vanhoucke",
        "relation": "author_of",
        "tail": "Rethinking the Inception Architecture for Computer Vision"
      },
      {
        "head": "Sergey Ioffe",
        "relation": "author_of",
        "tail": "Rethinking the Inception Architecture for Computer Vision"
      },
      {
        "head": "Jonathon Shlens",
        "relation": "author_of",
        "tail": "Rethinking the Inception Architecture for Computer Vision"
      },
      {
        "head": "Zbigniew Wojna",
        "relation": "author_of",
        "tail": "Rethinking the Inception Architecture for Computer Vision"
      },
      {
        "head": "Shaina Raza",
        "relation": "author_of",
        "tail": "A comprehensive review of recommender systems: Transitioning from theory to practice"
      },
      {
        "head": "Mizanur Rahman",
        "relation": "author_of",
        "tail": "A comprehensive review of recommender systems: Transitioning from theory to practice"
      },
      {
        "head": "Safiullah Kamawal",
        "relation": "author_of",
        "tail": "A comprehensive review of recommender systems: Transitioning from theory to practice"
      },
      {
        "head": "Armin Toroghi",
        "relation": "author_of",
        "tail": "A comprehensive review of recommender systems: Transitioning from theory to practice"
      },
      {
        "head": "Ananya Raval",
        "relation": "author_of",
        "tail": "A comprehensive review of recommender systems: Transitioning from theory to practice"
      },
      {
        "head": "F. Navah",
        "relation": "author_of",
        "tail": "A comprehensive review of recommender systems: Transitioning from theory to practice"
      },
      {
        "head": "Amirmohammad Kazemeini",
        "relation": "author_of",
        "tail": "A comprehensive review of recommender systems: Transitioning from theory to practice"
      },
      {
        "head": "Zhuoran Yang",
        "relation": "author_of",
        "tail": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation"
      },
      {
        "head": "Xi Guo",
        "relation": "author_of",
        "tail": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation"
      },
      {
        "head": "Chenjing Ding",
        "relation": "author_of",
        "tail": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation"
      },
      {
        "head": "Chiyu Wang",
        "relation": "author_of",
        "tail": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation"
      },
      {
        "head": "Wei Wu",
        "relation": "author_of",
        "tail": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation"
      },
      {
        "head": "Yanyong Zhang",
        "relation": "author_of",
        "tail": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation"
      },
      {
        "head": "Zisheng Wang",
        "relation": "author_of",
        "tail": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection"
      },
      {
        "head": "Junjie Chen",
        "relation": "author_of",
        "tail": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection"
      },
      {
        "head": "Chisen Wang",
        "relation": "author_of",
        "tail": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection"
      },
      {
        "head": "Cong Peng",
        "relation": "author_of",
        "tail": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection"
      },
      {
        "head": "Jianping Xuan",
        "relation": "author_of",
        "tail": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection"
      },
      {
        "head": "Tielin Shi",
        "relation": "author_of",
        "tail": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection"
      },
      {
        "head": "Ming J. Zuo",
        "relation": "author_of",
        "tail": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection"
      },
      {
        "head": "Abdullah Al Ahad Khan",
        "relation": "author_of",
        "tail": "Enhanced Transformer-BiLSTM Deep Learning Framework for Day-Ahead Energy Price Forecasting"
      },
      {
        "head": "Md Habib Ullah",
        "relation": "author_of",
        "tail": "Enhanced Transformer-BiLSTM Deep Learning Framework for Day-Ahead Energy Price Forecasting"
      },
      {
        "head": "Ruchira Tabassum",
        "relation": "author_of",
        "tail": "Enhanced Transformer-BiLSTM Deep Learning Framework for Day-Ahead Energy Price Forecasting"
      },
      {
        "head": "Md Faisal Kabir",
        "relation": "author_of",
        "tail": "Enhanced Transformer-BiLSTM Deep Learning Framework for Day-Ahead Energy Price Forecasting"
      },
      {
        "head": "Jinghuan Zhang",
        "relation": "author_of",
        "tail": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation"
      },
      {
        "head": "Wang Chen",
        "relation": "author_of",
        "tail": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation"
      },
      {
        "head": "Jian Zhang",
        "relation": "author_of",
        "tail": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation"
      },
      {
        "head": "A. Krizhevsky",
        "relation": "author_of",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "I. Sutskever",
        "relation": "author_of",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "Geoffrey E. Hinton",
        "relation": "author_of",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "Karen Simonyan",
        "relation": "author_of",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "Andrew Zisserman",
        "relation": "author_of",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "P. Cochat",
        "relation": "author_of",
        "tail": "Et al"
      },
      {
        "head": "L. Vaucoret",
        "relation": "author_of",
        "tail": "Et al"
      },
      {
        "head": "J. Sarles",
        "relation": "author_of",
        "tail": "Et al"
      },
      {
        "head": "Shaoqing Ren",
        "relation": "author_of",
        "tail": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
      },
      {
        "head": "Kaiming He",
        "relation": "author_of",
        "tail": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
      },
      {
        "head": "Ross Girshick",
        "relation": "author_of",
        "tail": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
      },
      {
        "head": "Jian Sun",
        "relation": "author_of",
        "tail": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
      },
      {
        "head": "Yuqi Cheng",
        "relation": "author_of",
        "tail": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects"
      },
      {
        "head": "Yunkang Cao",
        "relation": "author_of",
        "tail": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects"
      },
      {
        "head": "Haiming Yao",
        "relation": "author_of",
        "tail": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects"
      },
      {
        "head": "Wei Luo",
        "relation": "author_of",
        "tail": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects"
      },
      {
        "head": "Cheng Jiang",
        "relation": "author_of",
        "tail": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects"
      },
      {
        "head": "Hui Zhang",
        "relation": "author_of",
        "tail": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects"
      },
      {
        "head": "Weiming Shen",
        "relation": "author_of",
        "tail": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects"
      },
      {
        "head": "Naveen Kumar Srinivasa",
        "relation": "author_of",
        "tail": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning"
      },
      {
        "head": "Ajeet Rao Chalamala",
        "relation": "author_of",
        "tail": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning"
      },
      {
        "head": "Kumar Singh",
        "relation": "author_of",
        "tail": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning"
      },
      {
        "head": "Ieee Krishna Mohan Senior Member",
        "relation": "author_of",
        "tail": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning"
      },
      {
        "head": "K. Naveen",
        "relation": "author_of",
        "tail": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning"
      },
      {
        "head": "Srinivasa Rao",
        "relation": "author_of",
        "tail": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning"
      },
      {
        "head": "Ajeet Kumar Singh",
        "relation": "author_of",
        "tail": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning"
      },
      {
        "head": "Manlin Zhang",
        "relation": "author_of",
        "tail": "DiffusionEngine: Diffusion model is scalable data engine for object detection"
      },
      {
        "head": "Jie Wu",
        "relation": "author_of",
        "tail": "DiffusionEngine: Diffusion model is scalable data engine for object detection"
      },
      {
        "head": "Yuxi Ren",
        "relation": "author_of",
        "tail": "DiffusionEngine: Diffusion model is scalable data engine for object detection"
      },
      {
        "head": "Jiahong Yang",
        "relation": "author_of",
        "tail": "DiffusionEngine: Diffusion model is scalable data engine for object detection"
      },
      {
        "head": "Ming Li",
        "relation": "author_of",
        "tail": "DiffusionEngine: Diffusion model is scalable data engine for object detection"
      },
      {
        "head": "Andy J. Ma",
        "relation": "author_of",
        "tail": "DiffusionEngine: Diffusion model is scalable data engine for object detection"
      },
      {
        "head": "Salma Haidar",
        "relation": "author_of",
        "tail": "Enhancing hyperspectral image prediction with contrastive learning in low-label regimes"
      },
      {
        "head": "José Oramas",
        "relation": "author_of",
        "tail": "Enhancing hyperspectral image prediction with contrastive learning in low-label regimes"
      },
      {
        "head": "Hongbo Jiang",
        "relation": "author_of",
        "tail": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID"
      },
      {
        "head": "Lei Ye",
        "relation": "author_of",
        "tail": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID"
      },
      {
        "head": "Jingyang Hu",
        "relation": "author_of",
        "tail": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID"
      },
      {
        "head": "Xiaotian Chen",
        "relation": "author_of",
        "tail": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID"
      },
      {
        "head": "Siyu Chen",
        "relation": "author_of",
        "tail": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID"
      },
      {
        "head": "Wei Zhang",
        "relation": "author_of",
        "tail": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID"
      },
      {
        "head": "Kehua Yang",
        "relation": "author_of",
        "tail": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID"
      },
      {
        "head": "Diederik P Kingma",
        "relation": "author_of",
        "tail": "Auto-Encoding Variational Bayes"
      },
      {
        "head": "Max Welling",
        "relation": "author_of",
        "tail": "Auto-Encoding Variational Bayes"
      },
      {
        "head": "Geoffrey E. Hinton",
        "relation": "author_of",
        "tail": "Supporting Online Material for Reducing the Dimensionality of Data with Neural Networks"
      },
      {
        "head": "R. Salakhutdinov",
        "relation": "author_of",
        "tail": "Supporting Online Material for Reducing the Dimensionality of Data with Neural Networks"
      },
      {
        "head": "John C. Duchi",
        "relation": "author_of",
        "tail": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"
      },
      {
        "head": "Elad Hazan",
        "relation": "author_of",
        "tail": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"
      },
      {
        "head": "Y. Singer",
        "relation": "author_of",
        "tail": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"
      },
      {
        "head": "Alex Graves",
        "relation": "author_of",
        "tail": "Speech recognition with deep recurrent neural networks"
      },
      {
        "head": "Abdel-rahman Mohamed",
        "relation": "author_of",
        "tail": "Speech recognition with deep recurrent neural networks"
      },
      {
        "head": "Geoffrey Hinton",
        "relation": "author_of",
        "tail": "Speech recognition with deep recurrent neural networks"
      },
      {
        "head": "Nian Wang",
        "relation": "author_of",
        "tail": "Weakly Supervised Image Dehazing via Physics-Based Decomposition"
      },
      {
        "head": "Zhigao Cui",
        "relation": "author_of",
        "tail": "Weakly Supervised Image Dehazing via Physics-Based Decomposition"
      },
      {
        "head": "Yanzhao Su",
        "relation": "author_of",
        "tail": "Weakly Supervised Image Dehazing via Physics-Based Decomposition"
      },
      {
        "head": "Yunwei Lan",
        "relation": "author_of",
        "tail": "Weakly Supervised Image Dehazing via Physics-Based Decomposition"
      },
      {
        "head": "Yuanliang Xue",
        "relation": "author_of",
        "tail": "Weakly Supervised Image Dehazing via Physics-Based Decomposition"
      },
      {
        "head": "Cong Zhang",
        "relation": "author_of",
        "tail": "Weakly Supervised Image Dehazing via Physics-Based Decomposition"
      },
      {
        "head": "Aihua Li",
        "relation": "author_of",
        "tail": "Weakly Supervised Image Dehazing via Physics-Based Decomposition"
      },
      {
        "head": "Leong Kah Meng",
        "relation": "author_of",
        "tail": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer"
      },
      {
        "head": "Ho Hooi Yi",
        "relation": "author_of",
        "tail": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer"
      },
      {
        "head": "Ng Bo Wei",
        "relation": "author_of",
        "tail": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer"
      },
      {
        "head": "Lim Jia Xin",
        "relation": "author_of",
        "tail": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer"
      },
      {
        "head": "Zailan Arabee Abdul Salam",
        "relation": "author_of",
        "tail": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer"
      },
      {
        "head": "Xin Cheng",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Wangding Zeng",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Damai Dai",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Qinyu Chen",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Bingxuan Wang",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Zhenda Xie",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Kezhao Huang",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Xingkai Yu",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Zhewen Hao",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Yukun Li",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Han Zhang",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Huishuai Zhang",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Dongyan Zhao",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Wenfeng Liang",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Mostafa Saberian",
        "relation": "author_of",
        "tail": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction"
      },
      {
        "head": "Vidya Samadi",
        "relation": "author_of",
        "tail": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction"
      },
      {
        "head": "Ioana Popescu",
        "relation": "author_of",
        "tail": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction"
      },
      {
        "head": "Husheng Fang",
        "relation": "author_of",
        "tail": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model"
      },
      {
        "head": "Shunlin Liang",
        "relation": "author_of",
        "tail": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model"
      },
      {
        "head": "Wenyuan Li",
        "relation": "author_of",
        "tail": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model"
      },
      {
        "head": "Yongzhe Chen",
        "relation": "author_of",
        "tail": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model"
      },
      {
        "head": "Han Ma",
        "relation": "author_of",
        "tail": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model"
      },
      {
        "head": "Jianglei Xu",
        "relation": "author_of",
        "tail": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model"
      },
      {
        "head": "Yichuan Ma",
        "relation": "author_of",
        "tail": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model"
      },
      {
        "head": "Tao He",
        "relation": "author_of",
        "tail": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model"
      },
      {
        "head": "Feng Tian",
        "relation": "author_of",
        "tail": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model"
      },
      {
        "head": "Fengjiao Zhang",
        "relation": "author_of",
        "tail": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model"
      },
      {
        "head": "Hui Liang",
        "relation": "author_of",
        "tail": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model"
      },
      {
        "head": "Christian Szegedy",
        "relation": "author_of",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "Wei Liu",
        "relation": "author_of",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "Yangqing Jia",
        "relation": "author_of",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "Pierre Sermanet",
        "relation": "author_of",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "Scott Reed",
        "relation": "author_of",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "Dragomir Anguelov",
        "relation": "author_of",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "Dumitru Erhan",
        "relation": "author_of",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "Vincent Vanhoucke",
        "relation": "author_of",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "Andrew Rabinovich",
        "relation": "author_of",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "Sergey Ioffe",
        "relation": "author_of",
        "tail": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
      },
      {
        "head": "Christian Szegedy",
        "relation": "author_of",
        "tail": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
      },
      {
        "head": "Olga Russakovsky",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Jia Deng",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Hao Su",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Jonathan Krause",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Sanjeev Satheesh",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Sean Ma",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Zhiheng Huang",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Andrej Karpathy",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Aditya Khosla",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Michael Bernstein",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Alexander C. Berg",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Li Fei-Fei",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Boyang Zheng",
        "relation": "author_of",
        "tail": "Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "Nanye Ma",
        "relation": "author_of",
        "tail": "Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "Shengbang Tong",
        "relation": "author_of",
        "tail": "Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "Saining Xie",
        "relation": "author_of",
        "tail": "Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "S. Rizvi",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Daniel Levine",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Aakash Patel",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Shiyang Zhang",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Eric Wang",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Curtis Jamison Perry",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Ivan Vrkic",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Nicole Mayerli Constante",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Zirui Fu",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Sizhuang He",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "David Zhang",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Cerise Tang",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Zhuoyang Lyu",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Rayyan Y Darji",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Chang Li",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Emily Sun",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "David Jeong",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Lawrence Zhao",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "J. Kwan",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "David Braun",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Brian Hafler",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Hattie Chung",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "R. M. Dhodapkar",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Paul F. Jaeger",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Bryan Perozzi",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Jeffrey Ishizuka",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Shekoofeh Azizi",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "D. van Dijk",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Şafak Kılıç",
        "relation": "author_of",
        "tail": "HybridVisionNet: An advanced hybrid deep learning framework for automated multi-class ocular disease diagnosis using fundus imaging"
      },
      {
        "head": "Zhengyu Zhao",
        "relation": "author_of",
        "tail": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights"
      },
      {
        "head": "Hanwei Zhang",
        "relation": "author_of",
        "tail": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights"
      },
      {
        "head": "Renjue Li",
        "relation": "author_of",
        "tail": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights"
      },
      {
        "head": "R. Sicre",
        "relation": "author_of",
        "tail": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights"
      },
      {
        "head": "L. Amsaleg",
        "relation": "author_of",
        "tail": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights"
      },
      {
        "head": "Michael Backes",
        "relation": "author_of",
        "tail": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights"
      },
      {
        "head": "Qi Li",
        "relation": "author_of",
        "tail": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights"
      },
      {
        "head": "Chao Shen",
        "relation": "author_of",
        "tail": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights"
      },
      {
        "head": "Yifei Ge",
        "relation": "author_of",
        "tail": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot"
      },
      {
        "head": "Zhuo Li",
        "relation": "author_of",
        "tail": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot"
      },
      {
        "head": "Xuebin Yue",
        "relation": "author_of",
        "tail": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot"
      },
      {
        "head": "Hengyi Li",
        "relation": "author_of",
        "tail": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot"
      },
      {
        "head": "Lin Meng",
        "relation": "author_of",
        "tail": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot"
      },
      {
        "head": "Ashish Vaswani",
        "relation": "author_of",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Noam Shazeer",
        "relation": "author_of",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Niki Parmar",
        "relation": "author_of",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Jakob Uszkoreit",
        "relation": "author_of",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Llion Jones",
        "relation": "author_of",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Aidan N. Gomez",
        "relation": "author_of",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Lukasz Kaiser",
        "relation": "author_of",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Illia Polosukhin",
        "relation": "author_of",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Colin Raffel",
        "relation": "author_of",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "Noam Shazeer",
        "relation": "author_of",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "Adam Roberts",
        "relation": "author_of",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "Katherine Lee",
        "relation": "author_of",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "Sharan Narang",
        "relation": "author_of",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "Michael Matena",
        "relation": "author_of",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "Yanqi Zhou",
        "relation": "author_of",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "Wei Li",
        "relation": "author_of",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "Peter J. Liu",
        "relation": "author_of",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "M. Heusel",
        "relation": "author_of",
        "tail": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"
      },
      {
        "head": "Hubert Ramsauer",
        "relation": "author_of",
        "tail": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"
      },
      {
        "head": "Thomas Unterthiner",
        "relation": "author_of",
        "tail": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"
      },
      {
        "head": "Bernhard Nessler",
        "relation": "author_of",
        "tail": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"
      },
      {
        "head": "Sepp Hochreiter",
        "relation": "author_of",
        "tail": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"
      },
      {
        "head": "Holger Caesar",
        "relation": "author_of",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "Varun Bankiti",
        "relation": "author_of",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "Alex H. Lang",
        "relation": "author_of",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "Sourabh Vora",
        "relation": "author_of",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "Venice Erin Liong",
        "relation": "author_of",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "Qiang Xu",
        "relation": "author_of",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "Anush Krishnan",
        "relation": "author_of",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "Yu Pan",
        "relation": "author_of",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "Giancarlo Baldan",
        "relation": "author_of",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "Oscar Beijbom",
        "relation": "author_of",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "Alexey Dosovitskiy",
        "relation": "author_of",
        "tail": "CARLA: An Open Urban Driving Simulator"
      },
      {
        "head": "German Ros",
        "relation": "author_of",
        "tail": "CARLA: An Open Urban Driving Simulator"
      },
      {
        "head": "Felipe Codevilla",
        "relation": "author_of",
        "tail": "CARLA: An Open Urban Driving Simulator"
      },
      {
        "head": "Antonio Lopez",
        "relation": "author_of",
        "tail": "CARLA: An Open Urban Driving Simulator"
      },
      {
        "head": "Vladlen Koltun",
        "relation": "author_of",
        "tail": "CARLA: An Open Urban Driving Simulator"
      },
      {
        "head": "Fachrina Dewi Puspitasari",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Chaoning Zhang",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Joseph Cho",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Adnan Haider",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Noor Ul Eman",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Omer Amin",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Alexis Mankowski",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Muhammad Umair",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Jingyao Zheng",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Sheng Zheng",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Lik-Hang Lee",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Caiyan Qin",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Tae-Ho Kim",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Choong Seon Hong",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Yang Yang",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Heng Tao Shen",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Bohan Li",
        "relation": "author_of",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "Zhuang Ma",
        "relation": "author_of",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "Dalong Du",
        "relation": "author_of",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "Baorui Peng",
        "relation": "author_of",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "Zhujin Liang",
        "relation": "author_of",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "Zhenqiang Liu",
        "relation": "author_of",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "Chao Ma",
        "relation": "author_of",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "Yueming Jin",
        "relation": "author_of",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "Hao Zhao",
        "relation": "author_of",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "Wenjun Zeng",
        "relation": "author_of",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "Xin Jin",
        "relation": "author_of",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "Zhuoran Yang",
        "relation": "author_of",
        "tail": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask"
      },
      {
        "head": "Yanyong Zhang",
        "relation": "author_of",
        "tail": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask"
      },
      {
        "head": "Guosheng Zhao",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Yaozeng Wang",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Xiaofeng Wang",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Zheng Zhu",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Tingdong Yu",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Guan Huang",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Yongchen Zai",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Ji Jiao",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Changliang Xue",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Xiaole Wang",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Zhen Yang",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Futang Zhu",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Xingang Wang",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Ahmad Rahimi",
        "relation": "author_of",
        "tail": "MAD: Motion Appearance Decoupling for efficient Driving World Models"
      },
      {
        "head": "Valentin Gerard",
        "relation": "author_of",
        "tail": "MAD: Motion Appearance Decoupling for efficient Driving World Models"
      },
      {
        "head": "Eloi Zablocki",
        "relation": "author_of",
        "tail": "MAD: Motion Appearance Decoupling for efficient Driving World Models"
      },
      {
        "head": "Matthieu Cord",
        "relation": "author_of",
        "tail": "MAD: Motion Appearance Decoupling for efficient Driving World Models"
      },
      {
        "head": "Alexandre Alahi",
        "relation": "author_of",
        "tail": "MAD: Motion Appearance Decoupling for efficient Driving World Models"
      },
      {
        "head": "W. Marsden",
        "relation": "author_of",
        "tail": "I and J"
      },
      {
        "head": "Jia Deng",
        "relation": "author_of",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "Wei Dong",
        "relation": "author_of",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "R. Socher",
        "relation": "author_of",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "Li-Jia Li",
        "relation": "author_of",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "K. Li",
        "relation": "author_of",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "Li Fei-Fei",
        "relation": "author_of",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "R. Stephenson",
        "relation": "author_of",
        "tail": "A and V"
      },
      {
        "head": "Jaskirat Singh",
        "relation": "author_of",
        "tail": "What matters for Representation Alignment: Global Information or Spatial Structure?"
      },
      {
        "head": "Xingjian Leng",
        "relation": "author_of",
        "tail": "What matters for Representation Alignment: Global Information or Spatial Structure?"
      },
      {
        "head": "Zongze Wu",
        "relation": "author_of",
        "tail": "What matters for Representation Alignment: Global Information or Spatial Structure?"
      },
      {
        "head": "Liang Zheng",
        "relation": "author_of",
        "tail": "What matters for Representation Alignment: Global Information or Spatial Structure?"
      },
      {
        "head": "Richard Zhang",
        "relation": "author_of",
        "tail": "What matters for Representation Alignment: Global Information or Spatial Structure?"
      },
      {
        "head": "Eli Shechtman",
        "relation": "author_of",
        "tail": "What matters for Representation Alignment: Global Information or Spatial Structure?"
      },
      {
        "head": "Saining Xie",
        "relation": "author_of",
        "tail": "What matters for Representation Alignment: Global Information or Spatial Structure?"
      },
      {
        "head": "Zhifeng Wang",
        "relation": "author_of",
        "tail": "SCB-DETR: Multiscale Deformable Transformers for Occlusion-Resilient Student Learning Behavior Detection in Smart Classroom"
      },
      {
        "head": "Minghui Wang",
        "relation": "author_of",
        "tail": "SCB-DETR: Multiscale Deformable Transformers for Occlusion-Resilient Student Learning Behavior Detection in Smart Classroom"
      },
      {
        "head": "Chunyan Zeng",
        "relation": "author_of",
        "tail": "SCB-DETR: Multiscale Deformable Transformers for Occlusion-Resilient Student Learning Behavior Detection in Smart Classroom"
      },
      {
        "head": "Longlong Li",
        "relation": "author_of",
        "tail": "SCB-DETR: Multiscale Deformable Transformers for Occlusion-Resilient Student Learning Behavior Detection in Smart Classroom"
      },
      {
        "head": "Ehsan Zakeri",
        "relation": "author_of",
        "tail": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination"
      },
      {
        "head": "Amanda Spilkin",
        "relation": "author_of",
        "tail": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination"
      },
      {
        "head": "Hanae Elmekki",
        "relation": "author_of",
        "tail": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination"
      },
      {
        "head": "Antonela Zanuttini",
        "relation": "author_of",
        "tail": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination"
      },
      {
        "head": "L. Kadem",
        "relation": "author_of",
        "tail": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination"
      },
      {
        "head": "Jamal Bentahar",
        "relation": "author_of",
        "tail": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination"
      },
      {
        "head": "Wen-Fang Xie",
        "relation": "author_of",
        "tail": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination"
      },
      {
        "head": "Philippe Pibarot",
        "relation": "author_of",
        "tail": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination"
      },
      {
        "head": "Ana Davila",
        "relation": "author_of",
        "tail": "Bio-Inspired Fine-Tuning for Selective Transfer Learning in Image Classification"
      },
      {
        "head": "Jacinto Colan",
        "relation": "author_of",
        "tail": "Bio-Inspired Fine-Tuning for Selective Transfer Learning in Image Classification"
      },
      {
        "head": "Yasuhisa Hasegawa",
        "relation": "author_of",
        "tail": "Bio-Inspired Fine-Tuning for Selective Transfer Learning in Image Classification"
      },
      {
        "head": "Subham Sharma",
        "relation": "author_of",
        "tail": "Hand Sign Language Detection Using Deep Learning"
      },
      {
        "head": "Sharmila Subudhi",
        "relation": "author_of",
        "tail": "Hand Sign Language Detection Using Deep Learning"
      },
      {
        "head": "Yoshua Bengio",
        "relation": "author_of",
        "tail": "Representation Learning: A Review and New Perspectives"
      },
      {
        "head": "Aaron Courville",
        "relation": "author_of",
        "tail": "Representation Learning: A Review and New Perspectives"
      },
      {
        "head": "Pascal Vincent",
        "relation": "author_of",
        "tail": "Representation Learning: A Review and New Perspectives"
      },
      {
        "head": "D. Touretzky",
        "relation": "author_of",
        "tail": "In Advances in Neural Information Processing Systems"
      },
      {
        "head": "M. C. Mozer",
        "relation": "author_of",
        "tail": "In Advances in Neural Information Processing Systems"
      },
      {
        "head": "M. E. Hasselmo",
        "relation": "author_of",
        "tail": "In Advances in Neural Information Processing Systems"
      },
      {
        "head": "RegressionChristopher",
        "relation": "author_of",
        "tail": "In Advances in Neural Information Processing Systems"
      },
      {
        "head": "I. K.",
        "relation": "author_of",
        "tail": "In Advances in Neural Information Processing Systems"
      },
      {
        "head": "WilliamsNeural",
        "relation": "author_of",
        "tail": "In Advances in Neural Information Processing Systems"
      },
      {
        "head": "GroupAston",
        "relation": "author_of",
        "tail": "In Advances in Neural Information Processing Systems"
      },
      {
        "head": "UniversityBirmingham",
        "relation": "author_of",
        "tail": "In Advances in Neural Information Processing Systems"
      },
      {
        "head": "Danilo Jimenez Rezende",
        "relation": "author_of",
        "tail": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models"
      },
      {
        "head": "S. Mohamed",
        "relation": "author_of",
        "tail": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models"
      },
      {
        "head": "Daan Wierstra",
        "relation": "author_of",
        "tail": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models"
      },
      {
        "head": "Shanchuan Lin",
        "relation": "author_of",
        "tail": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation"
      },
      {
        "head": "Anran Wang",
        "relation": "author_of",
        "tail": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation"
      },
      {
        "head": "Xiao Yang",
        "relation": "author_of",
        "tail": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation"
      },
      {
        "head": "Zhiyuan Chen",
        "relation": "author_of",
        "tail": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions"
      },
      {
        "head": "Jiajiong Cao",
        "relation": "author_of",
        "tail": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions"
      },
      {
        "head": "Zhiquan Chen",
        "relation": "author_of",
        "tail": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions"
      },
      {
        "head": "Yuming Li",
        "relation": "author_of",
        "tail": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions"
      },
      {
        "head": "Chenguang Ma",
        "relation": "author_of",
        "tail": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions"
      },
      {
        "head": "Wenzhao Zheng",
        "relation": "author_of",
        "tail": "GenAD: Generative End-to-End Autonomous Driving"
      },
      {
        "head": "Ruiqi Song",
        "relation": "author_of",
        "tail": "GenAD: Generative End-to-End Autonomous Driving"
      },
      {
        "head": "Xianda Guo",
        "relation": "author_of",
        "tail": "GenAD: Generative End-to-End Autonomous Driving"
      },
      {
        "head": "Chenming Zhang",
        "relation": "author_of",
        "tail": "GenAD: Generative End-to-End Autonomous Driving"
      },
      {
        "head": "Long Chen",
        "relation": "author_of",
        "tail": "GenAD: Generative End-to-End Autonomous Driving"
      },
      {
        "head": "Shiyin Lu",
        "relation": "author_of",
        "tail": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model"
      },
      {
        "head": "Yang Li",
        "relation": "author_of",
        "tail": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model"
      },
      {
        "head": "Qing-Guo Chen",
        "relation": "author_of",
        "tail": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model"
      },
      {
        "head": "Zhao Xu",
        "relation": "author_of",
        "tail": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model"
      },
      {
        "head": "Weihua Luo",
        "relation": "author_of",
        "tail": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model"
      },
      {
        "head": "Kaifu Zhang",
        "relation": "author_of",
        "tail": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model"
      },
      {
        "head": "Han-Jia Ye",
        "relation": "author_of",
        "tail": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model"
      },
      {
        "head": "Jie Liu",
        "relation": "author_of",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Gongye Liu",
        "relation": "author_of",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Jiajun Liang",
        "relation": "author_of",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Ziyang Yuan",
        "relation": "author_of",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Xiaokun Liu",
        "relation": "author_of",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Mingwu Zheng",
        "relation": "author_of",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Xiele Wu",
        "relation": "author_of",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Qiulin Wang",
        "relation": "author_of",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Menghan Xia",
        "relation": "author_of",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Xintao Wang",
        "relation": "author_of",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Xiaohong Liu",
        "relation": "author_of",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Fei Yang",
        "relation": "author_of",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Pengfei Wan",
        "relation": "author_of",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Di Zhang",
        "relation": "author_of",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Kun Gai",
        "relation": "author_of",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Yujiu Yang",
        "relation": "author_of",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Wanli Ouyang",
        "relation": "author_of",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Alex Graves",
        "relation": "author_of",
        "tail": "Speech Recognition with Deep Recurrent Neural Networks"
      },
      {
        "head": "Abdel-rahman Mohamed",
        "relation": "author_of",
        "tail": "Speech Recognition with Deep Recurrent Neural Networks"
      },
      {
        "head": "Geoffrey Hinton",
        "relation": "author_of",
        "tail": "Speech Recognition with Deep Recurrent Neural Networks"
      },
      {
        "head": "D. Rumelhart",
        "relation": "author_of",
        "tail": "Learning representations by back-propagating errors"
      },
      {
        "head": "Geoffrey E. Hinton",
        "relation": "author_of",
        "tail": "Learning representations by back-propagating errors"
      },
      {
        "head": "Ronald J. Williams",
        "relation": "author_of",
        "tail": "Learning representations by back-propagating errors"
      },
      {
        "head": "M. Schuster",
        "relation": "author_of",
        "tail": "Bidirectional recurrent neural networks"
      },
      {
        "head": "K. Paliwal",
        "relation": "author_of",
        "tail": "Bidirectional recurrent neural networks"
      },
      {
        "head": "Alex Graves",
        "relation": "author_of",
        "tail": "Connectionist Temporal Classiﬁcation: Labelling Unsegmented Sequence Data with Recurrent Neural Networks"
      },
      {
        "head": "Santiago Fern´andez",
        "relation": "author_of",
        "tail": "Connectionist Temporal Classiﬁcation: Labelling Unsegmented Sequence Data with Recurrent Neural Networks"
      },
      {
        "head": "Faustino J. Gomez",
        "relation": "author_of",
        "tail": "Connectionist Temporal Classiﬁcation: Labelling Unsegmented Sequence Data with Recurrent Neural Networks"
      },
      {
        "head": "J¨urgen Schmidhuber",
        "relation": "author_of",
        "tail": "Connectionist Temporal Classiﬁcation: Labelling Unsegmented Sequence Data with Recurrent Neural Networks"
      },
      {
        "head": "Alex Graves",
        "relation": "author_of",
        "tail": "2005 Special Issue: Framewise phoneme classification with bidirectional LSTM and other neural network architectures"
      },
      {
        "head": "J. Schmidhuber",
        "relation": "author_of",
        "tail": "2005 Special Issue: Framewise phoneme classification with bidirectional LSTM and other neural network architectures"
      },
      {
        "head": "Tianming Sun",
        "relation": "author_of",
        "tail": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses"
      },
      {
        "head": "Bin Feng",
        "relation": "author_of",
        "tail": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses"
      },
      {
        "head": "Jinpeng Huo",
        "relation": "author_of",
        "tail": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses"
      },
      {
        "head": "Yu Xiao",
        "relation": "author_of",
        "tail": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses"
      },
      {
        "head": "Wengan Wang",
        "relation": "author_of",
        "tail": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses"
      },
      {
        "head": "Jin Peng",
        "relation": "author_of",
        "tail": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses"
      },
      {
        "head": "Zehua Li",
        "relation": "author_of",
        "tail": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses"
      },
      {
        "head": "Chengjie Du",
        "relation": "author_of",
        "tail": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses"
      },
      {
        "head": "Wenxian Wang",
        "relation": "author_of",
        "tail": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses"
      },
      {
        "head": "G. Zou",
        "relation": "author_of",
        "tail": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses"
      },
      {
        "head": "Lei Liu",
        "relation": "author_of",
        "tail": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses"
      },
      {
        "head": "Francis R. Willett",
        "relation": "author_of",
        "tail": "A high-performance speech neuroprosthesis"
      },
      {
        "head": "Erin M. Kunz",
        "relation": "author_of",
        "tail": "A high-performance speech neuroprosthesis"
      },
      {
        "head": "Chaofei Fan",
        "relation": "author_of",
        "tail": "A high-performance speech neuroprosthesis"
      },
      {
        "head": "Donald T. Avansino",
        "relation": "author_of",
        "tail": "A high-performance speech neuroprosthesis"
      },
      {
        "head": "G. Wilson",
        "relation": "author_of",
        "tail": "A high-performance speech neuroprosthesis"
      },
      {
        "head": "Eun Young Choi",
        "relation": "author_of",
        "tail": "A high-performance speech neuroprosthesis"
      },
      {
        "head": "Foram B. Kamdar",
        "relation": "author_of",
        "tail": "A high-performance speech neuroprosthesis"
      },
      {
        "head": "M. Glasser",
        "relation": "author_of",
        "tail": "A high-performance speech neuroprosthesis"
      },
      {
        "head": "L. Hochberg",
        "relation": "author_of",
        "tail": "A high-performance speech neuroprosthesis"
      },
      {
        "head": "S. Druckmann",
        "relation": "author_of",
        "tail": "A high-performance speech neuroprosthesis"
      },
      {
        "head": "K. Shenoy",
        "relation": "author_of",
        "tail": "A high-performance speech neuroprosthesis"
      },
      {
        "head": "J. Henderson",
        "relation": "author_of",
        "tail": "A high-performance speech neuroprosthesis"
      },
      {
        "head": "Shibhansh Dohare",
        "relation": "author_of",
        "tail": "Loss of plasticity in deep continual learning"
      },
      {
        "head": "J. F. Hernandez-Garcia",
        "relation": "author_of",
        "tail": "Loss of plasticity in deep continual learning"
      },
      {
        "head": "Qingfeng Lan",
        "relation": "author_of",
        "tail": "Loss of plasticity in deep continual learning"
      },
      {
        "head": "Parash Rahman",
        "relation": "author_of",
        "tail": "Loss of plasticity in deep continual learning"
      },
      {
        "head": "A. Mahmood",
        "relation": "author_of",
        "tail": "Loss of plasticity in deep continual learning"
      },
      {
        "head": "R. Sutton",
        "relation": "author_of",
        "tail": "Loss of plasticity in deep continual learning"
      },
      {
        "head": "S. Ambrogio",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "P. Narayanan",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "A. Okazaki",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "A. Fasoli",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "C. Mackin",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "K. Hosokawa",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "A. Nomura",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "Takeo Yasuda",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "An Chen",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "A. Friz",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "M. Ishii",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "J. Luquin",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "Y. Kohda",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "N. Saulnier",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "K. Brew",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "Samuel Choi",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "I. Ok",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "Timothy Philip",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "Victor Chan",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "M. Silvestre",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "Ishtiaq Ahsan",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "Vijay Narayanan",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "H. Tsai",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "Geoffrey W. Burr",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "J. Shin",
        "relation": "author_of",
        "tail": "A Mathematical Theory of Communication"
      },
      {
        "head": "Sang Joon Kim",
        "relation": "author_of",
        "tail": "A Mathematical Theory of Communication"
      },
      {
        "head": "Yike Sun",
        "relation": "author_of",
        "tail": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers"
      },
      {
        "head": "Haotong Yang",
        "relation": "author_of",
        "tail": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers"
      },
      {
        "head": "Zhouchen Lin",
        "relation": "author_of",
        "tail": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers"
      },
      {
        "head": "Muhan Zhang",
        "relation": "author_of",
        "tail": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers"
      },
      {
        "head": "Ning Ding",
        "relation": "author_of",
        "tail": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling"
      },
      {
        "head": "Fangcheng Liu",
        "relation": "author_of",
        "tail": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling"
      },
      {
        "head": "Kyungrae Kim",
        "relation": "author_of",
        "tail": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling"
      },
      {
        "head": "Linji Hao",
        "relation": "author_of",
        "tail": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling"
      },
      {
        "head": "Kyeng-Hun Lee",
        "relation": "author_of",
        "tail": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling"
      },
      {
        "head": "Hyeonmok Ko",
        "relation": "author_of",
        "tail": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling"
      },
      {
        "head": "Yehui Tang",
        "relation": "author_of",
        "tail": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling"
      },
      {
        "head": "Huinan Xu",
        "relation": "author_of",
        "tail": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram"
      },
      {
        "head": "Xuyang Feng",
        "relation": "author_of",
        "tail": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram"
      },
      {
        "head": "Junhong Chen",
        "relation": "author_of",
        "tail": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram"
      },
      {
        "head": "Junchen Liu",
        "relation": "author_of",
        "tail": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram"
      },
      {
        "head": "Kaiwen Deng",
        "relation": "author_of",
        "tail": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram"
      },
      {
        "head": "Kai Ding",
        "relation": "author_of",
        "tail": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram"
      },
      {
        "head": "Shengning Long",
        "relation": "author_of",
        "tail": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram"
      },
      {
        "head": "Jiaxue Shuai",
        "relation": "author_of",
        "tail": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram"
      },
      {
        "head": "Zhaorong Li",
        "relation": "author_of",
        "tail": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram"
      },
      {
        "head": "Shiping Liu",
        "relation": "author_of",
        "tail": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram"
      },
      {
        "head": "Guirong Xue",
        "relation": "author_of",
        "tail": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram"
      },
      {
        "head": "Zhan Xiao",
        "relation": "author_of",
        "tail": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram"
      },
      {
        "head": "Albert Tseng",
        "relation": "author_of",
        "tail": "L$^3$: Large Lookup Layers"
      },
      {
        "head": "Christopher De Sa",
        "relation": "author_of",
        "tail": "L$^3$: Large Lookup Layers"
      },
      {
        "head": "Hong Liu",
        "relation": "author_of",
        "tail": "Scaling Embeddings Outperforms Scaling Experts in Language Models"
      },
      {
        "head": "Jiaqi Zhang",
        "relation": "author_of",
        "tail": "Scaling Embeddings Outperforms Scaling Experts in Language Models"
      },
      {
        "head": "Chao Wang",
        "relation": "author_of",
        "tail": "Scaling Embeddings Outperforms Scaling Experts in Language Models"
      },
      {
        "head": "Xing Hu",
        "relation": "author_of",
        "tail": "Scaling Embeddings Outperforms Scaling Experts in Language Models"
      },
      {
        "head": "Linkun Lyu",
        "relation": "author_of",
        "tail": "Scaling Embeddings Outperforms Scaling Experts in Language Models"
      },
      {
        "head": "Jiaqi Sun",
        "relation": "author_of",
        "tail": "Scaling Embeddings Outperforms Scaling Experts in Language Models"
      },
      {
        "head": "Xurui Yang",
        "relation": "author_of",
        "tail": "Scaling Embeddings Outperforms Scaling Experts in Language Models"
      },
      {
        "head": "Bo Wang",
        "relation": "author_of",
        "tail": "Scaling Embeddings Outperforms Scaling Experts in Language Models"
      },
      {
        "head": "Fengcun Li",
        "relation": "author_of",
        "tail": "Scaling Embeddings Outperforms Scaling Experts in Language Models"
      },
      {
        "head": "Yulei Qian",
        "relation": "author_of",
        "tail": "Scaling Embeddings Outperforms Scaling Experts in Language Models"
      },
      {
        "head": "Lingtong Si",
        "relation": "author_of",
        "tail": "Scaling Embeddings Outperforms Scaling Experts in Language Models"
      },
      {
        "head": "Yerui Sun",
        "relation": "author_of",
        "tail": "Scaling Embeddings Outperforms Scaling Experts in Language Models"
      },
      {
        "head": "Rumei Li",
        "relation": "author_of",
        "tail": "Scaling Embeddings Outperforms Scaling Experts in Language Models"
      },
      {
        "head": "Peng Pei",
        "relation": "author_of",
        "tail": "Scaling Embeddings Outperforms Scaling Experts in Language Models"
      },
      {
        "head": "Yuchen Xie",
        "relation": "author_of",
        "tail": "Scaling Embeddings Outperforms Scaling Experts in Language Models"
      },
      {
        "head": "Xunliang Cai",
        "relation": "author_of",
        "tail": "Scaling Embeddings Outperforms Scaling Experts in Language Models"
      },
      {
        "head": "Christian Szegedy",
        "relation": "author_of",
        "tail": "Going Deeper with Convolutions"
      },
      {
        "head": "Wei Liu",
        "relation": "author_of",
        "tail": "Going Deeper with Convolutions"
      },
      {
        "head": "Yangqing Jia",
        "relation": "author_of",
        "tail": "Going Deeper with Convolutions"
      },
      {
        "head": "Pierre Sermanet",
        "relation": "author_of",
        "tail": "Going Deeper with Convolutions"
      },
      {
        "head": "Scott Reed",
        "relation": "author_of",
        "tail": "Going Deeper with Convolutions"
      },
      {
        "head": "Dragomir Anguelov",
        "relation": "author_of",
        "tail": "Going Deeper with Convolutions"
      },
      {
        "head": "Dumitru Erhan",
        "relation": "author_of",
        "tail": "Going Deeper with Convolutions"
      },
      {
        "head": "Vincent Vanhoucke",
        "relation": "author_of",
        "tail": "Going Deeper with Convolutions"
      },
      {
        "head": "Andrew Rabinovich",
        "relation": "author_of",
        "tail": "Going Deeper with Convolutions"
      },
      {
        "head": "Yann LeCun",
        "relation": "author_of",
        "tail": "Gradient-based learning applied to document recognition"
      },
      {
        "head": "L. Bottou",
        "relation": "author_of",
        "tail": "Gradient-based learning applied to document recognition"
      },
      {
        "head": "Yoshua Bengio",
        "relation": "author_of",
        "tail": "Gradient-based learning applied to document recognition"
      },
      {
        "head": "P. Haffner",
        "relation": "author_of",
        "tail": "Gradient-based learning applied to document recognition"
      },
      {
        "head": "R. Tibshirani",
        "relation": "author_of",
        "tail": "Regression Shrinkage and Selection via the Lasso"
      },
      {
        "head": "Tsung-Yi Lin",
        "relation": "author_of",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Michael Maire",
        "relation": "author_of",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Serge Belongie",
        "relation": "author_of",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Lubomir Bourdev",
        "relation": "author_of",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Ross Girshick",
        "relation": "author_of",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "James Hays",
        "relation": "author_of",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Pietro Perona",
        "relation": "author_of",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Deva Ramanan",
        "relation": "author_of",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "C. Lawrence Zitnick",
        "relation": "author_of",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Piotr Dollár",
        "relation": "author_of",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Herve Goeau",
        "relation": "author_of",
        "tail": "LifeCLEF Plant Identification Task 2015"
      },
      {
        "head": "Pierre Bonnet",
        "relation": "author_of",
        "tail": "LifeCLEF Plant Identification Task 2015"
      },
      {
        "head": "Alexis Joly",
        "relation": "author_of",
        "tail": "LifeCLEF Plant Identification Task 2015"
      },
      {
        "head": "Safa Ben Atitallah",
        "relation": "author_of",
        "tail": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics"
      },
      {
        "head": "Maha Driss",
        "relation": "author_of",
        "tail": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics"
      },
      {
        "head": "Henda Ben Ghezela",
        "relation": "author_of",
        "tail": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics"
      },
      {
        "head": "Sareer Ul Amin",
        "relation": "author_of",
        "tail": "Enhancing pine wilt disease detection with synthetic data and external attention-based transformers"
      },
      {
        "head": "Yonghoon Jung",
        "relation": "author_of",
        "tail": "Enhancing pine wilt disease detection with synthetic data and external attention-based transformers"
      },
      {
        "head": "Muhammad Fayaz",
        "relation": "author_of",
        "tail": "Enhancing pine wilt disease detection with synthetic data and external attention-based transformers"
      },
      {
        "head": "Bumsoo Kim",
        "relation": "author_of",
        "tail": "Enhancing pine wilt disease detection with synthetic data and external attention-based transformers"
      },
      {
        "head": "Sanghyun Seo",
        "relation": "author_of",
        "tail": "Enhancing pine wilt disease detection with synthetic data and external attention-based transformers"
      },
      {
        "head": "Ayşe Aybilge Murat",
        "relation": "author_of",
        "tail": "A comprehensive review on YOLO versions for object detection"
      },
      {
        "head": "M. S. Kıran",
        "relation": "author_of",
        "tail": "A comprehensive review on YOLO versions for object detection"
      },
      {
        "head": "Hongyan Zhu",
        "relation": "author_of",
        "tail": "Harnessing large vision and language models in agriculture: a review"
      },
      {
        "head": "Shuai Qin",
        "relation": "author_of",
        "tail": "Harnessing large vision and language models in agriculture: a review"
      },
      {
        "head": "Min Su",
        "relation": "author_of",
        "tail": "Harnessing large vision and language models in agriculture: a review"
      },
      {
        "head": "Chengzhi Lin",
        "relation": "author_of",
        "tail": "Harnessing large vision and language models in agriculture: a review"
      },
      {
        "head": "Anjie Li",
        "relation": "author_of",
        "tail": "Harnessing large vision and language models in agriculture: a review"
      },
      {
        "head": "Junfeng Gao",
        "relation": "author_of",
        "tail": "Harnessing large vision and language models in agriculture: a review"
      },
      {
        "head": "Abdul Rehman Khan",
        "relation": "author_of",
        "tail": "Multi-axis vision transformer for medical image segmentation"
      },
      {
        "head": "Asifullah Khan",
        "relation": "author_of",
        "tail": "Multi-axis vision transformer for medical image segmentation"
      },
      {
        "head": "D. E. Boukhari",
        "relation": "author_of",
        "tail": "A comprehensive review of facial beauty prediction using deep learning techniques"
      },
      {
        "head": "F. Dornaika",
        "relation": "author_of",
        "tail": "A comprehensive review of facial beauty prediction using deep learning techniques"
      },
      {
        "head": "A. Chemsa",
        "relation": "author_of",
        "tail": "A comprehensive review of facial beauty prediction using deep learning techniques"
      },
      {
        "head": "Abdelmalik Taleb-Ahmed",
        "relation": "author_of",
        "tail": "A comprehensive review of facial beauty prediction using deep learning techniques"
      },
      {
        "head": "Benjamin DeMeo",
        "relation": "author_of",
        "tail": "Active learning framework leveraging transcriptomics identifies modulators of disease phenotypes."
      },
      {
        "head": "Charlotte Nesbitt",
        "relation": "author_of",
        "tail": "Active learning framework leveraging transcriptomics identifies modulators of disease phenotypes."
      },
      {
        "head": "S. A. Miller",
        "relation": "author_of",
        "tail": "Active learning framework leveraging transcriptomics identifies modulators of disease phenotypes."
      },
      {
        "head": "Daniel B. Burkhardt",
        "relation": "author_of",
        "tail": "Active learning framework leveraging transcriptomics identifies modulators of disease phenotypes."
      },
      {
        "head": "Inna Lipchina",
        "relation": "author_of",
        "tail": "Active learning framework leveraging transcriptomics identifies modulators of disease phenotypes."
      },
      {
        "head": "Doris Fu",
        "relation": "author_of",
        "tail": "Active learning framework leveraging transcriptomics identifies modulators of disease phenotypes."
      },
      {
        "head": "Peter Holderreith",
        "relation": "author_of",
        "tail": "Active learning framework leveraging transcriptomics identifies modulators of disease phenotypes."
      },
      {
        "head": "David Kim",
        "relation": "author_of",
        "tail": "Active learning framework leveraging transcriptomics identifies modulators of disease phenotypes."
      },
      {
        "head": "Sergey Kolchenko",
        "relation": "author_of",
        "tail": "Active learning framework leveraging transcriptomics identifies modulators of disease phenotypes."
      },
      {
        "head": "Artur Szałata",
        "relation": "author_of",
        "tail": "Active learning framework leveraging transcriptomics identifies modulators of disease phenotypes."
      },
      {
        "head": "Ishan Gupta",
        "relation": "author_of",
        "tail": "Active learning framework leveraging transcriptomics identifies modulators of disease phenotypes."
      },
      {
        "head": "Christine Kerr",
        "relation": "author_of",
        "tail": "Active learning framework leveraging transcriptomics identifies modulators of disease phenotypes."
      },
      {
        "head": "Thomas Pfefer",
        "relation": "author_of",
        "tail": "Active learning framework leveraging transcriptomics identifies modulators of disease phenotypes."
      },
      {
        "head": "Raziel Rojas-Rodriguez",
        "relation": "author_of",
        "tail": "Active learning framework leveraging transcriptomics identifies modulators of disease phenotypes."
      },
      {
        "head": "Sunil Kuppassani",
        "relation": "author_of",
        "tail": "Active learning framework leveraging transcriptomics identifies modulators of disease phenotypes."
      },
      {
        "head": "Laurens Kruidenier",
        "relation": "author_of",
        "tail": "Active learning framework leveraging transcriptomics identifies modulators of disease phenotypes."
      },
      {
        "head": "Parul B Doshi",
        "relation": "author_of",
        "tail": "Active learning framework leveraging transcriptomics identifies modulators of disease phenotypes."
      },
      {
        "head": "Mahdi Zamanighomi",
        "relation": "author_of",
        "tail": "Active learning framework leveraging transcriptomics identifies modulators of disease phenotypes."
      },
      {
        "head": "James J. Collins",
        "relation": "author_of",
        "tail": "Active learning framework leveraging transcriptomics identifies modulators of disease phenotypes."
      },
      {
        "head": "A. Shalek",
        "relation": "author_of",
        "tail": "Active learning framework leveraging transcriptomics identifies modulators of disease phenotypes."
      },
      {
        "head": "F. Theis",
        "relation": "author_of",
        "tail": "Active learning framework leveraging transcriptomics identifies modulators of disease phenotypes."
      },
      {
        "head": "Mauricio Cortes",
        "relation": "author_of",
        "tail": "Active learning framework leveraging transcriptomics identifies modulators of disease phenotypes."
      },
      {
        "head": "D. Lowe",
        "relation": "author_of",
        "tail": "Distinctive Image Features from Scale-Invariant Keypoints"
      },
      {
        "head": "Xiang An",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Yin Xie",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Kaicheng Yang",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Wenkang Zhang",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Xiuwei Zhao",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Zheng Cheng",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Yirui Wang",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Songcen Xu",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Changrui Chen",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Didi Zhu",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Chunsheng Wu",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Huajie Tan",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Chunyuan Li",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Jing Yang",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Jie Yu",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Xiyao Wang",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Bin Qin",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Yumeng Wang",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Zizhen Yan",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Ziyong Feng",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Ziwei Liu",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Bo Li",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Jiankang Deng",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Kento Kawaharazuka",
        "relation": "author_of",
        "tail": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications"
      },
      {
        "head": "Jihoon Oh",
        "relation": "author_of",
        "tail": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications"
      },
      {
        "head": "Jun Yamada",
        "relation": "author_of",
        "tail": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications"
      },
      {
        "head": "Ingmar Posner",
        "relation": "author_of",
        "tail": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications"
      },
      {
        "head": "Yuke Zhu",
        "relation": "author_of",
        "tail": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications"
      },
      {
        "head": "Lukas Muttenthaler",
        "relation": "author_of",
        "tail": "Aligning machine and human visual representations across abstraction levels"
      },
      {
        "head": "Klaus Greff",
        "relation": "author_of",
        "tail": "Aligning machine and human visual representations across abstraction levels"
      },
      {
        "head": "Frieda Born",
        "relation": "author_of",
        "tail": "Aligning machine and human visual representations across abstraction levels"
      },
      {
        "head": "Bernhard Spitzer",
        "relation": "author_of",
        "tail": "Aligning machine and human visual representations across abstraction levels"
      },
      {
        "head": "Simon Kornblith",
        "relation": "author_of",
        "tail": "Aligning machine and human visual representations across abstraction levels"
      },
      {
        "head": "M. C. Mozer",
        "relation": "author_of",
        "tail": "Aligning machine and human visual representations across abstraction levels"
      },
      {
        "head": "Klaus-Robert Muller",
        "relation": "author_of",
        "tail": "Aligning machine and human visual representations across abstraction levels"
      },
      {
        "head": "Thomas Unterthiner",
        "relation": "author_of",
        "tail": "Aligning machine and human visual representations across abstraction levels"
      },
      {
        "head": "Andrew Kyle Lampinen",
        "relation": "author_of",
        "tail": "Aligning machine and human visual representations across abstraction levels"
      },
      {
        "head": "Alexey Dosovitskiy",
        "relation": "author_of",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "Lucas Beyer",
        "relation": "author_of",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "Alexander Kolesnikov",
        "relation": "author_of",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "Dirk Weissenborn",
        "relation": "author_of",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "Xiaohua Zhai",
        "relation": "author_of",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "Thomas Unterthiner",
        "relation": "author_of",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "Mostafa Dehghani",
        "relation": "author_of",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "Matthias Minderer",
        "relation": "author_of",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "Georg Heigold",
        "relation": "author_of",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "Sylvain Gelly",
        "relation": "author_of",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "Jakob Uszkoreit",
        "relation": "author_of",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "Neil Houlsby",
        "relation": "author_of",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "Alec Radford",
        "relation": "author_of",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Jong Wook Kim",
        "relation": "author_of",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Chris Hallacy",
        "relation": "author_of",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Aditya Ramesh",
        "relation": "author_of",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Gabriel Goh",
        "relation": "author_of",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Sandhini Agarwal",
        "relation": "author_of",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Girish Sastry",
        "relation": "author_of",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Amanda Askell",
        "relation": "author_of",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Pamela Mishkin",
        "relation": "author_of",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Jack Clark",
        "relation": "author_of",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Gretchen Krueger",
        "relation": "author_of",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Ilya Sutskever",
        "relation": "author_of",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Individualized Treat",
        "relation": "author_of",
        "tail": "GENERATIVE ADVERSARIAL NETS"
      },
      {
        "head": "Jinsung Yoon",
        "relation": "author_of",
        "tail": "GENERATIVE ADVERSARIAL NETS"
      },
      {
        "head": "Zhengyang Geng",
        "relation": "author_of",
        "tail": "Improved Mean Flows: On the Challenges of Fastforward Generative Models"
      },
      {
        "head": "Yiyang Lu",
        "relation": "author_of",
        "tail": "Improved Mean Flows: On the Challenges of Fastforward Generative Models"
      },
      {
        "head": "Zongze Wu",
        "relation": "author_of",
        "tail": "Improved Mean Flows: On the Challenges of Fastforward Generative Models"
      },
      {
        "head": "Eli Shechtman",
        "relation": "author_of",
        "tail": "Improved Mean Flows: On the Challenges of Fastforward Generative Models"
      },
      {
        "head": "J. Zico Kolter",
        "relation": "author_of",
        "tail": "Improved Mean Flows: On the Challenges of Fastforward Generative Models"
      },
      {
        "head": "Kaiming He",
        "relation": "author_of",
        "tail": "Improved Mean Flows: On the Challenges of Fastforward Generative Models"
      },
      {
        "head": "Jiachen Lei",
        "relation": "author_of",
        "tail": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training"
      },
      {
        "head": "Keli Liu",
        "relation": "author_of",
        "tail": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training"
      },
      {
        "head": "Julius Berner",
        "relation": "author_of",
        "tail": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training"
      },
      {
        "head": "Haiming Yu",
        "relation": "author_of",
        "tail": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training"
      },
      {
        "head": "Hongkai Zheng",
        "relation": "author_of",
        "tail": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training"
      },
      {
        "head": "Jiahong Wu",
        "relation": "author_of",
        "tail": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training"
      },
      {
        "head": "Xiangxiang Chu",
        "relation": "author_of",
        "tail": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training"
      },
      {
        "head": "Minglei Shi",
        "relation": "author_of",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "Haolin Wang",
        "relation": "author_of",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "Borui Zhang",
        "relation": "author_of",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "Wenzhao Zheng",
        "relation": "author_of",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "Bohan Zeng",
        "relation": "author_of",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "Ziyang Yuan",
        "relation": "author_of",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "Xiaoshi Wu",
        "relation": "author_of",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "Yuanxing Zhang",
        "relation": "author_of",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "Huan Yang",
        "relation": "author_of",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "Xintao Wang",
        "relation": "author_of",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "Pengfei Wan",
        "relation": "author_of",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "Kun Gai",
        "relation": "author_of",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "Jie Zhou",
        "relation": "author_of",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "Jiwen Lu",
        "relation": "author_of",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "Yongsheng Yu",
        "relation": "author_of",
        "tail": "PixelDiT: Pixel Diffusion Transformers for Image Generation"
      },
      {
        "head": "Wei Xiong",
        "relation": "author_of",
        "tail": "PixelDiT: Pixel Diffusion Transformers for Image Generation"
      },
      {
        "head": "Weili Nie",
        "relation": "author_of",
        "tail": "PixelDiT: Pixel Diffusion Transformers for Image Generation"
      },
      {
        "head": "Yichen Sheng",
        "relation": "author_of",
        "tail": "PixelDiT: Pixel Diffusion Transformers for Image Generation"
      },
      {
        "head": "Shiqiu Liu",
        "relation": "author_of",
        "tail": "PixelDiT: Pixel Diffusion Transformers for Image Generation"
      },
      {
        "head": "Jiebo Luo",
        "relation": "author_of",
        "tail": "PixelDiT: Pixel Diffusion Transformers for Image Generation"
      },
      {
        "head": "Zhiheng Liu",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Weiming Ren",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Haozhe Liu",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Zijian Zhou",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Shoufa Chen",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Haonan Qiu",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Xiaoke Huang",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Zhaochong An",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Fanny Yang",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Aditya Patel",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Viktar Atliha",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Tony Ng",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Xiao Han",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Chuyan Zhu",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Chenyang Zhang",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Ding Liu",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Juan-Manuel Perez-Rua",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Sen He",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Jürgen Schmidhuber",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Wenhu Chen",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Ping Luo",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Wei Liu",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Tao Xiang",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Jonas Schult",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Yuren Cong",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Jacob Devlin",
        "relation": "author_of",
        "tail": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "Ming-Wei Chang",
        "relation": "author_of",
        "tail": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "Kenton Lee",
        "relation": "author_of",
        "tail": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "Kristina Toutanova",
        "relation": "author_of",
        "tail": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "Vrushali Pagire",
        "relation": "author_of",
        "tail": "A comprehensive review of object detection with traditional and deep learning methods"
      },
      {
        "head": "M. Chavali",
        "relation": "author_of",
        "tail": "A comprehensive review of object detection with traditional and deep learning methods"
      },
      {
        "head": "Ashish Kale",
        "relation": "author_of",
        "tail": "A comprehensive review of object detection with traditional and deep learning methods"
      },
      {
        "head": "Jinjie Ni",
        "relation": "author_of",
        "tail": "Diffusion Language Models are Super Data Learners"
      },
      {
        "head": "Qian Liu",
        "relation": "author_of",
        "tail": "Diffusion Language Models are Super Data Learners"
      },
      {
        "head": "Longxu Dou",
        "relation": "author_of",
        "tail": "Diffusion Language Models are Super Data Learners"
      },
      {
        "head": "Chao Du",
        "relation": "author_of",
        "tail": "Diffusion Language Models are Super Data Learners"
      },
      {
        "head": "Zili Wang",
        "relation": "author_of",
        "tail": "Diffusion Language Models are Super Data Learners"
      },
      {
        "head": "Hang Yan",
        "relation": "author_of",
        "tail": "Diffusion Language Models are Super Data Learners"
      },
      {
        "head": "Tianyu Pang",
        "relation": "author_of",
        "tail": "Diffusion Language Models are Super Data Learners"
      },
      {
        "head": "Michael Qizhe Shieh",
        "relation": "author_of",
        "tail": "Diffusion Language Models are Super Data Learners"
      },
      {
        "head": "Zirui Wu",
        "relation": "author_of",
        "tail": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas"
      },
      {
        "head": "Lin Zheng",
        "relation": "author_of",
        "tail": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas"
      },
      {
        "head": "Zhihui Xie",
        "relation": "author_of",
        "tail": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas"
      },
      {
        "head": "Jiacheng Ye",
        "relation": "author_of",
        "tail": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas"
      },
      {
        "head": "Jiahui Gao",
        "relation": "author_of",
        "tail": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas"
      },
      {
        "head": "Shansan Gong",
        "relation": "author_of",
        "tail": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas"
      },
      {
        "head": "Yansong Feng",
        "relation": "author_of",
        "tail": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas"
      },
      {
        "head": "Zhenguo Li",
        "relation": "author_of",
        "tail": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas"
      },
      {
        "head": "Wei Bi",
        "relation": "author_of",
        "tail": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas"
      },
      {
        "head": "Guorui Zhou",
        "relation": "author_of",
        "tail": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas"
      },
      {
        "head": "Lingpeng Kong",
        "relation": "author_of",
        "tail": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas"
      },
      {
        "head": "Zhicheng Cai",
        "relation": "author_of",
        "tail": "FLEX: Continuous Agent Evolution via Forward Learning from Experience"
      },
      {
        "head": "Xinyuan Guo",
        "relation": "author_of",
        "tail": "FLEX: Continuous Agent Evolution via Forward Learning from Experience"
      },
      {
        "head": "Yu Pei",
        "relation": "author_of",
        "tail": "FLEX: Continuous Agent Evolution via Forward Learning from Experience"
      },
      {
        "head": "Jiangtao Feng",
        "relation": "author_of",
        "tail": "FLEX: Continuous Agent Evolution via Forward Learning from Experience"
      },
      {
        "head": "Jinsong Su",
        "relation": "author_of",
        "tail": "FLEX: Continuous Agent Evolution via Forward Learning from Experience"
      },
      {
        "head": "Jiangjie Chen",
        "relation": "author_of",
        "tail": "FLEX: Continuous Agent Evolution via Forward Learning from Experience"
      },
      {
        "head": "Ya-Qin Zhang",
        "relation": "author_of",
        "tail": "FLEX: Continuous Agent Evolution via Forward Learning from Experience"
      },
      {
        "head": "Wei-Ying Ma",
        "relation": "author_of",
        "tail": "FLEX: Continuous Agent Evolution via Forward Learning from Experience"
      },
      {
        "head": "Mingxuan Wang",
        "relation": "author_of",
        "tail": "FLEX: Continuous Agent Evolution via Forward Learning from Experience"
      },
      {
        "head": "Hao Zhou",
        "relation": "author_of",
        "tail": "FLEX: Continuous Agent Evolution via Forward Learning from Experience"
      },
      {
        "head": "Mingyue Cheng",
        "relation": "author_of",
        "tail": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning"
      },
      {
        "head": "Jie Ouyang",
        "relation": "author_of",
        "tail": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning"
      },
      {
        "head": "Shuo Yu",
        "relation": "author_of",
        "tail": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning"
      },
      {
        "head": "Ruiran Yan",
        "relation": "author_of",
        "tail": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning"
      },
      {
        "head": "Yucong Luo",
        "relation": "author_of",
        "tail": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning"
      },
      {
        "head": "Zirui Liu",
        "relation": "author_of",
        "tail": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning"
      },
      {
        "head": "Daoyu Wang",
        "relation": "author_of",
        "tail": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning"
      },
      {
        "head": "Qi Liu",
        "relation": "author_of",
        "tail": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning"
      },
      {
        "head": "Enhong Chen",
        "relation": "author_of",
        "tail": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning"
      },
      {
        "head": "Holger Caesar",
        "relation": "author_of",
        "tail": "nuScenes: A multimodal dataset for autonomous driving"
      },
      {
        "head": "Varun Bankiti",
        "relation": "author_of",
        "tail": "nuScenes: A multimodal dataset for autonomous driving"
      },
      {
        "head": "Alex H. Lang",
        "relation": "author_of",
        "tail": "nuScenes: A multimodal dataset for autonomous driving"
      },
      {
        "head": "Sourabh Vora",
        "relation": "author_of",
        "tail": "nuScenes: A multimodal dataset for autonomous driving"
      },
      {
        "head": "Venice Erin Liong",
        "relation": "author_of",
        "tail": "nuScenes: A multimodal dataset for autonomous driving"
      },
      {
        "head": "Qiang Xu",
        "relation": "author_of",
        "tail": "nuScenes: A multimodal dataset for autonomous driving"
      },
      {
        "head": "Anush Krishnan",
        "relation": "author_of",
        "tail": "nuScenes: A multimodal dataset for autonomous driving"
      },
      {
        "head": "Yu Pan",
        "relation": "author_of",
        "tail": "nuScenes: A multimodal dataset for autonomous driving"
      },
      {
        "head": "Giancarlo Baldan",
        "relation": "author_of",
        "tail": "nuScenes: A multimodal dataset for autonomous driving"
      },
      {
        "head": "Oscar Beijbom",
        "relation": "author_of",
        "tail": "nuScenes: A multimodal dataset for autonomous driving"
      },
      {
        "head": "Jie Hu",
        "relation": "author_of",
        "tail": "Squeeze-and-Excitation Networks"
      },
      {
        "head": "Li Shen",
        "relation": "author_of",
        "tail": "Squeeze-and-Excitation Networks"
      },
      {
        "head": "Samuel Albanie",
        "relation": "author_of",
        "tail": "Squeeze-and-Excitation Networks"
      },
      {
        "head": "Gang Sun",
        "relation": "author_of",
        "tail": "Squeeze-and-Excitation Networks"
      },
      {
        "head": "Enhua Wu",
        "relation": "author_of",
        "tail": "Squeeze-and-Excitation Networks"
      },
      {
        "head": "I. Loshchilov",
        "relation": "author_of",
        "tail": "Decoupled Weight Decay Regularization"
      },
      {
        "head": "F. Hutter",
        "relation": "author_of",
        "tail": "Decoupled Weight Decay Regularization"
      },
      {
        "head": "Mingxing Tan",
        "relation": "author_of",
        "tail": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"
      },
      {
        "head": "Quoc V. Le",
        "relation": "author_of",
        "tail": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"
      },
      {
        "head": "Tianqi Liu",
        "relation": "author_of",
        "tail": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control"
      },
      {
        "head": "Zhaoxi Chen",
        "relation": "author_of",
        "tail": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control"
      },
      {
        "head": "Zihao Huang",
        "relation": "author_of",
        "tail": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control"
      },
      {
        "head": "Shaocong Xu",
        "relation": "author_of",
        "tail": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control"
      },
      {
        "head": "Saining Zhang",
        "relation": "author_of",
        "tail": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control"
      },
      {
        "head": "Chongjie Ye",
        "relation": "author_of",
        "tail": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control"
      },
      {
        "head": "Bohan Li",
        "relation": "author_of",
        "tail": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control"
      },
      {
        "head": "Zhiguo Cao",
        "relation": "author_of",
        "tail": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control"
      },
      {
        "head": "Wei Li",
        "relation": "author_of",
        "tail": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control"
      },
      {
        "head": "Hao Zhao",
        "relation": "author_of",
        "tail": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control"
      },
      {
        "head": "Ziwei Liu",
        "relation": "author_of",
        "tail": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control"
      },
      {
        "head": "Tianze Xia",
        "relation": "author_of",
        "tail": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World"
      },
      {
        "head": "Yongkang Li",
        "relation": "author_of",
        "tail": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World"
      },
      {
        "head": "Lijun Zhou",
        "relation": "author_of",
        "tail": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World"
      },
      {
        "head": "Jingfeng Yao",
        "relation": "author_of",
        "tail": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World"
      },
      {
        "head": "Kaixin Xiong",
        "relation": "author_of",
        "tail": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World"
      },
      {
        "head": "Haiyang Sun",
        "relation": "author_of",
        "tail": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World"
      },
      {
        "head": "Bing Wang",
        "relation": "author_of",
        "tail": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World"
      },
      {
        "head": "Kun Ma",
        "relation": "author_of",
        "tail": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World"
      },
      {
        "head": "Guang Chen",
        "relation": "author_of",
        "tail": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World"
      },
      {
        "head": "Hangjun Ye",
        "relation": "author_of",
        "tail": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World"
      },
      {
        "head": "Wenyu Liu",
        "relation": "author_of",
        "tail": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World"
      },
      {
        "head": "Xinggang Wang",
        "relation": "author_of",
        "tail": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World"
      },
      {
        "head": "Sicheng Zuo",
        "relation": "author_of",
        "tail": "DVGT: Driving Visual Geometry Transformer"
      },
      {
        "head": "Zixun Xie",
        "relation": "author_of",
        "tail": "DVGT: Driving Visual Geometry Transformer"
      },
      {
        "head": "Wenzhao Zheng",
        "relation": "author_of",
        "tail": "DVGT: Driving Visual Geometry Transformer"
      },
      {
        "head": "Shaoqing Xu",
        "relation": "author_of",
        "tail": "DVGT: Driving Visual Geometry Transformer"
      },
      {
        "head": "Fang Li",
        "relation": "author_of",
        "tail": "DVGT: Driving Visual Geometry Transformer"
      },
      {
        "head": "Shengyin Jiang",
        "relation": "author_of",
        "tail": "DVGT: Driving Visual Geometry Transformer"
      },
      {
        "head": "Long Chen",
        "relation": "author_of",
        "tail": "DVGT: Driving Visual Geometry Transformer"
      },
      {
        "head": "Zhi-Xin Yang",
        "relation": "author_of",
        "tail": "DVGT: Driving Visual Geometry Transformer"
      },
      {
        "head": "Jiwen Lu",
        "relation": "author_of",
        "tail": "DVGT: Driving Visual Geometry Transformer"
      },
      {
        "head": "Lvmin Zhang",
        "relation": "author_of",
        "tail": "Adding Conditional Control to Text-to-Image Diffusion Models"
      },
      {
        "head": "Anyi Rao",
        "relation": "author_of",
        "tail": "Adding Conditional Control to Text-to-Image Diffusion Models"
      },
      {
        "head": "Maneesh Agrawala",
        "relation": "author_of",
        "tail": "Adding Conditional Control to Text-to-Image Diffusion Models"
      },
      {
        "head": "Hyung Won Chung",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Le Hou",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Shayne Longpre",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Barret Zoph",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Yi Tay",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "William Fedus",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Yunxuan Li",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Xuezhi Wang",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Mostafa Dehghani",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Siddhartha Brahma",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Albert Webson",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Shixiang Shane Gu",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Zhuyun Dai",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Mirac Suzgun",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Xinyun Chen",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Aakanksha Chowdhery",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Alex Castro-Ros",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Marie Pellat",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Kevin Robinson",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Dasha Valter",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Sharan Narang",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Gaurav Mishra",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Adams Yu",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Vincent Zhao",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Yanping Huang",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Andrew Dai",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Hongkun Yu",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Slav Petrov",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Ed H. Chi",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Jeff Dean",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Jacob Devlin",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Adam Roberts",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Denny Zhou",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Quoc V. Le",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Jason Wei",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Robin Rombach",
        "relation": "author_of",
        "tail": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "head": "Andreas Blattmann",
        "relation": "author_of",
        "tail": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "head": "Dominik Lorenz",
        "relation": "author_of",
        "tail": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "head": "Patrick Esser",
        "relation": "author_of",
        "tail": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "head": "Björn Ommer",
        "relation": "author_of",
        "tail": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "head": "Romain Lopez",
        "relation": "author_of",
        "tail": "AUTO-ENCODING VARIATIONAL BAYES"
      },
      {
        "head": "Pierre Boyeau",
        "relation": "author_of",
        "tail": "AUTO-ENCODING VARIATIONAL BAYES"
      },
      {
        "head": "N. Yosef",
        "relation": "author_of",
        "tail": "AUTO-ENCODING VARIATIONAL BAYES"
      },
      {
        "head": "Michael I. Jordan",
        "relation": "author_of",
        "tail": "AUTO-ENCODING VARIATIONAL BAYES"
      },
      {
        "head": "J. Regier",
        "relation": "author_of",
        "tail": "AUTO-ENCODING VARIATIONAL BAYES"
      },
      {
        "head": "Richard Zhang",
        "relation": "author_of",
        "tail": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric"
      },
      {
        "head": "Phillip Isola",
        "relation": "author_of",
        "tail": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric"
      },
      {
        "head": "Alexei A. Efros",
        "relation": "author_of",
        "tail": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric"
      },
      {
        "head": "Eli Shechtman",
        "relation": "author_of",
        "tail": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric"
      },
      {
        "head": "Oliver Wang",
        "relation": "author_of",
        "tail": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric"
      },
      {
        "head": "Pei Sun",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Henrik Kretzschmar",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Xerxes Dotiwalla",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Aurelien Chouard",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Vijaysai Patnaik",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Paul Tsui",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "James Guo",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Yin Zhou",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Yuning Chai",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Benjamin Caine",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Vijay Vasudevan",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Wei Han",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Jiquan Ngiam",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Hang Zhao",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Aleksei Timofeev",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Scott Ettinger",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Maxim Krivokon",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Amy Gao",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Aditya Joshi",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Sheng Zhao",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Shuyang Cheng",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Yu Zhang",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Jonathon Shlens",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Zhifeng Chen",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Dragomir Anguelov",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Navneet Dalal",
        "relation": "author_of",
        "tail": "Histograms of oriented gradients for human detection"
      },
      {
        "head": "B. Triggs",
        "relation": "author_of",
        "tail": "Histograms of oriented gradients for human detection"
      },
      {
        "head": "Gilad Cohen",
        "relation": "author_of",
        "tail": "Generative Adversarial Networks"
      },
      {
        "head": "Raja Giryes",
        "relation": "author_of",
        "tail": "Generative Adversarial Networks"
      },
      {
        "head": "Zekai Zhang",
        "relation": "author_of",
        "tail": "Generalization of Diffusion Models Arises with a Balanced Representation Space"
      },
      {
        "head": "Xiao Li",
        "relation": "author_of",
        "tail": "Generalization of Diffusion Models Arises with a Balanced Representation Space"
      },
      {
        "head": "Xiang Li",
        "relation": "author_of",
        "tail": "Generalization of Diffusion Models Arises with a Balanced Representation Space"
      },
      {
        "head": "Lianghe Shi",
        "relation": "author_of",
        "tail": "Generalization of Diffusion Models Arises with a Balanced Representation Space"
      },
      {
        "head": "Meng Wu",
        "relation": "author_of",
        "tail": "Generalization of Diffusion Models Arises with a Balanced Representation Space"
      },
      {
        "head": "Molei Tao",
        "relation": "author_of",
        "tail": "Generalization of Diffusion Models Arises with a Balanced Representation Space"
      },
      {
        "head": "Qing Qu",
        "relation": "author_of",
        "tail": "Generalization of Diffusion Models Arises with a Balanced Representation Space"
      },
      {
        "head": "Donglin Yang",
        "relation": "author_of",
        "tail": "Stable Velocity: A Variance Perspective on Flow Matching"
      },
      {
        "head": "Yongxing Zhang",
        "relation": "author_of",
        "tail": "Stable Velocity: A Variance Perspective on Flow Matching"
      },
      {
        "head": "Xin Yu",
        "relation": "author_of",
        "tail": "Stable Velocity: A Variance Perspective on Flow Matching"
      },
      {
        "head": "Liang Hou",
        "relation": "author_of",
        "tail": "Stable Velocity: A Variance Perspective on Flow Matching"
      },
      {
        "head": "Xin Tao",
        "relation": "author_of",
        "tail": "Stable Velocity: A Variance Perspective on Flow Matching"
      },
      {
        "head": "Pengfei Wan",
        "relation": "author_of",
        "tail": "Stable Velocity: A Variance Perspective on Flow Matching"
      },
      {
        "head": "Xiaojuan Qi",
        "relation": "author_of",
        "tail": "Stable Velocity: A Variance Perspective on Flow Matching"
      },
      {
        "head": "Renjie Liao",
        "relation": "author_of",
        "tail": "Stable Velocity: A Variance Perspective on Flow Matching"
      },
      {
        "head": "Ramón Calvo-González",
        "relation": "author_of",
        "tail": "Laminating Representation Autoencoders for Efficient Diffusion"
      },
      {
        "head": "François Fleuret",
        "relation": "author_of",
        "tail": "Laminating Representation Autoencoders for Efficient Diffusion"
      },
      {
        "head": "Yao Teng",
        "relation": "author_of",
        "tail": "Adaptive 1D Video Diffusion Autoencoder"
      },
      {
        "head": "Minxuan Lin",
        "relation": "author_of",
        "tail": "Adaptive 1D Video Diffusion Autoencoder"
      },
      {
        "head": "Xian Liu",
        "relation": "author_of",
        "tail": "Adaptive 1D Video Diffusion Autoencoder"
      },
      {
        "head": "Shuai Wang",
        "relation": "author_of",
        "tail": "Adaptive 1D Video Diffusion Autoencoder"
      },
      {
        "head": "Xiao Yang",
        "relation": "author_of",
        "tail": "Adaptive 1D Video Diffusion Autoencoder"
      },
      {
        "head": "Xihui Liu",
        "relation": "author_of",
        "tail": "Adaptive 1D Video Diffusion Autoencoder"
      },
      {
        "head": "Nicolas Sereyjol-Garros",
        "relation": "author_of",
        "tail": "Test-Time Conditioning with Representation-Aligned Visual Features"
      },
      {
        "head": "Ellington Kirby",
        "relation": "author_of",
        "tail": "Test-Time Conditioning with Representation-Aligned Visual Features"
      },
      {
        "head": "Victor Letzelter",
        "relation": "author_of",
        "tail": "Test-Time Conditioning with Representation-Aligned Visual Features"
      },
      {
        "head": "Victor Besnier",
        "relation": "author_of",
        "tail": "Test-Time Conditioning with Representation-Aligned Visual Features"
      },
      {
        "head": "Nermin Samet",
        "relation": "author_of",
        "tail": "Test-Time Conditioning with Representation-Aligned Visual Features"
      },
      {
        "head": "Ana Davila",
        "relation": "author_of",
        "tail": "Bio-inspired fine-tuning for selective transfer learning in image classification"
      },
      {
        "head": "Jacinto Colan",
        "relation": "author_of",
        "tail": "Bio-inspired fine-tuning for selective transfer learning in image classification"
      },
      {
        "head": "Yasuhisa Hasegawa",
        "relation": "author_of",
        "tail": "Bio-inspired fine-tuning for selective transfer learning in image classification"
      },
      {
        "head": "Subham Sharma",
        "relation": "author_of",
        "tail": "VGG Induced Deep Hand Sign Language Detection"
      },
      {
        "head": "Sharmila Subudhi",
        "relation": "author_of",
        "tail": "VGG Induced Deep Hand Sign Language Detection"
      },
      {
        "head": "Camillo Lugaresi",
        "relation": "author_of",
        "tail": "MediaPipe: A Framework for Building Perception Pipelines"
      },
      {
        "head": "Jiuqiang Tang",
        "relation": "author_of",
        "tail": "MediaPipe: A Framework for Building Perception Pipelines"
      },
      {
        "head": "Hadon Nash",
        "relation": "author_of",
        "tail": "MediaPipe: A Framework for Building Perception Pipelines"
      },
      {
        "head": "Chris McClanahan",
        "relation": "author_of",
        "tail": "MediaPipe: A Framework for Building Perception Pipelines"
      },
      {
        "head": "Esha Uboweja",
        "relation": "author_of",
        "tail": "MediaPipe: A Framework for Building Perception Pipelines"
      },
      {
        "head": "Michael Hays",
        "relation": "author_of",
        "tail": "MediaPipe: A Framework for Building Perception Pipelines"
      },
      {
        "head": "Fan Zhang",
        "relation": "author_of",
        "tail": "MediaPipe: A Framework for Building Perception Pipelines"
      },
      {
        "head": "Chuo-Ling Chang",
        "relation": "author_of",
        "tail": "MediaPipe: A Framework for Building Perception Pipelines"
      },
      {
        "head": "Ming Guang Yong",
        "relation": "author_of",
        "tail": "MediaPipe: A Framework for Building Perception Pipelines"
      },
      {
        "head": "Juhyun Lee",
        "relation": "author_of",
        "tail": "MediaPipe: A Framework for Building Perception Pipelines"
      },
      {
        "head": "Wan-Teh Chang",
        "relation": "author_of",
        "tail": "MediaPipe: A Framework for Building Perception Pipelines"
      },
      {
        "head": "Wei Hua",
        "relation": "author_of",
        "tail": "MediaPipe: A Framework for Building Perception Pipelines"
      },
      {
        "head": "Manfred Georg",
        "relation": "author_of",
        "tail": "MediaPipe: A Framework for Building Perception Pipelines"
      },
      {
        "head": "Matthias Grundmann",
        "relation": "author_of",
        "tail": "MediaPipe: A Framework for Building Perception Pipelines"
      },
      {
        "head": "Cem Keskin",
        "relation": "author_of",
        "tail": "Hand Pose Estimation and Hand Shape Classification Using Multi-layered Randomized Decision Forests"
      },
      {
        "head": "Mustafa Furkan Kıraç",
        "relation": "author_of",
        "tail": "Hand Pose Estimation and Hand Shape Classification Using Multi-layered Randomized Decision Forests"
      },
      {
        "head": "Yunus Emre Kara",
        "relation": "author_of",
        "tail": "Hand Pose Estimation and Hand Shape Classification Using Multi-layered Randomized Decision Forests"
      },
      {
        "head": "L. Akarun",
        "relation": "author_of",
        "tail": "Hand Pose Estimation and Hand Shape Classification Using Multi-layered Randomized Decision Forests"
      },
      {
        "head": "S. P. Priyal",
        "relation": "author_of",
        "tail": "A robust static hand gesture recognition system using geometry based normalizations and Krawtchouk moments"
      },
      {
        "head": "P. Bora",
        "relation": "author_of",
        "tail": "A robust static hand gesture recognition system using geometry based normalizations and Krawtchouk moments"
      },
      {
        "head": "Octavian Dudas",
        "relation": "author_of",
        "tail": "Hand signal classification system for sign language communication in Virtual Reality"
      },
      {
        "head": "C. Nandra",
        "relation": "author_of",
        "tail": "Hand signal classification system for sign language communication in Virtual Reality"
      },
      {
        "head": "C. Mocan",
        "relation": "author_of",
        "tail": "Hand signal classification system for sign language communication in Virtual Reality"
      },
      {
        "head": "D. Gorgan",
        "relation": "author_of",
        "tail": "Hand signal classification system for sign language communication in Virtual Reality"
      },
      {
        "head": "Avinash Dhiran",
        "relation": "author_of",
        "tail": "Real-Time Static Hand Sign Recognition System using MediaPipe and Fully Connected Neural Network"
      },
      {
        "head": "Anurag Kumbhare",
        "relation": "author_of",
        "tail": "Real-Time Static Hand Sign Recognition System using MediaPipe and Fully Connected Neural Network"
      },
      {
        "head": "Achal Patil",
        "relation": "author_of",
        "tail": "Real-Time Static Hand Sign Recognition System using MediaPipe and Fully Connected Neural Network"
      },
      {
        "head": "Mrugank Vichare",
        "relation": "author_of",
        "tail": "Real-Time Static Hand Sign Recognition System using MediaPipe and Fully Connected Neural Network"
      },
      {
        "head": "Dhananjay Patel",
        "relation": "author_of",
        "tail": "Real-Time Static Hand Sign Recognition System using MediaPipe and Fully Connected Neural Network"
      },
      {
        "head": "Saransh Mishra",
        "relation": "author_of",
        "tail": "Real-Time Gesture Recognition to Aid Communication in Children with Motor Impairments"
      },
      {
        "head": "Pavan Nair",
        "relation": "author_of",
        "tail": "Real-Time Gesture Recognition to Aid Communication in Children with Motor Impairments"
      },
      {
        "head": "Pushpalatha M",
        "relation": "author_of",
        "tail": "Real-Time Gesture Recognition to Aid Communication in Children with Motor Impairments"
      },
      {
        "head": "Poornima S",
        "relation": "author_of",
        "tail": "Real-Time Gesture Recognition to Aid Communication in Children with Motor Impairments"
      },
      {
        "head": "A. Dempster",
        "relation": "author_of",
        "tail": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
      },
      {
        "head": "N. Laird",
        "relation": "author_of",
        "tail": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
      },
      {
        "head": "D. Rubin",
        "relation": "author_of",
        "tail": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
      },
      {
        "head": "L. Maaten",
        "relation": "author_of",
        "tail": "Visualizing Data using t-SNE"
      },
      {
        "head": "Geoffrey E. Hinton",
        "relation": "author_of",
        "tail": "Visualizing Data using t-SNE"
      },
      {
        "head": "A. Krizhevsky",
        "relation": "author_of",
        "tail": "Learning Multiple Layers of Features from Tiny Images"
      },
      {
        "head": "Jacy Reese Anthis",
        "relation": "author_of",
        "tail": "LLM Social Simulations Are a Promising Research Method"
      },
      {
        "head": "Ryan Liu",
        "relation": "author_of",
        "tail": "LLM Social Simulations Are a Promising Research Method"
      },
      {
        "head": "Sean M. Richardson",
        "relation": "author_of",
        "tail": "LLM Social Simulations Are a Promising Research Method"
      },
      {
        "head": "Austin C. Kozlowski",
        "relation": "author_of",
        "tail": "LLM Social Simulations Are a Promising Research Method"
      },
      {
        "head": "Bernard Koch",
        "relation": "author_of",
        "tail": "LLM Social Simulations Are a Promising Research Method"
      },
      {
        "head": "James Evans",
        "relation": "author_of",
        "tail": "LLM Social Simulations Are a Promising Research Method"
      },
      {
        "head": "Erik Brynjolfsson",
        "relation": "author_of",
        "tail": "LLM Social Simulations Are a Promising Research Method"
      },
      {
        "head": "Michael Bernstein",
        "relation": "author_of",
        "tail": "LLM Social Simulations Are a Promising Research Method"
      },
      {
        "head": "Zhiwen Xiao",
        "relation": "author_of",
        "tail": "Federated Contrastive Learning With Feature-Based Distillation for Human Activity Recognition"
      },
      {
        "head": "Huagang Tong",
        "relation": "author_of",
        "tail": "Federated Contrastive Learning With Feature-Based Distillation for Human Activity Recognition"
      },
      {
        "head": "Runqian Wang",
        "relation": "author_of",
        "tail": "Diffuse and Disperse: Image Generation with Representation Regularization"
      },
      {
        "head": "Kaiming He",
        "relation": "author_of",
        "tail": "Diffuse and Disperse: Image Generation with Representation Regularization"
      },
      {
        "head": "Ibomoiye Domor Mienye",
        "relation": "author_of",
        "tail": "Deep Autoencoder Neural Networks: A Comprehensive Review and New Perspectives"
      },
      {
        "head": "Theo G. Swart",
        "relation": "author_of",
        "tail": "Deep Autoencoder Neural Networks: A Comprehensive Review and New Perspectives"
      },
      {
        "head": "Jusheng Zhang",
        "relation": "author_of",
        "tail": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems"
      },
      {
        "head": "Zimeng Huang",
        "relation": "author_of",
        "tail": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems"
      },
      {
        "head": "Yijia Fan",
        "relation": "author_of",
        "tail": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems"
      },
      {
        "head": "Ningyuan Liu",
        "relation": "author_of",
        "tail": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems"
      },
      {
        "head": "Mingyan Li",
        "relation": "author_of",
        "tail": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems"
      },
      {
        "head": "Zhuojie Yang",
        "relation": "author_of",
        "tail": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems"
      },
      {
        "head": "Jiawei Yao",
        "relation": "author_of",
        "tail": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems"
      },
      {
        "head": "Jian Wang",
        "relation": "author_of",
        "tail": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems"
      },
      {
        "head": "Keze Wang",
        "relation": "author_of",
        "tail": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems"
      },
      {
        "head": "Olaf Ronneberger",
        "relation": "author_of",
        "tail": "U-Net: Convolutional Networks for Biomedical Image Segmentation"
      },
      {
        "head": "Philipp Fischer",
        "relation": "author_of",
        "tail": "U-Net: Convolutional Networks for Biomedical Image Segmentation"
      },
      {
        "head": "Thomas Brox",
        "relation": "author_of",
        "tail": "U-Net: Convolutional Networks for Biomedical Image Segmentation"
      },
      {
        "head": "Tianwei Yin",
        "relation": "author_of",
        "tail": "Improved Distribution Matching Distillation for Fast Image Synthesis"
      },
      {
        "head": "Michaël Gharbi",
        "relation": "author_of",
        "tail": "Improved Distribution Matching Distillation for Fast Image Synthesis"
      },
      {
        "head": "Taesung Park",
        "relation": "author_of",
        "tail": "Improved Distribution Matching Distillation for Fast Image Synthesis"
      },
      {
        "head": "Richard Zhang",
        "relation": "author_of",
        "tail": "Improved Distribution Matching Distillation for Fast Image Synthesis"
      },
      {
        "head": "Eli Shechtman",
        "relation": "author_of",
        "tail": "Improved Distribution Matching Distillation for Fast Image Synthesis"
      },
      {
        "head": "Fredo Durand",
        "relation": "author_of",
        "tail": "Improved Distribution Matching Distillation for Fast Image Synthesis"
      },
      {
        "head": "William T. Freeman",
        "relation": "author_of",
        "tail": "Improved Distribution Matching Distillation for Fast Image Synthesis"
      },
      {
        "head": "Axel Sauer",
        "relation": "author_of",
        "tail": "Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation"
      },
      {
        "head": "Frederic Boesel",
        "relation": "author_of",
        "tail": "Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation"
      },
      {
        "head": "Tim Dockhorn",
        "relation": "author_of",
        "tail": "Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation"
      },
      {
        "head": "Andreas Blattmann",
        "relation": "author_of",
        "tail": "Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation"
      },
      {
        "head": "Patrick Esser",
        "relation": "author_of",
        "tail": "Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation"
      },
      {
        "head": "Robin Rombach",
        "relation": "author_of",
        "tail": "Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation"
      },
      {
        "head": "Takuya Akiba",
        "relation": "author_of",
        "tail": "Evolutionary optimization of model merging recipes"
      },
      {
        "head": "Makoto Shing",
        "relation": "author_of",
        "tail": "Evolutionary optimization of model merging recipes"
      },
      {
        "head": "Yujin Tang",
        "relation": "author_of",
        "tail": "Evolutionary optimization of model merging recipes"
      },
      {
        "head": "Qi Sun",
        "relation": "author_of",
        "tail": "Evolutionary optimization of model merging recipes"
      },
      {
        "head": "David Ha",
        "relation": "author_of",
        "tail": "Evolutionary optimization of model merging recipes"
      },
      {
        "head": "Tianwei Yin",
        "relation": "author_of",
        "tail": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
      },
      {
        "head": "Qiang Zhang",
        "relation": "author_of",
        "tail": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
      },
      {
        "head": "Richard Zhang",
        "relation": "author_of",
        "tail": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
      },
      {
        "head": "William T. Freeman",
        "relation": "author_of",
        "tail": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
      },
      {
        "head": "Fredo Durand",
        "relation": "author_of",
        "tail": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
      },
      {
        "head": "Eli Shechtman",
        "relation": "author_of",
        "tail": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
      },
      {
        "head": "Xun Huang",
        "relation": "author_of",
        "tail": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
      },
      {
        "head": "Zinan Guo",
        "relation": "author_of",
        "tail": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment"
      },
      {
        "head": "Yanze Wu",
        "relation": "author_of",
        "tail": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment"
      },
      {
        "head": "Zhuowei Chen",
        "relation": "author_of",
        "tail": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment"
      },
      {
        "head": "Lang Chen",
        "relation": "author_of",
        "tail": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment"
      },
      {
        "head": "Peng Zhang",
        "relation": "author_of",
        "tail": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment"
      },
      {
        "head": "Qian He",
        "relation": "author_of",
        "tail": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment"
      },
      {
        "head": "Jonathan Ho",
        "relation": "author_of",
        "tail": "Denoising Diffusion Probabilistic Models"
      },
      {
        "head": "Ajay Jain",
        "relation": "author_of",
        "tail": "Denoising Diffusion Probabilistic Models"
      },
      {
        "head": "Pieter Abbeel",
        "relation": "author_of",
        "tail": "Denoising Diffusion Probabilistic Models"
      },
      {
        "head": "Weijie Kong",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Qi Tian",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Zijian Zhang",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Rox Min",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Zuozhuo Dai",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Jin Zhou",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Jiangfeng Xiong",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Xin Li",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Bo Wu",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Jianwei Zhang",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Kathrina Wu",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Qin Lin",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Junkun Yuan",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Yanxin Long",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Aladdin Wang",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Andong Wang",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Changlin Li",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Duojun Huang",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Fang Yang",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Hao Tan",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Hongmei Wang",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Jacob Song",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Jiawang Bai",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Jianbing Wu",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Jinbao Xue",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Joey Wang",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Kai Wang",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Mengyang Liu",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Pengyu Li",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Shuai Li",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Weiyan Wang",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Wenqing Yu",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Xinchi Deng",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Yang Li",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Yi Chen",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Yutao Cui",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Yuanbo Peng",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Zhentao Yu",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Zhiyu He",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Zhiyong Xu",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Zixiang Zhou",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Zunnan Xu",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Yangyu Tao",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Qinglin Lu",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Songtao Liu",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Dax Zhou",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Hongfa Wang",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Yong Yang",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Di Wang",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Yuhong Liu",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Jie Jiang",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Caesar Zhong",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Jianwen Jiang",
        "relation": "author_of",
        "tail": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency"
      },
      {
        "head": "Chao Liang",
        "relation": "author_of",
        "tail": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency"
      },
      {
        "head": "Jiaqi Yang",
        "relation": "author_of",
        "tail": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency"
      },
      {
        "head": "Gaojie Lin",
        "relation": "author_of",
        "tail": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency"
      },
      {
        "head": "Tianyun Zhong",
        "relation": "author_of",
        "tail": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency"
      },
      {
        "head": "Yanbo Zheng",
        "relation": "author_of",
        "tail": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency"
      },
      {
        "head": "Gaojie Lin",
        "relation": "author_of",
        "tail": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models"
      },
      {
        "head": "Jianwen Jiang",
        "relation": "author_of",
        "tail": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models"
      },
      {
        "head": "Jiaqi Yang",
        "relation": "author_of",
        "tail": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models"
      },
      {
        "head": "Zerong Zheng",
        "relation": "author_of",
        "tail": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models"
      },
      {
        "head": "Chao Liang",
        "relation": "author_of",
        "tail": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models"
      },
      {
        "head": "Jiahao Cui",
        "relation": "author_of",
        "tail": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation"
      },
      {
        "head": "Hui Li",
        "relation": "author_of",
        "tail": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation"
      },
      {
        "head": "Yao Yao",
        "relation": "author_of",
        "tail": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation"
      },
      {
        "head": "Hao Zhu",
        "relation": "author_of",
        "tail": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation"
      },
      {
        "head": "Hanlin Shang",
        "relation": "author_of",
        "tail": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation"
      },
      {
        "head": "Kaihui Cheng",
        "relation": "author_of",
        "tail": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation"
      },
      {
        "head": "Hang Zhou",
        "relation": "author_of",
        "tail": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation"
      },
      {
        "head": "Siyu Zhu",
        "relation": "author_of",
        "tail": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation"
      },
      {
        "head": "Jingdong Wang",
        "relation": "author_of",
        "tail": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation"
      },
      {
        "head": "Rang Meng",
        "relation": "author_of",
        "tail": "EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation"
      },
      {
        "head": "Xingyu Zhang",
        "relation": "author_of",
        "tail": "EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation"
      },
      {
        "head": "Yuming Li",
        "relation": "author_of",
        "tail": "EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation"
      },
      {
        "head": "Chenguang Ma",
        "relation": "author_of",
        "tail": "EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation"
      },
      {
        "head": "Tsung-Yi Lin",
        "relation": "author_of",
        "tail": "Feature Pyramid Networks for Object Detection"
      },
      {
        "head": "Piotr Dollár",
        "relation": "author_of",
        "tail": "Feature Pyramid Networks for Object Detection"
      },
      {
        "head": "Ross Girshick",
        "relation": "author_of",
        "tail": "Feature Pyramid Networks for Object Detection"
      },
      {
        "head": "Kaiming He",
        "relation": "author_of",
        "tail": "Feature Pyramid Networks for Object Detection"
      },
      {
        "head": "Bharath Hariharan",
        "relation": "author_of",
        "tail": "Feature Pyramid Networks for Object Detection"
      },
      {
        "head": "Serge Belongie",
        "relation": "author_of",
        "tail": "Feature Pyramid Networks for Object Detection"
      },
      {
        "head": "Kyunghyun Cho",
        "relation": "author_of",
        "tail": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"
      },
      {
        "head": "Bart van Merrienboer",
        "relation": "author_of",
        "tail": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"
      },
      {
        "head": "Caglar Gulcehre",
        "relation": "author_of",
        "tail": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"
      },
      {
        "head": "Dzmitry Bahdanau",
        "relation": "author_of",
        "tail": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"
      },
      {
        "head": "Fethi Bougares",
        "relation": "author_of",
        "tail": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"
      },
      {
        "head": "Holger Schwenk",
        "relation": "author_of",
        "tail": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"
      },
      {
        "head": "Yoshua Bengio",
        "relation": "author_of",
        "tail": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"
      },
      {
        "head": "Erfei Cui",
        "relation": "author_of",
        "tail": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving"
      },
      {
        "head": "Wenhai Wang",
        "relation": "author_of",
        "tail": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving"
      },
      {
        "head": "Zhiqi Li",
        "relation": "author_of",
        "tail": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving"
      },
      {
        "head": "Jiangwei Xie",
        "relation": "author_of",
        "tail": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving"
      },
      {
        "head": "Haoming Zou",
        "relation": "author_of",
        "tail": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving"
      },
      {
        "head": "Hanming Deng",
        "relation": "author_of",
        "tail": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving"
      },
      {
        "head": "Gen Luo",
        "relation": "author_of",
        "tail": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving"
      },
      {
        "head": "Lewei Lu",
        "relation": "author_of",
        "tail": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving"
      },
      {
        "head": "Xizhou Zhu",
        "relation": "author_of",
        "tail": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving"
      },
      {
        "head": "Jifeng Dai",
        "relation": "author_of",
        "tail": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving"
      },
      {
        "head": "Shenyuan Gao",
        "relation": "author_of",
        "tail": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability"
      },
      {
        "head": "Jiazhi Yang",
        "relation": "author_of",
        "tail": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability"
      },
      {
        "head": "Li Chen",
        "relation": "author_of",
        "tail": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability"
      },
      {
        "head": "Kashyap Chitta",
        "relation": "author_of",
        "tail": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability"
      },
      {
        "head": "Yihang Qiu",
        "relation": "author_of",
        "tail": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability"
      },
      {
        "head": "Andreas Geiger",
        "relation": "author_of",
        "tail": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability"
      },
      {
        "head": "Jun Zhang",
        "relation": "author_of",
        "tail": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability"
      },
      {
        "head": "Hongyang Li",
        "relation": "author_of",
        "tail": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability"
      },
      {
        "head": "Bencheng Liao",
        "relation": "author_of",
        "tail": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving"
      },
      {
        "head": "Shaoyu Chen",
        "relation": "author_of",
        "tail": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving"
      },
      {
        "head": "Haoran Yin",
        "relation": "author_of",
        "tail": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving"
      },
      {
        "head": "Bo Jiang",
        "relation": "author_of",
        "tail": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving"
      },
      {
        "head": "Cheng Wang",
        "relation": "author_of",
        "tail": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving"
      },
      {
        "head": "Sixu Yan",
        "relation": "author_of",
        "tail": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving"
      },
      {
        "head": "Xinbang Zhang",
        "relation": "author_of",
        "tail": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving"
      },
      {
        "head": "Xiangyu Li",
        "relation": "author_of",
        "tail": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving"
      },
      {
        "head": "Ying Zhang",
        "relation": "author_of",
        "tail": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving"
      },
      {
        "head": "Qian Zhang",
        "relation": "author_of",
        "tail": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving"
      },
      {
        "head": "Xinggang Wang",
        "relation": "author_of",
        "tail": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving"
      },
      {
        "head": "Bingyi Kang",
        "relation": "author_of",
        "tail": "How Far is Video Generation from World Model: A Physical Law Perspective"
      },
      {
        "head": "Yang Yue",
        "relation": "author_of",
        "tail": "How Far is Video Generation from World Model: A Physical Law Perspective"
      },
      {
        "head": "Rui Lu",
        "relation": "author_of",
        "tail": "How Far is Video Generation from World Model: A Physical Law Perspective"
      },
      {
        "head": "Zhijie Lin",
        "relation": "author_of",
        "tail": "How Far is Video Generation from World Model: A Physical Law Perspective"
      },
      {
        "head": "Yang Zhao",
        "relation": "author_of",
        "tail": "How Far is Video Generation from World Model: A Physical Law Perspective"
      },
      {
        "head": "Kaixin Wang",
        "relation": "author_of",
        "tail": "How Far is Video Generation from World Model: A Physical Law Perspective"
      },
      {
        "head": "Gao Huang",
        "relation": "author_of",
        "tail": "How Far is Video Generation from World Model: A Physical Law Perspective"
      },
      {
        "head": "Jiashi Feng",
        "relation": "author_of",
        "tail": "How Far is Video Generation from World Model: A Physical Law Perspective"
      },
      {
        "head": "Jyh-Jing Hwang",
        "relation": "author_of",
        "tail": "EMMA: End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "Runsheng Xu",
        "relation": "author_of",
        "tail": "EMMA: End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "Hubert Lin",
        "relation": "author_of",
        "tail": "EMMA: End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "Wei-Chih Hung",
        "relation": "author_of",
        "tail": "EMMA: End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "Jingwei Ji",
        "relation": "author_of",
        "tail": "EMMA: End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "Kristy Choi",
        "relation": "author_of",
        "tail": "EMMA: End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "Di Huang",
        "relation": "author_of",
        "tail": "EMMA: End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "Tong He",
        "relation": "author_of",
        "tail": "EMMA: End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "Paul Covington",
        "relation": "author_of",
        "tail": "EMMA: End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "Benjamin Sapp",
        "relation": "author_of",
        "tail": "EMMA: End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "Yin Zhou",
        "relation": "author_of",
        "tail": "EMMA: End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "James Guo",
        "relation": "author_of",
        "tail": "EMMA: End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "Dragomir Anguelov",
        "relation": "author_of",
        "tail": "EMMA: End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "Mingxing Tan",
        "relation": "author_of",
        "tail": "EMMA: End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "Tom B. Brown",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Benjamin Mann",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Nick Ryder",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Melanie Subbiah",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Jared Kaplan",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Prafulla Dhariwal",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Arvind Neelakantan",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Pranav Shyam",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Girish Sastry",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Amanda Askell",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Sandhini Agarwal",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Ariel Herbert-Voss",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Gretchen Krueger",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Tom Henighan",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Rewon Child",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Aditya Ramesh",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Daniel M. Ziegler",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Jeffrey Wu",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Clemens Winter",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Christopher Hesse",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Mark Chen",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Eric Sigler",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Mateusz Litwin",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Scott Gray",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Benjamin Chess",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Jack Clark",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Christopher Berner",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Sam McCandlish",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Alec Radford",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Ilya Sutskever",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Dario Amodei",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Zhe Chen",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Weiyun Wang",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Yue Cao",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Yangzhou Liu",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Zhangwei Gao",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Erfei Cui",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Jinguo Zhu",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Shenglong Ye",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Hao Tian",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Zhaoyang Liu",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Lixin Gu",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Xuehui Wang",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Qingyun Li",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Yiming Ren",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Zixuan Chen",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Jiapeng Luo",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Jiahao Wang",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Tan Jiang",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Bo Wang",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Conghui He",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Botian Shi",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Xingcheng Zhang",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Han Lv",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Yi Wang",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Wenqi Shao",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Pei Chu",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Zhongying Tu",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Tong He",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Zhiyong Wu",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Huipeng Deng",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Jiaye Ge",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Kai Chen",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Kaipeng Zhang",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Limin Wang",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Min Dou",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Lewei Lu",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Xizhou Zhu",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Tong Lu",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Dahua Lin",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Yu Qiao",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Jifeng Dai",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Wenhai Wang",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Jinguo Zhu",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Weiyun Wang",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Zhe Chen",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Zhaoyang Liu",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Shenglong Ye",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Lixin Gu",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Hao Tian",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Yuchen Duan",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Weijie Su",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Jie Shao",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Zhangwei Gao",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Erfei Cui",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Xuehui Wang",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Yue Cao",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Yangzhou Liu",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Xingguang Wei",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Hongjie Zhang",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Haomin Wang",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Weiye Xu",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Hao Li",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Jiahao Wang",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Nianchen Deng",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Songze Li",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Yinan He",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Tan Jiang",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Jiapeng Luo",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Yi Wang",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Conghui He",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Botian Shi",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Xingcheng Zhang",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Wenqi Shao",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Junjun He",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Yingtong Xiong",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Wenwen Qu",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Peng Sun",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Penglong Jiao",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Han Lv",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Lijun Wu",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Kaipeng Zhang",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Huipeng Deng",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Jiaye Ge",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Kai Chen",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Limin Wang",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Min Dou",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Lewei Lu",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Xizhou Zhu",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Tong Lu",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Dahua Lin",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Yu Qiao",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Jifeng Dai",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Wenhai Wang",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Haodong Duan",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Xinyu Fang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Junming Yang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Xiangyu Zhao",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Yuxuan Qiao",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Mo Li",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Amit Agarwal",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Zhe Chen",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Lin Chen",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Yuan Liu",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Yubo Ma",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Hailong Sun",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Yifan Zhang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Shiyin Lu",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Tack Hwa Wong",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Weiyun Wang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Peiheng Zhou",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Xiaozhe Li",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Chaoyou Fu",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Junbo Cui",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Jixuan Chen",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Enxin Song",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Song Mao",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Shengyuan Ding",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Tianhao Liang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Zicheng Zhang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Xiaoyi Dong",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Yuhang Zang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Pan Zhang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Jiaqi Wang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Dahua Lin",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Kai Chen",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Guowei Xu",
        "relation": "author_of",
        "tail": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step"
      },
      {
        "head": "Peng Jin",
        "relation": "author_of",
        "tail": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step"
      },
      {
        "head": "Ziang Wu",
        "relation": "author_of",
        "tail": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step"
      },
      {
        "head": "Hao Li",
        "relation": "author_of",
        "tail": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step"
      },
      {
        "head": "Yibing Song",
        "relation": "author_of",
        "tail": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step"
      },
      {
        "head": "Lichao Sun",
        "relation": "author_of",
        "tail": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step"
      },
      {
        "head": "Li Yuan",
        "relation": "author_of",
        "tail": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step"
      },
      {
        "head": "Weiyun Wang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Zhangwei Gao",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Lixin Gu",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Hengjun Pu",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Long Cui",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Xingguang Wei",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Zhaoyang Liu",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Linglin Jing",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Shenglong Ye",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Jie Shao",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Zhaokai Wang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Zhe Chen",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Hongjie Zhang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Ganlin Yang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Haomin Wang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Qi Wei",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Jinhui Yin",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Wenhao Li",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Erfei Cui",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Guanzhou Chen",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Zichen Ding",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Changyao Tian",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Zhenyu Wu",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Jingjing Xie",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Zehao Li",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Bowen Yang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Yuchen Duan",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Xuehui Wang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Zhi Hou",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Haoran Hao",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Tianyi Zhang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Songze Li",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Xiangyu Zhao",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Haodong Duan",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Nianchen Deng",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Bin Fu",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Yinan He",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Yi Wang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Conghui He",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Botian Shi",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Junjun He",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Yingtong Xiong",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Han Lv",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Lijun Wu",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Wenqi Shao",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Kaipeng Zhang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Huipeng Deng",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Biqing Qi",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Jiaye Ge",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Qipeng Guo",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Wenwei Zhang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Songyang Zhang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Maosong Cao",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Junyao Lin",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Kexian Tang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Jianfei Gao",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Haian Huang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Yuzhe Gu",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Chengqi Lyu",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Huanze Tang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Rui Wang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Haijun Lv",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Wanli Ouyang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Limin Wang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Min Dou",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Xizhou Zhu",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Tong Lu",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Dahua Lin",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Jifeng Dai",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Weijie Su",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Bowen Zhou",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Kai Chen",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Yu Qiao",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Wenhai Wang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Gen Luo",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "John Schulman",
        "relation": "author_of",
        "tail": "Proximal Policy Optimization Algorithms"
      },
      {
        "head": "Filip Wolski",
        "relation": "author_of",
        "tail": "Proximal Policy Optimization Algorithms"
      },
      {
        "head": "Prafulla Dhariwal",
        "relation": "author_of",
        "tail": "Proximal Policy Optimization Algorithms"
      },
      {
        "head": "Alec Radford",
        "relation": "author_of",
        "tail": "Proximal Policy Optimization Algorithms"
      },
      {
        "head": "Oleg Klimov",
        "relation": "author_of",
        "tail": "Proximal Policy Optimization Algorithms"
      },
      {
        "head": "Jie Liu",
        "relation": "author_of",
        "tail": "Flow-GRPO: Training Flow Matching Models via Online RL"
      },
      {
        "head": "Gongye Liu",
        "relation": "author_of",
        "tail": "Flow-GRPO: Training Flow Matching Models via Online RL"
      },
      {
        "head": "Jiajun Liang",
        "relation": "author_of",
        "tail": "Flow-GRPO: Training Flow Matching Models via Online RL"
      },
      {
        "head": "Yangguang Li",
        "relation": "author_of",
        "tail": "Flow-GRPO: Training Flow Matching Models via Online RL"
      },
      {
        "head": "Jiaheng Liu",
        "relation": "author_of",
        "tail": "Flow-GRPO: Training Flow Matching Models via Online RL"
      },
      {
        "head": "Xintao Wang",
        "relation": "author_of",
        "tail": "Flow-GRPO: Training Flow Matching Models via Online RL"
      },
      {
        "head": "Pengfei Wan",
        "relation": "author_of",
        "tail": "Flow-GRPO: Training Flow Matching Models via Online RL"
      },
      {
        "head": "Di Zhang",
        "relation": "author_of",
        "tail": "Flow-GRPO: Training Flow Matching Models via Online RL"
      },
      {
        "head": "Wanli Ouyang",
        "relation": "author_of",
        "tail": "Flow-GRPO: Training Flow Matching Models via Online RL"
      },
      {
        "head": "Zeyue Xue",
        "relation": "author_of",
        "tail": "DanceGRPO: Unleashing GRPO on Visual Generation"
      },
      {
        "head": "Jie Wu",
        "relation": "author_of",
        "tail": "DanceGRPO: Unleashing GRPO on Visual Generation"
      },
      {
        "head": "Yu Gao",
        "relation": "author_of",
        "tail": "DanceGRPO: Unleashing GRPO on Visual Generation"
      },
      {
        "head": "Fangyuan Kong",
        "relation": "author_of",
        "tail": "DanceGRPO: Unleashing GRPO on Visual Generation"
      },
      {
        "head": "Lingting Zhu",
        "relation": "author_of",
        "tail": "DanceGRPO: Unleashing GRPO on Visual Generation"
      },
      {
        "head": "Mengzhao Chen",
        "relation": "author_of",
        "tail": "DanceGRPO: Unleashing GRPO on Visual Generation"
      },
      {
        "head": "Zhiheng Liu",
        "relation": "author_of",
        "tail": "DanceGRPO: Unleashing GRPO on Visual Generation"
      },
      {
        "head": "Wei Liu",
        "relation": "author_of",
        "tail": "DanceGRPO: Unleashing GRPO on Visual Generation"
      },
      {
        "head": "Qiushan Guo",
        "relation": "author_of",
        "tail": "DanceGRPO: Unleashing GRPO on Visual Generation"
      },
      {
        "head": "Weilin Huang",
        "relation": "author_of",
        "tail": "DanceGRPO: Unleashing GRPO on Visual Generation"
      },
      {
        "head": "Ping Luo",
        "relation": "author_of",
        "tail": "DanceGRPO: Unleashing GRPO on Visual Generation"
      },
      {
        "head": "Yu Gao",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Haoyuan Guo",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Tuyen Hoang",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Weilin Huang",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Lu Jiang",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Fangyuan Kong",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Huixia Li",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Jiashi Li",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Liang Li",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Xiaojie Li",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Xunsong Li",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Yifu Li",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Shanchuan Lin",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Zhijie Lin",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Jiawei Liu",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Shu Liu",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Xiaonan Nie",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Zhiwu Qing",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Yuxi Ren",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Li Sun",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Zhi Tian",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Rui Wang",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Sen Wang",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Guoqiang Wei",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Guohong Wu",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Jie Wu",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Ruiqi Xia",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Fei Xiao",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Xuefeng Xiao",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Jiangqiao Yan",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Ceyuan Yang",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Jianchao Yang",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Runkai Yang",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Tao Yang",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Yihang Yang",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Zilyu Ye",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Xuejiao Zeng",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Yan Zeng",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Heng Zhang",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Yang Zhao",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Xiaozheng Zheng",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Peihao Zhu",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Jiaxin Zou",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Feilong Zuo",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Guibin Chen",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Dixuan Lin",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Jiangping Yang",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Chunze Lin",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Junchen Zhu",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Mingyuan Fan",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Hao Zhang",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Sheng Chen",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Zheng Chen",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Chengcheng Ma",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Weiming Xiong",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Wei Wang",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Nuo Pang",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Kang Kang",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Zhiheng Xu",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Yuzhe Jin",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Yupeng Liang",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Yubing Song",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Peng Zhao",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Boyuan Xu",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Di Qiu",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Debang Li",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Zhengcong Fei",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Yang Li",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Yahui Zhou",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Yibin Wang",
        "relation": "author_of",
        "tail": "Unified Reward Model for Multimodal Understanding and Generation"
      },
      {
        "head": "Yuhang Zang",
        "relation": "author_of",
        "tail": "Unified Reward Model for Multimodal Understanding and Generation"
      },
      {
        "head": "Hao Li",
        "relation": "author_of",
        "tail": "Unified Reward Model for Multimodal Understanding and Generation"
      },
      {
        "head": "Cheng Jin",
        "relation": "author_of",
        "tail": "Unified Reward Model for Multimodal Understanding and Generation"
      },
      {
        "head": "Jiaqi Wang",
        "relation": "author_of",
        "tail": "Unified Reward Model for Multimodal Understanding and Generation"
      },
      {
        "head": "知秀 柴田",
        "relation": "author_of",
        "tail": "5分で分かる!? 有名論文ナナメ読み：Jacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "Pranav Rajpurkar",
        "relation": "author_of",
        "tail": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"
      },
      {
        "head": "Jian Zhang",
        "relation": "author_of",
        "tail": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"
      },
      {
        "head": "Konstantin Lopyrev",
        "relation": "author_of",
        "tail": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"
      },
      {
        "head": "Percy Liang",
        "relation": "author_of",
        "tail": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"
      },
      {
        "head": "Rico Sennrich",
        "relation": "author_of",
        "tail": "Neural Machine Translation of Rare Words with Subword Units"
      },
      {
        "head": "Barry Haddow",
        "relation": "author_of",
        "tail": "Neural Machine Translation of Rare Words with Subword Units"
      },
      {
        "head": "Alexandra Birch",
        "relation": "author_of",
        "tail": "Neural Machine Translation of Rare Words with Subword Units"
      },
      {
        "head": "Taku Kudo",
        "relation": "author_of",
        "tail": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"
      },
      {
        "head": "John Richardson",
        "relation": "author_of",
        "tail": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"
      },
      {
        "head": "OpenAI",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Josh Achiam",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Steven Adler",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Sandhini Agarwal",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Lama Ahmad",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Ilge Akkaya",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Florencia Leoni Aleman",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Diogo Almeida",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Janko Altenschmidt",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Sam Altman",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Shyamal Anadkat",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Red Avila",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Igor Babuschkin",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Suchir Balaji",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Valerie Balcom",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Paul Baltescu",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Haiming Bao",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Mohammad Bavarian",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jeff Belgum",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Irwan Bello",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jake Berdine",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Gabriel Bernadett-Shapiro",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Christopher Berner",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Lenny Bogdonoff",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Oleg Boiko",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Madelaine Boyd",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Anna-Luisa Brakman",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Greg Brockman",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Tim Brooks",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Miles Brundage",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Kevin Button",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Trevor Cai",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Rosie Campbell",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Andrew Cann",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Brittany Carey",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Chelsea Carlson",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Rory Carmichael",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Brooke Chan",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Che Chang",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Fotis Chantzis",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Derek Chen",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Sully Chen",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Ruby Chen",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jason Chen",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Mark Chen",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Ben Chess",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Chester Cho",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Casey Chu",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Hyung Won Chung",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Dave Cummings",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jeremiah Currier",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Yunxing Dai",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Cory Decareaux",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Thomas Degry",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Noah Deutsch",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Damien Deville",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Arka Dhar",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "David Dohan",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Steve Dowling",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Sheila Dunning",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Adrien Ecoffet",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Atty Eleti",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Tyna Eloundou",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "David Farhi",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Liam Fedus",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Niko Felix",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Simón Posada Fishman",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Juston Forte",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Isabella Fulford",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Leo Gao",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Elie Georges",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Christian Gibson",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Vik Goel",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Tarun Gogineni",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Gabriel Goh",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Rapha Gontijo-Lopes",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jonathan Gordon",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Morgan Grafstein",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Scott Gray",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Ryan Greene",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Joshua Gross",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Shixiang Shane Gu",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Yufei Guo",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Chris Hallacy",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jesse Han",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jeff Harris",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Yuchen He",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Mike Heaton",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Johannes Heidecke",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Chris Hesse",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Alan Hickey",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Wade Hickey",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Peter Hoeschele",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Brandon Houghton",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Kenny Hsu",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Shengli Hu",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Xin Hu",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Joost Huizinga",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Shantanu Jain",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Shawn Jain",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Joanne Jang",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Angela Jiang",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Roger Jiang",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Haozhun Jin",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Denny Jin",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Shino Jomoto",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Billie Jonn",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Heewoo Jun",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Tomer Kaftan",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Łukasz Kaiser",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Ali Kamali",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Ingmar Kanitscheider",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Nitish Shirish Keskar",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Tabarak Khan",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Logan Kilpatrick",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jong Wook Kim",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Christina Kim",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Yongjik Kim",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jan Hendrik Kirchner",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jamie Kiros",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Matt Knight",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Daniel Kokotajlo",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Łukasz Kondraciuk",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Andrew Kondrich",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Aris Konstantinidis",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Kyle Kosic",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Gretchen Krueger",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Vishal Kuo",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Michael Lampe",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Ikai Lan",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Teddy Lee",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jan Leike",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jade Leung",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Daniel Levy",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Chak Ming Li",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Rachel Lim",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Molly Lin",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Stephanie Lin",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Mateusz Litwin",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Theresa Lopez",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Ryan Lowe",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Patricia Lue",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Anna Makanju",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Kim Malfacini",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Sam Manning",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Todor Markov",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Yaniv Markovski",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Bianca Martin",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Katie Mayer",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Andrew Mayne",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Bob McGrew",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Scott Mayer McKinney",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Christine McLeavey",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Paul McMillan",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jake McNeil",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "David Medina",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Aalok Mehta",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jacob Menick",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Luke Metz",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Andrey Mishchenko",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Pamela Mishkin",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Vinnie Monaco",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Evan Morikawa",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Daniel Mossing",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Tong Mu",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Mira Murati",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Oleg Murk",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "David Mély",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Ashvin Nair",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Reiichiro Nakano",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Rajeev Nayak",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Arvind Neelakantan",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Richard Ngo",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Hyeonwoo Noh",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Long Ouyang",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Cullen O'Keefe",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jakub Pachocki",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Alex Paino",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Joe Palermo",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Ashley Pantuliano",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Giambattista Parascandolo",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Joel Parish",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Emy Parparita",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Alex Passos",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Mikhail Pavlov",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Andrew Peng",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Adam Perelman",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Filipe de Avila Belbute Peres",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Michael Petrov",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Henrique Ponde de Oliveira Pinto",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Michael",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Pokorny",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Michelle Pokrass",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Vitchyr H. Pong",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Tolly Powell",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Alethea Power",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Boris Power",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Elizabeth Proehl",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Raul Puri",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Alec Radford",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jack Rae",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Aditya Ramesh",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Cameron Raymond",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Francis Real",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Kendra Rimbach",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Carl Ross",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Bob Rotsted",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Henri Roussez",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Nick Ryder",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Mario Saltarelli",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Ted Sanders",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Shibani Santurkar",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Girish Sastry",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Heather Schmidt",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "David Schnurr",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "John Schulman",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Daniel Selsam",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Kyla Sheppard",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Toki Sherbakov",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jessica Shieh",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Sarah Shoker",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Pranav Shyam",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Szymon Sidor",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Eric Sigler",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Maddie Simens",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jordan Sitkin",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Katarina Slama",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Ian Sohl",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Benjamin Sokolowsky",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Yang Song",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Natalie Staudacher",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Felipe Petroski Such",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Natalie Summers",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Ilya Sutskever",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jie Tang",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Nikolas Tezak",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Madeleine B. Thompson",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Phil Tillet",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Amin Tootoonchian",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Elizabeth Tseng",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Preston Tuggle",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Nick Turley",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jerry Tworek",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Juan Felipe Cerón Uribe",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Andrea Vallone",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Arun Vijayvergiya",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Chelsea Voss",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Carroll Wainwright",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Justin Jay Wang",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Alvin Wang",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Ben Wang",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jonathan Ward",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jason Wei",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "CJ Weinmann",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Akila Welihinda",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Peter Welinder",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jiayi Weng",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Lilian Weng",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Matt Wiethoff",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Dave Willner",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Clemens Winter",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Samuel Wolrich",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Hannah Wong",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Lauren Workman",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Sherwin Wu",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jeff Wu",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Michael Wu",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Kai Xiao",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Tao Xu",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Sarah Yoo",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Kevin Yu",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Qiming Yuan",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Wojciech Zaremba",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Rowan Zellers",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Chong Zhang",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Marvin Zhang",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Shengjia Zhao",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Tianhao Zheng",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Juntang Zhuang",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "William Zhuk",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Barret Zoph",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Hugo Touvron",
        "relation": "author_of",
        "tail": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "head": "Thibaut Lavril",
        "relation": "author_of",
        "tail": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "head": "Gautier Izacard",
        "relation": "author_of",
        "tail": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "head": "Xavier Martinet",
        "relation": "author_of",
        "tail": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "head": "Marie-Anne Lachaux",
        "relation": "author_of",
        "tail": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "head": "Timothée Lacroix",
        "relation": "author_of",
        "tail": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "head": "Baptiste Rozière",
        "relation": "author_of",
        "tail": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "head": "Naman Goyal",
        "relation": "author_of",
        "tail": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "head": "Eric Hambro",
        "relation": "author_of",
        "tail": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "head": "Faisal Azhar",
        "relation": "author_of",
        "tail": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "head": "Aurelien Rodriguez",
        "relation": "author_of",
        "tail": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "head": "Armand Joulin",
        "relation": "author_of",
        "tail": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "head": "Edouard Grave",
        "relation": "author_of",
        "tail": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "head": "Guillaume Lample",
        "relation": "author_of",
        "tail": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "head": "Jason Wei",
        "relation": "author_of",
        "tail": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Xuezhi Wang",
        "relation": "author_of",
        "tail": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Dale Schuurmans",
        "relation": "author_of",
        "tail": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Maarten Bosma",
        "relation": "author_of",
        "tail": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Brian Ichter",
        "relation": "author_of",
        "tail": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Fei Xia",
        "relation": "author_of",
        "tail": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Ed Chi",
        "relation": "author_of",
        "tail": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Quoc Le",
        "relation": "author_of",
        "tail": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Denny Zhou",
        "relation": "author_of",
        "tail": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Karl Cobbe",
        "relation": "author_of",
        "tail": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "head": "Vineet Kosaraju",
        "relation": "author_of",
        "tail": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "head": "Mohammad Bavarian",
        "relation": "author_of",
        "tail": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "head": "Mark Chen",
        "relation": "author_of",
        "tail": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "head": "Heewoo Jun",
        "relation": "author_of",
        "tail": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "head": "Lukasz Kaiser",
        "relation": "author_of",
        "tail": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "head": "Matthias Plappert",
        "relation": "author_of",
        "tail": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "head": "Jerry Tworek",
        "relation": "author_of",
        "tail": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "head": "Jacob Hilton",
        "relation": "author_of",
        "tail": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "head": "Reiichiro Nakano",
        "relation": "author_of",
        "tail": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "head": "Christopher Hesse",
        "relation": "author_of",
        "tail": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "head": "John Schulman",
        "relation": "author_of",
        "tail": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "head": "Albert Gu",
        "relation": "author_of",
        "tail": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"
      },
      {
        "head": "Tri Dao",
        "relation": "author_of",
        "tail": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"
      },
      {
        "head": "Stephen M. Mount",
        "relation": "author_of",
        "tail": "A catalogue of splice junction sequences."
      },
      {
        "head": "F. Crick",
        "relation": "author_of",
        "tail": "Origin of the Genetic Code"
      },
      {
        "head": "Ilya Loshchilov",
        "relation": "author_of",
        "tail": "SGDR: Stochastic Gradient Descent with Warm Restarts"
      },
      {
        "head": "Frank Hutter",
        "relation": "author_of",
        "tail": "SGDR: Stochastic Gradient Descent with Warm Restarts"
      },
      {
        "head": "Noam Shazeer",
        "relation": "author_of",
        "tail": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"
      },
      {
        "head": "Azalia Mirhoseini",
        "relation": "author_of",
        "tail": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"
      },
      {
        "head": "Krzysztof Maziarz",
        "relation": "author_of",
        "tail": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"
      },
      {
        "head": "Andy Davis",
        "relation": "author_of",
        "tail": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"
      },
      {
        "head": "Quoc Le",
        "relation": "author_of",
        "tail": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"
      },
      {
        "head": "Geoffrey Hinton",
        "relation": "author_of",
        "tail": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"
      },
      {
        "head": "Jeff Dean",
        "relation": "author_of",
        "tail": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"
      },
      {
        "head": "J. Ziv",
        "relation": "author_of",
        "tail": "Compression of individual sequences via variable-rate coding"
      },
      {
        "head": "A. Lempel",
        "relation": "author_of",
        "tail": "Compression of individual sequences via variable-rate coding"
      },
      {
        "head": "Stephen Merity",
        "relation": "author_of",
        "tail": "Pointer Sentinel Mixture Models"
      },
      {
        "head": "Caiming Xiong",
        "relation": "author_of",
        "tail": "Pointer Sentinel Mixture Models"
      },
      {
        "head": "James Bradbury",
        "relation": "author_of",
        "tail": "Pointer Sentinel Mixture Models"
      },
      {
        "head": "Richard Socher",
        "relation": "author_of",
        "tail": "Pointer Sentinel Mixture Models"
      },
      {
        "head": "Dan Hendrycks",
        "relation": "author_of",
        "tail": "Measuring Massive Multitask Language Understanding"
      },
      {
        "head": "Collin Burns",
        "relation": "author_of",
        "tail": "Measuring Massive Multitask Language Understanding"
      },
      {
        "head": "Steven Basart",
        "relation": "author_of",
        "tail": "Measuring Massive Multitask Language Understanding"
      },
      {
        "head": "Andy Zou",
        "relation": "author_of",
        "tail": "Measuring Massive Multitask Language Understanding"
      },
      {
        "head": "Mantas Mazeika",
        "relation": "author_of",
        "tail": "Measuring Massive Multitask Language Understanding"
      },
      {
        "head": "Dawn Song",
        "relation": "author_of",
        "tail": "Measuring Massive Multitask Language Understanding"
      },
      {
        "head": "Jacob Steinhardt",
        "relation": "author_of",
        "tail": "Measuring Massive Multitask Language Understanding"
      },
      {
        "head": "Hunter Lightman",
        "relation": "author_of",
        "tail": "Let's Verify Step by Step"
      },
      {
        "head": "Vineet Kosaraju",
        "relation": "author_of",
        "tail": "Let's Verify Step by Step"
      },
      {
        "head": "Yura Burda",
        "relation": "author_of",
        "tail": "Let's Verify Step by Step"
      },
      {
        "head": "Harri Edwards",
        "relation": "author_of",
        "tail": "Let's Verify Step by Step"
      },
      {
        "head": "Bowen Baker",
        "relation": "author_of",
        "tail": "Let's Verify Step by Step"
      },
      {
        "head": "Teddy Lee",
        "relation": "author_of",
        "tail": "Let's Verify Step by Step"
      },
      {
        "head": "Jan Leike",
        "relation": "author_of",
        "tail": "Let's Verify Step by Step"
      },
      {
        "head": "John Schulman",
        "relation": "author_of",
        "tail": "Let's Verify Step by Step"
      },
      {
        "head": "Ilya Sutskever",
        "relation": "author_of",
        "tail": "Let's Verify Step by Step"
      },
      {
        "head": "Karl Cobbe",
        "relation": "author_of",
        "tail": "Let's Verify Step by Step"
      },
      {
        "head": "David Rein",
        "relation": "author_of",
        "tail": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark"
      },
      {
        "head": "Betty Li Hou",
        "relation": "author_of",
        "tail": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark"
      },
      {
        "head": "Asa Cooper Stickland",
        "relation": "author_of",
        "tail": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark"
      },
      {
        "head": "Jackson Petty",
        "relation": "author_of",
        "tail": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark"
      },
      {
        "head": "Richard Yuanzhe Pang",
        "relation": "author_of",
        "tail": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark"
      },
      {
        "head": "Julien Dirani",
        "relation": "author_of",
        "tail": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark"
      },
      {
        "head": "Julian Michael",
        "relation": "author_of",
        "tail": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark"
      },
      {
        "head": "Samuel R. Bowman",
        "relation": "author_of",
        "tail": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark"
      },
      {
        "head": "Dmitry Lepikhin",
        "relation": "author_of",
        "tail": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"
      },
      {
        "head": "HyoukJoong Lee",
        "relation": "author_of",
        "tail": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"
      },
      {
        "head": "Yuanzhong Xu",
        "relation": "author_of",
        "tail": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"
      },
      {
        "head": "Dehao Chen",
        "relation": "author_of",
        "tail": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"
      },
      {
        "head": "Orhan Firat",
        "relation": "author_of",
        "tail": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"
      },
      {
        "head": "Yanping Huang",
        "relation": "author_of",
        "tail": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"
      },
      {
        "head": "Maxim Krikun",
        "relation": "author_of",
        "tail": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"
      },
      {
        "head": "Noam Shazeer",
        "relation": "author_of",
        "tail": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"
      },
      {
        "head": "Zhifeng Chen",
        "relation": "author_of",
        "tail": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"
      },
      {
        "head": "Ross Girshick",
        "relation": "author_of",
        "tail": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation"
      },
      {
        "head": "Jeff Donahue",
        "relation": "author_of",
        "tail": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation"
      },
      {
        "head": "Trevor Darrell",
        "relation": "author_of",
        "tail": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation"
      },
      {
        "head": "Jitendra Malik",
        "relation": "author_of",
        "tail": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation"
      },
      {
        "head": "Shansong Liu",
        "relation": "author_of",
        "tail": "MuMu-LLaMA: Multi-modal music understanding and generation via large language models"
      },
      {
        "head": "Atin Sakkeer Hussain",
        "relation": "author_of",
        "tail": "MuMu-LLaMA: Multi-modal music understanding and generation via large language models"
      },
      {
        "head": "Qilong Wu",
        "relation": "author_of",
        "tail": "MuMu-LLaMA: Multi-modal music understanding and generation via large language models"
      },
      {
        "head": "Chenshuo Sun",
        "relation": "author_of",
        "tail": "MuMu-LLaMA: Multi-modal music understanding and generation via large language models"
      },
      {
        "head": "Ying Shan",
        "relation": "author_of",
        "tail": "MuMu-LLaMA: Multi-modal music understanding and generation via large language models"
      },
      {
        "head": "Jiahang Tu",
        "relation": "author_of",
        "tail": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy"
      },
      {
        "head": "Ye Li",
        "relation": "author_of",
        "tail": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy"
      },
      {
        "head": "Yiming Wu",
        "relation": "author_of",
        "tail": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy"
      },
      {
        "head": "Hanbin Zhao",
        "relation": "author_of",
        "tail": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy"
      },
      {
        "head": "Chao Zhang",
        "relation": "author_of",
        "tail": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy"
      },
      {
        "head": "Hui Qian",
        "relation": "author_of",
        "tail": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy"
      },
      {
        "head": "Haotian Lv",
        "relation": "author_of",
        "tail": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism"
      },
      {
        "head": "Yuhui Zhang",
        "relation": "author_of",
        "tail": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism"
      },
      {
        "head": "Jiangbo Dai",
        "relation": "author_of",
        "tail": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism"
      },
      {
        "head": "Hanli Wu",
        "relation": "author_of",
        "tail": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism"
      },
      {
        "head": "Jiaji Wang",
        "relation": "author_of",
        "tail": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism"
      },
      {
        "head": "Dawei Wang",
        "relation": "author_of",
        "tail": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism"
      },
      {
        "head": "Mingxin Li",
        "relation": "author_of",
        "tail": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking"
      },
      {
        "head": "Yanzhao Zhang",
        "relation": "author_of",
        "tail": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking"
      },
      {
        "head": "Dingkun Long",
        "relation": "author_of",
        "tail": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking"
      },
      {
        "head": "Keqin Chen",
        "relation": "author_of",
        "tail": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking"
      },
      {
        "head": "Sibo Song",
        "relation": "author_of",
        "tail": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking"
      },
      {
        "head": "Shuai Bai",
        "relation": "author_of",
        "tail": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking"
      },
      {
        "head": "Zhibo Yang",
        "relation": "author_of",
        "tail": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking"
      },
      {
        "head": "Pengjun Xie",
        "relation": "author_of",
        "tail": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking"
      },
      {
        "head": "An Yang",
        "relation": "author_of",
        "tail": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking"
      },
      {
        "head": "Dayiheng Liu",
        "relation": "author_of",
        "tail": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking"
      },
      {
        "head": "Jingren Zhou",
        "relation": "author_of",
        "tail": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking"
      },
      {
        "head": "Junyang Lin",
        "relation": "author_of",
        "tail": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking"
      },
      {
        "head": "Mingyue Chen",
        "relation": "author_of",
        "tail": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness"
      },
      {
        "head": "Xin Liao",
        "relation": "author_of",
        "tail": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness"
      },
      {
        "head": "Han Fang",
        "relation": "author_of",
        "tail": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness"
      },
      {
        "head": "Jinlin Guo",
        "relation": "author_of",
        "tail": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness"
      },
      {
        "head": "Yanxiang Chen",
        "relation": "author_of",
        "tail": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness"
      },
      {
        "head": "Xiaoshuai Wu",
        "relation": "author_of",
        "tail": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness"
      },
      {
        "head": "Radford M. Neal",
        "relation": "author_of",
        "tail": "Pattern Recognition and Machine Learning"
      },
      {
        "head": "L. Breiman",
        "relation": "author_of",
        "tail": "Bagging Predictors"
      },
      {
        "head": "Guan Wang",
        "relation": "author_of",
        "tail": "Automatic Image-Based Plant Disease Severity Estimation Using Deep Learning"
      },
      {
        "head": "Yu Sun",
        "relation": "author_of",
        "tail": "Automatic Image-Based Plant Disease Severity Estimation Using Deep Learning"
      },
      {
        "head": "Jianxin Wang",
        "relation": "author_of",
        "tail": "Automatic Image-Based Plant Disease Severity Estimation Using Deep Learning"
      },
      {
        "head": "Mostafa Mehdipour-Ghazi",
        "relation": "author_of",
        "tail": "Plant identification using deep neural networks via optimization of transfer learning parameters"
      },
      {
        "head": "B. Yanikoglu",
        "relation": "author_of",
        "tail": "Plant identification using deep neural networks via optimization of transfer learning parameters"
      },
      {
        "head": "E. Aptoula",
        "relation": "author_of",
        "tail": "Plant identification using deep neural networks via optimization of transfer learning parameters"
      },
      {
        "head": "Sue Han Lee",
        "relation": "author_of",
        "tail": "New perspectives on plant disease characterization based on deep learning"
      },
      {
        "head": "H. Goëau",
        "relation": "author_of",
        "tail": "New perspectives on plant disease characterization based on deep learning"
      },
      {
        "head": "P. Bonnet",
        "relation": "author_of",
        "tail": "New perspectives on plant disease characterization based on deep learning"
      },
      {
        "head": "A. Joly",
        "relation": "author_of",
        "tail": "New perspectives on plant disease characterization based on deep learning"
      },
      {
        "head": "Yu Sun",
        "relation": "author_of",
        "tail": "Deep Learning for Plant Identification in Natural Environment"
      },
      {
        "head": "Yuan Liu",
        "relation": "author_of",
        "tail": "Deep Learning for Plant Identification in Natural Environment"
      },
      {
        "head": "Guan Wang",
        "relation": "author_of",
        "tail": "Deep Learning for Plant Identification in Natural Environment"
      },
      {
        "head": "Haiyan Zhang",
        "relation": "author_of",
        "tail": "Deep Learning for Plant Identification in Natural Environment"
      },
      {
        "head": "Jose Carranza-Rojas",
        "relation": "author_of",
        "tail": "Going deeper in the automated identification of Herbarium specimens"
      },
      {
        "head": "Hervé Goeau",
        "relation": "author_of",
        "tail": "Going deeper in the automated identification of Herbarium specimens"
      },
      {
        "head": "P. Bonnet",
        "relation": "author_of",
        "tail": "Going deeper in the automated identification of Herbarium specimens"
      },
      {
        "head": "Erick Mata-Montero",
        "relation": "author_of",
        "tail": "Going deeper in the automated identification of Herbarium specimens"
      },
      {
        "head": "A. Joly",
        "relation": "author_of",
        "tail": "Going deeper in the automated identification of Herbarium specimens"
      },
      {
        "head": "Gao Huang",
        "relation": "author_of",
        "tail": "Densely Connected Convolutional Networks"
      },
      {
        "head": "Zhuang Liu",
        "relation": "author_of",
        "tail": "Densely Connected Convolutional Networks"
      },
      {
        "head": "Laurens van der Maaten",
        "relation": "author_of",
        "tail": "Densely Connected Convolutional Networks"
      },
      {
        "head": "Kilian Q. Weinberger",
        "relation": "author_of",
        "tail": "Densely Connected Convolutional Networks"
      },
      {
        "head": "Mark Sandler",
        "relation": "author_of",
        "tail": "MobileNetV2: Inverted Residuals and Linear Bottlenecks"
      },
      {
        "head": "Andrew Howard",
        "relation": "author_of",
        "tail": "MobileNetV2: Inverted Residuals and Linear Bottlenecks"
      },
      {
        "head": "Menglong Zhu",
        "relation": "author_of",
        "tail": "MobileNetV2: Inverted Residuals and Linear Bottlenecks"
      },
      {
        "head": "Andrey Zhmoginov",
        "relation": "author_of",
        "tail": "MobileNetV2: Inverted Residuals and Linear Bottlenecks"
      },
      {
        "head": "Liang-Chieh Chen",
        "relation": "author_of",
        "tail": "MobileNetV2: Inverted Residuals and Linear Bottlenecks"
      },
      {
        "head": "H. Brendan McMahan",
        "relation": "author_of",
        "tail": "Communication-Efficient Learning of Deep Networks from Decentralized Data"
      },
      {
        "head": "Eider Moore",
        "relation": "author_of",
        "tail": "Communication-Efficient Learning of Deep Networks from Decentralized Data"
      },
      {
        "head": "Daniel Ramage",
        "relation": "author_of",
        "tail": "Communication-Efficient Learning of Deep Networks from Decentralized Data"
      },
      {
        "head": "Seth Hampson",
        "relation": "author_of",
        "tail": "Communication-Efficient Learning of Deep Networks from Decentralized Data"
      },
      {
        "head": "Blaise Agüera y Arcas",
        "relation": "author_of",
        "tail": "Communication-Efficient Learning of Deep Networks from Decentralized Data"
      },
      {
        "head": "Fuzhen Zhuang",
        "relation": "author_of",
        "tail": "A Comprehensive Survey on Transfer Learning"
      },
      {
        "head": "Zhiyuan Qi",
        "relation": "author_of",
        "tail": "A Comprehensive Survey on Transfer Learning"
      },
      {
        "head": "Keyu Duan",
        "relation": "author_of",
        "tail": "A Comprehensive Survey on Transfer Learning"
      },
      {
        "head": "Dongbo Xi",
        "relation": "author_of",
        "tail": "A Comprehensive Survey on Transfer Learning"
      },
      {
        "head": "Yongchun Zhu",
        "relation": "author_of",
        "tail": "A Comprehensive Survey on Transfer Learning"
      },
      {
        "head": "Hengshu Zhu",
        "relation": "author_of",
        "tail": "A Comprehensive Survey on Transfer Learning"
      },
      {
        "head": "Hui Xiong",
        "relation": "author_of",
        "tail": "A Comprehensive Survey on Transfer Learning"
      },
      {
        "head": "Qing He",
        "relation": "author_of",
        "tail": "A Comprehensive Survey on Transfer Learning"
      },
      {
        "head": "A. Khan",
        "relation": "author_of",
        "tail": "Privacy Preserved and Decentralized Smartphone Recommendation System"
      },
      {
        "head": "Maha Driss",
        "relation": "author_of",
        "tail": "Privacy Preserved and Decentralized Smartphone Recommendation System"
      },
      {
        "head": "Wadii Boulila",
        "relation": "author_of",
        "tail": "Privacy Preserved and Decentralized Smartphone Recommendation System"
      },
      {
        "head": "G. A. Sampedro",
        "relation": "author_of",
        "tail": "Privacy Preserved and Decentralized Smartphone Recommendation System"
      },
      {
        "head": "Sidra Abbas",
        "relation": "author_of",
        "tail": "Privacy Preserved and Decentralized Smartphone Recommendation System"
      },
      {
        "head": "Chitapong Wechtaisong",
        "relation": "author_of",
        "tail": "Privacy Preserved and Decentralized Smartphone Recommendation System"
      },
      {
        "head": "Tesfahunegn Minwuyelet Mengistu",
        "relation": "author_of",
        "tail": "A Survey on Heterogeneity Taxonomy, Security and Privacy Preservation in the Integration of IoT, Wireless Sensor Networks and Federated Learning"
      },
      {
        "head": "Taewoon Kim",
        "relation": "author_of",
        "tail": "A Survey on Heterogeneity Taxonomy, Security and Privacy Preservation in the Integration of IoT, Wireless Sensor Networks and Federated Learning"
      },
      {
        "head": "Jenn-Wei Lin",
        "relation": "author_of",
        "tail": "A Survey on Heterogeneity Taxonomy, Security and Privacy Preservation in the Integration of IoT, Wireless Sensor Networks and Federated Learning"
      },
      {
        "head": "A. Alamer",
        "relation": "author_of",
        "tail": "A privacy-preserving federated learning with a secure collaborative for malware detection models using Internet of Things resources"
      },
      {
        "head": "Manel Khazri Khlifi",
        "relation": "author_of",
        "tail": "Graph-based deep learning techniques for remote sensing applications: Techniques, taxonomy, and applications - A comprehensive review"
      },
      {
        "head": "Wadii Boulila",
        "relation": "author_of",
        "tail": "Graph-based deep learning techniques for remote sensing applications: Techniques, taxonomy, and applications - A comprehensive review"
      },
      {
        "head": "I. Farah",
        "relation": "author_of",
        "tail": "Graph-based deep learning techniques for remote sensing applications: Techniques, taxonomy, and applications - A comprehensive review"
      },
      {
        "head": "Anwesha Mukherjee",
        "relation": "author_of",
        "tail": "Federated Learning Architectures: A Performance Evaluation With Crop Yield Prediction Application"
      },
      {
        "head": "Rajkumar Buyya",
        "relation": "author_of",
        "tail": "Federated Learning Architectures: A Performance Evaluation With Crop Yield Prediction Application"
      },
      {
        "head": "Alexander Kirillov",
        "relation": "author_of",
        "tail": "Segment Anything"
      },
      {
        "head": "Eric Mintun",
        "relation": "author_of",
        "tail": "Segment Anything"
      },
      {
        "head": "Nikhila Ravi",
        "relation": "author_of",
        "tail": "Segment Anything"
      },
      {
        "head": "Hanzi Mao",
        "relation": "author_of",
        "tail": "Segment Anything"
      },
      {
        "head": "Chloe Rolland",
        "relation": "author_of",
        "tail": "Segment Anything"
      },
      {
        "head": "Laura Gustafson",
        "relation": "author_of",
        "tail": "Segment Anything"
      },
      {
        "head": "Tete Xiao",
        "relation": "author_of",
        "tail": "Segment Anything"
      },
      {
        "head": "Spencer Whitehead",
        "relation": "author_of",
        "tail": "Segment Anything"
      },
      {
        "head": "Alexander C. Berg",
        "relation": "author_of",
        "tail": "Segment Anything"
      },
      {
        "head": "Wan-Yen Lo",
        "relation": "author_of",
        "tail": "Segment Anything"
      },
      {
        "head": "Piotr Dollár",
        "relation": "author_of",
        "tail": "Segment Anything"
      },
      {
        "head": "Ross Girshick",
        "relation": "author_of",
        "tail": "Segment Anything"
      },
      {
        "head": "Haotian Liu",
        "relation": "author_of",
        "tail": "Visual Instruction Tuning"
      },
      {
        "head": "Chunyuan Li",
        "relation": "author_of",
        "tail": "Visual Instruction Tuning"
      },
      {
        "head": "Qingyang Wu",
        "relation": "author_of",
        "tail": "Visual Instruction Tuning"
      },
      {
        "head": "Yong Jae Lee",
        "relation": "author_of",
        "tail": "Visual Instruction Tuning"
      },
      {
        "head": "Haotian Liu",
        "relation": "author_of",
        "tail": "Improved Baselines with Visual Instruction Tuning"
      },
      {
        "head": "Chunyuan Li",
        "relation": "author_of",
        "tail": "Improved Baselines with Visual Instruction Tuning"
      },
      {
        "head": "Yuheng Li",
        "relation": "author_of",
        "tail": "Improved Baselines with Visual Instruction Tuning"
      },
      {
        "head": "Yong Jae Lee",
        "relation": "author_of",
        "tail": "Improved Baselines with Visual Instruction Tuning"
      },
      {
        "head": "Zhiyuan You",
        "relation": "author_of",
        "tail": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset"
      },
      {
        "head": "Jinjin Gu",
        "relation": "author_of",
        "tail": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset"
      },
      {
        "head": "Xin Cai",
        "relation": "author_of",
        "tail": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset"
      },
      {
        "head": "Zheyuan Li",
        "relation": "author_of",
        "tail": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset"
      },
      {
        "head": "Kaiwen Zhu",
        "relation": "author_of",
        "tail": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset"
      },
      {
        "head": "Chao Dong",
        "relation": "author_of",
        "tail": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset"
      },
      {
        "head": "Tianfan Xue",
        "relation": "author_of",
        "tail": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset"
      },
      {
        "head": "Xintong Zhang",
        "relation": "author_of",
        "tail": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs"
      },
      {
        "head": "Zhi Gao",
        "relation": "author_of",
        "tail": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs"
      },
      {
        "head": "Bofei Zhang",
        "relation": "author_of",
        "tail": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs"
      },
      {
        "head": "Pengxiang Li",
        "relation": "author_of",
        "tail": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs"
      },
      {
        "head": "Xiaowen Zhang",
        "relation": "author_of",
        "tail": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs"
      },
      {
        "head": "Yang Liu",
        "relation": "author_of",
        "tail": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs"
      },
      {
        "head": "Tao Yuan",
        "relation": "author_of",
        "tail": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs"
      },
      {
        "head": "Yuwei Wu",
        "relation": "author_of",
        "tail": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs"
      },
      {
        "head": "Yunde Jia",
        "relation": "author_of",
        "tail": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs"
      },
      {
        "head": "Song-Chun Zhu",
        "relation": "author_of",
        "tail": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs"
      },
      {
        "head": "Qing Li",
        "relation": "author_of",
        "tail": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs"
      },
      {
        "head": "Zhangquan Chen",
        "relation": "author_of",
        "tail": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views"
      },
      {
        "head": "Manyuan Zhang",
        "relation": "author_of",
        "tail": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views"
      },
      {
        "head": "Xinlei Yu",
        "relation": "author_of",
        "tail": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views"
      },
      {
        "head": "Xufang Luo",
        "relation": "author_of",
        "tail": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views"
      },
      {
        "head": "Mingze Sun",
        "relation": "author_of",
        "tail": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views"
      },
      {
        "head": "Zihao Pan",
        "relation": "author_of",
        "tail": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views"
      },
      {
        "head": "Yan Feng",
        "relation": "author_of",
        "tail": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views"
      },
      {
        "head": "Peng Pei",
        "relation": "author_of",
        "tail": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views"
      },
      {
        "head": "Xunliang Cai",
        "relation": "author_of",
        "tail": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views"
      },
      {
        "head": "Ruqi Huang",
        "relation": "author_of",
        "tail": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views"
      },
      {
        "head": "Tiancheng Gu",
        "relation": "author_of",
        "tail": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning"
      },
      {
        "head": "Kaicheng Yang",
        "relation": "author_of",
        "tail": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning"
      },
      {
        "head": "Kaichen Zhang",
        "relation": "author_of",
        "tail": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning"
      },
      {
        "head": "Xiang An",
        "relation": "author_of",
        "tail": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning"
      },
      {
        "head": "Ziyong Feng",
        "relation": "author_of",
        "tail": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning"
      },
      {
        "head": "Yueyi Zhang",
        "relation": "author_of",
        "tail": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning"
      },
      {
        "head": "Weidong Cai",
        "relation": "author_of",
        "tail": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning"
      },
      {
        "head": "Jiankang Deng",
        "relation": "author_of",
        "tail": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning"
      },
      {
        "head": "Lidong Bing",
        "relation": "author_of",
        "tail": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning"
      },
      {
        "head": "Kaichen Zhang",
        "relation": "author_of",
        "tail": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe"
      },
      {
        "head": "Keming Wu",
        "relation": "author_of",
        "tail": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe"
      },
      {
        "head": "Zuhao Yang",
        "relation": "author_of",
        "tail": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe"
      },
      {
        "head": "Bo Li",
        "relation": "author_of",
        "tail": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe"
      },
      {
        "head": "Kairui Hu",
        "relation": "author_of",
        "tail": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe"
      },
      {
        "head": "Bin Wang",
        "relation": "author_of",
        "tail": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe"
      },
      {
        "head": "Ziwei Liu",
        "relation": "author_of",
        "tail": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe"
      },
      {
        "head": "Xingxuan Li",
        "relation": "author_of",
        "tail": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe"
      },
      {
        "head": "Lidong Bing",
        "relation": "author_of",
        "tail": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe"
      },
      {
        "head": "Ranjan Sapkota",
        "relation": "author_of",
        "tail": "Vision-Language-Action (VLA) Models: Concepts, Progress, Applications and Challenges"
      },
      {
        "head": "Yang Cao",
        "relation": "author_of",
        "tail": "Vision-Language-Action (VLA) Models: Concepts, Progress, Applications and Challenges"
      },
      {
        "head": "Konstantinos I. Roumeliotis",
        "relation": "author_of",
        "tail": "Vision-Language-Action (VLA) Models: Concepts, Progress, Applications and Challenges"
      },
      {
        "head": "Manoj Karkee",
        "relation": "author_of",
        "tail": "Vision-Language-Action (VLA) Models: Concepts, Progress, Applications and Challenges"
      },
      {
        "head": "Kohei Sendai",
        "relation": "author_of",
        "tail": "Leave No Observation Behind: Real-time Correction for VLA Action Chunks"
      },
      {
        "head": "Maxime Alvarez",
        "relation": "author_of",
        "tail": "Leave No Observation Behind: Real-time Correction for VLA Action Chunks"
      },
      {
        "head": "Tatsuya Matsushima",
        "relation": "author_of",
        "tail": "Leave No Observation Behind: Real-time Correction for VLA Action Chunks"
      },
      {
        "head": "Yutaka Matsuo",
        "relation": "author_of",
        "tail": "Leave No Observation Behind: Real-time Correction for VLA Action Chunks"
      },
      {
        "head": "Yusuke Iwasawa",
        "relation": "author_of",
        "tail": "Leave No Observation Behind: Real-time Correction for VLA Action Chunks"
      },
      {
        "head": "Shuhan Tan",
        "relation": "author_of",
        "tail": "Latent Chain-of-Thought World Modeling for End-to-End Driving"
      },
      {
        "head": "Kashyap Chitta",
        "relation": "author_of",
        "tail": "Latent Chain-of-Thought World Modeling for End-to-End Driving"
      },
      {
        "head": "Yuxiao Chen",
        "relation": "author_of",
        "tail": "Latent Chain-of-Thought World Modeling for End-to-End Driving"
      },
      {
        "head": "Ran Tian",
        "relation": "author_of",
        "tail": "Latent Chain-of-Thought World Modeling for End-to-End Driving"
      },
      {
        "head": "Yurong You",
        "relation": "author_of",
        "tail": "Latent Chain-of-Thought World Modeling for End-to-End Driving"
      },
      {
        "head": "Yan Wang",
        "relation": "author_of",
        "tail": "Latent Chain-of-Thought World Modeling for End-to-End Driving"
      },
      {
        "head": "Wenjie Luo",
        "relation": "author_of",
        "tail": "Latent Chain-of-Thought World Modeling for End-to-End Driving"
      },
      {
        "head": "Yulong Cao",
        "relation": "author_of",
        "tail": "Latent Chain-of-Thought World Modeling for End-to-End Driving"
      },
      {
        "head": "Philipp Krahenbuhl",
        "relation": "author_of",
        "tail": "Latent Chain-of-Thought World Modeling for End-to-End Driving"
      },
      {
        "head": "Marco Pavone",
        "relation": "author_of",
        "tail": "Latent Chain-of-Thought World Modeling for End-to-End Driving"
      },
      {
        "head": "Boris Ivanovic",
        "relation": "author_of",
        "tail": "Latent Chain-of-Thought World Modeling for End-to-End Driving"
      },
      {
        "head": "Zheng Xiong",
        "relation": "author_of",
        "tail": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks"
      },
      {
        "head": "Kang Li",
        "relation": "author_of",
        "tail": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks"
      },
      {
        "head": "Zilin Wang",
        "relation": "author_of",
        "tail": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks"
      },
      {
        "head": "Matthew Jackson",
        "relation": "author_of",
        "tail": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks"
      },
      {
        "head": "Jakob Foerster",
        "relation": "author_of",
        "tail": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks"
      },
      {
        "head": "Shimon Whiteson",
        "relation": "author_of",
        "tail": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks"
      },
      {
        "head": "Yifan Ye",
        "relation": "author_of",
        "tail": "Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models"
      },
      {
        "head": "Jiaqi Ma",
        "relation": "author_of",
        "tail": "Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models"
      },
      {
        "head": "Jun Cen",
        "relation": "author_of",
        "tail": "Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models"
      },
      {
        "head": "Zhihe Lu",
        "relation": "author_of",
        "tail": "Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models"
      },
      {
        "head": "Hai Liu",
        "relation": "author_of",
        "tail": "TransSIL: A Silhouette Cue-Aware Image Classification Framework for Bird Ecological Monitoring Systems"
      },
      {
        "head": "Yu Song",
        "relation": "author_of",
        "tail": "TransSIL: A Silhouette Cue-Aware Image Classification Framework for Bird Ecological Monitoring Systems"
      },
      {
        "head": "Tingting Liu",
        "relation": "author_of",
        "tail": "TransSIL: A Silhouette Cue-Aware Image Classification Framework for Bird Ecological Monitoring Systems"
      },
      {
        "head": "Lin Chen",
        "relation": "author_of",
        "tail": "TransSIL: A Silhouette Cue-Aware Image Classification Framework for Bird Ecological Monitoring Systems"
      },
      {
        "head": "Zhaoli Zhang",
        "relation": "author_of",
        "tail": "TransSIL: A Silhouette Cue-Aware Image Classification Framework for Bird Ecological Monitoring Systems"
      },
      {
        "head": "Xiaolan Yang",
        "relation": "author_of",
        "tail": "TransSIL: A Silhouette Cue-Aware Image Classification Framework for Bird Ecological Monitoring Systems"
      },
      {
        "head": "Neal N. Xiong",
        "relation": "author_of",
        "tail": "TransSIL: A Silhouette Cue-Aware Image Classification Framework for Bird Ecological Monitoring Systems"
      },
      {
        "head": "Honghu Chu",
        "relation": "author_of",
        "tail": "CBRFormer: rendering technology-based transformer for refinement segmentation of bridge crack images"
      },
      {
        "head": "Jiahao Gai",
        "relation": "author_of",
        "tail": "CBRFormer: rendering technology-based transformer for refinement segmentation of bridge crack images"
      },
      {
        "head": "Weiwei Chen",
        "relation": "author_of",
        "tail": "CBRFormer: rendering technology-based transformer for refinement segmentation of bridge crack images"
      },
      {
        "head": "Jun Ma",
        "relation": "author_of",
        "tail": "CBRFormer: rendering technology-based transformer for refinement segmentation of bridge crack images"
      },
      {
        "head": "A. S. Demirkol",
        "relation": "author_of",
        "tail": "A Fast and Compact Threshold Switch-Based Cellular Nonlinear Network Cell"
      },
      {
        "head": "A. Ascoli",
        "relation": "author_of",
        "tail": "A Fast and Compact Threshold Switch-Based Cellular Nonlinear Network Cell"
      },
      {
        "head": "I. Messaris",
        "relation": "author_of",
        "tail": "A Fast and Compact Threshold Switch-Based Cellular Nonlinear Network Cell"
      },
      {
        "head": "V. Ntinas",
        "relation": "author_of",
        "tail": "A Fast and Compact Threshold Switch-Based Cellular Nonlinear Network Cell"
      },
      {
        "head": "D. Prousalis",
        "relation": "author_of",
        "tail": "A Fast and Compact Threshold Switch-Based Cellular Nonlinear Network Cell"
      },
      {
        "head": "R. Tetzlaff",
        "relation": "author_of",
        "tail": "A Fast and Compact Threshold Switch-Based Cellular Nonlinear Network Cell"
      },
      {
        "head": "Yinjun Jia",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Bowen Gao",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Jiaxin Tan",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Jiqing Zheng",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Xin Hong",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Wenyu Zhu",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Haichuan Tan",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Yuan Xiao",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Liping Tan",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Hongyi Cai",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Yanwen Huang",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Zhiheng Deng",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Xiangwei Wu",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Yue Jin",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Yafei Yuan",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Jiekang Tian",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Wei He",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Weiying Ma",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Ya-Qin Zhang",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Lei Liu",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Chuangye Yan",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Wei Zhang",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Yanyan Lan",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Guodong Fan",
        "relation": "author_of",
        "tail": "LLaVA-based semantic feature modulation diffusion model for underwater image enhancement"
      },
      {
        "head": "Shengning Zhou",
        "relation": "author_of",
        "tail": "LLaVA-based semantic feature modulation diffusion model for underwater image enhancement"
      },
      {
        "head": "Zhen Hua",
        "relation": "author_of",
        "tail": "LLaVA-based semantic feature modulation diffusion model for underwater image enhancement"
      },
      {
        "head": "Jinjiang Li",
        "relation": "author_of",
        "tail": "LLaVA-based semantic feature modulation diffusion model for underwater image enhancement"
      },
      {
        "head": "Jingchun Zhou",
        "relation": "author_of",
        "tail": "LLaVA-based semantic feature modulation diffusion model for underwater image enhancement"
      },
      {
        "head": "Jiahua Dong",
        "relation": "author_of",
        "tail": "3DGS-Drag: Dragging Gaussians for Intuitive Point-Based 3D Editing"
      },
      {
        "head": "Yu-Xiong Wang",
        "relation": "author_of",
        "tail": "3DGS-Drag: Dragging Gaussians for Intuitive Point-Based 3D Editing"
      },
      {
        "head": "Joohyung Yun",
        "relation": "author_of",
        "tail": "LILaC: Late Interacting in Layered Component Graph for Open-domain Multimodal Multihop Retrieval"
      },
      {
        "head": "Doyup Lee",
        "relation": "author_of",
        "tail": "LILaC: Late Interacting in Layered Component Graph for Open-domain Multimodal Multihop Retrieval"
      },
      {
        "head": "Wook-Shin Han",
        "relation": "author_of",
        "tail": "LILaC: Late Interacting in Layered Component Graph for Open-domain Multimodal Multihop Retrieval"
      },
      {
        "head": "Yiyang Lu",
        "relation": "author_of",
        "tail": "Bidirectional Normalizing Flow: From Data to Noise and Back"
      },
      {
        "head": "Qiao Sun",
        "relation": "author_of",
        "tail": "Bidirectional Normalizing Flow: From Data to Noise and Back"
      },
      {
        "head": "Xianbang Wang",
        "relation": "author_of",
        "tail": "Bidirectional Normalizing Flow: From Data to Noise and Back"
      },
      {
        "head": "Zhicheng Jiang",
        "relation": "author_of",
        "tail": "Bidirectional Normalizing Flow: From Data to Noise and Back"
      },
      {
        "head": "Hanhong Zhao",
        "relation": "author_of",
        "tail": "Bidirectional Normalizing Flow: From Data to Noise and Back"
      },
      {
        "head": "Kaiming He",
        "relation": "author_of",
        "tail": "Bidirectional Normalizing Flow: From Data to Noise and Back"
      },
      {
        "head": "Yiyang Lu",
        "relation": "author_of",
        "tail": "One-step Latent-free Image Generation with Pixel Mean Flows"
      },
      {
        "head": "Susie Lu",
        "relation": "author_of",
        "tail": "One-step Latent-free Image Generation with Pixel Mean Flows"
      },
      {
        "head": "Qiao Sun",
        "relation": "author_of",
        "tail": "One-step Latent-free Image Generation with Pixel Mean Flows"
      },
      {
        "head": "Hanhong Zhao",
        "relation": "author_of",
        "tail": "One-step Latent-free Image Generation with Pixel Mean Flows"
      },
      {
        "head": "Zhicheng Jiang",
        "relation": "author_of",
        "tail": "One-step Latent-free Image Generation with Pixel Mean Flows"
      },
      {
        "head": "Xianbang Wang",
        "relation": "author_of",
        "tail": "One-step Latent-free Image Generation with Pixel Mean Flows"
      },
      {
        "head": "Tianhong Li",
        "relation": "author_of",
        "tail": "One-step Latent-free Image Generation with Pixel Mean Flows"
      },
      {
        "head": "Zhengyang Geng",
        "relation": "author_of",
        "tail": "One-step Latent-free Image Generation with Pixel Mean Flows"
      },
      {
        "head": "Kaiming He",
        "relation": "author_of",
        "tail": "One-step Latent-free Image Generation with Pixel Mean Flows"
      },
      {
        "head": "Peter Potaptchik",
        "relation": "author_of",
        "tail": "Meta Flow Maps enable scalable reward alignment"
      },
      {
        "head": "Adhi Saravanan",
        "relation": "author_of",
        "tail": "Meta Flow Maps enable scalable reward alignment"
      },
      {
        "head": "Abbas Mammadov",
        "relation": "author_of",
        "tail": "Meta Flow Maps enable scalable reward alignment"
      },
      {
        "head": "Alvaro Prat",
        "relation": "author_of",
        "tail": "Meta Flow Maps enable scalable reward alignment"
      },
      {
        "head": "Michael S. Albergo",
        "relation": "author_of",
        "tail": "Meta Flow Maps enable scalable reward alignment"
      },
      {
        "head": "Yee Whye Teh",
        "relation": "author_of",
        "tail": "Meta Flow Maps enable scalable reward alignment"
      },
      {
        "head": "Yinan Huang",
        "relation": "author_of",
        "tail": "Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective"
      },
      {
        "head": "Hans Hao-Hsun Hsu",
        "relation": "author_of",
        "tail": "Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective"
      },
      {
        "head": "Junran Wang",
        "relation": "author_of",
        "tail": "Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective"
      },
      {
        "head": "Bo Dai",
        "relation": "author_of",
        "tail": "Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective"
      },
      {
        "head": "Pan Li",
        "relation": "author_of",
        "tail": "Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective"
      },
      {
        "head": "Mingyang Deng",
        "relation": "author_of",
        "tail": "Generative Modeling via Drifting"
      },
      {
        "head": "He Li",
        "relation": "author_of",
        "tail": "Generative Modeling via Drifting"
      },
      {
        "head": "Tianhong Li",
        "relation": "author_of",
        "tail": "Generative Modeling via Drifting"
      },
      {
        "head": "Yilun Du",
        "relation": "author_of",
        "tail": "Generative Modeling via Drifting"
      },
      {
        "head": "Kaiming He",
        "relation": "author_of",
        "tail": "Generative Modeling via Drifting"
      },
      {
        "head": "Ting Chen",
        "relation": "author_of",
        "tail": "A Simple Framework for Contrastive Learning of Visual Representations"
      },
      {
        "head": "Simon Kornblith",
        "relation": "author_of",
        "tail": "A Simple Framework for Contrastive Learning of Visual Representations"
      },
      {
        "head": "Mohammad Norouzi",
        "relation": "author_of",
        "tail": "A Simple Framework for Contrastive Learning of Visual Representations"
      },
      {
        "head": "Geoffrey Hinton",
        "relation": "author_of",
        "tail": "A Simple Framework for Contrastive Learning of Visual Representations"
      },
      {
        "head": "Chubin Chen",
        "relation": "author_of",
        "tail": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning"
      },
      {
        "head": "Sujie Hu",
        "relation": "author_of",
        "tail": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning"
      },
      {
        "head": "Jiashu Zhu",
        "relation": "author_of",
        "tail": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning"
      },
      {
        "head": "Meiqi Wu",
        "relation": "author_of",
        "tail": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning"
      },
      {
        "head": "Jintao Chen",
        "relation": "author_of",
        "tail": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning"
      },
      {
        "head": "Yanxun Li",
        "relation": "author_of",
        "tail": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning"
      },
      {
        "head": "Nisha Huang",
        "relation": "author_of",
        "tail": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning"
      },
      {
        "head": "Chengyu Fang",
        "relation": "author_of",
        "tail": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning"
      },
      {
        "head": "Jiahong Wu",
        "relation": "author_of",
        "tail": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning"
      },
      {
        "head": "Xiangxiang Chu",
        "relation": "author_of",
        "tail": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning"
      },
      {
        "head": "Xiu Li",
        "relation": "author_of",
        "tail": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning"
      },
      {
        "head": "Meiqi Wu",
        "relation": "author_of",
        "tail": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints"
      },
      {
        "head": "Jiashu Zhu",
        "relation": "author_of",
        "tail": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints"
      },
      {
        "head": "Xiaokun Feng",
        "relation": "author_of",
        "tail": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints"
      },
      {
        "head": "Chubin Chen",
        "relation": "author_of",
        "tail": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints"
      },
      {
        "head": "Chen Zhu",
        "relation": "author_of",
        "tail": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints"
      },
      {
        "head": "Bingze Song",
        "relation": "author_of",
        "tail": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints"
      },
      {
        "head": "Fangyuan Mao",
        "relation": "author_of",
        "tail": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints"
      },
      {
        "head": "Jiahong Wu",
        "relation": "author_of",
        "tail": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints"
      },
      {
        "head": "Xiangxiang Chu",
        "relation": "author_of",
        "tail": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints"
      },
      {
        "head": "Kaiqi Huang",
        "relation": "author_of",
        "tail": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints"
      },
      {
        "head": "Shengbang Tong",
        "relation": "author_of",
        "tail": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "Boyang Zheng",
        "relation": "author_of",
        "tail": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "Ziteng Wang",
        "relation": "author_of",
        "tail": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "Bingda Tang",
        "relation": "author_of",
        "tail": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "Nanye Ma",
        "relation": "author_of",
        "tail": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "Ellis Brown",
        "relation": "author_of",
        "tail": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "Jihan Yang",
        "relation": "author_of",
        "tail": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "Rob Fergus",
        "relation": "author_of",
        "tail": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "Yann LeCun",
        "relation": "author_of",
        "tail": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "Saining Xie",
        "relation": "author_of",
        "tail": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "Jingtong Yue",
        "relation": "author_of",
        "tail": "Simulating the Visual World with Artificial Intelligence: A Roadmap"
      },
      {
        "head": "Ziqi Huang",
        "relation": "author_of",
        "tail": "Simulating the Visual World with Artificial Intelligence: A Roadmap"
      },
      {
        "head": "Zhaoxi Chen",
        "relation": "author_of",
        "tail": "Simulating the Visual World with Artificial Intelligence: A Roadmap"
      },
      {
        "head": "Xintao Wang",
        "relation": "author_of",
        "tail": "Simulating the Visual World with Artificial Intelligence: A Roadmap"
      },
      {
        "head": "Pengfei Wan",
        "relation": "author_of",
        "tail": "Simulating the Visual World with Artificial Intelligence: A Roadmap"
      },
      {
        "head": "Ziwei Liu",
        "relation": "author_of",
        "tail": "Simulating the Visual World with Artificial Intelligence: A Roadmap"
      },
      {
        "head": "Bohan Zeng",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Kaixin Zhu",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Daili Hua",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Bozhou Li",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Chengzhuo Tong",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Yuran Wang",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Xinyi Huang",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Yifan Dai",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Zixiang Zhang",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Yifan Yang",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Zhou Liu",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Hao Liang",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Xiaochen Ma",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Ruichuan An",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Tianyi Bai",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Hongcheng Gao",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Junbo Niu",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Yang Shi",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Xinlong Chen",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Yue Ding",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Minglei Shi",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Kai Zeng",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Yiwen Tang",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Yuanxing Zhang",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Pengfei Wan",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Xintao Wang",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Wentao Zhang",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Qingyu Shi",
        "relation": "author_of",
        "tail": "RecTok: Reconstruction Distillation along Rectified Flow"
      },
      {
        "head": "Size Wu",
        "relation": "author_of",
        "tail": "RecTok: Reconstruction Distillation along Rectified Flow"
      },
      {
        "head": "Jinbin Bai",
        "relation": "author_of",
        "tail": "RecTok: Reconstruction Distillation along Rectified Flow"
      },
      {
        "head": "Kaidong Yu",
        "relation": "author_of",
        "tail": "RecTok: Reconstruction Distillation along Rectified Flow"
      },
      {
        "head": "Yujing Wang",
        "relation": "author_of",
        "tail": "RecTok: Reconstruction Distillation along Rectified Flow"
      },
      {
        "head": "Yunhai Tong",
        "relation": "author_of",
        "tail": "RecTok: Reconstruction Distillation along Rectified Flow"
      },
      {
        "head": "Xiangtai Li",
        "relation": "author_of",
        "tail": "RecTok: Reconstruction Distillation along Rectified Flow"
      },
      {
        "head": "Xuelong Li",
        "relation": "author_of",
        "tail": "RecTok: Reconstruction Distillation along Rectified Flow"
      },
      {
        "head": "Guanfang Dong",
        "relation": "author_of",
        "tail": "RePack then Refine: Efficient Diffusion Transformer with Vision Foundation Model"
      },
      {
        "head": "Luke Schultz",
        "relation": "author_of",
        "tail": "RePack then Refine: Efficient Diffusion Transformer with Vision Foundation Model"
      },
      {
        "head": "Negar Hassanpour",
        "relation": "author_of",
        "tail": "RePack then Refine: Efficient Diffusion Transformer with Vision Foundation Model"
      },
      {
        "head": "Chao Gao",
        "relation": "author_of",
        "tail": "RePack then Refine: Efficient Diffusion Transformer with Vision Foundation Model"
      },
      {
        "head": "Prafulla Dhariwal",
        "relation": "author_of",
        "tail": "Diffusion Models Beat GANs on Image Synthesis"
      },
      {
        "head": "Alex Nichol",
        "relation": "author_of",
        "tail": "Diffusion Models Beat GANs on Image Synthesis"
      },
      {
        "head": "Maxime Oquab",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Timothée Darcet",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Théo Moutakanni",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Huy Vo",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Marc Szafraniec",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Vasil Khalidov",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Pierre Fernandez",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Daniel Haziza",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Francisco Massa",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Alaaeldin El-Nouby",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Mahmoud Assran",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Nicolas Ballas",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Wojciech Galuba",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Russell Howes",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Po-Yao Huang",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Shang-Wen Li",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Ishan Misra",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Michael Rabbat",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Vasu Sharma",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Gabriel Synnaeve",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Hu Xu",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Hervé Jegou",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Julien Mairal",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Patrick Labatut",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Armand Joulin",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Piotr Bojanowski",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Zehong Ma",
        "relation": "author_of",
        "tail": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss"
      },
      {
        "head": "Ruihan Xu",
        "relation": "author_of",
        "tail": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss"
      },
      {
        "head": "Shiliang Zhang",
        "relation": "author_of",
        "tail": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss"
      },
      {
        "head": "Shanshan Zhao",
        "relation": "author_of",
        "tail": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities"
      },
      {
        "head": "Xinjie Zhang",
        "relation": "author_of",
        "tail": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities"
      },
      {
        "head": "Jintao Guo",
        "relation": "author_of",
        "tail": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities"
      },
      {
        "head": "Jiakui Hu",
        "relation": "author_of",
        "tail": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities"
      },
      {
        "head": "Lunhao Duan",
        "relation": "author_of",
        "tail": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities"
      },
      {
        "head": "Minghao Fu",
        "relation": "author_of",
        "tail": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities"
      },
      {
        "head": "Yong Xien Chng",
        "relation": "author_of",
        "tail": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities"
      },
      {
        "head": "Guo-Hua Wang",
        "relation": "author_of",
        "tail": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities"
      },
      {
        "head": "Qing-Guo Chen",
        "relation": "author_of",
        "tail": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities"
      },
      {
        "head": "Zhao Xu",
        "relation": "author_of",
        "tail": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities"
      },
      {
        "head": "Weihua Luo",
        "relation": "author_of",
        "tail": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities"
      },
      {
        "head": "Kaifu Zhang",
        "relation": "author_of",
        "tail": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities"
      },
      {
        "head": "Jana Zeller",
        "relation": "author_of",
        "tail": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery"
      },
      {
        "head": "Thaddäus Wiedemer",
        "relation": "author_of",
        "tail": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery"
      },
      {
        "head": "Fanfei Li",
        "relation": "author_of",
        "tail": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery"
      },
      {
        "head": "Thomas Klein",
        "relation": "author_of",
        "tail": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery"
      },
      {
        "head": "Prasanna Mayilvahanan",
        "relation": "author_of",
        "tail": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery"
      },
      {
        "head": "Matthias Bethge",
        "relation": "author_of",
        "tail": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery"
      },
      {
        "head": "Felix Wichmann",
        "relation": "author_of",
        "tail": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery"
      },
      {
        "head": "Ryan Cotterell",
        "relation": "author_of",
        "tail": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery"
      },
      {
        "head": "Wieland Brendel",
        "relation": "author_of",
        "tail": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery"
      },
      {
        "head": "Letian Zhang",
        "relation": "author_of",
        "tail": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation"
      },
      {
        "head": "Sucheng Ren",
        "relation": "author_of",
        "tail": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation"
      },
      {
        "head": "Yanqing Liu",
        "relation": "author_of",
        "tail": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation"
      },
      {
        "head": "Xianhang Li",
        "relation": "author_of",
        "tail": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation"
      },
      {
        "head": "Zeyu Wang",
        "relation": "author_of",
        "tail": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation"
      },
      {
        "head": "Yuyin Zhou",
        "relation": "author_of",
        "tail": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation"
      },
      {
        "head": "Huaxiu Yao",
        "relation": "author_of",
        "tail": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation"
      },
      {
        "head": "Zeyu Zheng",
        "relation": "author_of",
        "tail": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation"
      },
      {
        "head": "Weili Nie",
        "relation": "author_of",
        "tail": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation"
      },
      {
        "head": "Guilin Liu",
        "relation": "author_of",
        "tail": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation"
      },
      {
        "head": "Zhiding Yu",
        "relation": "author_of",
        "tail": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation"
      },
      {
        "head": "Cihang Xie",
        "relation": "author_of",
        "tail": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation"
      },
      {
        "head": "Tomas Mikolov",
        "relation": "author_of",
        "tail": "Distributed Representations of Words and Phrases and their Compositionality"
      },
      {
        "head": "Ilya Sutskever",
        "relation": "author_of",
        "tail": "Distributed Representations of Words and Phrases and their Compositionality"
      },
      {
        "head": "Kai Chen",
        "relation": "author_of",
        "tail": "Distributed Representations of Words and Phrases and their Compositionality"
      },
      {
        "head": "Greg Corrado",
        "relation": "author_of",
        "tail": "Distributed Representations of Words and Phrases and their Compositionality"
      },
      {
        "head": "Jeffrey Dean",
        "relation": "author_of",
        "tail": "Distributed Representations of Words and Phrases and their Compositionality"
      },
      {
        "head": "Jeffrey Pennington",
        "relation": "author_of",
        "tail": "GloVe: Global Vectors for Word Representation"
      },
      {
        "head": "R. Socher",
        "relation": "author_of",
        "tail": "GloVe: Global Vectors for Word Representation"
      },
      {
        "head": "Christopher D. Manning",
        "relation": "author_of",
        "tail": "GloVe: Global Vectors for Word Representation"
      },
      {
        "head": "Matthew E. Peters",
        "relation": "author_of",
        "tail": "Deep Contextualized Word Representations"
      },
      {
        "head": "Mark Neumann",
        "relation": "author_of",
        "tail": "Deep Contextualized Word Representations"
      },
      {
        "head": "Mohit Iyyer",
        "relation": "author_of",
        "tail": "Deep Contextualized Word Representations"
      },
      {
        "head": "Matt Gardner",
        "relation": "author_of",
        "tail": "Deep Contextualized Word Representations"
      },
      {
        "head": "Christopher Clark",
        "relation": "author_of",
        "tail": "Deep Contextualized Word Representations"
      },
      {
        "head": "Kenton Lee",
        "relation": "author_of",
        "tail": "Deep Contextualized Word Representations"
      },
      {
        "head": "Luke Zettlemoyer",
        "relation": "author_of",
        "tail": "Deep Contextualized Word Representations"
      },
      {
        "head": "Quentin Fournier",
        "relation": "author_of",
        "tail": "Protein Language Models: Is Scaling Necessary?"
      },
      {
        "head": "Robert M. Vernon",
        "relation": "author_of",
        "tail": "Protein Language Models: Is Scaling Necessary?"
      },
      {
        "head": "Almer M. van der Sloot",
        "relation": "author_of",
        "tail": "Protein Language Models: Is Scaling Necessary?"
      },
      {
        "head": "Benjamin Schulz",
        "relation": "author_of",
        "tail": "Protein Language Models: Is Scaling Necessary?"
      },
      {
        "head": "Sarath Chandar",
        "relation": "author_of",
        "tail": "Protein Language Models: Is Scaling Necessary?"
      },
      {
        "head": "C. Langmead",
        "relation": "author_of",
        "tail": "Protein Language Models: Is Scaling Necessary?"
      },
      {
        "head": "Scott Friedman",
        "relation": "author_of",
        "tail": "Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis"
      },
      {
        "head": "Sonja Schmer-Galunder",
        "relation": "author_of",
        "tail": "Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis"
      },
      {
        "head": "Anthony Chen",
        "relation": "author_of",
        "tail": "Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis"
      },
      {
        "head": "Jeffrey Rye",
        "relation": "author_of",
        "tail": "Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis"
      },
      {
        "head": "Zhaohu Xing",
        "relation": "author_of",
        "tail": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation"
      },
      {
        "head": "Tian Ye",
        "relation": "author_of",
        "tail": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation"
      },
      {
        "head": "Yijun Yang",
        "relation": "author_of",
        "tail": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation"
      },
      {
        "head": "D. Cai",
        "relation": "author_of",
        "tail": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation"
      },
      {
        "head": "Baowen Gai",
        "relation": "author_of",
        "tail": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation"
      },
      {
        "head": "Xiao-Jian Wu",
        "relation": "author_of",
        "tail": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation"
      },
      {
        "head": "Feng Gao",
        "relation": "author_of",
        "tail": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation"
      },
      {
        "head": "Lei Zhu",
        "relation": "author_of",
        "tail": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation"
      },
      {
        "head": "Alexander C. Li",
        "relation": "author_of",
        "tail": "Generative Classifiers Avoid Shortcut Solutions"
      },
      {
        "head": "Ananya Kumar",
        "relation": "author_of",
        "tail": "Generative Classifiers Avoid Shortcut Solutions"
      },
      {
        "head": "Deepak Pathak",
        "relation": "author_of",
        "tail": "Generative Classifiers Avoid Shortcut Solutions"
      },
      {
        "head": "Y. Sun",
        "relation": "author_of",
        "tail": "Edge Large AI Model Agent-Empowered Cognitive Multimodal Semantic Communication"
      },
      {
        "head": "Yinqiu Liu",
        "relation": "author_of",
        "tail": "Edge Large AI Model Agent-Empowered Cognitive Multimodal Semantic Communication"
      },
      {
        "head": "Shaoyong Guo",
        "relation": "author_of",
        "tail": "Edge Large AI Model Agent-Empowered Cognitive Multimodal Semantic Communication"
      },
      {
        "head": "Xuesong Qiu",
        "relation": "author_of",
        "tail": "Edge Large AI Model Agent-Empowered Cognitive Multimodal Semantic Communication"
      },
      {
        "head": "Jiewei Chen",
        "relation": "author_of",
        "tail": "Edge Large AI Model Agent-Empowered Cognitive Multimodal Semantic Communication"
      },
      {
        "head": "Jiakai Hao",
        "relation": "author_of",
        "tail": "Edge Large AI Model Agent-Empowered Cognitive Multimodal Semantic Communication"
      },
      {
        "head": "Dusist Niyato",
        "relation": "author_of",
        "tail": "Edge Large AI Model Agent-Empowered Cognitive Multimodal Semantic Communication"
      },
      {
        "head": "Alec Radford",
        "relation": "author_of",
        "tail": "Language Models are Unsupervised Multitask Learners"
      },
      {
        "head": "Jeff Wu",
        "relation": "author_of",
        "tail": "Language Models are Unsupervised Multitask Learners"
      },
      {
        "head": "R. Child",
        "relation": "author_of",
        "tail": "Language Models are Unsupervised Multitask Learners"
      },
      {
        "head": "D. Luan",
        "relation": "author_of",
        "tail": "Language Models are Unsupervised Multitask Learners"
      },
      {
        "head": "Dario Amodei",
        "relation": "author_of",
        "tail": "Language Models are Unsupervised Multitask Learners"
      },
      {
        "head": "I. Sutskever",
        "relation": "author_of",
        "tail": "Language Models are Unsupervised Multitask Learners"
      },
      {
        "head": "Aaron Grattafiori",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Abhimanyu Dubey",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Abhinav Jauhri",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Abhinav Pandey",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Abhishek Kadian",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ahmad Al-Dahle",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Aiesha Letman",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Akhil Mathur",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Alan Schelten",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Alex Vaughan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Amy Yang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Angela Fan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Anirudh Goyal",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Anthony Hartshorn",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Aobo Yang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Archi Mitra",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Archie Sravankumar",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Artem Korenev",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Arthur Hinsvark",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Arun Rao",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Aston Zhang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Aurelien Rodriguez",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Austen Gregerson",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ava Spataru",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Baptiste Roziere",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Bethany Biron",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Binh Tang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Bobbie Chern",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Charlotte Caucheteux",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Chaya Nayak",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Chloe Bi",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Chris Marra",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Chris McConnell",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Christian Keller",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Christophe Touret",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Chunyang Wu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Corinne Wong",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Cristian Canton Ferrer",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Cyrus Nikolaidis",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Damien Allonsius",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Daniel Song",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Danielle Pintz",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Danny Livshits",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Danny Wyatt",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "David Esiobu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Dhruv Choudhary",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Dhruv Mahajan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Diego Garcia-Olano",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Diego Perino",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Dieuwke Hupkes",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Egor Lakomkin",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ehab AlBadawy",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Elina Lobanova",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Emily Dinan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Eric Michael Smith",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Filip Radenovic",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Francisco Guzmán",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Frank Zhang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Gabriel Synnaeve",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Gabrielle Lee",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Georgia Lewis Anderson",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Govind Thattai",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Graeme Nail",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Gregoire Mialon",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Guan Pang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Guillem Cucurell",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Hailey Nguyen",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Hannah Korevaar",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Hu Xu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Hugo Touvron",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Iliyan Zarov",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Imanol Arrieta Ibarra",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Isabel Kloumann",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ishan Misra",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ivan Evtimov",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jack Zhang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jade Copet",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jaewon Lee",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jan Geffert",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jana Vranes",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jason Park",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jay Mahadeokar",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jeet Shah",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jelmer van der Linde",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jennifer Billock",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jenny Hong",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jenya Lee",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jeremy Fu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jianfeng Chi",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jianyu Huang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jiawen Liu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jie Wang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jiecao Yu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Joanna Bitton",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Joe Spisak",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jongsoo Park",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Joseph Rocca",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Joshua Johnstun",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Joshua Saxe",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Junteng Jia",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kalyan Vasuden Alwala",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Karthik Prasad",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kartikeya Upasani",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kate Plawiak",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ke Li",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kenneth Heafield",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kevin Stone",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Khalid El-Arini",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Krithika Iyer",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kshitiz Malik",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kuenley Chiu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kunal Bhalla",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kushal Lakhotia",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Lauren Rantala-Yeary",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Laurens van der Maaten",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Lawrence Chen",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Liang Tan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Liz Jenkins",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Louis Martin",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Lovish Madaan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Lubo Malo",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Lukas Blecher",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Lukas Landzaat",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Luke de Oliveira",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Madeline Muzzi",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Mahesh Pasupuleti",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Mannat Singh",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Manohar Paluri",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Marcin Kardas",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Maria Tsimpoukelli",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Mathew Oldham",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Mathieu Rita",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Maya Pavlova",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Melanie Kambadur",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Mike Lewis",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Min Si",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Mitesh Kumar Singh",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Mona Hassan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Naman Goyal",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Narjes Torabi",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Nikolay Bashlykov",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Nikolay Bogoychev",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Niladri Chatterji",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ning Zhang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Olivier Duchenne",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Onur Çelebi",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Patrick Alrassy",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Pengchuan Zhang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Pengwei Li",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Petar Vasic",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Peter Weng",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Prajjwal Bhargava",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Pratik Dubal",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Praveen Krishnan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Punit Singh Koura",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Puxin Xu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Qing He",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Qingxiao Dong",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ragavan Srinivasan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Raj Ganapathy",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ramon Calderer",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ricardo Silveira Cabral",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Robert Stojnic",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Roberta Raileanu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Rohan Maheswari",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Rohit Girdhar",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Rohit Patel",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Romain Sauvestre",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ronnie Polidoro",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Roshan Sumbaly",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ross Taylor",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ruan Silva",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Rui Hou",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Rui Wang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Saghar Hosseini",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sahana Chennabasappa",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sanjay Singh",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sean Bell",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Seohyun Sonia Kim",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sergey Edunov",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Shaoliang Nie",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sharan Narang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sharath Raparthy",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sheng Shen",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Shengye Wan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Shruti Bhosale",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Shun Zhang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Simon Vandenhende",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Soumya Batra",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Spencer Whitman",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sten Sootla",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Stephane Collot",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Suchin Gururangan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sydney Borodinsky",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Tamar Herman",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Tara Fowler",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Tarek Sheasha",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Thomas Georgiou",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Thomas Scialom",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Tobias Speckbacher",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Todor Mihaylov",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Tong Xiao",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ujjwal Karn",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Vedanuj Goswami",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Vibhor Gupta",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Vignesh Ramanathan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Viktor Kerkez",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Vincent Gonguet",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Virginie Do",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Vish Vogeti",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Vítor Albiero",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Vladan Petrovic",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Weiwei Chu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Wenhan Xiong",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Wenyin Fu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Whitney Meers",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Xavier Martinet",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Xiaodong Wang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Xiaofang Wang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Xiaoqing Ellen Tan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Xide Xia",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Xinfeng Xie",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Xuchao Jia",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Xuewei Wang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yaelle Goldschlag",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yashesh Gaur",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yasmine Babaei",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yi Wen",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yiwen Song",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yuchen Zhang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yue Li",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yuning Mao",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Zacharie Delpierre Coudert",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Zheng Yan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Zhengxing Chen",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Zoe Papakipos",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Aaditya Singh",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Aayushi Srivastava",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Abha Jain",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Adam Kelsey",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Adam Shajnfeld",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Adithya Gangidi",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Adolfo Victoria",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ahuva Goldstand",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ajay Menon",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ajay Sharma",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Alex Boesenberg",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Alexei Baevski",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Allie Feinstein",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Amanda Kallet",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Amit Sangani",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Amos Teo",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Anam Yunus",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Andrei Lupu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Andres Alvarado",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Andrew Caples",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Andrew Gu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Andrew Ho",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Andrew Poulton",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Andrew Ryan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ankit Ramchandani",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Annie Dong",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Annie Franco",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Anuj Goyal",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Aparajita Saraf",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Arkabandhu Chowdhury",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ashley Gabriel",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ashwin Bharambe",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Assaf Eisenman",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Azadeh Yazdan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Beau James",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ben Maurer",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Benjamin Leonhardi",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Bernie Huang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Beth Loyd",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Beto De Paola",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Bhargavi Paranjape",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Bing Liu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Bo Wu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Boyu Ni",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Braden Hancock",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Bram Wasti",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Brandon Spence",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Brani Stojkovic",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Brian Gamido",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Britt Montalvo",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Carl Parker",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Carly Burton",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Catalina Mejia",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ce Liu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Changhan Wang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Changkyu Kim",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Chao Zhou",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Chester Hu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ching-Hsiang Chu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Chris Cai",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Chris Tindal",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Christoph Feichtenhofer",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Cynthia Gao",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Damon Civin",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Dana Beaty",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Daniel Kreymer",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Daniel Li",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "David Adkins",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "David Xu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Davide Testuggine",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Delia David",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Devi Parikh",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Diana Liskovich",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Didem Foss",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Dingkang Wang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Duc Le",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Dustin Holland",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Edward Dowling",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Eissa Jamil",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Elaine Montgomery",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Eleonora Presani",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Emily Hahn",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Emily Wood",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Eric-Tuan Le",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Erik Brinkman",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Esteban Arcaute",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Evan Dunbar",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Evan Smothers",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Fei Sun",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Felix Kreuk",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Feng Tian",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Filippos Kokkinos",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Firat Ozgenel",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Francesco Caggioni",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Frank Kanayet",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Frank Seide",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Gabriela Medina Florez",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Gabriella Schwarz",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Gada Badeer",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Georgia Swee",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Gil Halpern",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Grant Herman",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Grigory Sizov",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Guangyi",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Zhang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Guna Lakshminarayanan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Hakan Inan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Hamid Shojanazeri",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Han Zou",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Hannah Wang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Hanwen Zha",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Haroun Habeeb",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Harrison Rudolph",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Helen Suk",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Henry Aspegren",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Hunter Goldman",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Hongyuan Zhan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ibrahim Damlaj",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Igor Molybog",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Igor Tufanov",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ilias Leontiadis",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Irina-Elena Veliche",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Itai Gat",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jake Weissman",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "James Geboski",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "James Kohli",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Janice Lam",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Japhet Asher",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jean-Baptiste Gaya",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jeff Marcus",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jeff Tang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jennifer Chan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jenny Zhen",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jeremy Reizenstein",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jeremy Teboul",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jessica Zhong",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jian Jin",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jingyi Yang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Joe Cummings",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jon Carvill",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jon Shepard",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jonathan McPhie",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jonathan Torres",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Josh Ginsburg",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Junjie Wang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kai Wu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kam Hou U",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Karan Saxena",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kartikay Khandelwal",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Katayoun Zand",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kathy Matosich",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kaushik Veeraraghavan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kelly Michelena",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Keqian Li",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kiran Jagadeesh",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kun Huang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kunal Chawla",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kyle Huang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Lailin Chen",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Lakshya Garg",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Lavender A",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Leandro Silva",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Lee Bell",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Lei Zhang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Liangpeng Guo",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Licheng Yu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Liron Moshkovich",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Luca Wehrstedt",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Madian Khabsa",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Manav Avalani",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Manish Bhatt",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Martynas Mankus",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Matan Hasson",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Matthew Lennie",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Matthias Reso",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Maxim Groshev",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Maxim Naumov",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Maya Lathi",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Meghan Keneally",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Miao Liu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Michael L. Seltzer",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Michal Valko",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Michelle Restrepo",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Mihir Patel",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Mik Vyatskov",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Mikayel Samvelyan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Mike Clark",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Mike Macey",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Mike Wang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Miquel Jubert Hermoso",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Mo Metanat",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Mohammad Rastegari",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Munish Bansal",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Nandhini Santhanam",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Natascha Parks",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Natasha White",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Navyata Bawa",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Nayan Singhal",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Nick Egebo",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Nicolas Usunier",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Nikhil Mehta",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Nikolay Pavlovich Laptev",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ning Dong",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Norman Cheng",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Oleg Chernoguz",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Olivia Hart",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Omkar Salpekar",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ozlem Kalinli",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Parkin Kent",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Parth Parekh",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Paul Saab",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Pavan Balaji",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Pedro Rittner",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Philip Bontrager",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Pierre Roux",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Piotr Dollar",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Polina Zvyagina",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Prashant Ratanchandani",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Pritish Yuvraj",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Qian Liang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Rachad Alao",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Rachel Rodriguez",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Rafi Ayub",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Raghotham Murthy",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Raghu Nayani",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Rahul Mitra",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Rangaprabhu Parthasarathy",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Raymond Li",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Rebekkah Hogan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Robin Battey",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Rocky Wang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Russ Howes",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ruty Rinott",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sachin Mehta",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sachin Siby",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sai Jayesh Bondu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Samyak Datta",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sara Chugh",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sara Hunt",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sargun Dhillon",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sasha Sidorov",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Satadru Pan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Saurabh Mahajan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Saurabh Verma",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Seiji Yamamoto",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sharadh Ramaswamy",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Shaun Lindsay",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Shaun Lindsay",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sheng Feng",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Shenghao Lin",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Shengxin Cindy Zha",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Shishir Patil",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Shiva Shankar",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Shuqiang Zhang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Shuqiang Zhang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sinong Wang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sneha Agarwal",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Soji Sajuyigbe",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Soumith Chintala",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Stephanie Max",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Stephen Chen",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Steve Kehoe",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Steve Satterfield",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sudarshan Govindaprasad",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sumit Gupta",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Summer Deng",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sungmin Cho",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sunny Virk",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Suraj Subramanian",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sy Choudhury",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sydney Goldman",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Tal Remez",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Tamar Glaser",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Tamara Best",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Thilo Koehler",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Thomas Robinson",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Tianhe Li",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Tianjun Zhang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Tim Matthews",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Timothy Chou",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Tzook Shaked",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Varun Vontimitta",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Victoria Ajayi",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Victoria Montanez",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Vijai Mohan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Vinay Satish Kumar",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Vishal Mangla",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Vlad Ionescu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Vlad Poenaru",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Vlad Tiberiu Mihailescu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Vladimir Ivanov",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Wei Li",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Wenchen Wang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Wenwen Jiang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Wes Bouaziz",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Will Constable",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Xiaocheng Tang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Xiaojian Wu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Xiaolan Wang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Xilun Wu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Xinbo Gao",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yaniv Kleinman",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yanjun Chen",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ye Hu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ye Jia",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ye Qi",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yenda Li",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yilin Zhang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ying Zhang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yossi Adi",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Youngjin Nam",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Wang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yu Zhao",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yuchen Hao",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yundi Qian",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yunlu Li",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yuzi He",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Zach Rait",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Zachary DeVito",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Zef Rosnbrick",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Zhaoduo Wen",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Zhenyu Yang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Zhiwei Zhao",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Zhiyu Ma",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Tianyi Li",
        "relation": "author_of",
        "tail": "A Survey on Diffusion Language Models"
      },
      {
        "head": "Mingda Chen",
        "relation": "author_of",
        "tail": "A Survey on Diffusion Language Models"
      },
      {
        "head": "Bowei Guo",
        "relation": "author_of",
        "tail": "A Survey on Diffusion Language Models"
      },
      {
        "head": "Zhiqiang Shen",
        "relation": "author_of",
        "tail": "A Survey on Diffusion Language Models"
      },
      {
        "head": "Jinjie Ni",
        "relation": "author_of",
        "tail": "Training Optimal Large Diffusion Language Models"
      },
      {
        "head": "Qian Liu",
        "relation": "author_of",
        "tail": "Training Optimal Large Diffusion Language Models"
      },
      {
        "head": "Chao Du",
        "relation": "author_of",
        "tail": "Training Optimal Large Diffusion Language Models"
      },
      {
        "head": "Longxu Dou",
        "relation": "author_of",
        "tail": "Training Optimal Large Diffusion Language Models"
      },
      {
        "head": "Hang Yan",
        "relation": "author_of",
        "tail": "Training Optimal Large Diffusion Language Models"
      },
      {
        "head": "Zili Wang",
        "relation": "author_of",
        "tail": "Training Optimal Large Diffusion Language Models"
      },
      {
        "head": "Tianyu Pang",
        "relation": "author_of",
        "tail": "Training Optimal Large Diffusion Language Models"
      },
      {
        "head": "Michael Qizhe Shieh",
        "relation": "author_of",
        "tail": "Training Optimal Large Diffusion Language Models"
      },
      {
        "head": "Siyan Zhao",
        "relation": "author_of",
        "tail": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models"
      },
      {
        "head": "Mengchen Liu",
        "relation": "author_of",
        "tail": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models"
      },
      {
        "head": "Jing Huang",
        "relation": "author_of",
        "tail": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models"
      },
      {
        "head": "Miao Liu",
        "relation": "author_of",
        "tail": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models"
      },
      {
        "head": "Chenyu Wang",
        "relation": "author_of",
        "tail": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models"
      },
      {
        "head": "Bo Liu",
        "relation": "author_of",
        "tail": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models"
      },
      {
        "head": "Yuandong Tian",
        "relation": "author_of",
        "tail": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models"
      },
      {
        "head": "Guan Pang",
        "relation": "author_of",
        "tail": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models"
      },
      {
        "head": "Sean Bell",
        "relation": "author_of",
        "tail": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models"
      },
      {
        "head": "Aditya Grover",
        "relation": "author_of",
        "tail": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models"
      },
      {
        "head": "Feiyu Chen",
        "relation": "author_of",
        "tail": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models"
      },
      {
        "head": "Marianne Arriola",
        "relation": "author_of",
        "tail": "Encoder-Decoder Diffusion Language Models for Efficient Training and Inference"
      },
      {
        "head": "Yair Schiff",
        "relation": "author_of",
        "tail": "Encoder-Decoder Diffusion Language Models for Efficient Training and Inference"
      },
      {
        "head": "Hao Phung",
        "relation": "author_of",
        "tail": "Encoder-Decoder Diffusion Language Models for Efficient Training and Inference"
      },
      {
        "head": "Aaron Gokaslan",
        "relation": "author_of",
        "tail": "Encoder-Decoder Diffusion Language Models for Efficient Training and Inference"
      },
      {
        "head": "Volodymyr Kuleshov",
        "relation": "author_of",
        "tail": "Encoder-Decoder Diffusion Language Models for Efficient Training and Inference"
      },
      {
        "head": "Chenghao Fan",
        "relation": "author_of",
        "tail": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model"
      },
      {
        "head": "Wen Heng",
        "relation": "author_of",
        "tail": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model"
      },
      {
        "head": "Bo Li",
        "relation": "author_of",
        "tail": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model"
      },
      {
        "head": "Sichen Liu",
        "relation": "author_of",
        "tail": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model"
      },
      {
        "head": "Yuxuan Song",
        "relation": "author_of",
        "tail": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model"
      },
      {
        "head": "Jing Su",
        "relation": "author_of",
        "tail": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model"
      },
      {
        "head": "Xiaoye Qu",
        "relation": "author_of",
        "tail": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model"
      },
      {
        "head": "Kai Shen",
        "relation": "author_of",
        "tail": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model"
      },
      {
        "head": "Wei Wei",
        "relation": "author_of",
        "tail": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model"
      },
      {
        "head": "Chang Yang",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Chuang Zhou",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Yilin Xiao",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Su Dong",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Luyao Zhuang",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Yujing Zhang",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Zhu Wang",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Zijin Hong",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Zheng Yuan",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Zhishang Xiang",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Shengyuan Chen",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Huachi Zhou",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Qinggang Zhang",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Ninghao Liu",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Jinsong Su",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Xinrun Wang",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Yi Chang",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Xiao Huang",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Hao Lu",
        "relation": "author_of",
        "tail": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search"
      },
      {
        "head": "Haoyuan Huang",
        "relation": "author_of",
        "tail": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search"
      },
      {
        "head": "Yulin Zhou",
        "relation": "author_of",
        "tail": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search"
      },
      {
        "head": "Chen Li",
        "relation": "author_of",
        "tail": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search"
      },
      {
        "head": "Ningxin Zhu",
        "relation": "author_of",
        "tail": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search"
      },
      {
        "head": "Yu Cheng",
        "relation": "author_of",
        "tail": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking"
      },
      {
        "head": "Jiuan Zhou",
        "relation": "author_of",
        "tail": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking"
      },
      {
        "head": "Yongkang Hu",
        "relation": "author_of",
        "tail": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking"
      },
      {
        "head": "Yihang Chen",
        "relation": "author_of",
        "tail": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking"
      },
      {
        "head": "Huichi Zhou",
        "relation": "author_of",
        "tail": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking"
      },
      {
        "head": "Mingang Chen",
        "relation": "author_of",
        "tail": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking"
      },
      {
        "head": "Zhizhong Zhang",
        "relation": "author_of",
        "tail": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking"
      },
      {
        "head": "Kun Shao",
        "relation": "author_of",
        "tail": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking"
      },
      {
        "head": "Yuan Xie",
        "relation": "author_of",
        "tail": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking"
      },
      {
        "head": "Zhaoxia Yin",
        "relation": "author_of",
        "tail": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking"
      },
      {
        "head": "Qirui Mi",
        "relation": "author_of",
        "tail": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents"
      },
      {
        "head": "Zhijian Ma",
        "relation": "author_of",
        "tail": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents"
      },
      {
        "head": "Mengyue Yang",
        "relation": "author_of",
        "tail": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents"
      },
      {
        "head": "Haoxuan Li",
        "relation": "author_of",
        "tail": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents"
      },
      {
        "head": "Yisen Wang",
        "relation": "author_of",
        "tail": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents"
      },
      {
        "head": "Haifeng Zhang",
        "relation": "author_of",
        "tail": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents"
      },
      {
        "head": "Jun Wang",
        "relation": "author_of",
        "tail": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents"
      },
      {
        "head": "Mingyue Cheng",
        "relation": "author_of",
        "tail": "Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting"
      },
      {
        "head": "Xiaoyu Tao",
        "relation": "author_of",
        "tail": "Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting"
      },
      {
        "head": "Qi Liu",
        "relation": "author_of",
        "tail": "Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting"
      },
      {
        "head": "Ze Guo",
        "relation": "author_of",
        "tail": "Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting"
      },
      {
        "head": "Enhong Chen",
        "relation": "author_of",
        "tail": "Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting"
      },
      {
        "head": "Long Ouyang",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Jeff Wu",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Xu Jiang",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Diogo Almeida",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Carroll L. Wainwright",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Pamela Mishkin",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Chong Zhang",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Sandhini Agarwal",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Katarina Slama",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Alex Ray",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "John Schulman",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Jacob Hilton",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Fraser Kelton",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Luke Miller",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Maddie Simens",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Amanda Askell",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Peter Welinder",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Paul Christiano",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Jan Leike",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Ryan Lowe",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Aakanksha Chowdhery",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Sharan Narang",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Jacob Devlin",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Maarten Bosma",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Gaurav Mishra",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Adam Roberts",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Paul Barham",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Hyung Won Chung",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Charles Sutton",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Sebastian Gehrmann",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Parker Schuh",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Kensen Shi",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Sasha Tsvyashchenko",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Joshua Maynez",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Abhishek Rao",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Parker Barnes",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Yi Tay",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Noam Shazeer",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Vinodkumar Prabhakaran",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Emily Reif",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Nan Du",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Ben Hutchinson",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Reiner Pope",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "James Bradbury",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Jacob Austin",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Michael Isard",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Guy Gur-Ari",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Pengcheng Yin",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Toju Duke",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Anselm Levskaya",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Sanjay Ghemawat",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Sunipa Dev",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Henryk Michalewski",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Xavier Garcia",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Vedant Misra",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Kevin Robinson",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Liam Fedus",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Denny Zhou",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Daphne Ippolito",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "David Luan",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Hyeontaek Lim",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Barret Zoph",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Alexander Spiridonov",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Ryan Sepassi",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "David Dohan",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Shivani Agrawal",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Mark Omernick",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Andrew M. Dai",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Thanumalayan Sankaranarayana Pillai",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Marie Pellat",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Aitor Lewkowycz",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Erica Moreira",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Rewon Child",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Oleksandr Polozov",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Katherine Lee",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Zongwei Zhou",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Xuezhi Wang",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Brennan Saeta",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Mark Diaz",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Orhan Firat",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Michele Catasta",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Jason Wei",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Kathy Meier-Hellstern",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Douglas Eck",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Jeff Dean",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Slav Petrov",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Noah Fiedel",
        "relation": "author_of",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Shunyu Yao",
        "relation": "author_of",
        "tail": "ReAct: Synergizing Reasoning and Acting in Language Models"
      },
      {
        "head": "Jeffrey Zhao",
        "relation": "author_of",
        "tail": "ReAct: Synergizing Reasoning and Acting in Language Models"
      },
      {
        "head": "Dian Yu",
        "relation": "author_of",
        "tail": "ReAct: Synergizing Reasoning and Acting in Language Models"
      },
      {
        "head": "Nan Du",
        "relation": "author_of",
        "tail": "ReAct: Synergizing Reasoning and Acting in Language Models"
      },
      {
        "head": "Izhak Shafran",
        "relation": "author_of",
        "tail": "ReAct: Synergizing Reasoning and Acting in Language Models"
      },
      {
        "head": "Karthik Narasimhan",
        "relation": "author_of",
        "tail": "ReAct: Synergizing Reasoning and Acting in Language Models"
      },
      {
        "head": "Yuan Cao",
        "relation": "author_of",
        "tail": "ReAct: Synergizing Reasoning and Acting in Language Models"
      },
      {
        "head": "Zhong-Zhi Li",
        "relation": "author_of",
        "tail": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
      },
      {
        "head": "Duzhen Zhang",
        "relation": "author_of",
        "tail": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
      },
      {
        "head": "Ming-Liang Zhang",
        "relation": "author_of",
        "tail": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
      },
      {
        "head": "Jiaxin Zhang",
        "relation": "author_of",
        "tail": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
      },
      {
        "head": "Zengyan Liu",
        "relation": "author_of",
        "tail": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
      },
      {
        "head": "Yuxuan Yao",
        "relation": "author_of",
        "tail": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
      },
      {
        "head": "Haotian Xu",
        "relation": "author_of",
        "tail": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
      },
      {
        "head": "Junhao Zheng",
        "relation": "author_of",
        "tail": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
      },
      {
        "head": "Pei-Jie Wang",
        "relation": "author_of",
        "tail": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
      },
      {
        "head": "Xiuyi Chen",
        "relation": "author_of",
        "tail": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
      },
      {
        "head": "Yingying Zhang",
        "relation": "author_of",
        "tail": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
      },
      {
        "head": "Fei Yin",
        "relation": "author_of",
        "tail": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
      },
      {
        "head": "Jiahua Dong",
        "relation": "author_of",
        "tail": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
      },
      {
        "head": "Zhiwei Li",
        "relation": "author_of",
        "tail": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
      },
      {
        "head": "Bao-Long Bi",
        "relation": "author_of",
        "tail": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
      },
      {
        "head": "Ling-Rui Mei",
        "relation": "author_of",
        "tail": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
      },
      {
        "head": "Junfeng Fang",
        "relation": "author_of",
        "tail": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
      },
      {
        "head": "Xiao Liang",
        "relation": "author_of",
        "tail": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
      },
      {
        "head": "Zhijiang Guo",
        "relation": "author_of",
        "tail": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
      },
      {
        "head": "Le Song",
        "relation": "author_of",
        "tail": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
      },
      {
        "head": "Cheng-Lin Liu",
        "relation": "author_of",
        "tail": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
      },
      {
        "head": "Yuxuan Huang",
        "relation": "author_of",
        "tail": "Deep Research Agents: A Systematic Examination And Roadmap"
      },
      {
        "head": "Yihang Chen",
        "relation": "author_of",
        "tail": "Deep Research Agents: A Systematic Examination And Roadmap"
      },
      {
        "head": "Haozheng Zhang",
        "relation": "author_of",
        "tail": "Deep Research Agents: A Systematic Examination And Roadmap"
      },
      {
        "head": "Kang Li",
        "relation": "author_of",
        "tail": "Deep Research Agents: A Systematic Examination And Roadmap"
      },
      {
        "head": "Huichi Zhou",
        "relation": "author_of",
        "tail": "Deep Research Agents: A Systematic Examination And Roadmap"
      },
      {
        "head": "Meng Fang",
        "relation": "author_of",
        "tail": "Deep Research Agents: A Systematic Examination And Roadmap"
      },
      {
        "head": "Linyi Yang",
        "relation": "author_of",
        "tail": "Deep Research Agents: A Systematic Examination And Roadmap"
      },
      {
        "head": "Xiaoguang Li",
        "relation": "author_of",
        "tail": "Deep Research Agents: A Systematic Examination And Roadmap"
      },
      {
        "head": "Lifeng Shang",
        "relation": "author_of",
        "tail": "Deep Research Agents: A Systematic Examination And Roadmap"
      },
      {
        "head": "Songcen Xu",
        "relation": "author_of",
        "tail": "Deep Research Agents: A Systematic Examination And Roadmap"
      },
      {
        "head": "Jianye Hao",
        "relation": "author_of",
        "tail": "Deep Research Agents: A Systematic Examination And Roadmap"
      },
      {
        "head": "Kun Shao",
        "relation": "author_of",
        "tail": "Deep Research Agents: A Systematic Examination And Roadmap"
      },
      {
        "head": "Jun Wang",
        "relation": "author_of",
        "tail": "Deep Research Agents: A Systematic Examination And Roadmap"
      },
      {
        "head": "Zheyuan Yang",
        "relation": "author_of",
        "tail": "Table-R1: Inference-Time Scaling for Table Reasoning"
      },
      {
        "head": "Lyuhao Chen",
        "relation": "author_of",
        "tail": "Table-R1: Inference-Time Scaling for Table Reasoning"
      },
      {
        "head": "Arman Cohan",
        "relation": "author_of",
        "tail": "Table-R1: Inference-Time Scaling for Table Reasoning"
      },
      {
        "head": "Yilun Zhao",
        "relation": "author_of",
        "tail": "Table-R1: Inference-Time Scaling for Table Reasoning"
      },
      {
        "head": "Mingyue Cheng",
        "relation": "author_of",
        "tail": "Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis"
      },
      {
        "head": "Daoyu Wang",
        "relation": "author_of",
        "tail": "Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis"
      },
      {
        "head": "Qi Liu",
        "relation": "author_of",
        "tail": "Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis"
      },
      {
        "head": "Shuo Yu",
        "relation": "author_of",
        "tail": "Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis"
      },
      {
        "head": "Xiaoyu Tao",
        "relation": "author_of",
        "tail": "Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis"
      },
      {
        "head": "Yuqian Wang",
        "relation": "author_of",
        "tail": "Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis"
      },
      {
        "head": "Chengzhong Chu",
        "relation": "author_of",
        "tail": "Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis"
      },
      {
        "head": "Yu Duan",
        "relation": "author_of",
        "tail": "Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis"
      },
      {
        "head": "Mingkang Long",
        "relation": "author_of",
        "tail": "Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis"
      },
      {
        "head": "Enhong Chen",
        "relation": "author_of",
        "tail": "Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis"
      },
      {
        "head": "Tingyue Pan",
        "relation": "author_of",
        "tail": "PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware Sequence-Level Policy Optimization"
      },
      {
        "head": "Jie Ouyang",
        "relation": "author_of",
        "tail": "PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware Sequence-Level Policy Optimization"
      },
      {
        "head": "Mingyue Cheng",
        "relation": "author_of",
        "tail": "PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware Sequence-Level Policy Optimization"
      },
      {
        "head": "Qingchuan Li",
        "relation": "author_of",
        "tail": "PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware Sequence-Level Policy Optimization"
      },
      {
        "head": "Zirui Liu",
        "relation": "author_of",
        "tail": "PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware Sequence-Level Policy Optimization"
      },
      {
        "head": "Mingfan Pan",
        "relation": "author_of",
        "tail": "PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware Sequence-Level Policy Optimization"
      },
      {
        "head": "Shuo Yu",
        "relation": "author_of",
        "tail": "PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware Sequence-Level Policy Optimization"
      },
      {
        "head": "Qi Liu",
        "relation": "author_of",
        "tail": "PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware Sequence-Level Policy Optimization"
      },
      {
        "head": "Jinmiao Zhao",
        "relation": "author_of",
        "tail": "Gradient-Guided Learning Network for Infrared Small Target Detection"
      },
      {
        "head": "Chuang Yu",
        "relation": "author_of",
        "tail": "Gradient-Guided Learning Network for Infrared Small Target Detection"
      },
      {
        "head": "Zelin Shi",
        "relation": "author_of",
        "tail": "Gradient-Guided Learning Network for Infrared Small Target Detection"
      },
      {
        "head": "Yunpeng Liu",
        "relation": "author_of",
        "tail": "Gradient-Guided Learning Network for Infrared Small Target Detection"
      },
      {
        "head": "Yingdi Zhang",
        "relation": "author_of",
        "tail": "Gradient-Guided Learning Network for Infrared Small Target Detection"
      },
      {
        "head": "Waleed Khalid",
        "relation": "author_of",
        "tail": "A Retrieval-Augmented Generation Approach to Extracting Algorithmic Logic from Neural Networks"
      },
      {
        "head": "Dmitry Ignatov",
        "relation": "author_of",
        "tail": "A Retrieval-Augmented Generation Approach to Extracting Algorithmic Logic from Neural Networks"
      },
      {
        "head": "Radu Timofte",
        "relation": "author_of",
        "tail": "A Retrieval-Augmented Generation Approach to Extracting Algorithmic Logic from Neural Networks"
      },
      {
        "head": "Yu Tian",
        "relation": "author_of",
        "tail": "CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation"
      },
      {
        "head": "Zhongheng Yang",
        "relation": "author_of",
        "tail": "CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation"
      },
      {
        "head": "Chenshi Liu",
        "relation": "author_of",
        "tail": "CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation"
      },
      {
        "head": "Yiyun Su",
        "relation": "author_of",
        "tail": "CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation"
      },
      {
        "head": "Ziwei Hong",
        "relation": "author_of",
        "tail": "CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation"
      },
      {
        "head": "Zexi Gong",
        "relation": "author_of",
        "tail": "CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation"
      },
      {
        "head": "Jingyuan Xu",
        "relation": "author_of",
        "tail": "CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation"
      },
      {
        "head": "Yang Lu",
        "relation": "author_of",
        "tail": "IMobileTransformer: A fusion-based lightweight model for rice disease identification"
      },
      {
        "head": "Haoyang Zhou",
        "relation": "author_of",
        "tail": "IMobileTransformer: A fusion-based lightweight model for rice disease identification"
      },
      {
        "head": "Peng Wang",
        "relation": "author_of",
        "tail": "IMobileTransformer: A fusion-based lightweight model for rice disease identification"
      },
      {
        "head": "Erzhi Wang",
        "relation": "author_of",
        "tail": "IMobileTransformer: A fusion-based lightweight model for rice disease identification"
      },
      {
        "head": "Gongfa Li",
        "relation": "author_of",
        "tail": "IMobileTransformer: A fusion-based lightweight model for rice disease identification"
      },
      {
        "head": "Tongjian Yu",
        "relation": "author_of",
        "tail": "IMobileTransformer: A fusion-based lightweight model for rice disease identification"
      },
      {
        "head": "Isaac Robinson",
        "relation": "author_of",
        "tail": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers"
      },
      {
        "head": "Peter Robicheaux",
        "relation": "author_of",
        "tail": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers"
      },
      {
        "head": "Matvei Popov",
        "relation": "author_of",
        "tail": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers"
      },
      {
        "head": "Deva Ramanan",
        "relation": "author_of",
        "tail": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers"
      },
      {
        "head": "Neehar Peri",
        "relation": "author_of",
        "tail": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers"
      },
      {
        "head": "Jinhui Yi",
        "relation": "author_of",
        "tail": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images"
      },
      {
        "head": "Gina Lopez",
        "relation": "author_of",
        "tail": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images"
      },
      {
        "head": "S. Hadir",
        "relation": "author_of",
        "tail": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images"
      },
      {
        "head": "Jan Weyler",
        "relation": "author_of",
        "tail": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images"
      },
      {
        "head": "Lasse Klingbeil",
        "relation": "author_of",
        "tail": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images"
      },
      {
        "head": "Marion Deichmann",
        "relation": "author_of",
        "tail": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images"
      },
      {
        "head": "Juergen Gall",
        "relation": "author_of",
        "tail": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images"
      },
      {
        "head": "S. J. Seidel",
        "relation": "author_of",
        "tail": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images"
      },
      {
        "head": "Haoding Xu",
        "relation": "author_of",
        "tail": "Deep learning models for efficient geotechnical predictions: reducing training effort and data requirements with transfer learning"
      },
      {
        "head": "Xuzhen He",
        "relation": "author_of",
        "tail": "Deep learning models for efficient geotechnical predictions: reducing training effort and data requirements with transfer learning"
      },
      {
        "head": "Shaoheng Dai",
        "relation": "author_of",
        "tail": "Deep learning models for efficient geotechnical predictions: reducing training effort and data requirements with transfer learning"
      },
      {
        "head": "Caihui Zhu",
        "relation": "author_of",
        "tail": "Deep learning models for efficient geotechnical predictions: reducing training effort and data requirements with transfer learning"
      },
      {
        "head": "F. Shan",
        "relation": "author_of",
        "tail": "Deep learning models for efficient geotechnical predictions: reducing training effort and data requirements with transfer learning"
      },
      {
        "head": "Qin Zhao",
        "relation": "author_of",
        "tail": "Deep learning models for efficient geotechnical predictions: reducing training effort and data requirements with transfer learning"
      },
      {
        "head": "Faning Dang",
        "relation": "author_of",
        "tail": "Deep learning models for efficient geotechnical predictions: reducing training effort and data requirements with transfer learning"
      },
      {
        "head": "Daichao Sheng",
        "relation": "author_of",
        "tail": "Deep learning models for efficient geotechnical predictions: reducing training effort and data requirements with transfer learning"
      },
      {
        "head": "Zhou Wang",
        "relation": "author_of",
        "tail": "Image quality assessment: from error visibility to structural similarity"
      },
      {
        "head": "A. Bovik",
        "relation": "author_of",
        "tail": "Image quality assessment: from error visibility to structural similarity"
      },
      {
        "head": "H. Sheikh",
        "relation": "author_of",
        "tail": "Image quality assessment: from error visibility to structural similarity"
      },
      {
        "head": "Eero P. Simoncelli",
        "relation": "author_of",
        "tail": "Image quality assessment: from error visibility to structural similarity"
      },
      {
        "head": "Wei Cao",
        "relation": "author_of",
        "tail": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction"
      },
      {
        "head": "Hao Zhang",
        "relation": "author_of",
        "tail": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction"
      },
      {
        "head": "Fengrui Tian",
        "relation": "author_of",
        "tail": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction"
      },
      {
        "head": "Yulun Wu",
        "relation": "author_of",
        "tail": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction"
      },
      {
        "head": "Yingying Li",
        "relation": "author_of",
        "tail": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction"
      },
      {
        "head": "Shenlong Wang",
        "relation": "author_of",
        "tail": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction"
      },
      {
        "head": "Ning Yu",
        "relation": "author_of",
        "tail": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction"
      },
      {
        "head": "Yaoyao Liu",
        "relation": "author_of",
        "tail": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction"
      },
      {
        "head": "Yuxue Yang",
        "relation": "author_of",
        "tail": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos"
      },
      {
        "head": "Lue Fan",
        "relation": "author_of",
        "tail": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos"
      },
      {
        "head": "Ziqi Shi",
        "relation": "author_of",
        "tail": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos"
      },
      {
        "head": "Junran Peng",
        "relation": "author_of",
        "tail": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos"
      },
      {
        "head": "Feng Wang",
        "relation": "author_of",
        "tail": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos"
      },
      {
        "head": "Zhaoxiang Zhang",
        "relation": "author_of",
        "tail": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos"
      },
      {
        "head": "William Peebles",
        "relation": "author_of",
        "tail": "Scalable Diffusion Models with Transformers"
      },
      {
        "head": "Saining Xie",
        "relation": "author_of",
        "tail": "Scalable Diffusion Models with Transformers"
      },
      {
        "head": "Jianlin Su",
        "relation": "author_of",
        "tail": "RoFormer: Enhanced Transformer with Rotary Position Embedding"
      },
      {
        "head": "Yu Lu",
        "relation": "author_of",
        "tail": "RoFormer: Enhanced Transformer with Rotary Position Embedding"
      },
      {
        "head": "Shengfeng Pan",
        "relation": "author_of",
        "tail": "RoFormer: Enhanced Transformer with Rotary Position Embedding"
      },
      {
        "head": "Ahmed Murtadha",
        "relation": "author_of",
        "tail": "RoFormer: Enhanced Transformer with Rotary Position Embedding"
      },
      {
        "head": "Bo Wen",
        "relation": "author_of",
        "tail": "RoFormer: Enhanced Transformer with Rotary Position Embedding"
      },
      {
        "head": "Yunfeng Liu",
        "relation": "author_of",
        "tail": "RoFormer: Enhanced Transformer with Rotary Position Embedding"
      },
      {
        "head": "Sifan Tu",
        "relation": "author_of",
        "tail": "The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey"
      },
      {
        "head": "Xin Zhou",
        "relation": "author_of",
        "tail": "The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey"
      },
      {
        "head": "Dingkang Liang",
        "relation": "author_of",
        "tail": "The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey"
      },
      {
        "head": "Xingyu Jiang",
        "relation": "author_of",
        "tail": "The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey"
      },
      {
        "head": "Yumeng Zhang",
        "relation": "author_of",
        "tail": "The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey"
      },
      {
        "head": "Xiaofan Li",
        "relation": "author_of",
        "tail": "The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey"
      },
      {
        "head": "Xiang Bai",
        "relation": "author_of",
        "tail": "The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey"
      },
      {
        "head": "Andreas Geiger",
        "relation": "author_of",
        "tail": "Vision meets robotics: The KITTI dataset"
      },
      {
        "head": "Philip Lenz",
        "relation": "author_of",
        "tail": "Vision meets robotics: The KITTI dataset"
      },
      {
        "head": "C. Stiller",
        "relation": "author_of",
        "tail": "Vision meets robotics: The KITTI dataset"
      },
      {
        "head": "R. Urtasun",
        "relation": "author_of",
        "tail": "Vision meets robotics: The KITTI dataset"
      },
      {
        "head": "S. Umeyama",
        "relation": "author_of",
        "tail": "Least-Squares Estimation of Transformation Parameters Between Two Point Patterns"
      },
      {
        "head": "Run Wang",
        "relation": "author_of",
        "tail": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations"
      },
      {
        "head": "Chaoyi Zhou",
        "relation": "author_of",
        "tail": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations"
      },
      {
        "head": "Amir Salarpour",
        "relation": "author_of",
        "tail": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations"
      },
      {
        "head": "Xi Liu",
        "relation": "author_of",
        "tail": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations"
      },
      {
        "head": "Zhi-Qi Cheng",
        "relation": "author_of",
        "tail": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations"
      },
      {
        "head": "Feng Luo",
        "relation": "author_of",
        "tail": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations"
      },
      {
        "head": "Mert D. Pesé",
        "relation": "author_of",
        "tail": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations"
      },
      {
        "head": "Siyu Huang",
        "relation": "author_of",
        "tail": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations"
      },
      {
        "head": "Xingbang Hao",
        "relation": "author_of",
        "tail": "Deep Learning"
      },
      {
        "head": "Guigang Zhang",
        "relation": "author_of",
        "tail": "Deep Learning"
      },
      {
        "head": "Shang Ma",
        "relation": "author_of",
        "tail": "Deep Learning"
      },
      {
        "head": "Nikhil Keetha",
        "relation": "author_of",
        "tail": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction"
      },
      {
        "head": "Norman Müller",
        "relation": "author_of",
        "tail": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction"
      },
      {
        "head": "Johannes Schönberger",
        "relation": "author_of",
        "tail": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction"
      },
      {
        "head": "Lorenzo Porzi",
        "relation": "author_of",
        "tail": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction"
      },
      {
        "head": "Yuchen Zhang",
        "relation": "author_of",
        "tail": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction"
      },
      {
        "head": "Tobias Fischer",
        "relation": "author_of",
        "tail": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction"
      },
      {
        "head": "Arno Knapitsch",
        "relation": "author_of",
        "tail": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction"
      },
      {
        "head": "Duncan Zauss",
        "relation": "author_of",
        "tail": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction"
      },
      {
        "head": "Ethan Weber",
        "relation": "author_of",
        "tail": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction"
      },
      {
        "head": "Nelson Antunes",
        "relation": "author_of",
        "tail": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction"
      },
      {
        "head": "Jonathon Luiten",
        "relation": "author_of",
        "tail": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction"
      },
      {
        "head": "Manuel Lopez-Antequera",
        "relation": "author_of",
        "tail": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction"
      },
      {
        "head": "Samuel Rota Bulò",
        "relation": "author_of",
        "tail": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction"
      },
      {
        "head": "Christian Richardt",
        "relation": "author_of",
        "tail": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction"
      },
      {
        "head": "Deva Ramanan",
        "relation": "author_of",
        "tail": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction"
      },
      {
        "head": "Sebastian Scherer",
        "relation": "author_of",
        "tail": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction"
      },
      {
        "head": "Peter Kontschieder",
        "relation": "author_of",
        "tail": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction"
      },
      {
        "head": "Team Seedream",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": ":",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Yunpeng Chen",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Yu Gao",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Lixue Gong",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Meng Guo",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Qiushan Guo",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Zhiyao Guo",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Xiaoxia Hou",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Weilin Huang",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Yixuan Huang",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Xiaowen Jian",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Huafeng Kuang",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Zhichao Lai",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Fanshi Li",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Liang Li",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Xiaochen Lian",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Chao Liao",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Liyang Liu",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Wei Liu",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Yanzuo Lu",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Zhengxiong Luo",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Tongtong Ou",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Guang Shi",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Yichun Shi",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Shiqi Sun",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Yu Tian",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Zhi Tian",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Peng Wang",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Rui Wang",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Xun Wang",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Ye Wang",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Guofeng Wu",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Jie Wu",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Wenxu Wu",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Yonghui Wu",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Xin Xia",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Xuefeng Xiao",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Shuang Xu",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Xin Yan",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Ceyuan Yang",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Jianchao Yang",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Zhonghua Zhai",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Chenlin Zhang",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Heng Zhang",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Qi Zhang",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Xinyu Zhang",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Yuwei Zhang",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Shijia Zhao",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Wenliang Zhao",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Wenjia Zhu",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Junyan Ye",
        "relation": "author_of",
        "tail": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation"
      },
      {
        "head": "Dongzhi Jiang",
        "relation": "author_of",
        "tail": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation"
      },
      {
        "head": "Zihao Wang",
        "relation": "author_of",
        "tail": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation"
      },
      {
        "head": "Leqi Zhu",
        "relation": "author_of",
        "tail": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation"
      },
      {
        "head": "Zhenghao Hu",
        "relation": "author_of",
        "tail": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation"
      },
      {
        "head": "Zilong Huang",
        "relation": "author_of",
        "tail": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation"
      },
      {
        "head": "Jun He",
        "relation": "author_of",
        "tail": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation"
      },
      {
        "head": "Zhiyuan Yan",
        "relation": "author_of",
        "tail": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation"
      },
      {
        "head": "Jinghua Yu",
        "relation": "author_of",
        "tail": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation"
      },
      {
        "head": "Hongsheng Li",
        "relation": "author_of",
        "tail": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation"
      },
      {
        "head": "Conghui He",
        "relation": "author_of",
        "tail": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation"
      },
      {
        "head": "Weijia Li",
        "relation": "author_of",
        "tail": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation"
      },
      {
        "head": "Yi Xin",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Qi Qin",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Siqi Luo",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Kaiwen Zhu",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Juncheng Yan",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Yan Tai",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Jiayi Lei",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Yuewen Cao",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Keqi Wang",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Yibin Wang",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Jinbin Bai",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Qian Yu",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Dengyang Jiang",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Yuandong Pu",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Haoxing Chen",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Le Zhuo",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Junjun He",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Gen Luo",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Tianbin Li",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Ming Hu",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Jin Ye",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Shenglong Ye",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Bo Zhang",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Chang Xu",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Wenhai Wang",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Hongsheng Li",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Guangtao Zhai",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Tianfan Xue",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Bin Fu",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Xiaohong Liu",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Yu Qiao",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Yihao Liu",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "NextStep Team",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Chunrui Han",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Guopeng Li",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Jingwei Wu",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Quan Sun",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Yan Cai",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Yuang Peng",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Zheng Ge",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Deyu Zhou",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Haomiao Tang",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Hongyu Zhou",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Kenkun Liu",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Ailin Huang",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Bin Wang",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Changxin Miao",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Deshan Sun",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "En Yu",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Fukun Yin",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Gang Yu",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Hao Nie",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Haoran Lv",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Hanpeng Hu",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Jia Wang",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Jian Zhou",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Jianjian Sun",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Kaijun Tan",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Kang An",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Kangheng Lin",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Liang Zhao",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Mei Chen",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Peng Xing",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Rui Wang",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Shiyu Liu",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Shutao Xia",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Tianhao You",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Wei Ji",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Xianfang Zeng",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Xin Han",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Xuelin Zhang",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Yana Wei",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Yanming Xu",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Yimin Jiang",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Yingming Wang",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Yu Zhou",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Yucheng Han",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Ziyang Meng",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Binxing Jiao",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Daxin Jiang",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Xiangyu Zhang",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Yibo Zhu",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Yinhan Liu",
        "relation": "author_of",
        "tail": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
      },
      {
        "head": "Myle Ott",
        "relation": "author_of",
        "tail": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
      },
      {
        "head": "Naman Goyal",
        "relation": "author_of",
        "tail": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
      },
      {
        "head": "Jingfei Du",
        "relation": "author_of",
        "tail": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
      },
      {
        "head": "Mandar Joshi",
        "relation": "author_of",
        "tail": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
      },
      {
        "head": "Danqi Chen",
        "relation": "author_of",
        "tail": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
      },
      {
        "head": "Omer Levy",
        "relation": "author_of",
        "tail": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
      },
      {
        "head": "Mike Lewis",
        "relation": "author_of",
        "tail": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
      },
      {
        "head": "Luke Zettlemoyer",
        "relation": "author_of",
        "tail": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
      },
      {
        "head": "Veselin Stoyanov",
        "relation": "author_of",
        "tail": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
      },
      {
        "head": "DeepSeek-AI",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Daya Guo",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Dejian Yang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Haowei Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Junxiao Song",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Peiyi Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Qihao Zhu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Runxin Xu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Ruoyu Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Shirong Ma",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xiao Bi",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xiaokang Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xingkai Yu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yu Wu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Z. F. Wu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zhibin Gou",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zhihong Shao",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zhuoshu Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Ziyi Gao",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Aixin Liu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Bing Xue",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Bingxuan Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Bochao Wu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Bei Feng",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Chengda Lu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Chenggang Zhao",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Chengqi Deng",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Chenyu Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Chong Ruan",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Damai Dai",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Deli Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Dongjie Ji",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Erhang Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Fangyun Lin",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Fucong Dai",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Fuli Luo",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Guangbo Hao",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Guanting Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Guowei Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "H. Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Han Bao",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Hanwei Xu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Haocheng Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Honghui Ding",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Huajian Xin",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Huazuo Gao",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Hui Qu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Hui Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Jianzhong Guo",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Jiashi Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Jiawei Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Jingchang Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Jingyang Yuan",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Junjie Qiu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Junlong Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "J. L. Cai",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Jiaqi Ni",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Jian Liang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Jin Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Kai Dong",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Kai Hu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Kaige Gao",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Kang Guan",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Kexin Huang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Kuai Yu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Lean Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Lecong Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Liang Zhao",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Litong Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Liyue Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Lei Xu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Leyi Xia",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Mingchuan Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Minghua Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Minghui Tang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Meng Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Miaojun Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Mingming Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Ning Tian",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Panpan Huang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Peng Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Qiancheng Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Qinyu Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Qiushi Du",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Ruiqi Ge",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Ruisong Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Ruizhe Pan",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Runji Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "R. J. Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "R. L. Jin",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Ruyi Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Shanghao Lu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Shangyan Zhou",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Shanhuang Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Shengfeng Ye",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Shiyu Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Shuiping Yu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Shunfeng Zhou",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Shuting Pan",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "S. S. Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Shuang Zhou",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Shaoqing Wu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Shengfeng Ye",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Tao Yun",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Tian Pei",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Tianyu Sun",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "T. Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Wangding Zeng",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Wanjia Zhao",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Wen Liu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Wenfeng Liang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Wenjun Gao",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Wenqin Yu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Wentao Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "W. L. Xiao",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Wei An",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xiaodong Liu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xiaohan Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xiaokang Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xiaotao Nie",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xin Cheng",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xin Liu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xin Xie",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xingchao Liu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xinyu Yang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xinyuan Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xuecheng Su",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xuheng Lin",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "X. Q. Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xiangyue Jin",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xiaojin Shen",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xiaosha Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xiaowen Sun",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xiaoxiang Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xinnan Song",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xinyi Zhou",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xianzu Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xinxia Shan",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Y. K. Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Y. Q. Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Y. X. Wei",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yang Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yanhong Xu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yao Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yao Zhao",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yaofeng Sun",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yaohui Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yi Yu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yichao Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yifan Shi",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yiliang Xiong",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Ying He",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yishi Piao",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yisong Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yixuan Tan",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yiyang Ma",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yiyuan Liu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yongqiang Guo",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yuan Ou",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yuduan Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yue Gong",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yuheng Zou",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yujia He",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yunfan Xiong",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yuxiang Luo",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yuxiang You",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yuxuan Liu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yuyang Zhou",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Y. X. Zhu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yanhong Xu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yanping Huang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yaohui Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yi Zheng",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yuchen Zhu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yunxian Ma",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Ying Tang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yukun Zha",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yuting Yan",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Z. Z. Ren",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zehui Ren",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zhangli Sha",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zhe Fu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zhean Xu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zhenda Xie",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zhengyan Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zhewen Hao",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zhicheng Ma",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zhigang Yan",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zhiyu Wu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zihui Gu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zijia Zhu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zijun Liu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zilin Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Ziwei Xie",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Ziyang Song",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zizheng Pan",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zhen Huang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zhipeng Xu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zhongyu Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zhen Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Bowen Jin",
        "relation": "author_of",
        "tail": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning"
      },
      {
        "head": "Hansi Zeng",
        "relation": "author_of",
        "tail": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning"
      },
      {
        "head": "Zhenrui Yue",
        "relation": "author_of",
        "tail": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning"
      },
      {
        "head": "Jinsung Yoon",
        "relation": "author_of",
        "tail": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning"
      },
      {
        "head": "Sercan Arik",
        "relation": "author_of",
        "tail": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning"
      },
      {
        "head": "Dong Wang",
        "relation": "author_of",
        "tail": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning"
      },
      {
        "head": "Hamed Zamani",
        "relation": "author_of",
        "tail": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning"
      },
      {
        "head": "Jiawei Han",
        "relation": "author_of",
        "tail": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning"
      },
      {
        "head": "Karan Singhal",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Tao Tu",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Juraj Gottweis",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "R. Sayres",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Ellery Wulczyn",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Mohamed Amin",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Le Hou",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Kevin Clark",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Stephen R. Pfohl",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Heather Cole-Lewis",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Darlene Neal",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Q. Rashid",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Mike Schaekermann",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Amy Wang",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Dev Dash",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Jonathan H. Chen",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Nigam H. Shah",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Sami Lachgar",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "P. Mansfield",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Sushant Prakash",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Bradley Green",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Ewa Dominowska",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Blaise Agüera y Arcas",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Nenad Tomašev",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Yun Liu",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Renee Wong",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Christopher Semturs",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "S. Mahdavi",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Joelle K. Barral",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Dale R. Webster",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "G. Corrado",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Yossi Matias",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Shekoofeh Azizi",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "A. Karthikesalingam",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Vivek Natarajan",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Tianzhe Chu",
        "relation": "author_of",
        "tail": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training"
      },
      {
        "head": "Yuexiang Zhai",
        "relation": "author_of",
        "tail": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training"
      },
      {
        "head": "Jihan Yang",
        "relation": "author_of",
        "tail": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training"
      },
      {
        "head": "Shengbang Tong",
        "relation": "author_of",
        "tail": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training"
      },
      {
        "head": "Saining Xie",
        "relation": "author_of",
        "tail": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training"
      },
      {
        "head": "Dale Schuurmans",
        "relation": "author_of",
        "tail": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training"
      },
      {
        "head": "Quoc V. Le",
        "relation": "author_of",
        "tail": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training"
      },
      {
        "head": "Sergey Levine",
        "relation": "author_of",
        "tail": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training"
      },
      {
        "head": "Yi Ma",
        "relation": "author_of",
        "tail": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training"
      },
      {
        "head": "Peijia Lin",
        "relation": "author_of",
        "tail": "Equivariant Diffusion for Crystal Structure Prediction"
      },
      {
        "head": "Pin Chen",
        "relation": "author_of",
        "tail": "Equivariant Diffusion for Crystal Structure Prediction"
      },
      {
        "head": "Rui Jiao",
        "relation": "author_of",
        "tail": "Equivariant Diffusion for Crystal Structure Prediction"
      },
      {
        "head": "Qing Mo",
        "relation": "author_of",
        "tail": "Equivariant Diffusion for Crystal Structure Prediction"
      },
      {
        "head": "Jianhuan Cen",
        "relation": "author_of",
        "tail": "Equivariant Diffusion for Crystal Structure Prediction"
      },
      {
        "head": "Wenbing Huang",
        "relation": "author_of",
        "tail": "Equivariant Diffusion for Crystal Structure Prediction"
      },
      {
        "head": "Yang Liu",
        "relation": "author_of",
        "tail": "Equivariant Diffusion for Crystal Structure Prediction"
      },
      {
        "head": "Dan Huang",
        "relation": "author_of",
        "tail": "Equivariant Diffusion for Crystal Structure Prediction"
      },
      {
        "head": "Yutong Lu",
        "relation": "author_of",
        "tail": "Equivariant Diffusion for Crystal Structure Prediction"
      },
      {
        "head": "Wenqiang Sun",
        "relation": "author_of",
        "tail": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling"
      },
      {
        "head": "Haiyu Zhang",
        "relation": "author_of",
        "tail": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling"
      },
      {
        "head": "Haoyuan Wang",
        "relation": "author_of",
        "tail": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling"
      },
      {
        "head": "Junta Wu",
        "relation": "author_of",
        "tail": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling"
      },
      {
        "head": "Zehan Wang",
        "relation": "author_of",
        "tail": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling"
      },
      {
        "head": "Zhenwei Wang",
        "relation": "author_of",
        "tail": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling"
      },
      {
        "head": "Yunhong Wang",
        "relation": "author_of",
        "tail": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling"
      },
      {
        "head": "Jun Zhang",
        "relation": "author_of",
        "tail": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling"
      },
      {
        "head": "Tengfei Wang",
        "relation": "author_of",
        "tail": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling"
      },
      {
        "head": "Chunchao Guo",
        "relation": "author_of",
        "tail": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling"
      },
      {
        "head": "Jianfeng Xiang",
        "relation": "author_of",
        "tail": "Native and Compact Structured Latents for 3D Generation"
      },
      {
        "head": "Xiaoxue Chen",
        "relation": "author_of",
        "tail": "Native and Compact Structured Latents for 3D Generation"
      },
      {
        "head": "Sicheng Xu",
        "relation": "author_of",
        "tail": "Native and Compact Structured Latents for 3D Generation"
      },
      {
        "head": "Ruicheng Wang",
        "relation": "author_of",
        "tail": "Native and Compact Structured Latents for 3D Generation"
      },
      {
        "head": "Zelong Lv",
        "relation": "author_of",
        "tail": "Native and Compact Structured Latents for 3D Generation"
      },
      {
        "head": "Yu Deng",
        "relation": "author_of",
        "tail": "Native and Compact Structured Latents for 3D Generation"
      },
      {
        "head": "Hongyuan Zhu",
        "relation": "author_of",
        "tail": "Native and Compact Structured Latents for 3D Generation"
      },
      {
        "head": "Yue Dong",
        "relation": "author_of",
        "tail": "Native and Compact Structured Latents for 3D Generation"
      },
      {
        "head": "Hao Zhao",
        "relation": "author_of",
        "tail": "Native and Compact Structured Latents for 3D Generation"
      },
      {
        "head": "Nicholas Jing Yuan",
        "relation": "author_of",
        "tail": "Native and Compact Structured Latents for 3D Generation"
      },
      {
        "head": "Jiaolong Yang",
        "relation": "author_of",
        "tail": "Native and Compact Structured Latents for 3D Generation"
      },
      {
        "head": "Basile Terver",
        "relation": "author_of",
        "tail": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?"
      },
      {
        "head": "Tsung-Yen Yang",
        "relation": "author_of",
        "tail": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?"
      },
      {
        "head": "Jean Ponce",
        "relation": "author_of",
        "tail": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?"
      },
      {
        "head": "Adrien Bardes",
        "relation": "author_of",
        "tail": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?"
      },
      {
        "head": "Yann LeCun",
        "relation": "author_of",
        "tail": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?"
      },
      {
        "head": "Guangyi Zhang",
        "relation": "author_of",
        "tail": "Progressive Learned Image Transmission for Semantic Communication Using Hierarchical VAE"
      },
      {
        "head": "Hanlei Li",
        "relation": "author_of",
        "tail": "Progressive Learned Image Transmission for Semantic Communication Using Hierarchical VAE"
      },
      {
        "head": "Yunlong Cai",
        "relation": "author_of",
        "tail": "Progressive Learned Image Transmission for Semantic Communication Using Hierarchical VAE"
      },
      {
        "head": "Qiyu Hu",
        "relation": "author_of",
        "tail": "Progressive Learned Image Transmission for Semantic Communication Using Hierarchical VAE"
      },
      {
        "head": "Guanding Yu",
        "relation": "author_of",
        "tail": "Progressive Learned Image Transmission for Semantic Communication Using Hierarchical VAE"
      },
      {
        "head": "Zhijing Qin",
        "relation": "author_of",
        "tail": "Progressive Learned Image Transmission for Semantic Communication Using Hierarchical VAE"
      },
      {
        "head": "Wenjun Lin",
        "relation": "author_of",
        "tail": "STORM: Search-Guided Generative World Models for Robotic Manipulation"
      },
      {
        "head": "Jensen Zhang",
        "relation": "author_of",
        "tail": "STORM: Search-Guided Generative World Models for Robotic Manipulation"
      },
      {
        "head": "Kaitong Cai",
        "relation": "author_of",
        "tail": "STORM: Search-Guided Generative World Models for Robotic Manipulation"
      },
      {
        "head": "Keze Wang",
        "relation": "author_of",
        "tail": "STORM: Search-Guided Generative World Models for Robotic Manipulation"
      },
      {
        "head": "Takuya Akiba",
        "relation": "author_of",
        "tail": "Evolutionary Optimization of Model Merging Recipes"
      },
      {
        "head": "Makoto Shing",
        "relation": "author_of",
        "tail": "Evolutionary Optimization of Model Merging Recipes"
      },
      {
        "head": "Yujin Tang",
        "relation": "author_of",
        "tail": "Evolutionary Optimization of Model Merging Recipes"
      },
      {
        "head": "Qi Sun",
        "relation": "author_of",
        "tail": "Evolutionary Optimization of Model Merging Recipes"
      },
      {
        "head": "David Ha",
        "relation": "author_of",
        "tail": "Evolutionary Optimization of Model Merging Recipes"
      },
      {
        "head": "Kyunghyun Cho",
        "relation": "author_of",
        "tail": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation"
      },
      {
        "head": "Bart van Merrienboer",
        "relation": "author_of",
        "tail": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation"
      },
      {
        "head": "Caglar Gulcehre",
        "relation": "author_of",
        "tail": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation"
      },
      {
        "head": "Dzmitry Bahdanau",
        "relation": "author_of",
        "tail": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation"
      },
      {
        "head": "Fethi Bougares",
        "relation": "author_of",
        "tail": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation"
      },
      {
        "head": "Holger Schwenk",
        "relation": "author_of",
        "tail": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation"
      },
      {
        "head": "Yoshua Bengio",
        "relation": "author_of",
        "tail": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation"
      },
      {
        "head": "Erfei Cui",
        "relation": "author_of",
        "tail": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving"
      },
      {
        "head": "Wenhai Wang",
        "relation": "author_of",
        "tail": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving"
      },
      {
        "head": "Zhiqi Li",
        "relation": "author_of",
        "tail": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving"
      },
      {
        "head": "Jiangwei Xie",
        "relation": "author_of",
        "tail": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving"
      },
      {
        "head": "Haoming Zou",
        "relation": "author_of",
        "tail": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving"
      },
      {
        "head": "Hanming Deng",
        "relation": "author_of",
        "tail": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving"
      },
      {
        "head": "Gen Luo",
        "relation": "author_of",
        "tail": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving"
      },
      {
        "head": "Lewei Lu",
        "relation": "author_of",
        "tail": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving"
      },
      {
        "head": "Xizhou Zhu",
        "relation": "author_of",
        "tail": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving"
      },
      {
        "head": "Jifeng Dai",
        "relation": "author_of",
        "tail": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving"
      },
      {
        "head": "Haodong Duan",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Xinyu Fang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Junming Yang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Xiangyu Zhao",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Yuxuan Qiao",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Mo Li",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Amit Agarwal",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Zhe Chen",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Lin Chen",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Yuan Liu",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Yubo Ma",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Hailong Sun",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Yifan Zhang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Shiyin Lu",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Tack Hwa Wong",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Weiyun Wang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Peiheng Zhou",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Xiaozhe Li",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Chaoyou Fu",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Junbo Cui",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Jixuan Chen",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Enxin Song",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Song Mao",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Shengyuan Ding",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Tianhao Liang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Zicheng Zhang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Xiaoyi Dong",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Yuhang Zang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Pan Zhang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Jiaqi Wang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Dahua Lin",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Kai Chen",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Jason Wei",
        "relation": "author_of",
        "tail": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Xuezhi Wang",
        "relation": "author_of",
        "tail": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Dale Schuurmans",
        "relation": "author_of",
        "tail": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Maarten Bosma",
        "relation": "author_of",
        "tail": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Brian Ichter",
        "relation": "author_of",
        "tail": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Fei Xia",
        "relation": "author_of",
        "tail": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Ed Chi",
        "relation": "author_of",
        "tail": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Quoc Le",
        "relation": "author_of",
        "tail": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Denny Zhou",
        "relation": "author_of",
        "tail": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Ross Girshick",
        "relation": "author_of",
        "tail": "Rich feature hierarchies for accurate object detection and semantic segmentation"
      },
      {
        "head": "Jeff Donahue",
        "relation": "author_of",
        "tail": "Rich feature hierarchies for accurate object detection and semantic segmentation"
      },
      {
        "head": "Trevor Darrell",
        "relation": "author_of",
        "tail": "Rich feature hierarchies for accurate object detection and semantic segmentation"
      },
      {
        "head": "Jitendra Malik",
        "relation": "author_of",
        "tail": "Rich feature hierarchies for accurate object detection and semantic segmentation"
      },
      {
        "head": "Haotian Lv",
        "relation": "author_of",
        "tail": "Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism"
      },
      {
        "head": "Yuhui Zhang",
        "relation": "author_of",
        "tail": "Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism"
      },
      {
        "head": "Jiangbo Dai",
        "relation": "author_of",
        "tail": "Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism"
      },
      {
        "head": "Hanli Wu",
        "relation": "author_of",
        "tail": "Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism"
      },
      {
        "head": "Jiaji Wang",
        "relation": "author_of",
        "tail": "Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism"
      },
      {
        "head": "Dawei Wang",
        "relation": "author_of",
        "tail": "Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism"
      },
      {
        "head": "Anwesha Mukherjee",
        "relation": "author_of",
        "tail": "Federated Learning Architectures: A Performance Evaluation with Crop Yield Prediction Application"
      },
      {
        "head": "Rajkumar Buyya",
        "relation": "author_of",
        "tail": "Federated Learning Architectures: A Performance Evaluation with Crop Yield Prediction Application"
      },
      {
        "head": "Zhiyuan You",
        "relation": "author_of",
        "tail": "Enhancing Descriptive Image Quality Assessment with A Large-scale Multi-modal Dataset"
      },
      {
        "head": "Jinjin Gu",
        "relation": "author_of",
        "tail": "Enhancing Descriptive Image Quality Assessment with A Large-scale Multi-modal Dataset"
      },
      {
        "head": "Xin Cai",
        "relation": "author_of",
        "tail": "Enhancing Descriptive Image Quality Assessment with A Large-scale Multi-modal Dataset"
      },
      {
        "head": "Zheyuan Li",
        "relation": "author_of",
        "tail": "Enhancing Descriptive Image Quality Assessment with A Large-scale Multi-modal Dataset"
      },
      {
        "head": "Kaiwen Zhu",
        "relation": "author_of",
        "tail": "Enhancing Descriptive Image Quality Assessment with A Large-scale Multi-modal Dataset"
      },
      {
        "head": "Chao Dong",
        "relation": "author_of",
        "tail": "Enhancing Descriptive Image Quality Assessment with A Large-scale Multi-modal Dataset"
      },
      {
        "head": "Tianfan Xue",
        "relation": "author_of",
        "tail": "Enhancing Descriptive Image Quality Assessment with A Large-scale Multi-modal Dataset"
      },
      {
        "head": "Matthew E. Peters",
        "relation": "author_of",
        "tail": "Deep contextualized word representations"
      },
      {
        "head": "Mark Neumann",
        "relation": "author_of",
        "tail": "Deep contextualized word representations"
      },
      {
        "head": "Mohit Iyyer",
        "relation": "author_of",
        "tail": "Deep contextualized word representations"
      },
      {
        "head": "Matt Gardner",
        "relation": "author_of",
        "tail": "Deep contextualized word representations"
      },
      {
        "head": "Christopher Clark",
        "relation": "author_of",
        "tail": "Deep contextualized word representations"
      },
      {
        "head": "Kenton Lee",
        "relation": "author_of",
        "tail": "Deep contextualized word representations"
      },
      {
        "head": "Luke Zettlemoyer",
        "relation": "author_of",
        "tail": "Deep contextualized word representations"
      },
      {
        "head": "DeepSeek-AI",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Daya Guo",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Dejian Yang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Haowei Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Junxiao Song",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Peiyi Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Qihao Zhu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Runxin Xu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Ruoyu Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Shirong Ma",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xiao Bi",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xiaokang Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xingkai Yu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yu Wu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Z. F. Wu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zhibin Gou",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zhihong Shao",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zhuoshu Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Ziyi Gao",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Aixin Liu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Bing Xue",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Bingxuan Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Bochao Wu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Bei Feng",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Chengda Lu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Chenggang Zhao",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Chengqi Deng",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Chenyu Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Chong Ruan",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Damai Dai",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Deli Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Dongjie Ji",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Erhang Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Fangyun Lin",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Fucong Dai",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Fuli Luo",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Guangbo Hao",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Guanting Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Guowei Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "H. Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Han Bao",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Hanwei Xu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Haocheng Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Honghui Ding",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Huajian Xin",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Huazuo Gao",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Hui Qu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Hui Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Jianzhong Guo",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Jiashi Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Jiawei Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Jingchang Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Jingyang Yuan",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Junjie Qiu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Junlong Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "J. L. Cai",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Jiaqi Ni",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Jian Liang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Jin Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Kai Dong",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Kai Hu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Kaige Gao",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Kang Guan",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Kexin Huang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Kuai Yu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Lean Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Lecong Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Liang Zhao",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Litong Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Liyue Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Lei Xu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Leyi Xia",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Mingchuan Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Minghua Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Minghui Tang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Meng Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Miaojun Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Mingming Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Ning Tian",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Panpan Huang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Peng Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Qiancheng Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Qinyu Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Qiushi Du",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Ruiqi Ge",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Ruisong Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Ruizhe Pan",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Runji Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "R. J. Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "R. L. Jin",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Ruyi Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Shanghao Lu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Shangyan Zhou",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Shanhuang Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Shengfeng Ye",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Shiyu Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Shuiping Yu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Shunfeng Zhou",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Shuting Pan",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "S. S. Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Shuang Zhou",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Shaoqing Wu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Shengfeng Ye",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Tao Yun",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Tian Pei",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Tianyu Sun",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "T. Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Wangding Zeng",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Wanjia Zhao",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Wen Liu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Wenfeng Liang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Wenjun Gao",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Wenqin Yu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Wentao Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "W. L. Xiao",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Wei An",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xiaodong Liu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xiaohan Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xiaokang Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xiaotao Nie",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xin Cheng",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xin Liu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xin Xie",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xingchao Liu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xinyu Yang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xinyuan Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xuecheng Su",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xuheng Lin",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "X. Q. Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xiangyue Jin",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xiaojin Shen",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xiaosha Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xiaowen Sun",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xiaoxiang Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xinnan Song",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xinyi Zhou",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xianzu Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xinxia Shan",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Y. K. Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Y. Q. Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Y. X. Wei",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yang Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yanhong Xu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yao Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yao Zhao",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yaofeng Sun",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yaohui Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yi Yu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yichao Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yifan Shi",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yiliang Xiong",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Ying He",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yishi Piao",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yisong Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yixuan Tan",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yiyang Ma",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yiyuan Liu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yongqiang Guo",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yuan Ou",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yuduan Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yue Gong",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yuheng Zou",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yujia He",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yunfan Xiong",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yuxiang Luo",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yuxiang You",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yuxuan Liu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yuyang Zhou",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Y. X. Zhu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yanhong Xu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yanping Huang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yaohui Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yi Zheng",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yuchen Zhu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yunxian Ma",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Ying Tang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yukun Zha",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yuting Yan",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Z. Z. Ren",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zehui Ren",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zhangli Sha",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zhe Fu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zhean Xu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zhenda Xie",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zhengyan Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zhewen Hao",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zhicheng Ma",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zhigang Yan",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zhiyu Wu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zihui Gu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zijia Zhu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zijun Liu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zilin Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Ziwei Xie",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Ziyang Song",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zizheng Pan",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zhen Huang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zhipeng Xu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zhongyu Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zhen Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Attention Is All You Need",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Attention Is All You Need",
        "relation": "cites",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "Attention Is All You Need",
        "relation": "cites",
        "tail": "Long Short-Term Memory"
      },
      {
        "head": "Attention Is All You Need",
        "relation": "cites",
        "tail": "Dropout: a simple way to prevent neural networks from overfitting"
      },
      {
        "head": "Attention Is All You Need",
        "relation": "cites",
        "tail": "Rethinking the Inception Architecture for Computer Vision"
      },
      {
        "head": "A comprehensive review of recommender systems: Transitioning from theory to practice",
        "relation": "cites",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
        "relation": "cites",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection",
        "relation": "cites",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "Enhanced Transformer-BiLSTM Deep Learning Framework for Day-Ahead Energy Price Forecasting",
        "relation": "cites",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation",
        "relation": "cites",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "cites",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "cites",
        "tail": "Long Short-Term Memory"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "cites",
        "tail": "Et al"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "cites",
        "tail": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
      },
      {
        "head": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "DiffusionEngine: Diffusion model is scalable data engine for object detection",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Enhancing hyperspectral image prediction with contrastive learning in low-label regimes",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Adam: A Method for Stochastic Optimization",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "Adam: A Method for Stochastic Optimization",
        "relation": "cites",
        "tail": "Auto-Encoding Variational Bayes"
      },
      {
        "head": "Adam: A Method for Stochastic Optimization",
        "relation": "cites",
        "tail": "Supporting Online Material for Reducing the Dimensionality of Data with Neural Networks"
      },
      {
        "head": "Adam: A Method for Stochastic Optimization",
        "relation": "cites",
        "tail": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"
      },
      {
        "head": "Adam: A Method for Stochastic Optimization",
        "relation": "cites",
        "tail": "Speech recognition with deep recurrent neural networks"
      },
      {
        "head": "Weakly Supervised Image Dehazing via Physics-Based Decomposition",
        "relation": "cites",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer",
        "relation": "cites",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
        "relation": "cites",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction",
        "relation": "cites",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model",
        "relation": "cites",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "Rethinking the Inception Architecture for Computer Vision",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "Rethinking the Inception Architecture for Computer Vision",
        "relation": "cites",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "Rethinking the Inception Architecture for Computer Vision",
        "relation": "cites",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "Rethinking the Inception Architecture for Computer Vision",
        "relation": "cites",
        "tail": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
      },
      {
        "head": "Rethinking the Inception Architecture for Computer Vision",
        "relation": "cites",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Diffusion Transformers with Representation Autoencoders",
        "relation": "cites",
        "tail": "Rethinking the Inception Architecture for Computer Vision"
      },
      {
        "head": "Scaling Large Language Models for Next-Generation Single-Cell Analysis",
        "relation": "cites",
        "tail": "Rethinking the Inception Architecture for Computer Vision"
      },
      {
        "head": "HybridVisionNet: An advanced hybrid deep learning framework for automated multi-class ocular disease diagnosis using fundus imaging",
        "relation": "cites",
        "tail": "Rethinking the Inception Architecture for Computer Vision"
      },
      {
        "head": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights",
        "relation": "cites",
        "tail": "Rethinking the Inception Architecture for Computer Vision"
      },
      {
        "head": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot",
        "relation": "cites",
        "tail": "Rethinking the Inception Architecture for Computer Vision"
      },
      {
        "head": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
        "relation": "cites",
        "tail": "Attention is All you Need"
      },
      {
        "head": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
        "relation": "cites",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
        "relation": "cites",
        "tail": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"
      },
      {
        "head": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
        "relation": "cites",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
        "relation": "cites",
        "tail": "CARLA: An Open Urban Driving Simulator"
      },
      {
        "head": "Sora as a World Model? A Complete Survey on Text-to-Video Generation",
        "relation": "cites",
        "tail": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation"
      },
      {
        "head": "OmniNWM: Omniscient Driving Navigation World Models",
        "relation": "cites",
        "tail": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation"
      },
      {
        "head": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask",
        "relation": "cites",
        "tail": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation"
      },
      {
        "head": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving",
        "relation": "cites",
        "tail": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation"
      },
      {
        "head": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
        "relation": "cites",
        "tail": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation"
      },
      {
        "head": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
        "relation": "cites",
        "tail": "I and J"
      },
      {
        "head": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
        "relation": "cites",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
        "relation": "cites",
        "tail": "A and V"
      },
      {
        "head": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
        "relation": "cites",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "What matters for Representation Alignment: Global Information or Spatial Structure?",
        "relation": "cites",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "SCB-DETR: Multiscale Deformable Transformers for Occlusion-Resilient Student Learning Behavior Detection in Smart Classroom",
        "relation": "cites",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination",
        "relation": "cites",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "Bio-Inspired Fine-Tuning for Selective Transfer Learning in Image Classification",
        "relation": "cites",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "Hand Sign Language Detection Using Deep Learning",
        "relation": "cites",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "Auto-Encoding Variational Bayes",
        "relation": "cites",
        "tail": "Representation Learning: A Review and New Perspectives"
      },
      {
        "head": "Auto-Encoding Variational Bayes",
        "relation": "cites",
        "tail": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"
      },
      {
        "head": "Auto-Encoding Variational Bayes",
        "relation": "cites",
        "tail": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion"
      },
      {
        "head": "Auto-Encoding Variational Bayes",
        "relation": "cites",
        "tail": "In Advances in Neural Information Processing Systems"
      },
      {
        "head": "Auto-Encoding Variational Bayes",
        "relation": "cites",
        "tail": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models"
      },
      {
        "head": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation",
        "relation": "cites",
        "tail": "Auto-Encoding Variational Bayes"
      },
      {
        "head": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions",
        "relation": "cites",
        "tail": "Auto-Encoding Variational Bayes"
      },
      {
        "head": "GenAD: Generative End-to-End Autonomous Driving",
        "relation": "cites",
        "tail": "Auto-Encoding Variational Bayes"
      },
      {
        "head": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model",
        "relation": "cites",
        "tail": "Auto-Encoding Variational Bayes"
      },
      {
        "head": "Improving Video Generation with Human Feedback",
        "relation": "cites",
        "tail": "Auto-Encoding Variational Bayes"
      },
      {
        "head": "Speech Recognition with Deep Recurrent Neural Networks",
        "relation": "cites",
        "tail": "Long Short-Term Memory"
      },
      {
        "head": "Speech Recognition with Deep Recurrent Neural Networks",
        "relation": "cites",
        "tail": "Learning representations by back-propagating errors"
      },
      {
        "head": "Speech Recognition with Deep Recurrent Neural Networks",
        "relation": "cites",
        "tail": "Bidirectional recurrent neural networks"
      },
      {
        "head": "Speech Recognition with Deep Recurrent Neural Networks",
        "relation": "cites",
        "tail": "Connectionist Temporal Classiﬁcation: Labelling Unsegmented Sequence Data with Recurrent Neural Networks"
      },
      {
        "head": "Speech Recognition with Deep Recurrent Neural Networks",
        "relation": "cites",
        "tail": "2005 Special Issue: Framewise phoneme classification with bidirectional LSTM and other neural network architectures"
      },
      {
        "head": "A high-performance neuroprosthesis for speech decoding and avatar control",
        "relation": "cites",
        "tail": "Speech Recognition with Deep Recurrent Neural Networks"
      },
      {
        "head": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses",
        "relation": "cites",
        "tail": "Speech Recognition with Deep Recurrent Neural Networks"
      },
      {
        "head": "A high-performance speech neuroprosthesis",
        "relation": "cites",
        "tail": "Speech Recognition with Deep Recurrent Neural Networks"
      },
      {
        "head": "Loss of plasticity in deep continual learning",
        "relation": "cites",
        "tail": "Speech Recognition with Deep Recurrent Neural Networks"
      },
      {
        "head": "An analog-AI chip for energy-efficient speech recognition and transcription",
        "relation": "cites",
        "tail": "Speech Recognition with Deep Recurrent Neural Networks"
      },
      {
        "head": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
        "relation": "cites",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
        "relation": "cites",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
        "relation": "cites",
        "tail": "A Mathematical Theory of Communication"
      },
      {
        "head": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
        "relation": "cites",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers",
        "relation": "cites",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling",
        "relation": "cites",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram",
        "relation": "cites",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "L$^3$: Large Lookup Layers",
        "relation": "cites",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Scaling Embeddings Outperforms Scaling Experts in Language Models",
        "relation": "cites",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Going Deeper with Convolutions",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "Going Deeper with Convolutions",
        "relation": "cites",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "Going Deeper with Convolutions",
        "relation": "cites",
        "tail": "Gradient-based learning applied to document recognition"
      },
      {
        "head": "Going Deeper with Convolutions",
        "relation": "cites",
        "tail": "Regression Shrinkage and Selection via the Lasso"
      },
      {
        "head": "Going Deeper with Convolutions",
        "relation": "cites",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "LifeCLEF Plant Identification Task 2015",
        "relation": "cites",
        "tail": "Going Deeper with Convolutions"
      },
      {
        "head": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics",
        "relation": "cites",
        "tail": "Going Deeper with Convolutions"
      },
      {
        "head": "Enhancing pine wilt disease detection with synthetic data and external attention-based transformers",
        "relation": "cites",
        "tail": "Going Deeper with Convolutions"
      },
      {
        "head": "A comprehensive review on YOLO versions for object detection",
        "relation": "cites",
        "tail": "Going Deeper with Convolutions"
      },
      {
        "head": "Harnessing large vision and language models in agriculture: a review",
        "relation": "cites",
        "tail": "Going Deeper with Convolutions"
      },
      {
        "head": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
        "relation": "cites",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
        "relation": "cites",
        "tail": "Gradient-based learning applied to document recognition"
      },
      {
        "head": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
        "relation": "cites",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "A comprehensive review on YOLO versions for object detection",
        "relation": "cites",
        "tail": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
      },
      {
        "head": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer",
        "relation": "cites",
        "tail": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
      },
      {
        "head": "Multi-axis vision transformer for medical image segmentation",
        "relation": "cites",
        "tail": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
      },
      {
        "head": "A comprehensive review of facial beauty prediction using deep learning techniques",
        "relation": "cites",
        "tail": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
      },
      {
        "head": "Active learning framework leveraging transcriptomics identifies modulators of disease phenotypes.",
        "relation": "cites",
        "tail": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
      },
      {
        "head": "ImageNet Large Scale Visual Recognition Challenge",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "ImageNet Large Scale Visual Recognition Challenge",
        "relation": "cites",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "ImageNet Large Scale Visual Recognition Challenge",
        "relation": "cites",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "ImageNet Large Scale Visual Recognition Challenge",
        "relation": "cites",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "ImageNet Large Scale Visual Recognition Challenge",
        "relation": "cites",
        "tail": "Distinctive Image Features from Scale-Invariant Keypoints"
      },
      {
        "head": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training",
        "relation": "cites",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Diffusion Transformers with Representation Autoencoders",
        "relation": "cites",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications",
        "relation": "cites",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "A comprehensive review on YOLO versions for object detection",
        "relation": "cites",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Aligning machine and human visual representations across abstraction levels",
        "relation": "cites",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Diffusion Transformers with Representation Autoencoders",
        "relation": "cites",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "Diffusion Transformers with Representation Autoencoders",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Diffusion Transformers with Representation Autoencoders",
        "relation": "cites",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Diffusion Transformers with Representation Autoencoders",
        "relation": "cites",
        "tail": "GENERATIVE ADVERSARIAL NETS"
      },
      {
        "head": "Diffusion Transformers with Representation Autoencoders",
        "relation": "cites",
        "tail": "Rethinking the Inception Architecture for Computer Vision"
      },
      {
        "head": "Improved Mean Flows: On the Challenges of Fastforward Generative Models",
        "relation": "cites",
        "tail": "Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training",
        "relation": "cites",
        "tail": "Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
        "relation": "cites",
        "tail": "Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "PixelDiT: Pixel Diffusion Transformers for Image Generation",
        "relation": "cites",
        "tail": "Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models",
        "relation": "cites",
        "tail": "Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "relation": "cites",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "relation": "cites",
        "tail": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "relation": "cites",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "relation": "cites",
        "tail": "Dropout: a simple way to prevent neural networks from overfitting"
      },
      {
        "head": "A comprehensive review of object detection with traditional and deep learning methods",
        "relation": "cites",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "Diffusion Language Models are Super Data Learners",
        "relation": "cites",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas",
        "relation": "cites",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "FLEX: Continuous Agent Evolution via Forward Learning from Experience",
        "relation": "cites",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning",
        "relation": "cites",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "OmniNWM: Omniscient Driving Navigation World Models",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "OmniNWM: Omniscient Driving Navigation World Models",
        "relation": "cites",
        "tail": "Squeeze-and-Excitation Networks"
      },
      {
        "head": "OmniNWM: Omniscient Driving Navigation World Models",
        "relation": "cites",
        "tail": "Decoupled Weight Decay Regularization"
      },
      {
        "head": "OmniNWM: Omniscient Driving Navigation World Models",
        "relation": "cites",
        "tail": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"
      },
      {
        "head": "OmniNWM: Omniscient Driving Navigation World Models",
        "relation": "cites",
        "tail": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"
      },
      {
        "head": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control",
        "relation": "cites",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World",
        "relation": "cites",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "DVGT: Driving Visual Geometry Transformer",
        "relation": "cites",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask",
        "relation": "cites",
        "tail": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"
      },
      {
        "head": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask",
        "relation": "cites",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask",
        "relation": "cites",
        "tail": "Adding Conditional Control to Text-to-Image Diffusion Models"
      },
      {
        "head": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask",
        "relation": "cites",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving",
        "relation": "cites",
        "tail": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "head": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving",
        "relation": "cites",
        "tail": "AUTO-ENCODING VARIATIONAL BAYES"
      },
      {
        "head": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving",
        "relation": "cites",
        "tail": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"
      },
      {
        "head": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving",
        "relation": "cites",
        "tail": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric"
      },
      {
        "head": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving",
        "relation": "cites",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
        "relation": "cites",
        "tail": "Decoupled Weight Decay Regularization"
      },
      {
        "head": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
        "relation": "cites",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
        "relation": "cites",
        "tail": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"
      },
      {
        "head": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
        "relation": "cites",
        "tail": "CARLA: An Open Urban Driving Simulator"
      },
      {
        "head": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
        "relation": "cites",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "What matters for Representation Alignment: Global Information or Spatial Structure?",
        "relation": "cites",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "What matters for Representation Alignment: Global Information or Spatial Structure?",
        "relation": "cites",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "What matters for Representation Alignment: Global Information or Spatial Structure?",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "What matters for Representation Alignment: Global Information or Spatial Structure?",
        "relation": "cites",
        "tail": "Histograms of oriented gradients for human detection"
      },
      {
        "head": "What matters for Representation Alignment: Global Information or Spatial Structure?",
        "relation": "cites",
        "tail": "Generative Adversarial Networks"
      },
      {
        "head": "Generalization of Diffusion Models Arises with a Balanced Representation Space",
        "relation": "cites",
        "tail": "What matters for Representation Alignment: Global Information or Spatial Structure?"
      },
      {
        "head": "Stable Velocity: A Variance Perspective on Flow Matching",
        "relation": "cites",
        "tail": "What matters for Representation Alignment: Global Information or Spatial Structure?"
      },
      {
        "head": "Laminating Representation Autoencoders for Efficient Diffusion",
        "relation": "cites",
        "tail": "What matters for Representation Alignment: Global Information or Spatial Structure?"
      },
      {
        "head": "Adaptive 1D Video Diffusion Autoencoder",
        "relation": "cites",
        "tail": "What matters for Representation Alignment: Global Information or Spatial Structure?"
      },
      {
        "head": "Test-Time Conditioning with Representation-Aligned Visual Features",
        "relation": "cites",
        "tail": "What matters for Representation Alignment: Global Information or Spatial Structure?"
      },
      {
        "head": "VGG Induced Deep Hand Sign Language Detection",
        "relation": "cites",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "VGG Induced Deep Hand Sign Language Detection",
        "relation": "cites",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "VGG Induced Deep Hand Sign Language Detection",
        "relation": "cites",
        "tail": "MediaPipe: A Framework for Building Perception Pipelines"
      },
      {
        "head": "VGG Induced Deep Hand Sign Language Detection",
        "relation": "cites",
        "tail": "Hand Pose Estimation and Hand Shape Classification Using Multi-layered Randomized Decision Forests"
      },
      {
        "head": "VGG Induced Deep Hand Sign Language Detection",
        "relation": "cites",
        "tail": "A robust static hand gesture recognition system using geometry based normalizations and Krawtchouk moments"
      },
      {
        "head": "Hand signal classification system for sign language communication in Virtual Reality",
        "relation": "cites",
        "tail": "VGG Induced Deep Hand Sign Language Detection"
      },
      {
        "head": "Real-Time Static Hand Sign Recognition System using MediaPipe and Fully Connected Neural Network",
        "relation": "cites",
        "tail": "VGG Induced Deep Hand Sign Language Detection"
      },
      {
        "head": "Real-Time Gesture Recognition to Aid Communication in Children with Motor Impairments",
        "relation": "cites",
        "tail": "VGG Induced Deep Hand Sign Language Detection"
      },
      {
        "head": "Representation Learning: A Review and New Perspectives",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "Representation Learning: A Review and New Perspectives",
        "relation": "cites",
        "tail": "Gradient-based learning applied to document recognition"
      },
      {
        "head": "Representation Learning: A Review and New Perspectives",
        "relation": "cites",
        "tail": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
      },
      {
        "head": "Representation Learning: A Review and New Perspectives",
        "relation": "cites",
        "tail": "Visualizing Data using t-SNE"
      },
      {
        "head": "Representation Learning: A Review and New Perspectives",
        "relation": "cites",
        "tail": "Learning Multiple Layers of Features from Tiny Images"
      },
      {
        "head": "LLM Social Simulations Are a Promising Research Method",
        "relation": "cites",
        "tail": "Representation Learning: A Review and New Perspectives"
      },
      {
        "head": "Federated Contrastive Learning With Feature-Based Distillation for Human Activity Recognition",
        "relation": "cites",
        "tail": "Representation Learning: A Review and New Perspectives"
      },
      {
        "head": "Diffuse and Disperse: Image Generation with Representation Regularization",
        "relation": "cites",
        "tail": "Representation Learning: A Review and New Perspectives"
      },
      {
        "head": "Deep Autoencoder Neural Networks: A Comprehensive Review and New Perspectives",
        "relation": "cites",
        "tail": "Representation Learning: A Review and New Perspectives"
      },
      {
        "head": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems",
        "relation": "cites",
        "tail": "Representation Learning: A Review and New Perspectives"
      },
      {
        "head": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation",
        "relation": "cites",
        "tail": "U-Net: Convolutional Networks for Biomedical Image Segmentation"
      },
      {
        "head": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation",
        "relation": "cites",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation",
        "relation": "cites",
        "tail": "Generative Adversarial Networks"
      },
      {
        "head": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation",
        "relation": "cites",
        "tail": "Decoupled Weight Decay Regularization"
      },
      {
        "head": "Improved Distribution Matching Distillation for Fast Image Synthesis",
        "relation": "cites",
        "tail": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation"
      },
      {
        "head": "Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation",
        "relation": "cites",
        "tail": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation"
      },
      {
        "head": "Evolutionary optimization of model merging recipes",
        "relation": "cites",
        "tail": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation"
      },
      {
        "head": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
        "relation": "cites",
        "tail": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation"
      },
      {
        "head": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment",
        "relation": "cites",
        "tail": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation"
      },
      {
        "head": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions",
        "relation": "cites",
        "tail": "U-Net: Convolutional Networks for Biomedical Image Segmentation"
      },
      {
        "head": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions",
        "relation": "cites",
        "tail": "Denoising Diffusion Probabilistic Models"
      },
      {
        "head": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions",
        "relation": "cites",
        "tail": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "head": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions",
        "relation": "cites",
        "tail": "Auto-Encoding Variational Bayes"
      },
      {
        "head": "HunyuanVideo: A Systematic Framework For Large Video Generative Models",
        "relation": "cites",
        "tail": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions"
      },
      {
        "head": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency",
        "relation": "cites",
        "tail": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions"
      },
      {
        "head": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models",
        "relation": "cites",
        "tail": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions"
      },
      {
        "head": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation",
        "relation": "cites",
        "tail": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions"
      },
      {
        "head": "EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation",
        "relation": "cites",
        "tail": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions"
      },
      {
        "head": "GenAD: Generative End-to-End Autonomous Driving",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "GenAD: Generative End-to-End Autonomous Driving",
        "relation": "cites",
        "tail": "Attention is All you Need"
      },
      {
        "head": "GenAD: Generative End-to-End Autonomous Driving",
        "relation": "cites",
        "tail": "Decoupled Weight Decay Regularization"
      },
      {
        "head": "GenAD: Generative End-to-End Autonomous Driving",
        "relation": "cites",
        "tail": "Feature Pyramid Networks for Object Detection"
      },
      {
        "head": "GenAD: Generative End-to-End Autonomous Driving",
        "relation": "cites",
        "tail": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"
      },
      {
        "head": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving",
        "relation": "cites",
        "tail": "GenAD: Generative End-to-End Autonomous Driving"
      },
      {
        "head": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability",
        "relation": "cites",
        "tail": "GenAD: Generative End-to-End Autonomous Driving"
      },
      {
        "head": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving",
        "relation": "cites",
        "tail": "GenAD: Generative End-to-End Autonomous Driving"
      },
      {
        "head": "How Far is Video Generation from World Model: A Physical Law Perspective",
        "relation": "cites",
        "tail": "GenAD: Generative End-to-End Autonomous Driving"
      },
      {
        "head": "EMMA: End-to-End Multimodal Model for Autonomous Driving",
        "relation": "cites",
        "tail": "GenAD: Generative End-to-End Autonomous Driving"
      },
      {
        "head": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model",
        "relation": "cites",
        "tail": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model",
        "relation": "cites",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model",
        "relation": "cites",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model",
        "relation": "cites",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling",
        "relation": "cites",
        "tail": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model"
      },
      {
        "head": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models",
        "relation": "cites",
        "tail": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model"
      },
      {
        "head": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models",
        "relation": "cites",
        "tail": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model"
      },
      {
        "head": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step",
        "relation": "cites",
        "tail": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model"
      },
      {
        "head": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency",
        "relation": "cites",
        "tail": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model"
      },
      {
        "head": "Improving Video Generation with Human Feedback",
        "relation": "cites",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "Improving Video Generation with Human Feedback",
        "relation": "cites",
        "tail": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
      },
      {
        "head": "Improving Video Generation with Human Feedback",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Improving Video Generation with Human Feedback",
        "relation": "cites",
        "tail": "Denoising Diffusion Probabilistic Models"
      },
      {
        "head": "Improving Video Generation with Human Feedback",
        "relation": "cites",
        "tail": "Proximal Policy Optimization Algorithms"
      },
      {
        "head": "Flow-GRPO: Training Flow Matching Models via Online RL",
        "relation": "cites",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "DanceGRPO: Unleashing GRPO on Visual Generation",
        "relation": "cites",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Seedance 1.0: Exploring the Boundaries of Video Generation Models",
        "relation": "cites",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "SkyReels-V2: Infinite-length Film Generative Model",
        "relation": "cites",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Unified Reward Model for Multimodal Understanding and Generation",
        "relation": "cites",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers",
        "relation": "cites",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers",
        "relation": "cites",
        "tail": "5分で分かる!? 有名論文ナナメ読み：Jacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers",
        "relation": "cites",
        "tail": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"
      },
      {
        "head": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers",
        "relation": "cites",
        "tail": "Neural Machine Translation of Rare Words with Subword Units"
      },
      {
        "head": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers",
        "relation": "cites",
        "tail": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"
      },
      {
        "head": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling",
        "relation": "cites",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling",
        "relation": "cites",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling",
        "relation": "cites",
        "tail": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "head": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling",
        "relation": "cites",
        "tail": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling",
        "relation": "cites",
        "tail": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "head": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram",
        "relation": "cites",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram",
        "relation": "cites",
        "tail": "Crystal structure of the nucleosome core particle at 2.8 Å resolution"
      },
      {
        "head": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram",
        "relation": "cites",
        "tail": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"
      },
      {
        "head": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram",
        "relation": "cites",
        "tail": "A catalogue of splice junction sequences."
      },
      {
        "head": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram",
        "relation": "cites",
        "tail": "Origin of the Genetic Code"
      },
      {
        "head": "L$^3$: Large Lookup Layers",
        "relation": "cites",
        "tail": "Decoupled Weight Decay Regularization"
      },
      {
        "head": "L$^3$: Large Lookup Layers",
        "relation": "cites",
        "tail": "SGDR: Stochastic Gradient Descent with Warm Restarts"
      },
      {
        "head": "L$^3$: Large Lookup Layers",
        "relation": "cites",
        "tail": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"
      },
      {
        "head": "L$^3$: Large Lookup Layers",
        "relation": "cites",
        "tail": "Compression of individual sequences via variable-rate coding"
      },
      {
        "head": "L$^3$: Large Lookup Layers",
        "relation": "cites",
        "tail": "Pointer Sentinel Mixture Models"
      },
      {
        "head": "Scaling Embeddings Outperforms Scaling Experts in Language Models",
        "relation": "cites",
        "tail": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "head": "Scaling Embeddings Outperforms Scaling Experts in Language Models",
        "relation": "cites",
        "tail": "Measuring Massive Multitask Language Understanding"
      },
      {
        "head": "Scaling Embeddings Outperforms Scaling Experts in Language Models",
        "relation": "cites",
        "tail": "Let's Verify Step by Step"
      },
      {
        "head": "Scaling Embeddings Outperforms Scaling Experts in Language Models",
        "relation": "cites",
        "tail": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark"
      },
      {
        "head": "Scaling Embeddings Outperforms Scaling Experts in Language Models",
        "relation": "cites",
        "tail": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"
      },
      {
        "head": "Microsoft COCO: Common Objects in Context",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "Microsoft COCO: Common Objects in Context",
        "relation": "cites",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "Microsoft COCO: Common Objects in Context",
        "relation": "cites",
        "tail": "Learning Multiple Layers of Features from Tiny Images"
      },
      {
        "head": "Microsoft COCO: Common Objects in Context",
        "relation": "cites",
        "tail": "Histograms of oriented gradients for human detection"
      },
      {
        "head": "Microsoft COCO: Common Objects in Context",
        "relation": "cites",
        "tail": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation"
      },
      {
        "head": "MuMu-LLaMA: Multi-modal music understanding and generation via large language models",
        "relation": "cites",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy",
        "relation": "cites",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism",
        "relation": "cites",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
        "relation": "cites",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness",
        "relation": "cites",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "LifeCLEF Plant Identification Task 2015",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "LifeCLEF Plant Identification Task 2015",
        "relation": "cites",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "LifeCLEF Plant Identification Task 2015",
        "relation": "cites",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "LifeCLEF Plant Identification Task 2015",
        "relation": "cites",
        "tail": "Pattern Recognition and Machine Learning"
      },
      {
        "head": "LifeCLEF Plant Identification Task 2015",
        "relation": "cites",
        "tail": "Bagging Predictors"
      },
      {
        "head": "Automatic Image-Based Plant Disease Severity Estimation Using Deep Learning",
        "relation": "cites",
        "tail": "LifeCLEF Plant Identification Task 2015"
      },
      {
        "head": "Plant identification using deep neural networks via optimization of transfer learning parameters",
        "relation": "cites",
        "tail": "LifeCLEF Plant Identification Task 2015"
      },
      {
        "head": "New perspectives on plant disease characterization based on deep learning",
        "relation": "cites",
        "tail": "LifeCLEF Plant Identification Task 2015"
      },
      {
        "head": "Deep Learning for Plant Identification in Natural Environment",
        "relation": "cites",
        "tail": "LifeCLEF Plant Identification Task 2015"
      },
      {
        "head": "Going deeper in the automated identification of Herbarium specimens",
        "relation": "cites",
        "tail": "LifeCLEF Plant Identification Task 2015"
      },
      {
        "head": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics",
        "relation": "cites",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics",
        "relation": "cites",
        "tail": "Densely Connected Convolutional Networks"
      },
      {
        "head": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics",
        "relation": "cites",
        "tail": "MobileNetV2: Inverted Residuals and Linear Bottlenecks"
      },
      {
        "head": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics",
        "relation": "cites",
        "tail": "Communication-Efficient Learning of Deep Networks from Decentralized Data"
      },
      {
        "head": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics",
        "relation": "cites",
        "tail": "A Comprehensive Survey on Transfer Learning"
      },
      {
        "head": "Privacy Preserved and Decentralized Smartphone Recommendation System",
        "relation": "cites",
        "tail": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics"
      },
      {
        "head": "A Survey on Heterogeneity Taxonomy, Security and Privacy Preservation in the Integration of IoT, Wireless Sensor Networks and Federated Learning",
        "relation": "cites",
        "tail": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics"
      },
      {
        "head": "A privacy-preserving federated learning with a secure collaborative for malware detection models using Internet of Things resources",
        "relation": "cites",
        "tail": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics"
      },
      {
        "head": "Graph-based deep learning techniques for remote sensing applications: Techniques, taxonomy, and applications - A comprehensive review",
        "relation": "cites",
        "tail": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics"
      },
      {
        "head": "Federated Learning Architectures: A Performance Evaluation With Crop Yield Prediction Application",
        "relation": "cites",
        "tail": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics"
      },
      {
        "head": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training",
        "relation": "cites",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training",
        "relation": "cites",
        "tail": "Segment Anything"
      },
      {
        "head": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training",
        "relation": "cites",
        "tail": "Visual Instruction Tuning"
      },
      {
        "head": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training",
        "relation": "cites",
        "tail": "Improved Baselines with Visual Instruction Tuning"
      },
      {
        "head": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset",
        "relation": "cites",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs",
        "relation": "cites",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views",
        "relation": "cites",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning",
        "relation": "cites",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe",
        "relation": "cites",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications",
        "relation": "cites",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications",
        "relation": "cites",
        "tail": "Long Short-Term Memory"
      },
      {
        "head": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications",
        "relation": "cites",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications",
        "relation": "cites",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "Vision-Language-Action (VLA) Models: Concepts, Progress, Applications and Challenges",
        "relation": "cites",
        "tail": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications"
      },
      {
        "head": "Leave No Observation Behind: Real-time Correction for VLA Action Chunks",
        "relation": "cites",
        "tail": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications"
      },
      {
        "head": "Latent Chain-of-Thought World Modeling for End-to-End Driving",
        "relation": "cites",
        "tail": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications"
      },
      {
        "head": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks",
        "relation": "cites",
        "tail": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications"
      },
      {
        "head": "Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models",
        "relation": "cites",
        "tail": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications"
      },
      {
        "head": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
        "relation": "cites",
        "tail": "Attention is All you Need"
      },
      {
        "head": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
        "relation": "cites",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
        "relation": "cites",
        "tail": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects",
        "relation": "cites",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "TransSIL: A Silhouette Cue-Aware Image Classification Framework for Bird Ecological Monitoring Systems",
        "relation": "cites",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "CBRFormer: rendering technology-based transformer for refinement segmentation of bridge crack images",
        "relation": "cites",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation",
        "relation": "cites",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "A Fast and Compact Threshold Switch-Based Cellular Nonlinear Network Cell",
        "relation": "cites",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "Learning Transferable Visual Models From Natural Language Supervision",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Learning Transferable Visual Models From Natural Language Supervision",
        "relation": "cites",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Learning Transferable Visual Models From Natural Language Supervision",
        "relation": "cites",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "Learning Transferable Visual Models From Natural Language Supervision",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "Learning Transferable Visual Models From Natural Language Supervision",
        "relation": "cites",
        "tail": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "Deep contrastive learning enables genome-wide virtual screening.",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "LLaVA-based semantic feature modulation diffusion model for underwater image enhancement",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "3DGS-Drag: Dragging Gaussians for Intuitive Point-Based 3D Editing",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "LILaC: Late Interacting in Layered Component Graph for Open-domain Multimodal Multihop Retrieval",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Improved Mean Flows: On the Challenges of Fastforward Generative Models",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Improved Mean Flows: On the Challenges of Fastforward Generative Models",
        "relation": "cites",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Improved Mean Flows: On the Challenges of Fastforward Generative Models",
        "relation": "cites",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "Improved Mean Flows: On the Challenges of Fastforward Generative Models",
        "relation": "cites",
        "tail": "Denoising Diffusion Probabilistic Models"
      },
      {
        "head": "Improved Mean Flows: On the Challenges of Fastforward Generative Models",
        "relation": "cites",
        "tail": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "head": "Bidirectional Normalizing Flow: From Data to Noise and Back",
        "relation": "cites",
        "tail": "Improved Mean Flows: On the Challenges of Fastforward Generative Models"
      },
      {
        "head": "One-step Latent-free Image Generation with Pixel Mean Flows",
        "relation": "cites",
        "tail": "Improved Mean Flows: On the Challenges of Fastforward Generative Models"
      },
      {
        "head": "Meta Flow Maps enable scalable reward alignment",
        "relation": "cites",
        "tail": "Improved Mean Flows: On the Challenges of Fastforward Generative Models"
      },
      {
        "head": "Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective",
        "relation": "cites",
        "tail": "Improved Mean Flows: On the Challenges of Fastforward Generative Models"
      },
      {
        "head": "Generative Modeling via Drifting",
        "relation": "cites",
        "tail": "Improved Mean Flows: On the Challenges of Fastforward Generative Models"
      },
      {
        "head": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training",
        "relation": "cites",
        "tail": "Attention is All you Need"
      },
      {
        "head": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training",
        "relation": "cites",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training",
        "relation": "cites",
        "tail": "Denoising Diffusion Probabilistic Models"
      },
      {
        "head": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training",
        "relation": "cites",
        "tail": "A Simple Framework for Contrastive Learning of Visual Representations"
      },
      {
        "head": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training",
        "relation": "cites",
        "tail": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "head": "PixelDiT: Pixel Diffusion Transformers for Image Generation",
        "relation": "cites",
        "tail": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training"
      },
      {
        "head": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning",
        "relation": "cites",
        "tail": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training"
      },
      {
        "head": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints",
        "relation": "cites",
        "tail": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training"
      },
      {
        "head": "One-step Latent-free Image Generation with Pixel Mean Flows",
        "relation": "cites",
        "tail": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training"
      },
      {
        "head": "Generative Modeling via Drifting",
        "relation": "cites",
        "tail": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training"
      },
      {
        "head": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
        "relation": "cites",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
        "relation": "cites",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
        "relation": "cites",
        "tail": "GENERATIVE ADVERSARIAL NETS"
      },
      {
        "head": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
        "relation": "cites",
        "tail": "Denoising Diffusion Probabilistic Models"
      },
      {
        "head": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
        "relation": "cites",
        "tail": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "head": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders",
        "relation": "cites",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "Simulating the Visual World with Artificial Intelligence: A Roadmap",
        "relation": "cites",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks",
        "relation": "cites",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "RecTok: Reconstruction Distillation along Rectified Flow",
        "relation": "cites",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "RePack then Refine: Efficient Diffusion Transformer with Vision Foundation Model",
        "relation": "cites",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "PixelDiT: Pixel Diffusion Transformers for Image Generation",
        "relation": "cites",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "PixelDiT: Pixel Diffusion Transformers for Image Generation",
        "relation": "cites",
        "tail": "Decoupled Weight Decay Regularization"
      },
      {
        "head": "PixelDiT: Pixel Diffusion Transformers for Image Generation",
        "relation": "cites",
        "tail": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "head": "PixelDiT: Pixel Diffusion Transformers for Image Generation",
        "relation": "cites",
        "tail": "Diffusion Models Beat GANs on Image Synthesis"
      },
      {
        "head": "PixelDiT: Pixel Diffusion Transformers for Image Generation",
        "relation": "cites",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "One-step Latent-free Image Generation with Pixel Mean Flows",
        "relation": "cites",
        "tail": "PixelDiT: Pixel Diffusion Transformers for Image Generation"
      },
      {
        "head": "Generative Modeling via Drifting",
        "relation": "cites",
        "tail": "PixelDiT: Pixel Diffusion Transformers for Image Generation"
      },
      {
        "head": "Laminating Representation Autoencoders for Efficient Diffusion",
        "relation": "cites",
        "tail": "PixelDiT: Pixel Diffusion Transformers for Image Generation"
      },
      {
        "head": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss",
        "relation": "cites",
        "tail": "PixelDiT: Pixel Diffusion Transformers for Image Generation"
      },
      {
        "head": "RePack then Refine: Efficient Diffusion Transformer with Vision Foundation Model",
        "relation": "cites",
        "tail": "PixelDiT: Pixel Diffusion Transformers for Image Generation"
      },
      {
        "head": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models",
        "relation": "cites",
        "tail": "U-Net: Convolutional Networks for Biomedical Image Segmentation"
      },
      {
        "head": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models",
        "relation": "cites",
        "tail": "Decoupled Weight Decay Regularization"
      },
      {
        "head": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models",
        "relation": "cites",
        "tail": "Denoising Diffusion Probabilistic Models"
      },
      {
        "head": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models",
        "relation": "cites",
        "tail": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "head": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
        "relation": "cites",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery",
        "relation": "cites",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation",
        "relation": "cites",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "relation": "cites",
        "tail": "Attention is All you Need"
      },
      {
        "head": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "relation": "cites",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "relation": "cites",
        "tail": "Distributed Representations of Words and Phrases and their Compositionality"
      },
      {
        "head": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "relation": "cites",
        "tail": "GloVe: Global Vectors for Word Representation"
      },
      {
        "head": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "relation": "cites",
        "tail": "Deep Contextualized Word Representations"
      },
      {
        "head": "Protein Language Models: Is Scaling Necessary?",
        "relation": "cites",
        "tail": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis",
        "relation": "cites",
        "tail": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation",
        "relation": "cites",
        "tail": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "Generative Classifiers Avoid Shortcut Solutions",
        "relation": "cites",
        "tail": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "Edge Large AI Model Agent-Empowered Cognitive Multimodal Semantic Communication",
        "relation": "cites",
        "tail": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "Diffusion Language Models are Super Data Learners",
        "relation": "cites",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Diffusion Language Models are Super Data Learners",
        "relation": "cites",
        "tail": "Language Models are Unsupervised Multitask Learners"
      },
      {
        "head": "Diffusion Language Models are Super Data Learners",
        "relation": "cites",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "Diffusion Language Models are Super Data Learners",
        "relation": "cites",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Diffusion Language Models are Super Data Learners",
        "relation": "cites",
        "tail": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"
      },
      {
        "head": "A Survey on Diffusion Language Models",
        "relation": "cites",
        "tail": "Diffusion Language Models are Super Data Learners"
      },
      {
        "head": "Training Optimal Large Diffusion Language Models",
        "relation": "cites",
        "tail": "Diffusion Language Models are Super Data Learners"
      },
      {
        "head": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models",
        "relation": "cites",
        "tail": "Diffusion Language Models are Super Data Learners"
      },
      {
        "head": "Encoder-Decoder Diffusion Language Models for Efficient Training and Inference",
        "relation": "cites",
        "tail": "Diffusion Language Models are Super Data Learners"
      },
      {
        "head": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model",
        "relation": "cites",
        "tail": "Diffusion Language Models are Super Data Learners"
      },
      {
        "head": "FLEX: Continuous Agent Evolution via Forward Learning from Experience",
        "relation": "cites",
        "tail": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "FLEX: Continuous Agent Evolution via Forward Learning from Experience",
        "relation": "cites",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "FLEX: Continuous Agent Evolution via Forward Learning from Experience",
        "relation": "cites",
        "tail": "Learning representations by back-propagating errors"
      },
      {
        "head": "FLEX: Continuous Agent Evolution via Forward Learning from Experience",
        "relation": "cites",
        "tail": "Language Models are Unsupervised Multitask Learners"
      },
      {
        "head": "FLEX: Continuous Agent Evolution via Forward Learning from Experience",
        "relation": "cites",
        "tail": "Proximal Policy Optimization Algorithms"
      },
      {
        "head": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications",
        "relation": "cites",
        "tail": "FLEX: Continuous Agent Evolution via Forward Learning from Experience"
      },
      {
        "head": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search",
        "relation": "cites",
        "tail": "FLEX: Continuous Agent Evolution via Forward Learning from Experience"
      },
      {
        "head": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking",
        "relation": "cites",
        "tail": "FLEX: Continuous Agent Evolution via Forward Learning from Experience"
      },
      {
        "head": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents",
        "relation": "cites",
        "tail": "FLEX: Continuous Agent Evolution via Forward Learning from Experience"
      },
      {
        "head": "Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting",
        "relation": "cites",
        "tail": "FLEX: Continuous Agent Evolution via Forward Learning from Experience"
      },
      {
        "head": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning",
        "relation": "cites",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning",
        "relation": "cites",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning",
        "relation": "cites",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning",
        "relation": "cites",
        "tail": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "head": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning",
        "relation": "cites",
        "tail": "ReAct: Synergizing Reasoning and Acting in Language Models"
      },
      {
        "head": "From System 1 to System 2: A Survey of Reasoning Large Language Models",
        "relation": "cites",
        "tail": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning"
      },
      {
        "head": "Deep Research Agents: A Systematic Examination And Roadmap",
        "relation": "cites",
        "tail": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning"
      },
      {
        "head": "Table-R1: Inference-Time Scaling for Table Reasoning",
        "relation": "cites",
        "tail": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning"
      },
      {
        "head": "Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis",
        "relation": "cites",
        "tail": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning"
      },
      {
        "head": "PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware Sequence-Level Policy Optimization",
        "relation": "cites",
        "tail": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning"
      },
      {
        "head": "Squeeze-and-Excitation Networks",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Squeeze-and-Excitation Networks",
        "relation": "cites",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Squeeze-and-Excitation Networks",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "Squeeze-and-Excitation Networks",
        "relation": "cites",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "Squeeze-and-Excitation Networks",
        "relation": "cites",
        "tail": "Long Short-Term Memory"
      },
      {
        "head": "Gradient-Guided Learning Network for Infrared Small Target Detection",
        "relation": "cites",
        "tail": "Squeeze-and-Excitation Networks"
      },
      {
        "head": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism",
        "relation": "cites",
        "tail": "Squeeze-and-Excitation Networks"
      },
      {
        "head": "A Retrieval-Augmented Generation Approach to Extracting Algorithmic Logic from Neural Networks",
        "relation": "cites",
        "tail": "Squeeze-and-Excitation Networks"
      },
      {
        "head": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness",
        "relation": "cites",
        "tail": "Squeeze-and-Excitation Networks"
      },
      {
        "head": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation",
        "relation": "cites",
        "tail": "Squeeze-and-Excitation Networks"
      },
      {
        "head": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
        "relation": "cites",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
        "relation": "cites",
        "tail": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
      },
      {
        "head": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
        "relation": "cites",
        "tail": "Dropout: a simple way to prevent neural networks from overfitting"
      },
      {
        "head": "CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation",
        "relation": "cites",
        "tail": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"
      },
      {
        "head": "IMobileTransformer: A fusion-based lightweight model for rice disease identification",
        "relation": "cites",
        "tail": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"
      },
      {
        "head": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers",
        "relation": "cites",
        "tail": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"
      },
      {
        "head": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images",
        "relation": "cites",
        "tail": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"
      },
      {
        "head": "Deep learning models for efficient geotechnical predictions: reducing training effort and data requirements with transfer learning",
        "relation": "cites",
        "tail": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"
      },
      {
        "head": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control",
        "relation": "cites",
        "tail": "Image quality assessment: from error visibility to structural similarity"
      },
      {
        "head": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control",
        "relation": "cites",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control",
        "relation": "cites",
        "tail": "Generative Adversarial Networks"
      },
      {
        "head": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control",
        "relation": "cites",
        "tail": "Denoising Diffusion Probabilistic Models"
      },
      {
        "head": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction",
        "relation": "cites",
        "tail": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control"
      },
      {
        "head": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos",
        "relation": "cites",
        "tail": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control"
      },
      {
        "head": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World",
        "relation": "cites",
        "tail": "Attention is All you Need"
      },
      {
        "head": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World",
        "relation": "cites",
        "tail": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"
      },
      {
        "head": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World",
        "relation": "cites",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World",
        "relation": "cites",
        "tail": "Scalable Diffusion Models with Transformers"
      },
      {
        "head": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World",
        "relation": "cites",
        "tail": "RoFormer: Enhanced Transformer with Rotary Position Embedding"
      },
      {
        "head": "The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey",
        "relation": "cites",
        "tail": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World"
      },
      {
        "head": "DVGT: Driving Visual Geometry Transformer",
        "relation": "cites",
        "tail": "Decoupled Weight Decay Regularization"
      },
      {
        "head": "DVGT: Driving Visual Geometry Transformer",
        "relation": "cites",
        "tail": "Vision meets robotics: The KITTI dataset"
      },
      {
        "head": "DVGT: Driving Visual Geometry Transformer",
        "relation": "cites",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "DVGT: Driving Visual Geometry Transformer",
        "relation": "cites",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "DVGT: Driving Visual Geometry Transformer",
        "relation": "cites",
        "tail": "Least-Squares Estimation of Transformation Parameters Between Two Point Patterns"
      },
      {
        "head": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations",
        "relation": "cites",
        "tail": "DVGT: Driving Visual Geometry Transformer"
      },
      {
        "head": "Adding Conditional Control to Text-to-Image Diffusion Models",
        "relation": "cites",
        "tail": "U-Net: Convolutional Networks for Biomedical Image Segmentation"
      },
      {
        "head": "Adding Conditional Control to Text-to-Image Diffusion Models",
        "relation": "cites",
        "tail": "Et al"
      },
      {
        "head": "Adding Conditional Control to Text-to-Image Diffusion Models",
        "relation": "cites",
        "tail": "Deep Learning"
      },
      {
        "head": "Adding Conditional Control to Text-to-Image Diffusion Models",
        "relation": "cites",
        "tail": "Gradient-based learning applied to document recognition"
      },
      {
        "head": "Adding Conditional Control to Text-to-Image Diffusion Models",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction",
        "relation": "cites",
        "tail": "Adding Conditional Control to Text-to-Image Diffusion Models"
      },
      {
        "head": "Seedream 4.0: Toward Next-generation Multimodal Image Generation",
        "relation": "cites",
        "tail": "Adding Conditional Control to Text-to-Image Diffusion Models"
      },
      {
        "head": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation",
        "relation": "cites",
        "tail": "Adding Conditional Control to Text-to-Image Diffusion Models"
      },
      {
        "head": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding",
        "relation": "cites",
        "tail": "Adding Conditional Control to Text-to-Image Diffusion Models"
      },
      {
        "head": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale",
        "relation": "cites",
        "tail": "Adding Conditional Control to Text-to-Image Diffusion Models"
      },
      {
        "head": "Scaling Instruction-Finetuned Language Models",
        "relation": "cites",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Scaling Instruction-Finetuned Language Models",
        "relation": "cites",
        "tail": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
      },
      {
        "head": "Scaling Instruction-Finetuned Language Models",
        "relation": "cites",
        "tail": "Language Models are Unsupervised Multitask Learners"
      },
      {
        "head": "Scaling Instruction-Finetuned Language Models",
        "relation": "cites",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "Scaling Instruction-Finetuned Language Models",
        "relation": "cites",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning",
        "relation": "cites",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling",
        "relation": "cites",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning",
        "relation": "cites",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Toward expert-level medical question answering with large language models",
        "relation": "cites",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training",
        "relation": "cites",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "High-Resolution Image Synthesis with Latent Diffusion Models",
        "relation": "cites",
        "tail": "Attention is All you Need"
      },
      {
        "head": "High-Resolution Image Synthesis with Latent Diffusion Models",
        "relation": "cites",
        "tail": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "High-Resolution Image Synthesis with Latent Diffusion Models",
        "relation": "cites",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "High-Resolution Image Synthesis with Latent Diffusion Models",
        "relation": "cites",
        "tail": "U-Net: Convolutional Networks for Biomedical Image Segmentation"
      },
      {
        "head": "High-Resolution Image Synthesis with Latent Diffusion Models",
        "relation": "cites",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "Equivariant Diffusion for Crystal Structure Prediction",
        "relation": "cites",
        "tail": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "head": "Generative Classifiers Avoid Shortcut Solutions",
        "relation": "cites",
        "tail": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "head": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling",
        "relation": "cites",
        "tail": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "head": "Native and Compact Structured Latents for 3D Generation",
        "relation": "cites",
        "tail": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "head": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
        "relation": "cites",
        "tail": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "head": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
        "relation": "cites",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
        "relation": "cites",
        "tail": "and as an in"
      },
      {
        "head": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
        "relation": "cites",
        "tail": "Et al"
      },
      {
        "head": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
        "relation": "cites",
        "tail": "Image quality assessment: from error visibility to structural similarity"
      },
      {
        "head": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?",
        "relation": "cites",
        "tail": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric"
      },
      {
        "head": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness",
        "relation": "cites",
        "tail": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric"
      },
      {
        "head": "Progressive Learned Image Transmission for Semantic Communication Using Hierarchical VAE",
        "relation": "cites",
        "tail": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric"
      },
      {
        "head": "3DGS-Drag: Dragging Gaussians for Intuitive Point-Based 3D Editing",
        "relation": "cites",
        "tail": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric"
      },
      {
        "head": "STORM: Search-Guided Generative World Models for Robotic Manipulation",
        "relation": "cites",
        "tail": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric"
      },
      {
        "head": "Attention Is All You Need",
        "relation": "proposed_model",
        "tail": "Transformer"
      },
      {
        "head": "Transformer",
        "relation": "evaluated_on",
        "tail": "WMT 2014 English-to-German translation task"
      },
      {
        "head": "Transformer",
        "relation": "evaluated_on",
        "tail": "WMT 2014 English-to-French translation task"
      },
      {
        "head": "Transformer",
        "relation": "evaluated_on",
        "tail": "English constituency parsing"
      },
      {
        "head": "Transformer",
        "relation": "uses_metric",
        "tail": "BLEU"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "proposed_model",
        "tail": "residual learning framework"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "proposed_model",
        "tail": "residual networks"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "baseline_model",
        "tail": "VGG nets"
      },
      {
        "head": "residual networks",
        "relation": "evaluated_on",
        "tail": "ImageNet dataset"
      },
      {
        "head": "residual networks",
        "relation": "evaluated_on",
        "tail": "CIFAR-10"
      },
      {
        "head": "residual networks",
        "relation": "evaluated_on",
        "tail": "COCO object detection dataset"
      },
      {
        "head": "residual networks",
        "relation": "evaluated_on",
        "tail": "ILSVRC 2015 classification task"
      },
      {
        "head": "residual networks",
        "relation": "evaluated_on",
        "tail": "ILSVRC & COCO 2015 competitions"
      },
      {
        "head": "residual networks",
        "relation": "evaluated_on",
        "tail": "ImageNet detection"
      },
      {
        "head": "residual networks",
        "relation": "evaluated_on",
        "tail": "ImageNet localization"
      },
      {
        "head": "residual networks",
        "relation": "evaluated_on",
        "tail": "COCO detection"
      },
      {
        "head": "residual networks",
        "relation": "evaluated_on",
        "tail": "COCO segmentation"
      },
      {
        "head": "Adam: A Method for Stochastic Optimization",
        "relation": "proposed_model",
        "tail": "Adam"
      },
      {
        "head": "Adam: A Method for Stochastic Optimization",
        "relation": "proposed_model",
        "tail": "AdaMax"
      },
      {
        "head": "Adam: A Method for Stochastic Optimization",
        "relation": "cites",
        "tail": "other stochastic optimization methods"
      },
      {
        "head": "Dropout: a simple way to prevent neural networks from overfitting",
        "relation": "proposed_model",
        "tail": "Dropout"
      },
      {
        "head": "Dropout",
        "relation": "evalu_on",
        "tail": "neural networks"
      },
      {
        "head": "Rethinking the Inception Architecture for Computer Vision",
        "relation": "proposed_model",
        "tail": "Inception Architecture"
      },
      {
        "head": "Rethinking the Inception Architecture for Computer Vision",
        "relation": "evaluated_on",
        "tail": "ILSVRC 2012 classification challenge validation set"
      },
      {
        "head": "Rethinking the Inception Architecture for Computer Vision",
        "relation": "uses_metric",
        "tail": "top-1 error"
      },
      {
        "head": "Rethinking the Inception Architecture for Computer Vision",
        "relation": "uses_metric",
        "tail": "top-5 error"
      },
      {
        "head": "Rethinking the Inception Architecture for Computer Vision",
        "relation": "cites",
        "tail": "convolutional networks"
      },
      {
        "head": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
        "relation": "proposed_model",
        "tail": "InstaDrive"
      },
      {
        "head": "InstaDrive",
        "relation": "uses_metric",
        "tail": "nuScenes dataset"
      },
      {
        "head": "InstaDrive",
        "relation": "evaluated_on",
        "tail": "nuScenes dataset"
      },
      {
        "head": "InstaDrive",
        "relation": "cites",
        "tail": "CARLA"
      },
      {
        "head": "CNC-VLM",
        "relation": "proposed_model",
        "tail": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection"
      },
      {
        "head": "CNC-VLM",
        "relation": "uses_metric",
        "tail": "CNC fault detection"
      },
      {
        "head": "CNC-VLM",
        "relation": "evaluated_on",
        "tail": "CNC fault detection"
      },
      {
        "head": "Enhanced Transformer-BiLSTM Deep Learning Framework for Day-Ahead Energy Price Forecasting",
        "relation": "proposed_model",
        "tail": "Transformer"
      },
      {
        "head": "Enhanced Transformer-BiLSTM Deep Learning Framework for Day-Ahead Energy Price Forecasting",
        "relation": "proposed_model",
        "tail": "Bidirectional Long Short-Term Memory (BiLSTM)"
      },
      {
        "head": "Enhanced Transformer-BiLSTM Deep Learning Framework for Day-Ahead Energy Price Forecasting",
        "relation": "evaluated_on",
        "tail": "New York Independent System Operator"
      },
      {
        "head": "Enhanced Transformer-BiLSTM Deep Learning Framework for Day-Ahead Energy Price Forecasting",
        "relation": "uses_metric",
        "tail": "MAE"
      },
      {
        "head": "Enhanced Transformer-BiLSTM Deep Learning Framework for Day-Ahead Energy Price Forecasting",
        "relation": "uses_metric",
        "tail": "RMSE"
      },
      {
        "head": "Enhanced Transformer-BiLSTM Deep Learning Framework for Day-Ahead Energy Price Forecasting",
        "relation": "uses_metric",
        "tail": "sMAPE"
      },
      {
        "head": "Enhanced Transformer-BiLSTM Deep Learning Framework for Day-Ahead Energy Price Forecasting",
        "relation": "uses_metric",
        "tail": "MAPE"
      },
      {
        "head": "Enhanced Transformer-BiLSTM Deep Learning Framework for Day-Ahead Energy Price Forecasting",
        "relation": "uses_metric",
        "tail": "R²"
      },
      {
        "head": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation",
        "relation": "uses_metric",
        "tail": "four-point laser metric calibration"
      },
      {
        "head": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation",
        "relation": "evaluated_on",
        "tail": "mamba segmentation"
      },
      {
        "head": "Alex Krizhevsky",
        "relation": "author_of",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "Ilya Sutskever",
        "relation": "author_of",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "Geoffrey E. Hinton",
        "relation": "author_of",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "ImageNet classification with deep convolutional neural networks",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "ImageNet classification with deep convolutional neural networks",
        "relation": "proposed_model",
        "tail": "deep convolutional neural network"
      },
      {
        "head": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
        "relation": "evaluated_on",
        "tail": "ImageNet Challenge 2014"
      },
      {
        "head": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
        "relation": "proposed_model",
        "tail": "ConvNet models"
      },
      {
        "head": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
        "relation": "proposed_model",
        "tail": "Faster R-CNN"
      },
      {
        "head": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
        "relation": "proposed_model",
        "tail": "Region Proposal Network (RPN)"
      },
      {
        "head": "Faster R-CNN",
        "relation": "cites",
        "tail": "SPPnet"
      },
      {
        "head": "Faster R-CNN",
        "relation": "cites",
        "tail": "Fast R-CNN"
      },
      {
        "head": "Faster R-CNN",
        "relation": "evaluated_on",
        "tail": "PASCAL VOC 2007"
      },
      {
        "head": "Faster R-CNN",
        "relation": "evaluated_on",
        "tail": "PASCAL VOC 2012"
      },
      {
        "head": "Faster R-CNN",
        "relation": "evaluated_on",
        "tail": "MS COCO"
      },
      {
        "head": "Faster R-CNN",
        "relation": "baseline_model",
        "tail": "Fast R-CNN"
      },
      {
        "head": "Faster R-CNN",
        "relation": "uses_metric",
        "tail": "frame rate"
      },
      {
        "head": "Region Proposal Network (RPN)",
        "relation": "baseline_model",
        "tail": "Fast R-CNN"
      },
      {
        "head": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning",
        "relation": "proposed_model",
        "tail": "Federated Learning Optimal Transport (FLOT)"
      },
      {
        "head": "Federated Learning Optimal Transport (FLOT)",
        "relation": "evaluated_on",
        "tail": "GTSRB"
      },
      {
        "head": "Federated Learning Optimal Transport (FLOT)",
        "relation": "evaluated_on",
        "tail": "KBTS"
      },
      {
        "head": "Federated Learning Optimal Transport (FLOT)",
        "relation": "evaluated_on",
        "tail": "CIFAR10"
      },
      {
        "head": "Federated Learning Optimal Transport (FLOT)",
        "relation": "evaluated_on",
        "tail": "EMNIST"
      },
      {
        "head": "DiffusionEngine: Diffusion model is scalable data engine for object detection",
        "relation": "proposed_model",
        "tail": "DiffusionEngine"
      },
      {
        "head": "DiffusionEngine: Diffusion model is scalable data engine for object detection",
        "relation": "evaluated_on",
        "tail": "object detection"
      },
      {
        "head": "Enhancing hyperspectral image prediction with contrastive learning in low-label regimes",
        "relation": "proposed_model",
        "tail": "contrastive learning"
      },
      {
        "head": "Enhancing hyperspectral image prediction with contrastive learning in low-label regimes",
        "relation": "evaluated_on",
        "tail": "hyperspectral image prediction"
      },
      {
        "head": "WarmGait",
        "relation": "proposed_model",
        "tail": "thermal array sensors"
      },
      {
        "head": "WarmGait",
        "relation": "uses_metric",
        "tail": "average recognition accuracy of 87.3%"
      },
      {
        "head": "WarmGait",
        "relation": "evaluated_on",
        "tail": "average recognition accuracy of 87.3%"
      },
      {
        "head": "edge module",
        "relation": "cites",
        "tail": "Taylor Finite Difference (TFD)"
      },
      {
        "head": "WarmGait",
        "relation": "baseline_model",
        "tail": "gait recognition"
      },
      {
        "head": "WarmGait",
        "relation": "author_of",
        "tail": "Person re-identification (Re-ID)"
      },
      {
        "head": "Auto-Encoding Variational Bayes",
        "relation": "proposed_model",
        "tail": "stochastic variational inference and learning algorithm"
      },
      {
        "head": "stochastic variational inference and learning algorithm",
        "relation": "uses_metric",
        "tail": "variational lower bound"
      },
      {
        "head": "stochastic variational inference and learning algorithm",
        "relation": "evaluated_on",
        "tail": "i.i.d. datasets"
      },
      {
        "head": "stochastic variational inference and learning algorithm",
        "relation": "proposed_model",
        "tail": "lower bound estimator"
      },
      {
        "head": "lower bound estimator",
        "relation": "uses_metric",
        "tail": "variational lower bound"
      },
      {
        "head": "lower bound estimator",
        "relation": "baseline_model",
        "tail": "standard stochastic gradient methods"
      },
      {
        "head": "stochastic variational inference and learning algorithm",
        "relation": "proposed_model",
        "tail": "approximate inference model"
      },
      {
        "head": "approximate inference model",
        "relation": "cites",
        "tail": "recognition model"
      },
      {
        "head": "Supporting Online Material for Reducing the Dimensionality of Data with Neural Networks",
        "relation": "cites",
        "tail": "Reducing the Dimensionality of Data with Neural Networks"
      },
      {
        "head": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization",
        "relation": "evaluated_on",
        "tail": "Online Learning"
      },
      {
        "head": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization",
        "relation": "evaluated_on",
        "tail": "Stochastic Optimization"
      },
      {
        "head": "Speech recognition with deep recurrent neural networks",
        "relation": "proposed_model",
        "tail": "deep recurrent neural networks"
      },
      {
        "head": "Speech recognition with deep recurrent neural networks",
        "relation": "proposed_model",
        "tail": "deep Long Short-term Memory RNNs"
      },
      {
        "head": "deep Long Short-term Memory RNNs",
        "relation": "evaluated_on",
        "tail": "TIMIT phoneme recognition benchmark"
      },
      {
        "head": "deep Long Short-term Memory RNNs",
        "relation": "uses_metric",
        "tail": "test set error"
      },
      {
        "head": "Speech recognition with deep recurrent neural networks",
        "relation": "cites",
        "tail": "Recurrent neural networks"
      },
      {
        "head": "Speech recognition with deep recurrent neural networks",
        "relation": "cites",
        "tail": "Connectionist Temporal Classification"
      },
      {
        "head": "Speech recognition with deep recurrent neural networks",
        "relation": "cites",
        "tail": "Long Short-term Memory"
      },
      {
        "head": "Weakly Supervised Image Dehazing via Physics-Based Decomposition",
        "relation": "proposed_model",
        "tail": "PBD"
      },
      {
        "head": "Weakly Supervised Image Dehazing via Physics-Based Decomposition",
        "relation": "author_of",
        "tail": "NianWang-HJJGCDX"
      },
      {
        "head": "PBD",
        "relation": "uses_metric",
        "tail": "reconstruction loss"
      },
      {
        "head": "PBD",
        "relation": "evaluated_on",
        "tail": "seven benchmarks"
      },
      {
        "head": "PBD",
        "relation": "baseline_model",
        "tail": "GAN"
      },
      {
        "head": "PBD",
        "relation": "cites",
        "tail": "GAN"
      },
      {
        "head": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer",
        "relation": "proposed_model",
        "tail": "face mask detection model"
      },
      {
        "head": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer",
        "relation": "baseline_model",
        "tail": "Adam optimizer"
      },
      {
        "head": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer",
        "relation": "evaluated_on",
        "tail": "face mask detection model"
      },
      {
        "head": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer",
        "relation": "uses_metric",
        "tail": "accuracy rate"
      },
      {
        "head": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer",
        "relation": "cites",
        "tail": "AdamW optimizer"
      },
      {
        "head": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer",
        "relation": "cites",
        "tail": "Adam optimizer"
      },
      {
        "head": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer",
        "relation": "cites",
        "tail": "Adam with L2-regularization"
      },
      {
        "head": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
        "relation": "proposed_model",
        "tail": "Engram"
      },
      {
        "head": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
        "relation": "baseline_model",
        "tail": "Mixture-of-Experts (MoE)"
      },
      {
        "head": "Engram",
        "relation": "evaluated_on",
        "tail": "MMLU"
      },
      {
        "head": "Engram",
        "relation": "evaluated_on",
        "tail": "CMMLU"
      },
      {
        "head": "Engram",
        "relation": "evaluated_on",
        "tail": "BBH"
      },
      {
        "head": "Engram",
        "relation": "evaluated_on",
        "tail": "ARC-Challenge"
      },
      {
        "head": "Engram",
        "relation": "evaluated_on",
        "tail": "HumanEval"
      },
      {
        "head": "Engram",
        "relation": "evaluated_on",
        "tail": "MATH"
      },
      {
        "head": "Engram",
        "relation": "evaluated_on",
        "tail": "Multi-Query NIAH"
      },
      {
        "head": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
        "relation": "cites",
        "tail": "Transformers"
      },
      {
        "head": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction",
        "relation": "proposed_model",
        "tail": "Neural Hierarchical Interpolation for Time Series Forecasting (N-HiTS)"
      },
      {
        "head": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction",
        "relation": "proposed_model",
        "tail": "Network-Based Expansion Analysis for Interpretable Time Series Forecasting (N-BEATS)"
      },
      {
        "head": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction",
        "relation": "baseline_model",
        "tail": "long short-term memory (LSTM)"
      },
      {
        "head": "Neural Hierarchical Interpolation for Time Series Forecasting (N-HiTS)",
        "relation": "evaluated_on",
        "tail": "flood prediction"
      },
      {
        "head": "Network-Based Expansion Analysis for Interpretable Time Series Forecasting (N-BEATS)",
        "relation": "evaluated_on",
        "tail": "flood prediction"
      },
      {
        "head": "long short-term memory (LSTM)",
        "relation": "evaluated_on",
        "tail": "flood prediction"
      },
      {
        "head": "Neural Hierarchical Interpolation for Time Series Forecasting (N-HiTS)",
        "relation": "evaluated_on",
        "tail": "headwater streams in Georgia and North Carolina, USA"
      },
      {
        "head": "Network-Based Expansion Analysis for Interpretable Time Series Forecasting (N-BEATS)",
        "relation": "evaluated_on",
        "tail": "headwater streams in Georgia and North Carolina, USA"
      },
      {
        "head": "long short-term memory (LSTM)",
        "relation": "evaluated_on",
        "tail": "headwater streams in Georgia and North Carolina, USA"
      },
      {
        "head": "Neural Hierarchical Interpolation for Time Series Forecasting (N-HiTS)",
        "relation": "uses_metric",
        "tail": "Multi-Quantile Loss"
      },
      {
        "head": "Network-Based Expansion Analysis for Interpretable Time Series Forecasting (N-BEATS)",
        "relation": "uses_metric",
        "tail": "Multi-Quantile Loss"
      },
      {
        "head": "Multi-Quantile Loss",
        "relation": "uses_metric",
        "tail": "95th percentile prediction uncertainty (95 PPU)"
      },
      {
        "head": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model",
        "relation": "uses_metric",
        "tail": "harmonized Landsat and Sentinel-2 data"
      },
      {
        "head": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model",
        "relation": "proposed_model",
        "tail": "NASA-IBM geospatial foundation model"
      },
      {
        "head": "Going deeper with convolutions",
        "relation": "proposed_model",
        "tail": "Inception"
      },
      {
        "head": "Going deeper with convolutions",
        "relation": "proposed_model",
        "tail": "GoogLeNet"
      },
      {
        "head": "Inception",
        "relation": "evaluated_on",
        "tail": "ImageNet Large-Scale Visual Recognition Challenge 2014"
      },
      {
        "head": "Inception",
        "relation": "evaluated_on",
        "tail": "ILSVRC 2014"
      },
      {
        "head": "GoogLeNet",
        "relation": "evaluated_on",
        "tail": "ImageNet Large-Scale Visual Recognition Challenge 2014"
      },
      {
        "head": "GoogLeNet",
        "relation": "evaluated_on",
        "tail": "ILSVRC 2014"
      },
      {
        "head": "Batch Normalization",
        "relation": "proposed_model",
        "tail": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
      },
      {
        "head": "Batch Normalization",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "Batch Normalization",
        "relation": "uses_metric",
        "tail": "top-5 validation error"
      },
      {
        "head": "Batch Normalization",
        "relation": "uses_metric",
        "tail": "test error"
      },
      {
        "head": "Batch Normalization",
        "relation": "baseline_model",
        "tail": "state-of-the-art image classification model"
      },
      {
        "head": "Batch Normalization",
        "relation": "cites",
        "tail": "Dropout"
      },
      {
        "head": "This paper",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "This paper",
        "relation": "cites",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Diffusion Transformers with Representation Autoencoders",
        "relation": "proposed_model",
        "tail": "Representation Autoencoders (RAEs)"
      },
      {
        "head": "Diffusion Transformers with Representation Autoencoders",
        "relation": "baseline_model",
        "tail": "VAE encoder"
      },
      {
        "head": "Diffusion Transformers with Representation Autoencoders",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "Diffusion Transformers with Representation Autoencoders",
        "relation": "uses_metric",
        "tail": "FID"
      },
      {
        "head": "Representation Autoencoders (RAEs)",
        "relation": "cites",
        "tail": "DINO"
      },
      {
        "head": "Representation Autoencoders (RAEs)",
        "relation": "cites",
        "tail": "SigLIP"
      },
      {
        "head": "Representation Autoencoders (RAEs)",
        "relation": "cites",
        "tail": "MAE"
      },
      {
        "head": "Diffusion Transformers (DiT)",
        "relation": "cites",
        "tail": "VAE encoder"
      },
      {
        "head": "Diffusion Transformers with Representation Autoencoders",
        "relation": "cites",
        "tail": "Diffusion Transformers (DiT)"
      },
      {
        "head": "Diffusion Transformers with Representation Autoencoders",
        "relation": "cites",
        "tail": "DDT head"
      },
      {
        "head": "Scaling Large Language Models for Next-Generation Single-Cell Analysis",
        "relation": "proposed_model",
        "tail": "C2S-Scale"
      },
      {
        "head": "Scaling Large Language Models for Next-Generation Single-Cell Analysis",
        "relation": "baseline_model",
        "tail": "Cell2Sentence (C2S) framework"
      },
      {
        "head": "Scaling Large Language Models for Next-Generation Single-Cell Analysis",
        "relation": "baseline_model",
        "tail": "Large Language Models (LLMs)"
      },
      {
        "head": "C2S-Scale",
        "relation": "evaluated_on",
        "tail": "single-cell RNA sequencing"
      },
      {
        "head": "C2S-Scale",
        "relation": "evaluated_on",
        "tail": "transcriptomic data"
      },
      {
        "head": "C2S-Scale",
        "relation": "evaluated_on",
        "tail": "biological text"
      },
      {
        "head": "C2S-Scale",
        "relation": "evaluated_on",
        "tail": "metadata"
      },
      {
        "head": "C2S-Scale",
        "relation": "evaluated_on",
        "tail": "human cell models"
      },
      {
        "head": "C2S-Scale",
        "relation": "uses_metric",
        "tail": "predictive and generative capabilities"
      },
      {
        "head": "C2S-Scale",
        "relation": "uses_metric",
        "tail": "perturbation response prediction"
      },
      {
        "head": "C2S-Scale",
        "relation": "uses_metric",
        "tail": "natural language interpretation"
      },
      {
        "head": "C2S-Scale",
        "relation": "uses_metric",
        "tail": "complex biological reasoning"
      },
      {
        "head": "Scaling Large Language Models for Next-Generation Single-Cell Analysis",
        "relation": "cites",
        "tail": "Cell2Sentence (C2S) framework"
      },
      {
        "head": "C2S-Scale",
        "relation": "cites",
        "tail": "silmitasertib (CX-4945)"
      },
      {
        "head": "HybridVisionNet: An advanced hybrid deep learning framework for automated multi-class ocular disease diagnosis using fundus imaging",
        "relation": "proposed_model",
        "tail": "HybridVisionNet"
      },
      {
        "head": "HybridVisionNet",
        "relation": "evaluated_on",
        "tail": "fundus imaging"
      },
      {
        "head": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights",
        "relation": "author_of",
        "tail": "authors"
      },
      {
        "head": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights",
        "relation": "proposed_model",
        "tail": "transfer attacks"
      },
      {
        "head": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights",
        "relation": "evaluated_on",
        "tail": "adversarial images"
      },
      {
        "head": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights",
        "relation": "evaluated_on",
        "tail": "defenses"
      },
      {
        "head": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights",
        "relation": "cites",
        "tail": "DI"
      },
      {
        "head": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights",
        "relation": "cites",
        "tail": "DiffPure"
      },
      {
        "head": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights",
        "relation": "cites",
        "tail": "Google Cloud Vision"
      },
      {
        "head": "transfer attacks",
        "relation": "baseline_model",
        "tail": "DI"
      },
      {
        "head": "transfer attacks",
        "relation": "evaluated_on",
        "tail": "defenses"
      },
      {
        "head": "transfer attacks",
        "relation": "uses_metric",
        "tail": "imperceptibility metrics"
      },
      {
        "head": "transfer attacks",
        "relation": "uses_metric",
        "tail": "user study"
      },
      {
        "head": "transfer attacks",
        "relation": "uses_metric",
        "tail": "Lp constraint"
      },
      {
        "head": "defenses",
        "relation": "evaluated_on",
        "tail": "adversarial images"
      },
      {
        "head": "defenses",
        "relation": "includes",
        "tail": "DiffPure"
      },
      {
        "head": "defenses",
        "relation": "includes",
        "tail": "Google Cloud Vision"
      },
      {
        "head": "DI",
        "relation": "evaluated_on",
        "tail": "adversarial images"
      },
      {
        "head": "DiffPure",
        "relation": "evaluated_on",
        "tail": "adversarial images"
      },
      {
        "head": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot",
        "relation": "proposed_model",
        "tail": "YOLO-OG"
      },
      {
        "head": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot",
        "relation": "proposed_model",
        "tail": "OGNet"
      },
      {
        "head": "OGNet",
        "relation": "evaluated_on",
        "tail": "Dish-10"
      },
      {
        "head": "YOLO-OG",
        "relation": "evaluated_on",
        "tail": "Dish-10"
      },
      {
        "head": "YOLO-OG",
        "relation": "evaluated_on",
        "tail": "Dish-20"
      },
      {
        "head": "YOLO-OG",
        "relation": "uses_metric",
        "tail": "mean Average Precision (mAP)"
      },
      {
        "head": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot",
        "relation": "cites",
        "tail": "Empty-dish Recycling Robot"
      },
      {
        "head": "Attention is All you Need",
        "relation": "proposed_model",
        "tail": "Transformer"
      },
      {
        "head": "Transformer",
        "relation": "evaluated_on",
        "tail": "WMT 2014 English-to-German translation task"
      },
      {
        "head": "Transformer",
        "relation": "evaluated_on",
        "tail": "WMT 2014 English-to-French translation task"
      },
      {
        "head": "Transformer",
        "relation": "evaluated_on",
        "tail": "English constituency parsing"
      },
      {
        "head": "Transformer",
        "relation": "uses_metric",
        "tail": "BLEU"
      },
      {
        "head": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "relation": "proposed_model",
        "tail": "unified framework that converts all text-based language problems into a text-to-text format"
      },
      {
        "head": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "relation": "evaluated_on",
        "tail": "dozens of language understanding tasks"
      },
      {
        "head": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "relation": "uses_metric",
        "tail": "state-of-the-art results"
      },
      {
        "head": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "relation": "cites",
        "tail": "transfer learning"
      },
      {
        "head": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "relation": "author_of",
        "tail": "we"
      },
      {
        "head": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
        "relation": "proposed_model",
        "tail": "Two Time-Scale Update Rule"
      },
      {
        "head": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
        "relation": "evaluated_on",
        "tail": "Local Nash Equilibrium"
      },
      {
        "head": "nuScenes",
        "relation": "baseline_model",
        "tail": "baselines for lidar and image based detection and tracking"
      },
      {
        "head": "nuScenes",
        "relation": "evaluated_on",
        "tail": "3D detection and tracking metrics"
      },
      {
        "head": "nuScenes",
        "relation": "cites",
        "tail": "KITTI dataset"
      },
      {
        "head": "nuScenes",
        "relation": "author_of",
        "tail": "Robust detection and tracking of objects"
      },
      {
        "head": "nuScenes",
        "relation": "proposed_model",
        "tail": "nuTonomy scenes"
      },
      {
        "head": "Image based benchmark datasets",
        "relation": "uses_metric",
        "tail": "computer vision tasks"
      },
      {
        "head": "autonomous vehicle technology",
        "relation": "cites",
        "tail": "machine learning based methods for detection and tracking"
      },
      {
        "head": "CARLA: An Open Urban Driving Simulator",
        "relation": "proposed_model",
        "tail": "CARLA"
      },
      {
        "head": "CARLA",
        "relation": "evaluated_on",
        "tail": "autonomous driving research"
      },
      {
        "head": "CARLA",
        "relation": "uses_metric",
        "tail": "autonomous driving research"
      },
      {
        "head": "classic modular pipeline",
        "relation": "baseline_model",
        "tail": "CARLA"
      },
      {
        "head": "end-to-end model trained via imitation learning",
        "relation": "baseline_model",
        "tail": "CARLA"
      },
      {
        "head": "end-to-end model trained via reinforcement learning",
        "relation": "baseline_model",
        "tail": "CARLA"
      },
      {
        "head": "Sora as a World Model? A Complete Survey on Text-to-Video Generation",
        "relation": "author_of",
        "tail": "Person"
      },
      {
        "head": "Sora as a World Model? A Complete Survey on Text-to-Video Generation",
        "relation": "cites",
        "tail": "250+ studies on text-based video synthesis and world modeling"
      },
      {
        "head": "Sora",
        "relation": "proposed_model",
        "tail": "world modeling"
      },
      {
        "head": "text-to-video generation",
        "relation": "evaluated_on",
        "tail": "world modeling"
      },
      {
        "head": "text-to-video generation",
        "relation": "uses_metric",
        "tail": "completeness, consistency, invention, human interaction and control"
      },
      {
        "head": "OmniNWM: Omniscient Driving Navigation World Models",
        "relation": "proposed_model",
        "tail": "OmniNWM"
      },
      {
        "head": "OmniNWM",
        "relation": "baseline_model",
        "tail": "autonomous driving world models"
      },
      {
        "head": "OmniNWM",
        "relation": "evaluated_on",
        "tail": "panoramic videos"
      },
      {
        "head": "OmniNWM",
        "relation": "evaluated_on",
        "tail": "3D occupancy"
      },
      {
        "head": "OmniNWM",
        "relation": "uses_metric",
        "tail": "occupancy-grounded rewards"
      },
      {
        "head": "OmniNWM",
        "relation": "cites",
        "tail": "autonomous driving world models"
      },
      {
        "head": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask",
        "relation": "proposed_model",
        "tail": "ConsisDrive"
      },
      {
        "head": "ConsisDrive",
        "relation": "uses_metric",
        "tail": "Instance-Masked Attention"
      },
      {
        "head": "ConsisDrive",
        "relation": "uses_metric",
        "tail": "Instance-Masked Loss"
      },
      {
        "head": "ConsisDrive",
        "relation": "evaluated_on",
        "tail": "nuScenes"
      },
      {
        "head": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving",
        "relation": "proposed_model",
        "tail": "UniDriveDreamer"
      },
      {
        "head": "UniDriveDreamer",
        "relation": "uses_metric",
        "tail": "multi-camera video"
      },
      {
        "head": "UniDriveDreamer",
        "relation": "uses_metric",
        "tail": "LiDAR sequence"
      },
      {
        "head": "UniDriveDreamer",
        "relation": "evaluated_on",
        "tail": "autonomous driving"
      },
      {
        "head": "UniDriveDreamer",
        "relation": "baseline_model",
        "tail": "previous state-of-the-art methods"
      },
      {
        "head": "UniDriveDreamer",
        "relation": "cites",
        "tail": "World models"
      },
      {
        "head": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
        "relation": "proposed_model",
        "tail": "MAD-LTX"
      },
      {
        "head": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
        "relation": "baseline_model",
        "tail": "SVD"
      },
      {
        "head": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
        "relation": "evaluated_on",
        "tail": "autonomous driving"
      },
      {
        "head": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
        "relation": "cites",
        "tail": "video diffusion models"
      },
      {
        "head": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
        "relation": "cites",
        "tail": "driving world models"
      },
      {
        "head": "What matters for Representation Alignment: Global Information or Spatial Structure?",
        "relation": "author_of",
        "tail": "Person"
      },
      {
        "head": "REPA",
        "relation": "proposed_model",
        "tail": "What matters for Representation Alignment: Global Information or Spatial Structure?"
      },
      {
        "head": "iREPA",
        "relation": "proposed_model",
        "tail": "What matters for Representation Alignment: Global Information or Spatial Structure?"
      },
      {
        "head": "REPA",
        "relation": "evaluated_on",
        "tail": "ImageNet-1K"
      },
      {
        "head": "iREPA",
        "relation": "evaluated_on",
        "tail": "ImageNet-1K"
      },
      {
        "head": "iREPA",
        "relation": "baseline_model",
        "tail": "REPA"
      },
      {
        "head": "iREPA",
        "relation": "baseline_model",
        "tail": "REPA-E"
      },
      {
        "head": "iREPA",
        "relation": "baseline_model",
        "tail": "Meanflow"
      },
      {
        "head": "iREPA",
        "relation": "baseline_model",
        "tail": "JiT"
      },
      {
        "head": "SCB-DETR",
        "relation": "proposed_model",
        "tail": "SCB-DETR: Multiscale Deformable Transformers for Occlusion-Resilient Student Learning Behavior Detection in Smart Classroom"
      },
      {
        "head": "SCB-DETR: Multiscale Deformable Transformers for Occlusion-Resilient Student Learning Behavior Detection in Smart Classroom",
        "relation": "author_of",
        "tail": "We"
      },
      {
        "head": "SCB-DETR",
        "relation": "evaluated_on",
        "tail": "SCBehavior dataset"
      },
      {
        "head": "SCB-DETR",
        "relation": "uses_metric",
        "tail": "mean Average Precision (mAP)"
      },
      {
        "head": "SCB-DETR",
        "relation": "uses_metric",
        "tail": "AP50"
      },
      {
        "head": "SCB-DETR",
        "relation": "baseline_model",
        "tail": "baseline model"
      },
      {
        "head": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination",
        "relation": "proposed_model",
        "tail": "ultrasound-cardiac-feature-net (UCF-Net)"
      },
      {
        "head": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination",
        "relation": "proposed_model",
        "tail": "filtered integral quasi-super-twisting algorithm (FIQSTA)"
      },
      {
        "head": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination",
        "relation": "baseline_model",
        "tail": "proportional (P) controller"
      },
      {
        "head": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination",
        "relation": "baseline_model",
        "tail": "sliding mode controller"
      },
      {
        "head": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination",
        "relation": "baseline_model",
        "tail": "super-twisting algorithm (STA)"
      },
      {
        "head": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination",
        "relation": "baseline_model",
        "tail": "integral quasi-STA"
      },
      {
        "head": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination",
        "relation": "evaluated_on",
        "tail": "cardiac phantom"
      },
      {
        "head": "ultrasound-cardiac-feature-net (UCF-Net)",
        "relation": "uses_metric",
        "tail": "deep ultrasound image features"
      },
      {
        "head": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination",
        "relation": "cites",
        "tail": "proportional (P) controller"
      },
      {
        "head": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination",
        "relation": "cites",
        "tail": "sliding mode controller"
      },
      {
        "head": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination",
        "relation": "cites",
        "tail": "super-twisting algorithm (STA)"
      },
      {
        "head": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination",
        "relation": "cites",
        "tail": "integral quasi-STA"
      },
      {
        "head": "BioTune",
        "relation": "proposed_model",
        "tail": "Bio-Inspired Fine-Tuning for Selective Transfer Learning in Image Classification"
      },
      {
        "head": "BioTune",
        "relation": "evaluated_on",
        "tail": "nine image classification datasets"
      },
      {
        "head": "BioTune",
        "relation": "evaluated_on",
        "tail": "medical imaging"
      },
      {
        "head": "BioTune",
        "relation": "evaluated_on",
        "tail": "four different CNN architectures"
      },
      {
        "head": "BioTune",
        "relation": "baseline_model",
        "tail": "AutoRGN"
      },
      {
        "head": "BioTune",
        "relation": "baseline_model",
        "tail": "LoRA"
      },
      {
        "head": "Hand Sign Language Detection Using Deep Learning",
        "relation": "proposed_model",
        "tail": "VGG-16 net"
      },
      {
        "head": "Hand Sign Language Detection Using Deep Learning",
        "relation": "evaluated_on",
        "tail": "NUS dataset"
      },
      {
        "head": "Hand Sign Language Detection Using Deep Learning",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "VGG-16 net",
        "relation": "baseline_model",
        "tail": "convolutional neural network"
      },
      {
        "head": "VGG-16 net",
        "relation": "evaluated_on",
        "tail": "Google's open source Application Programming Interface (API)"
      },
      {
        "head": "VGG-16 net",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "Representation Learning: A Review and New Perspectives",
        "relation": "author_of",
        "tail": "Person"
      },
      {
        "head": "Representation Learning: A Review and New Perspectives",
        "relation": "cites",
        "tail": "unsupervised feature learning"
      },
      {
        "head": "Representation Learning: A Review and New Perspectives",
        "relation": "cites",
        "tail": "deep learning"
      },
      {
        "head": "Representation Learning: A Review and New Perspectives",
        "relation": "cites",
        "tail": "probabilistic models"
      },
      {
        "head": "Representation Learning: A Review and New Perspectives",
        "relation": "cites",
        "tail": "auto-encoders"
      },
      {
        "head": "Representation Learning: A Review and New Perspectives",
        "relation": "cites",
        "tail": "manifold learning"
      },
      {
        "head": "Representation Learning: A Review and New Perspectives",
        "relation": "cites",
        "tail": "deep networks"
      },
      {
        "head": "representation learning",
        "relation": "proposed_model",
        "tail": "machine learning algorithms"
      },
      {
        "head": "representation learning",
        "relation": "evaluated_on",
        "tail": "density estimation"
      },
      {
        "head": "representation learning",
        "relation": "evaluated_on",
        "tail": "manifold learning"
      },
      {
        "head": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion",
        "relation": "proposed_model",
        "tail": "Stacked Denoising Autoencoders"
      },
      {
        "head": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models",
        "relation": "proposed_model",
        "tail": "Stochastic Backpropagation"
      },
      {
        "head": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models",
        "relation": "proposed_model",
        "tail": "Deep Generative Models"
      },
      {
        "head": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation",
        "relation": "proposed_model",
        "tail": "SDXL-Lightning models"
      },
      {
        "head": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation",
        "relation": "uses_metric",
        "tail": "quality"
      },
      {
        "head": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation",
        "relation": "uses_metric",
        "tail": "mode coverage"
      },
      {
        "head": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation",
        "relation": "evaluated_on",
        "tail": "1024px text-to-image generation"
      },
      {
        "head": "SDXL-Lightning models",
        "relation": "baseline_model",
        "tail": "SDXL"
      },
      {
        "head": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions",
        "relation": "proposed_model",
        "tail": "EchoMimic"
      },
      {
        "head": "EchoMimic",
        "relation": "uses_metric",
        "tail": "audios"
      },
      {
        "head": "EchoMimic",
        "relation": "uses_metric",
        "tail": "facial landmarks"
      },
      {
        "head": "EchoMimic",
        "relation": "evaluated_on",
        "tail": "public datasets"
      },
      {
        "head": "EchoMimic",
        "relation": "evaluated_on",
        "tail": "collected dataset"
      },
      {
        "head": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions",
        "relation": "cites",
        "tail": "portrait image animation"
      },
      {
        "head": "GenAD: Generative End-to-End Autonomous Driving",
        "relation": "proposed_model",
        "tail": "GenAD"
      },
      {
        "head": "GenAD: Generative End-to-End Autonomous Driving",
        "relation": "evaluated_on",
        "tail": "nuScenes"
      },
      {
        "head": "GenAD",
        "relation": "uses_metric",
        "tail": "state-of-the-art performance"
      },
      {
        "head": "GenAD",
        "relation": "baseline_model",
        "tail": "end-to-end autonomous driving methods"
      },
      {
        "head": "GenAD",
        "relation": "cites",
        "tail": "end-to-end autonomous driving methods"
      },
      {
        "head": "GenAD",
        "relation": "author_of",
        "tail": "wzzheng"
      },
      {
        "head": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model",
        "relation": "proposed_model",
        "tail": "Ovis"
      },
      {
        "head": "Ovis",
        "relation": "baseline_model",
        "tail": "open-source MLLMs of similar parameter scales"
      },
      {
        "head": "Ovis",
        "relation": "baseline_model",
        "tail": "Qwen-VL-Plus"
      },
      {
        "head": "Ovis",
        "relation": "evaluated_on",
        "tail": "various multimodal benchmarks"
      },
      {
        "head": "Ovis",
        "relation": "cites",
        "tail": "Multimodal Large Language Models (MLLMs)"
      },
      {
        "head": "Multimodal Large Language Models (MLLMs)",
        "relation": "uses_metric",
        "tail": "structural textual embeddings"
      },
      {
        "head": "Multimodal Large Language Models (MLLMs)",
        "relation": "uses_metric",
        "tail": "continuous embeddings"
      },
      {
        "head": "Ovis",
        "relation": "uses_metric",
        "tail": "visual embedding table"
      },
      {
        "head": "Ovis",
        "relation": "uses_metric",
        "tail": "probabilistic combination"
      },
      {
        "head": "Improving Video Generation with Human Feedback",
        "relation": "proposed_model",
        "tail": "VideoReward"
      },
      {
        "head": "Improving Video Generation with Human Feedback",
        "relation": "proposed_model",
        "tail": "Flow-DPO"
      },
      {
        "head": "Improving Video Generation with Human Feedback",
        "relation": "proposed_model",
        "tail": "Flow-RWR"
      },
      {
        "head": "Improving Video Generation with Human Feedback",
        "relation": "proposed_model",
        "tail": "Flow-NRG"
      },
      {
        "head": "Improving Video Generation with Human Feedback",
        "relation": "baseline_model",
        "tail": "rectified flow techniques"
      },
      {
        "head": "Improving Video Generation with Human Feedback",
        "relation": "baseline_model",
        "tail": "supervised fine-tuning methods"
      },
      {
        "head": "VideoReward",
        "relation": "evaluated_on",
        "tail": "large-scale human preference dataset"
      },
      {
        "head": "Flow-DPO",
        "relation": "evaluated_on",
        "tail": "large-scale human preference dataset"
      },
      {
        "head": "Flow-RWR",
        "relation": "evaluated_on",
        "tail": "large-scale human preference dataset"
      },
      {
        "head": "Flow-NRG",
        "relation": "evaluated_on",
        "tail": "large-scale human preference dataset"
      },
      {
        "head": "VideoReward",
        "relation": "uses_metric",
        "tail": "rewarding efficacy"
      },
      {
        "head": "Flow-DPO",
        "relation": "uses_metric",
        "tail": "rewarding efficacy"
      },
      {
        "head": "Flow-RWR",
        "relation": "uses_metric",
        "tail": "rewarding efficacy"
      },
      {
        "head": "Flow-NRG",
        "relation": "uses_metric",
        "tail": "rewarding efficacy"
      },
      {
        "head": "Improving Video Generation with Human Feedback",
        "relation": "cites",
        "tail": "rectified flow techniques"
      },
      {
        "head": "Improving Video Generation with Human Feedback",
        "relation": "cites",
        "tail": "modern video generation models"
      },
      {
        "head": "Speech Recognition with Deep Recurrent Neural Networks",
        "relation": "proposed_model",
        "tail": "deep recurrent neural networks"
      },
      {
        "head": "Speech Recognition with Deep Recurrent Neural Networks",
        "relation": "proposed_model",
        "tail": "deep Long Short-term Memory RNNs"
      },
      {
        "head": "Speech Recognition with Deep Recurrent Neural Networks",
        "relation": "evaluated_on",
        "tail": "TIMIT phoneme recognition benchmark"
      },
      {
        "head": "deep recurrent neural networks",
        "relation": "cites",
        "tail": "Recurrent neural networks"
      },
      {
        "head": "deep recurrent neural networks",
        "relation": "cites",
        "tail": "Long Short-term Memory RNN"
      },
      {
        "head": "deep recurrent neural networks",
        "relation": "uses_metric",
        "tail": "test set error"
      },
      {
        "head": "Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks",
        "relation": "proposed_model",
        "tail": "Connectionist Temporal Classification"
      },
      {
        "head": "Connectionist Temporal Classification",
        "relation": "uses_metric",
        "tail": "Recurrent Neural Networks"
      },
      {
        "head": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures",
        "relation": "proposed_model",
        "tail": "bidirectional LSTM"
      },
      {
        "head": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures",
        "relation": "proposed_model",
        "tail": "neural network architectures"
      },
      {
        "head": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses",
        "relation": "proposed_model",
        "tail": "machine learning"
      },
      {
        "head": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses",
        "relation": "proposed_model",
        "tail": "artificial synapses"
      },
      {
        "head": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses",
        "relation": "evaluated_on",
        "tail": "flexible sensors"
      },
      {
        "head": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses",
        "relation": "evaluated_on",
        "tail": "flexible sensory systems"
      },
      {
        "head": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses",
        "relation": "evaluated_on",
        "tail": "soft/humanoid robotics"
      },
      {
        "head": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses",
        "relation": "evaluated_on",
        "tail": "human activity monitoring"
      },
      {
        "head": "A high-performance speech neuroprosthesis",
        "relation": "proposed_model",
        "tail": "speech-to-text BCI"
      },
      {
        "head": "speech-to-text BCI",
        "relation": "uses_metric",
        "tail": "word error rate"
      },
      {
        "head": "speech-to-text BCI",
        "relation": "evaluated_on",
        "tail": "50-word vocabulary"
      },
      {
        "head": "speech-to-text BCI",
        "relation": "evaluated_on",
        "tail": "125,000-word vocabulary"
      },
      {
        "head": "speech-to-text BCI",
        "relation": "cites",
        "tail": "speech brain–computer interfaces (BCIs)"
      },
      {
        "head": "A high-performance speech neuroprosthesis",
        "relation": "author_of",
        "tail": "study participant"
      },
      {
        "head": "Loss of plasticity in deep continual learning",
        "relation": "author_of",
        "tail": "artificial neural networks"
      },
      {
        "head": "Loss of plasticity in deep continual learning",
        "relation": "evaluated_on",
        "tail": "ImageNet dataset"
      },
      {
        "head": "Loss of plasticity in deep continual learning",
        "relation": "proposed_model",
        "tail": "continual backpropagation algorithm"
      },
      {
        "head": "Loss of plasticity in deep continual learning",
        "relation": "cites",
        "tail": "deep-learning methods"
      },
      {
        "head": "Loss of plasticity in deep continual learning",
        "relation": "cites",
        "tail": "backpropagation algorithm"
      },
      {
        "head": "Loss of plasticity in deep continual learning",
        "relation": "cites",
        "tail": "gradient descent"
      },
      {
        "head": "continual backpropagation algorithm",
        "relation": "baseline_model",
        "tail": "backpropagation algorithm"
      },
      {
        "head": "An analog-AI chip for energy-efficient speech recognition and transcription",
        "relation": "proposed_model",
        "tail": "analog-AI chip"
      },
      {
        "head": "analog-AI chip",
        "relation": "evaluated_on",
        "tail": "speech-recognition tasks"
      },
      {
        "head": "analog-AI chip",
        "relation": "evaluated_on",
        "tail": "MLPerf"
      },
      {
        "head": "analog-AI chip",
        "relation": "uses_metric",
        "tail": "TOPS/W"
      },
      {
        "head": "analog-AI chip",
        "relation": "baseline_model",
        "tail": "graphics processing units"
      },
      {
        "head": "analog-AI chip",
        "relation": "baseline_model",
        "tail": "central processing units"
      },
      {
        "head": "analog-AI chip",
        "relation": "cites",
        "tail": "analog in-memory computing (analog-AI)"
      },
      {
        "head": "keyword-spotting network",
        "relation": "evaluated_on",
        "tail": "speech-recognition tasks"
      },
      {
        "head": "recurrent neural-network transducer (RNNT)",
        "relation": "evaluated_on",
        "tail": "MLPerf"
      },
      {
        "head": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers",
        "relation": "proposed_model",
        "tail": "LiteToken"
      },
      {
        "head": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers",
        "relation": "evaluated_on",
        "tail": "intermediate merge residues"
      },
      {
        "head": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers",
        "relation": "evaluated_on",
        "tail": "token fragmentation"
      },
      {
        "head": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers",
        "relation": "evaluated_on",
        "tail": "noisy or misspelled inputs"
      },
      {
        "head": "LiteToken",
        "relation": "baseline_model",
        "tail": "BPE tokenizers"
      },
      {
        "head": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling",
        "relation": "proposed_model",
        "tail": "MeKi"
      },
      {
        "head": "MeKi",
        "relation": "baseline_model",
        "tail": "dense LLM baselines"
      },
      {
        "head": "MeKi",
        "relation": "evaluated_on",
        "tail": "Large Language Models"
      },
      {
        "head": "MeKi",
        "relation": "uses_metric",
        "tail": "inference speed"
      },
      {
        "head": "MeKi",
        "relation": "author_of",
        "tail": "ningding-o"
      },
      {
        "head": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram",
        "relation": "proposed_model",
        "tail": "Gengram"
      },
      {
        "head": "Gengram",
        "relation": "evaluated_on",
        "tail": "functional genomics tasks"
      },
      {
        "head": "Gengram",
        "relation": "baseline_model",
        "tail": "genomic foundation models"
      },
      {
        "head": "L$^3$: Large Lookup Layers",
        "relation": "proposed_model",
        "tail": "Large Lookup Layer (L$^3$)"
      },
      {
        "head": "L$^3$: Large Lookup Layers",
        "relation": "baseline_model",
        "tail": "Mixture-of-Experts (MoE)"
      },
      {
        "head": "L$^3$: Large Lookup Layers",
        "relation": "baseline_model",
        "tail": "dense models"
      },
      {
        "head": "L$^3$: Large Lookup Layers",
        "relation": "baseline_model",
        "tail": "iso-sparse MoEs"
      },
      {
        "head": "Large Lookup Layer (L$^3$)",
        "relation": "evaluated_on",
        "tail": "language modeling"
      },
      {
        "head": "Large Lookup Layer (L$^3$)",
        "relation": "evaluated_on",
        "tail": "downstream tasks"
      },
      {
        "head": "Large Lookup Layer (L$^3$)",
        "relation": "cites",
        "tail": "tokenizer embedding table"
      },
      {
        "head": "Large Lookup Layer (L$^3$)",
        "relation": "uses_metric",
        "tail": "language modeling"
      },
      {
        "head": "Large Lookup Layer (L$^3$)",
        "relation": "uses_metric",
        "tail": "downstream tasks"
      },
      {
        "head": "Scaling Embeddings Outperforms Scaling Experts in Language Models",
        "relation": "proposed_model",
        "tail": "LongCat-Flash-Lite"
      },
      {
        "head": "Scaling Embeddings Outperforms Scaling Experts in Language Models",
        "relation": "evaluated_on",
        "tail": "parameter-equivalent MoE baselines"
      },
      {
        "head": "Scaling Embeddings Outperforms Scaling Experts in Language Models",
        "relation": "evaluated_on",
        "tail": "existing models of comparable scale"
      },
      {
        "head": "Scaling Embeddings Outperforms Scaling Experts in Language Models",
        "relation": "cites",
        "tail": "Mixture-of-Experts (MoE) architectures"
      },
      {
        "head": "embedding scaling",
        "relation": "baseline_model",
        "tail": "expert scaling"
      },
      {
        "head": "Going Deeper with Convolutions",
        "relation": "proposed_model",
        "tail": "Inception"
      },
      {
        "head": "Inception",
        "relation": "evaluated_on",
        "tail": "ImageNet Large-Scale Visual Recognition Challenge 2014"
      },
      {
        "head": "Inception",
        "relation": "evaluated_on",
        "tail": "ILSVRC 2014"
      },
      {
        "head": "Going Deeper with Convolutions",
        "relation": "proposed_model",
        "tail": "GoogLeNet"
      },
      {
        "head": "GoogLeNet",
        "relation": "evaluated_on",
        "tail": "ImageNet Large-Scale Visual Recognition Challenge 2014"
      },
      {
        "head": "GoogLeNet",
        "relation": "evaluated_on",
        "tail": "ILSVRC 2014"
      },
      {
        "head": "Gradient-based learning applied to document recognition",
        "relation": "proposed_model",
        "tail": "LeNet"
      },
      {
        "head": "Gradient-based learning applied to document recognition",
        "relation": "evaluated_on",
        "tail": "MNIST"
      },
      {
        "head": "Regression Shrinkage and Selection via the Lasso",
        "relation": "proposed_model",
        "tail": "Lasso"
      },
      {
        "head": "Microsoft COCO: Common Objects in Context",
        "relation": "evaluated_on",
        "tail": "PASCAL"
      },
      {
        "head": "Microsoft COCO: Common Objects in Context",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "Microsoft COCO: Common Objects in Context",
        "relation": "evaluated_on",
        "tail": "SUN"
      },
      {
        "head": "Microsoft COCO: Common Objects in Context",
        "relation": "baseline_model",
        "tail": "Deformable Parts Model"
      },
      {
        "head": "LifeCLEF plant identification challenge",
        "relation": "evaluated_on",
        "tail": "dataset"
      },
      {
        "head": "dataset",
        "relation": "uses_metric",
        "tail": "participatory sensing plateform"
      },
      {
        "head": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics",
        "relation": "proposed_model",
        "tail": "FedMicro-IDA"
      },
      {
        "head": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics",
        "relation": "evaluated_on",
        "tail": "IoT-malware detection and classification use case"
      },
      {
        "head": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics",
        "relation": "evaluated_on",
        "tail": "MaleVis"
      },
      {
        "head": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics",
        "relation": "cites",
        "tail": "federated learning"
      },
      {
        "head": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics",
        "relation": "cites",
        "tail": "microservices-based architecture"
      },
      {
        "head": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics",
        "relation": "baseline_model",
        "tail": "existing state-of-the-art methods"
      },
      {
        "head": "Harnessing large vision and language models in agriculture: a review",
        "relation": "author_of",
        "tail": "Person"
      },
      {
        "head": "Harnessing large vision and language models in agriculture: a review",
        "relation": "cites",
        "tail": "large language models"
      },
      {
        "head": "Harnessing large vision and language models in agriculture: a review",
        "relation": "cites",
        "tail": "large vision models"
      },
      {
        "head": "Harnessing large vision and language models in agriculture: a review",
        "relation": "cites",
        "tail": "multimodal large language models"
      },
      {
        "head": "Harnessing large vision and language models in agriculture: a review",
        "relation": "evaluated_on",
        "tail": "Web of Science"
      },
      {
        "head": "Harnessing large vision and language models in agriculture: a review",
        "relation": "evaluated_on",
        "tail": "arXiv"
      },
      {
        "head": "Harnessing large vision and language models in agriculture: a review",
        "relation": "proposed_model",
        "tail": "agricultural question-answering"
      },
      {
        "head": "Harnessing large vision and language models in agriculture: a review",
        "relation": "proposed_model",
        "tail": "robotic automation"
      },
      {
        "head": "Harnessing large vision and language models in agriculture: a review",
        "relation": "proposed_model",
        "tail": "advanced image analysis"
      },
      {
        "head": "advanced image analysis",
        "relation": "uses_metric",
        "tail": "remote sensing"
      },
      {
        "head": "advanced image analysis",
        "relation": "uses_metric",
        "tail": "spectral data"
      },
      {
        "head": "Harnessing large vision and language models in agriculture: a review",
        "relation": "proposed_model",
        "tail": "pragmatic framework"
      },
      {
        "head": "Multi-axis vision transformer for medical image segmentation",
        "relation": "proposed_model",
        "tail": "Multi-axis vision transformer"
      },
      {
        "head": "Multi-axis vision transformer",
        "relation": "evaluated_on",
        "tail": "medical image segmentation"
      },
      {
        "head": "We",
        "relation": "author_of",
        "tail": "Active learning framework leveraging transcriptomics identifies modulators of disease phenotypes"
      },
      {
        "head": "active deep-learning framework",
        "relation": "proposed_model",
        "tail": "Active learning framework leveraging transcriptomics identifies modulators of disease phenotypes"
      },
      {
        "head": "active deep-learning framework",
        "relation": "uses_metric",
        "tail": "classical recall"
      },
      {
        "head": "generalizable algorithm",
        "relation": "proposed_model",
        "tail": "Active learning framework leveraging transcriptomics identifies modulators of disease phenotypes"
      },
      {
        "head": "generalizable algorithm",
        "relation": "evaluated_on",
        "tail": "hematological discovery campaigns"
      },
      {
        "head": "generalizable algorithm",
        "relation": "baseline_model",
        "tail": "state-of-the-art models"
      },
      {
        "head": "generalizable algorithm",
        "relation": "uses_metric",
        "tail": "classical recall"
      },
      {
        "head": "active deep-learning framework",
        "relation": "cites",
        "tail": "omics"
      },
      {
        "head": "LLaVA-OneVision-1.5",
        "relation": "proposed_model",
        "tail": "LLaVA-OneVision-1.5"
      },
      {
        "head": "LLaVA-OneVision-1.5",
        "relation": "uses_metric",
        "tail": "LLaVA-OneVision-1.5-Mid-Traning"
      },
      {
        "head": "LLaVA-OneVision-1.5",
        "relation": "uses_metric",
        "tail": "LLaVA-OneVision-1.5-Instruct"
      },
      {
        "head": "LLaVA-OneVision-1.5-8B",
        "relation": "evaluated_on",
        "tail": "Qwen2.5-VL-7B"
      },
      {
        "head": "LLaVA-OneVision-1.5-4B",
        "relation": "evaluated_on",
        "tail": "Qwen2.5-VL-3B"
      },
      {
        "head": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications",
        "relation": "author_of",
        "tail": "robotics community"
      },
      {
        "head": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications",
        "relation": "cites",
        "tail": "large language models (LLMs)"
      },
      {
        "head": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications",
        "relation": "cites",
        "tail": "vision-language models (VLMs)"
      },
      {
        "head": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications",
        "relation": "proposed_model",
        "tail": "Vision-Language-Action (VLA) models"
      },
      {
        "head": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications",
        "relation": "evaluated_on",
        "tail": "publicly available datasets"
      },
      {
        "head": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications",
        "relation": "evaluated_on",
        "tail": "evaluation benchmarks"
      },
      {
        "head": "teacher model",
        "relation": "proposed_model",
        "tail": "Aligning machine and human visual representations across abstraction levels"
      },
      {
        "head": "human-aligned models",
        "relation": "proposed_model",
        "tail": "Aligning machine and human visual representations across abstraction levels"
      },
      {
        "head": "Aligning machine and human visual representations across abstraction levels",
        "relation": "evaluated_on",
        "tail": "dataset of human judgements spanning multiple levels of semantic abstractions"
      },
      {
        "head": "Aligning machine and human visual representations across abstraction levels",
        "relation": "evaluated_on",
        "tail": "human judgements"
      },
      {
        "head": "human-aligned models",
        "relation": "uses_metric",
        "tail": "human judgements"
      },
      {
        "head": "teacher model",
        "relation": "uses_metric",
        "tail": "human judgements"
      },
      {
        "head": "human-aligned models",
        "relation": "baseline_model",
        "tail": "pretrained state-of-the-art vision foundation models"
      },
      {
        "head": "Vision Transformer (ViT)",
        "relation": "proposed_model",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "Vision Transformer (ViT)",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "Vision Transformer (ViT)",
        "relation": "evaluated_on",
        "tail": "CIFAR-100"
      },
      {
        "head": "Vision Transformer (ViT)",
        "relation": "evaluated_on",
        "tail": "VTAB"
      },
      {
        "head": "Vision Transformer (ViT)",
        "relation": "baseline_model",
        "tail": "convolutional networks"
      },
      {
        "head": "Vision Transformer (ViT)",
        "relation": "cites",
        "tail": "Transformer architecture"
      },
      {
        "head": "Learning Transferable Visual Models From Natural Language Supervision",
        "relation": "proposed_model",
        "tail": "CLIP"
      },
      {
        "head": "Learning Transferable Visual Models From Natural Language Supervision",
        "relation": "baseline_model",
        "tail": "ResNet-50"
      },
      {
        "head": "Learning Transferable Visual Models From Natural Language Supervision",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "CLIP",
        "relation": "author_of",
        "tail": "OpenAI"
      },
      {
        "head": "Improved Mean Flows: On the Challenges of Fastforward Generative Models",
        "relation": "proposed_model",
        "tail": "improved MeanFlow"
      },
      {
        "head": "Improved Mean Flows: On the Challenges of Fastforward Generative Models",
        "relation": "proposed_model",
        "tail": "iMF"
      },
      {
        "head": "improved MeanFlow",
        "relation": "baseline_model",
        "tail": "MeanFlow"
      },
      {
        "head": "iMF",
        "relation": "baseline_model",
        "tail": "MeanFlow"
      },
      {
        "head": "improved MeanFlow",
        "relation": "evaluated_on",
        "tail": "ImageNet 256×256"
      },
      {
        "head": "iMF",
        "relation": "evaluated_on",
        "tail": "ImageNet 256×256"
      },
      {
        "head": "improved MeanFlow",
        "relation": "uses_metric",
        "tail": "FID"
      },
      {
        "head": "iMF",
        "relation": "uses_metric",
        "tail": "FID"
      },
      {
        "head": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training",
        "relation": "proposed_model",
        "tail": "two-stage training framework"
      },
      {
        "head": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training",
        "relation": "proposed_model",
        "tail": "diffusion model"
      },
      {
        "head": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training",
        "relation": "proposed_model",
        "tail": "consistency model"
      },
      {
        "head": "two-stage training framework",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "diffusion model",
        "relation": "evaluated_on",
        "tail": "ImageNet-256"
      },
      {
        "head": "diffusion model",
        "relation": "evaluated_on",
        "tail": "ImageNet-512"
      },
      {
        "head": "diffusion model",
        "relation": "uses_metric",
        "tail": "FID"
      },
      {
        "head": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training",
        "relation": "cites",
        "tail": "DiT"
      },
      {
        "head": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training",
        "relation": "cites",
        "tail": "VAE"
      },
      {
        "head": "two-stage training framework",
        "relation": "baseline_model",
        "tail": "pixel-space diffusion"
      },
      {
        "head": "two-stage training framework",
        "relation": "baseline_model",
        "tail": "pixel-space consistency models"
      },
      {
        "head": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
        "relation": "proposed_model",
        "tail": "SVG-T2I"
      },
      {
        "head": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
        "relation": "baseline_model",
        "tail": "SVG (Self-supervised representations for Visual Generation)"
      },
      {
        "head": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
        "relation": "evaluated_on",
        "tail": "GenEval"
      },
      {
        "head": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
        "relation": "evaluated_on",
        "tail": "DPG-Bench"
      },
      {
        "head": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
        "relation": "uses_metric",
        "tail": "0.75 on GenEval"
      },
      {
        "head": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
        "relation": "uses_metric",
        "tail": "85.78 on DPG-Bench"
      },
      {
        "head": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
        "relation": "cites",
        "tail": "Visual Foundation Model (VFM)"
      },
      {
        "head": "PixelDiT",
        "relation": "proposed_model",
        "tail": "PixelDiT: Pixel Diffusion Transformers for Image Generation"
      },
      {
        "head": "PixelDiT",
        "relation": "baseline_model",
        "tail": "Diffusion Transformers (DiTs)"
      },
      {
        "head": "PixelDiT",
        "relation": "evaluated_on",
        "tail": "ImageNet 256x256"
      },
      {
        "head": "PixelDiT",
        "relation": "evaluated_on",
        "tail": "GenEval"
      },
      {
        "head": "PixelDiT",
        "relation": "evaluated_on",
        "tail": "DPG-bench"
      },
      {
        "head": "PixelDiT",
        "relation": "uses_metric",
        "tail": "FID"
      },
      {
        "head": "PixelDiT",
        "relation": "cites",
        "tail": "latent diffusion models"
      },
      {
        "head": "TUNA",
        "relation": "proposed_model",
        "tail": "Unified multimodal models (UMMs)"
      },
      {
        "head": "TUNA",
        "relation": "baseline_model",
        "tail": "prior UMMs with decoupled representations"
      },
      {
        "head": "TUNA",
        "relation": "evaluated_on",
        "tail": "multimodal understanding and generation benchmarks"
      },
      {
        "head": "TUNA",
        "relation": "evaluated_on",
        "tail": "image and video understanding"
      },
      {
        "head": "TUNA",
        "relation": "evaluated_on",
        "tail": "image and video generation"
      },
      {
        "head": "TUNA",
        "relation": "evaluated_on",
        "tail": "image editing"
      },
      {
        "head": "TUNA",
        "relation": "uses_metric",
        "tail": "performance"
      },
      {
        "head": "TUNA",
        "relation": "cites",
        "tail": "prior UMMs with decoupled representations"
      },
      {
        "head": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "relation": "proposed_model",
        "tail": "BERT"
      },
      {
        "head": "BERT",
        "relation": "evaluated_on",
        "tail": "GLUE"
      },
      {
        "head": "BERT",
        "relation": "evaluated_on",
        "tail": "MultiNLI"
      },
      {
        "head": "BERT",
        "relation": "evaluated_on",
        "tail": "SQuAD v1.1"
      },
      {
        "head": "BERT",
        "relation": "evaluated_on",
        "tail": "SQuAD v2.0"
      },
      {
        "head": "A comprehensive review of object detection with traditional and deep learning methods",
        "relation": "author_of",
        "tail": "object detection"
      },
      {
        "head": "A comprehensive review of object detection with traditional and deep learning methods",
        "relation": "cites",
        "tail": "traditional methods"
      },
      {
        "head": "A comprehensive review of object detection with traditional and deep learning methods",
        "relation": "cites",
        "tail": "deep learning methods"
      },
      {
        "head": "Diffusion Language Models are Super Data Learners",
        "relation": "proposed_model",
        "tail": "diffusion language models (DLMs)"
      },
      {
        "head": "Diffusion Language Models are Super Data Learners",
        "relation": "baseline_model",
        "tail": "autoregressive (AR) models"
      },
      {
        "head": "diffusion language models (DLMs)",
        "relation": "evaluated_on",
        "tail": "HellaSwag"
      },
      {
        "head": "diffusion language models (DLMs)",
        "relation": "evaluated_on",
        "tail": "MMLU"
      },
      {
        "head": "diffusion language models (DLMs)",
        "relation": "evaluated_on",
        "tail": "Python tokens"
      },
      {
        "head": "AR coder",
        "relation": "evaluated_on",
        "tail": "Python tokens"
      },
      {
        "head": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas",
        "relation": "proposed_model",
        "tail": "DreamOn"
      },
      {
        "head": "DreamOn",
        "relation": "baseline_model",
        "tail": "Diffusion Language Models (DLMs)"
      },
      {
        "head": "DreamOn",
        "relation": "evaluated_on",
        "tail": "HumanEval-Infilling"
      },
      {
        "head": "DreamOn",
        "relation": "evaluated_on",
        "tail": "SantaCoder-FIM"
      },
      {
        "head": "DreamOn",
        "relation": "uses_metric",
        "tail": "infilling performance"
      },
      {
        "head": "DreamOn",
        "relation": "cites",
        "tail": "Dream-Coder-7B"
      },
      {
        "head": "DreamOn",
        "relation": "cites",
        "tail": "DiffuCoder-7B"
      },
      {
        "head": "FLEX: Continuous Agent Evolution via Forward Learning from Experience",
        "relation": "proposed_model",
        "tail": "Forward Learning with EXperience (FLEX)"
      },
      {
        "head": "Forward Learning with EXperience (FLEX)",
        "relation": "evaluated_on",
        "tail": "AIME25"
      },
      {
        "head": "Forward Learning with EXperience (FLEX)",
        "relation": "evaluated_on",
        "tail": "USPTO50k"
      },
      {
        "head": "Forward Learning with EXperience (FLEX)",
        "relation": "evaluated_on",
        "tail": "ProteinGym"
      },
      {
        "head": "Forward Learning with EXperience (FLEX)",
        "relation": "uses_metric",
        "tail": "mathematical reasoning"
      },
      {
        "head": "Forward Learning with EXperience (FLEX)",
        "relation": "uses_metric",
        "tail": "chemical retrosynthesis"
      },
      {
        "head": "Forward Learning with EXperience (FLEX)",
        "relation": "uses_metric",
        "tail": "protein fitness prediction"
      },
      {
        "head": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning",
        "relation": "proposed_model",
        "tail": "Agent-R1"
      },
      {
        "head": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning",
        "relation": "cites",
        "tail": "Large Language Models (LLMs)"
      },
      {
        "head": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning",
        "relation": "cites",
        "tail": "Reinforcement Learning (RL)"
      },
      {
        "head": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning",
        "relation": "cites",
        "tail": "Markov Decision Process (MDP)"
      },
      {
        "head": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning",
        "relation": "evaluated_on",
        "tail": "Multihop QA benchmark tasks"
      },
      {
        "head": "nuScenes",
        "relation": "evaluated_on",
        "tail": "nuScenes"
      },
      {
        "head": "nuScenes",
        "relation": "baseline_model",
        "tail": "KITTI dataset"
      },
      {
        "head": "nuScenes",
        "relation": "cites",
        "tail": "KITTI dataset"
      },
      {
        "head": "Squeeze-and-Excitation Networks",
        "relation": "proposed_model",
        "tail": "Squeeze-and-Excitation block"
      },
      {
        "head": "Squeeze-and-Excitation block",
        "relation": "author_of",
        "tail": "SENet"
      },
      {
        "head": "SENet",
        "relation": "evaluated_on",
        "tail": "ILSVRC 2017 classification submission"
      },
      {
        "head": "Squeeze-and-Excitation Networks",
        "relation": "cites",
        "tail": "prior research"
      },
      {
        "head": "Squeeze-and-Excitation block",
        "relation": "uses_metric",
        "tail": "convolutional neural networks"
      },
      {
        "head": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
        "relation": "proposed_model",
        "tail": "EfficientNets"
      },
      {
        "head": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
        "relation": "baseline_model",
        "tail": "MobileNets"
      },
      {
        "head": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
        "relation": "baseline_model",
        "tail": "ResNet"
      },
      {
        "head": "EfficientNet-B7",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "EfficientNets",
        "relation": "evaluated_on",
        "tail": "CIFAR-100"
      },
      {
        "head": "EfficientNets",
        "relation": "evaluated_on",
        "tail": "Flowers"
      },
      {
        "head": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control",
        "relation": "proposed_model",
        "tail": "Light-X"
      },
      {
        "head": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control",
        "relation": "author_of",
        "tail": "Light-X"
      },
      {
        "head": "Light-X",
        "relation": "evaluated_on",
        "tail": "Light-Syn"
      },
      {
        "head": "Light-X",
        "relation": "baseline_model",
        "tail": "prior video relighting methods"
      },
      {
        "head": "Light-X",
        "relation": "uses_metric",
        "tail": "joint camera-illumination control"
      },
      {
        "head": "Light-X",
        "relation": "uses_metric",
        "tail": "text- and background-conditioned settings"
      },
      {
        "head": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control",
        "relation": "cites",
        "tail": "Recent advances in illumination control"
      },
      {
        "head": "DriveLaW",
        "relation": "proposed_model",
        "tail": "DriveLaW-Video"
      },
      {
        "head": "DriveLaW",
        "relation": "proposed_model",
        "tail": "DriveLaW-Act"
      },
      {
        "head": "DriveLaW",
        "relation": "evaluated_on",
        "tail": "NAVSIM"
      },
      {
        "head": "DriveLaW",
        "relation": "uses_metric",
        "tail": "FID"
      },
      {
        "head": "DriveLaW",
        "relation": "uses_metric",
        "tail": "FVD"
      },
      {
        "head": "DVGT: Driving Visual Geometry Transformer",
        "relation": "proposed_model",
        "tail": "DVGT: Driving Visual Geometry Transformer"
      },
      {
        "head": "DVGT: Driving Visual Geometry Transformer",
        "relation": "uses_metric",
        "tail": "metric-scaled geometry"
      },
      {
        "head": "DVGT: Driving Visual Geometry Transformer",
        "relation": "evaluated_on",
        "tail": "nuScenes"
      },
      {
        "head": "DVGT: Driving Visual Geometry Transformer",
        "relation": "evaluated_on",
        "tail": "OpenScene"
      },
      {
        "head": "DVGT: Driving Visual Geometry Transformer",
        "relation": "evaluated_on",
        "tail": "Waymo"
      },
      {
        "head": "DVGT: Driving Visual Geometry Transformer",
        "relation": "evaluated_on",
        "tail": "KITTI"
      },
      {
        "head": "DVGT: Driving Visual Geometry Transformer",
        "relation": "evaluated_on",
        "tail": "DDAD"
      },
      {
        "head": "DVGT: Driving Visual Geometry Transformer",
        "relation": "baseline_model",
        "tail": "existing models"
      },
      {
        "head": "Adding Conditional Control to Text-to-Image Diffusion Models",
        "relation": "proposed_model",
        "tail": "ControlNet"
      },
      {
        "head": "ControlNet",
        "relation": "evaluated_on",
        "tail": "Stable Diffusion"
      },
      {
        "head": "ControlNet",
        "relation": "baseline_model",
        "tail": "Stable Diffusion"
      },
      {
        "head": "ControlNet",
        "relation": "cites",
        "tail": "Stable Diffusion"
      },
      {
        "head": "Scaling Instruction-Finetuned Language Models",
        "relation": "proposed_model",
        "tail": "Flan-PaLM 540B"
      },
      {
        "head": "Scaling Instruction-Finetuned Language Models",
        "relation": "proposed_model",
        "tail": "Flan-T5"
      },
      {
        "head": "Scaling Instruction-Finetuned Language Models",
        "relation": "baseline_model",
        "tail": "PALM 540B"
      },
      {
        "head": "Scaling Instruction-Finetuned Language Models",
        "relation": "baseline_model",
        "tail": "PaLM 62B"
      },
      {
        "head": "Scaling Instruction-Finetuned Language Models",
        "relation": "evaluated_on",
        "tail": "MMLU"
      },
      {
        "head": "Scaling Instruction-Finetuned Language Models",
        "relation": "evaluated_on",
        "tail": "BBH"
      },
      {
        "head": "Scaling Instruction-Finetuned Language Models",
        "relation": "evaluated_on",
        "tail": "TyDiQA"
      },
      {
        "head": "Scaling Instruction-Finetuned Language Models",
        "relation": "evaluated_on",
        "tail": "MGSM"
      },
      {
        "head": "Flan-PaLM 540B",
        "relation": "uses_metric",
        "tail": "MMLU"
      },
      {
        "head": "Flan-PaLM 540B",
        "relation": "cites",
        "tail": "PaLM"
      },
      {
        "head": "Flan-PaLM 540B",
        "relation": "cites",
        "tail": "T5"
      },
      {
        "head": "Flan-PaLM 540B",
        "relation": "cites",
        "tail": "U-PaLM"
      },
      {
        "head": "Flan-T5",
        "relation": "cites",
        "tail": "T5"
      },
      {
        "head": "High-Resolution Image Synthesis with Latent Diffusion Models",
        "relation": "proposed_model",
        "tail": "latent diffusion models"
      },
      {
        "head": "latent diffusion models",
        "relation": "baseline_model",
        "tail": "diffusion models"
      },
      {
        "head": "latent diffusion models",
        "relation": "evaluated_on",
        "tail": "image inpainting"
      },
      {
        "head": "latent diffusion models",
        "relation": "evaluated_on",
        "tail": "unconditional image generation"
      },
      {
        "head": "latent diffusion models",
        "relation": "evaluated_on",
        "tail": "semantic scene synthesis"
      },
      {
        "head": "latent diffusion models",
        "relation": "evaluated_on",
        "tail": "super-resolution"
      },
      {
        "head": "High-Resolution Image Synthesis with Latent Diffusion Models",
        "relation": "cites",
        "tail": "diffusion models"
      },
      {
        "head": "High-Resolution Image Synthesis with Latent Diffusion Models",
        "relation": "author_of",
        "tail": "CompVis/latent-diffusion"
      },
      {
        "head": "AUTO-ENCODING VARIATIONAL BAYES",
        "relation": "proposed_model",
        "tail": "Variational Auto-Encoder"
      },
      {
        "head": "Diederik P. Kingma",
        "relation": "author_of",
        "tail": "AUTO-ENCODING VARIATIONAL BAYES"
      },
      {
        "head": "Max Welling",
        "relation": "author_of",
        "tail": "AUTO-ENCODING VARIATIONAL BAYES"
      },
      {
        "head": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
        "relation": "author_of",
        "tail": "Person"
      },
      {
        "head": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
        "relation": "evaluated_on",
        "tail": "new dataset of human perceptual similarity judgments"
      },
      {
        "head": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
        "relation": "uses_metric",
        "tail": "PSNR"
      },
      {
        "head": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
        "relation": "uses_metric",
        "tail": "SSIM"
      },
      {
        "head": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
        "relation": "cites",
        "tail": "VGG network"
      },
      {
        "head": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
        "relation": "cites",
        "tail": "ImageNet"
      },
      {
        "head": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset",
        "relation": "author_of",
        "tail": "we"
      },
      {
        "head": "Waymo Open Dataset",
        "relation": "author_of",
        "tail": "we"
      },
      {
        "head": "Waymo Open Dataset",
        "relation": "evaluated_on",
        "tail": "2D detection and tracking tasks"
      },
      {
        "head": "Waymo Open Dataset",
        "relation": "evaluated_on",
        "tail": "3D detection and tracking tasks"
      },
      {
        "head": "Waymo Open Dataset",
        "relation": "evaluated_on",
        "tail": "3D detection methods"
      },
      {
        "head": "Waymo Open Dataset",
        "relation": "uses_metric",
        "tail": "diversity metric"
      },
      {
        "head": "research community",
        "relation": "cites",
        "tail": "Waymo Open Dataset"
      },
      {
        "head": "Histograms of oriented gradients for human detection",
        "relation": "proposed_model",
        "tail": "Histograms of oriented gradients"
      },
      {
        "head": "Histograms of oriented gradients for human detection",
        "relation": "evaluated_on",
        "tail": "human detection"
      },
      {
        "head": "Generative Adversarial Networks",
        "relation": "proposed_model",
        "tail": "Generative Adversarial Networks (GANs)"
      },
      {
        "head": "Generative Adversarial Networks (GANs)",
        "relation": "cites",
        "tail": "GANs"
      },
      {
        "head": "GANs",
        "relation": "evaluated_on",
        "tail": "Data augmentation"
      },
      {
        "head": "GANs",
        "relation": "evaluated_on",
        "tail": "face images generation"
      },
      {
        "head": "Generalization of Diffusion Models Arises with a Balanced Representation Space",
        "relation": "author_of",
        "tail": "diffusion models"
      },
      {
        "head": "Generalization of Diffusion Models Arises with a Balanced Representation Space",
        "relation": "proposed_model",
        "tail": "two-layer ReLU denoising autoencoder (DAE)"
      },
      {
        "head": "Generalization of Diffusion Models Arises with a Balanced Representation Space",
        "relation": "evaluated_on",
        "tail": "real-world unconditional and text-to-image diffusion models"
      },
      {
        "head": "Generalization of Diffusion Models Arises with a Balanced Representation Space",
        "relation": "proposed_model",
        "tail": "representation-based method for detecting memorization"
      },
      {
        "head": "Generalization of Diffusion Models Arises with a Balanced Representation Space",
        "relation": "proposed_model",
        "tail": "training-free editing technique"
      },
      {
        "head": "Generalization of Diffusion Models Arises with a Balanced Representation Space",
        "relation": "cites",
        "tail": "representation learning"
      },
      {
        "head": "Generalization of Diffusion Models Arises with a Balanced Representation Space",
        "relation": "cites",
        "tail": "memorization"
      },
      {
        "head": "Generalization of Diffusion Models Arises with a Balanced Representation Space",
        "relation": "cites",
        "tail": "generalization"
      },
      {
        "head": "Generalization of Diffusion Models Arises with a Balanced Representation Space",
        "relation": "cites",
        "tail": "balanced representations"
      },
      {
        "head": "Generalization of Diffusion Models Arises with a Balanced Representation Space",
        "relation": "cites",
        "tail": "localized spiky representations"
      },
      {
        "head": "Generalization of Diffusion Models Arises with a Balanced Representation Space",
        "relation": "cites",
        "tail": "local data statistics"
      },
      {
        "head": "Generalization of Diffusion Models Arises with a Balanced Representation Space",
        "relation": "cites",
        "tail": "deep generative models"
      },
      {
        "head": "Generalization of Diffusion Models Arises with a Balanced Representation Space",
        "relation": "cites",
        "tail": "generative modeling"
      },
      {
        "head": "two-layer ReLU denoising autoencoder (DAE)",
        "relation": "baseline_model",
        "tail": "diffusion models"
      },
      {
        "head": "representation-based method for detecting memorization",
        "relation": "uses_metric",
        "tail": "memorization"
      },
      {
        "head": "training-free editing technique",
        "relation": "uses_metric",
        "tail": "balanced representations"
      },
      {
        "head": "Stable Velocity",
        "relation": "proposed_model",
        "tail": "Stable Velocity: A Variance Perspective on Flow Matching"
      },
      {
        "head": "Stable Velocity",
        "relation": "baseline_model",
        "tail": "flow matching"
      },
      {
        "head": "Stable Velocity",
        "relation": "evaluated_on",
        "tail": "ImageNet 256×256"
      },
      {
        "head": "Stable Velocity",
        "relation": "evaluated_on",
        "tail": "SD3.5"
      },
      {
        "head": "Stable Velocity",
        "relation": "evaluated_on",
        "tail": "Flux"
      },
      {
        "head": "Stable Velocity",
        "relation": "evaluated_on",
        "tail": "Qwen-Image"
      },
      {
        "head": "Stable Velocity",
        "relation": "evaluated_on",
        "tail": "Wan2.2"
      },
      {
        "head": "Stable Velocity",
        "relation": "uses_metric",
        "tail": "training efficiency"
      },
      {
        "head": "Stable Velocity",
        "relation": "uses_metric",
        "tail": "sampling speed"
      },
      {
        "head": "Stable Velocity",
        "relation": "uses_metric",
        "tail": "sample quality"
      },
      {
        "head": "Stable Velocity",
        "relation": "cites",
        "tail": "flow matching"
      },
      {
        "head": "FlatDINO",
        "relation": "proposed_model",
        "tail": "Laminating Representation Autoencoders for Efficient Diffusion"
      },
      {
        "head": "DINOv2",
        "relation": "baseline_model",
        "tail": "Laminating Representation Autoencoders for Efficient Diffusion"
      },
      {
        "head": "Laminating Representation Autoencoders for Efficient Diffusion",
        "relation": "evaluated_on",
        "tail": "ImageNet 256x256"
      },
      {
        "head": "Laminating Representation Autoencoders for Efficient Diffusion",
        "relation": "uses_metric",
        "tail": "gFID"
      },
      {
        "head": "Laminating Representation Autoencoders for Efficient Diffusion",
        "relation": "cites",
        "tail": "DINOv2"
      },
      {
        "head": "Laminating Representation Autoencoders for Efficient Diffusion",
        "relation": "cites",
        "tail": "DiT-XL"
      },
      {
        "head": "Adaptive 1D Video Diffusion Autoencoder",
        "relation": "proposed_model",
        "tail": "One-Dimensional Diffusion Video Autoencoder (One-DVA)"
      },
      {
        "head": "One-Dimensional Diffusion Video Autoencoder (One-DVA)",
        "relation": "baseline_model",
        "tail": "3D-CNN VAEs"
      },
      {
        "head": "One-Dimensional Diffusion Video Autoencoder (One-DVA)",
        "relation": "evaluated_on",
        "tail": "reconstruction metrics"
      },
      {
        "head": "One-Dimensional Diffusion Video Autoencoder (One-DVA)",
        "relation": "uses_metric",
        "tail": "reconstruction metrics"
      },
      {
        "head": "One-Dimensional Diffusion Video Autoencoder (One-DVA)",
        "relation": "cites",
        "tail": "video autoencoders"
      },
      {
        "head": "One-Dimensional Diffusion Video Autoencoder (One-DVA)",
        "relation": "cites",
        "tail": "video generation models"
      },
      {
        "head": "One-Dimensional Diffusion Video Autoencoder (One-DVA)",
        "relation": "author_of",
        "tail": "Adaptive 1D Video Diffusion Autoencoder"
      },
      {
        "head": "One-Dimensional Diffusion Video Autoencoder (One-DVA)",
        "relation": "proposed_model",
        "tail": "transformer-based framework"
      },
      {
        "head": "One-Dimensional Diffusion Video Autoencoder (One-DVA)",
        "relation": "proposed_model",
        "tail": "query-based vision transformers"
      },
      {
        "head": "One-Dimensional Diffusion Video Autoencoder (One-DVA)",
        "relation": "proposed_model",
        "tail": "pixel-space diffusion transformer"
      },
      {
        "head": "One-Dimensional Diffusion Video Autoencoder (One-DVA)",
        "relation": "proposed_model",
        "tail": "two-stage training strategy"
      },
      {
        "head": "One-Dimensional Diffusion Video Autoencoder (One-DVA)",
        "relation": "proposed_model",
        "tail": "variable-length dropout mechanism"
      },
      {
        "head": "One-Dimensional Diffusion Video Autoencoder (One-DVA)",
        "relation": "proposed_model",
        "tail": "generative modeling"
      },
      {
        "head": "One-Dimensional Diffusion Video Autoencoder (One-DVA)",
        "relation": "proposed_model",
        "tail": "latent distribution"
      },
      {
        "head": "One-Dimensional Diffusion Video Autoencoder (One-DVA)",
        "relation": "proposed_model",
        "tail": "decoder"
      },
      {
        "head": "One-Dimensional Diffusion Video Autoencoder (One-DVA)",
        "relation": "proposed_model",
        "tail": "encoder"
      },
      {
        "head": "Test-Time Conditioning with Representation-Aligned Visual Features",
        "relation": "proposed_model",
        "tail": "REPA-G"
      },
      {
        "head": "REPA-G",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "REPA-G",
        "relation": "evaluated_on",
        "tail": "COCO"
      },
      {
        "head": "Bio-inspired fine-tuning for selective transfer learning in image classification",
        "relation": "proposed_model",
        "tail": "BioTune"
      },
      {
        "head": "Bio-inspired fine-tuning for selective transfer learning in image classification",
        "relation": "evaluated_on",
        "tail": "nine image classification datasets"
      },
      {
        "head": "Bio-inspired fine-tuning for selective transfer learning in image classification",
        "relation": "evaluated_on",
        "tail": "medical imaging"
      },
      {
        "head": "Bio-inspired fine-tuning for selective transfer learning in image classification",
        "relation": "evaluated_on",
        "tail": "four different CNN architectures"
      },
      {
        "head": "BioTune",
        "relation": "baseline_model",
        "tail": "AutoRGN"
      },
      {
        "head": "BioTune",
        "relation": "baseline_model",
        "tail": "LoRA"
      },
      {
        "head": "BioTune",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "BioTune",
        "relation": "uses_metric",
        "tail": "efficiency"
      },
      {
        "head": "BioTune",
        "relation": "cites",
        "tail": "evolutionary optimization"
      },
      {
        "head": "VGG Induced Deep Hand Sign Language Detection",
        "relation": "proposed_model",
        "tail": "VGG-16 net"
      },
      {
        "head": "VGG Induced Deep Hand Sign Language Detection",
        "relation": "evaluated_on",
        "tail": "NUS dataset"
      },
      {
        "head": "VGG-16 net",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "VGG-16 net",
        "relation": "evaluated_on",
        "tail": "testing dataset of 10 classes"
      },
      {
        "head": "VGG-16 net",
        "relation": "uses_metric",
        "tail": "experimental results"
      },
      {
        "head": "MediaPipe: A Framework for Building Perception Pipelines",
        "relation": "proposed_model",
        "tail": "MediaPipe"
      },
      {
        "head": "Hand Pose Estimation and Hand Shape Classification Using Multi-layered Randomized Decision Forests",
        "relation": "proposed_model",
        "tail": "Multi-layered Randomized Decision Forests"
      },
      {
        "head": "A robust static hand gesture recognition system using geometry based normalizations and Krawtchouk moments",
        "relation": "proposed_model",
        "tail": "geometry based normalizations"
      },
      {
        "head": "A robust static hand gesture recognition system using geometry based normalizations and Krawtchouk moments",
        "relation": "proposed_model",
        "tail": "Krawtchouk moments"
      },
      {
        "head": "Hand signal classification system for sign language communication in Virtual Reality",
        "relation": "proposed_model",
        "tail": "machine learning model"
      },
      {
        "head": "Hand signal classification system for sign language communication in Virtual Reality",
        "relation": "uses_metric",
        "tail": "virtual reality headset"
      },
      {
        "head": "Hand signal classification system for sign language communication in Virtual Reality",
        "relation": "evaluated_on",
        "tail": "system meant to facilitate communication with hearing impaired individuals"
      },
      {
        "head": "Real-Time Static Hand Sign Recognition System using MediaPipe and Fully Connected Neural Network",
        "relation": "proposed_model",
        "tail": "fully connected neural network (FCNN)"
      },
      {
        "head": "Real-Time Static Hand Sign Recognition System using MediaPipe and Fully Connected Neural Network",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "Real-Time Static Hand Sign Recognition System using MediaPipe and Fully Connected Neural Network",
        "relation": "evaluated_on",
        "tail": "American Sign Language (ASL)"
      },
      {
        "head": "Real-Time Static Hand Sign Recognition System using MediaPipe and Fully Connected Neural Network",
        "relation": "cites",
        "tail": "World Health Organization"
      },
      {
        "head": "Real-Time Static Hand Sign Recognition System using MediaPipe and Fully Connected Neural Network",
        "relation": "cites",
        "tail": "Sign Language Recognition (SLR)"
      },
      {
        "head": "Real-Time Static Hand Sign Recognition System using MediaPipe and Fully Connected Neural Network",
        "relation": "uses_metric",
        "tail": "fast recognition"
      },
      {
        "head": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper",
        "relation": "proposed_model",
        "tail": "EM algorithm"
      },
      {
        "head": "LLM Social Simulations Are a Promising Research Method",
        "relation": "author_of",
        "tail": "LLM social simulations"
      },
      {
        "head": "LLM social simulations",
        "relation": "evaluated_on",
        "tail": "human research subjects"
      },
      {
        "head": "LLM social simulations",
        "relation": "proposed_model",
        "tail": "LLMs"
      },
      {
        "head": "LLM social simulations",
        "relation": "uses_metric",
        "tail": "social science datasets"
      },
      {
        "head": "LLM social simulations",
        "relation": "cites",
        "tail": "conceptual models"
      },
      {
        "head": "LLM social simulations",
        "relation": "cites",
        "tail": "iterative evaluations"
      },
      {
        "head": "Federated Contrastive Learning With Feature-Based Distillation for Human Activity Recognition",
        "relation": "proposed_model",
        "tail": "FCLFD"
      },
      {
        "head": "FCLFD",
        "relation": "uses_metric",
        "tail": "F1"
      },
      {
        "head": "FCLFD",
        "relation": "evaluated_on",
        "tail": "WISDM"
      },
      {
        "head": "FCLFD",
        "relation": "evaluated_on",
        "tail": "PAMAP2"
      },
      {
        "head": "FCLFD",
        "relation": "baseline_model",
        "tail": "federated learning algorithms"
      },
      {
        "head": "Diffuse and Disperse: Image Generation with Representation Regularization",
        "relation": "proposed_model",
        "tail": "Dispersive Loss"
      },
      {
        "head": "Diffuse and Disperse: Image Generation with Representation Regularization",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "Dispersive Loss",
        "relation": "uses_metric",
        "tail": "ImageNet"
      },
      {
        "head": "Dispersive Loss",
        "relation": "baseline_model",
        "tail": "representation alignment (REPA)"
      },
      {
        "head": "Dispersive Loss",
        "relation": "baseline_model",
        "tail": "diffusion-based generative models"
      },
      {
        "head": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems",
        "relation": "proposed_model",
        "tail": "Knowledge-Aware Bayesian Bandits (KABB)"
      },
      {
        "head": "Knowledge-Aware Bayesian Bandits (KABB)",
        "relation": "uses_metric",
        "tail": "three-dimensional knowledge distance model"
      },
      {
        "head": "Knowledge-Aware Bayesian Bandits (KABB)",
        "relation": "uses_metric",
        "tail": "dual-adaptation mechanism"
      },
      {
        "head": "Knowledge-Aware Bayesian Bandits (KABB)",
        "relation": "uses_metric",
        "tail": "knowledge-aware Thompson Sampling strategy"
      },
      {
        "head": "Knowledge-Aware Bayesian Bandits (KABB)",
        "relation": "evaluated_on",
        "tail": "multi-agent systems"
      },
      {
        "head": "multi-agent systems",
        "relation": "cites",
        "tail": "large language models"
      },
      {
        "head": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
        "relation": "proposed_model",
        "tail": "U-Net: Convolutional Networks for Biomedical Image Segmentation"
      },
      {
        "head": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
        "relation": "baseline_model",
        "tail": "sliding-window convolutional network"
      },
      {
        "head": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
        "relation": "evaluated_on",
        "tail": "ISBI challenge for segmentation of neuronal structures in electron microscopic stacks"
      },
      {
        "head": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
        "relation": "evaluated_on",
        "tail": "ISBI cell tracking challenge 2015"
      },
      {
        "head": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
        "relation": "uses_metric",
        "tail": "ISBI challenge for segmentation of neuronal structures in electron microscopic stacks"
      },
      {
        "head": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
        "relation": "uses_metric",
        "tail": "ISBI cell tracking challenge 2015"
      },
      {
        "head": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
        "relation": "cites",
        "tail": "Caffe"
      },
      {
        "head": "DMD2",
        "relation": "proposed_model",
        "tail": "Improved Distribution Matching Distillation for Fast Image Synthesis"
      },
      {
        "head": "DMD2",
        "relation": "baseline_model",
        "tail": "Distribution Matching Distillation (DMD)"
      },
      {
        "head": "DMD2",
        "relation": "evaluated_on",
        "tail": "ImageNet-64x64"
      },
      {
        "head": "DMD2",
        "relation": "evaluated_on",
        "tail": "COCO 2014"
      },
      {
        "head": "DMD2",
        "relation": "uses_metric",
        "tail": "FID"
      },
      {
        "head": "Improved Distribution Matching Distillation for Fast Image Synthesis",
        "relation": "cites",
        "tail": "Distribution Matching Distillation (DMD)"
      },
      {
        "head": "Latent Adversarial Diffusion Distillation (LADD)",
        "relation": "proposed_model",
        "tail": "Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation"
      },
      {
        "head": "adversarial diffusion distillation (ADD)",
        "relation": "baseline_model",
        "tail": "Latent Adversarial Diffusion Distillation (LADD)"
      },
      {
        "head": "adversarial diffusion distillation (ADD)",
        "relation": "uses_metric",
        "tail": "DINOv2"
      },
      {
        "head": "Latent Adversarial Diffusion Distillation (LADD)",
        "relation": "evaluated_on",
        "tail": "Stable Diffusion 3 (8B)"
      },
      {
        "head": "Latent Adversarial Diffusion Distillation (LADD)",
        "relation": "author_of",
        "tail": "We"
      },
      {
        "head": "Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation",
        "relation": "author_of",
        "tail": "We"
      },
      {
        "head": "SD3-Turbo",
        "relation": "cites",
        "tail": "Stable Diffusion 3 (8B)"
      },
      {
        "head": "Evolutionary optimization of model merging recipes",
        "relation": "proposed_model",
        "tail": "evolutionary approach"
      },
      {
        "head": "evolutionary approach",
        "relation": "proposed_model",
        "tail": "Japanese LLM with Math reasoning capabilities"
      },
      {
        "head": "evolutionary approach",
        "relation": "proposed_model",
        "tail": "culturally-aware Japanese VLM"
      },
      {
        "head": "Japanese Math LLM",
        "relation": "evaluated_on",
        "tail": "Japanese LLM benchmarks"
      },
      {
        "head": "culturally-aware Japanese VLM",
        "relation": "evaluated_on",
        "tail": "Japanese culture-specific content"
      },
      {
        "head": "evolutionary approach",
        "relation": "baseline_model",
        "tail": "model merging"
      },
      {
        "head": "evolutionary approach",
        "relation": "baseline_model",
        "tail": "previous Japanese VLMs"
      },
      {
        "head": "evolutionary approach",
        "relation": "uses_metric",
        "tail": "Japanese LLM benchmarks"
      },
      {
        "head": "evolutionary approach",
        "relation": "uses_metric",
        "tail": "Japanese culture-specific content"
      },
      {
        "head": "evolutionary approach",
        "relation": "cites",
        "tail": "Large language models (LLMs)"
      },
      {
        "head": "evolutionary approach",
        "relation": "cites",
        "tail": "open-source models"
      },
      {
        "head": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
        "relation": "proposed_model",
        "tail": "autoregressive transformer"
      },
      {
        "head": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
        "relation": "proposed_model",
        "tail": "student initialization scheme"
      },
      {
        "head": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
        "relation": "proposed_model",
        "tail": "asymmetric distillation strategy"
      },
      {
        "head": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
        "relation": "baseline_model",
        "tail": "bidirectional diffusion transformer"
      },
      {
        "head": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
        "relation": "evaluated_on",
        "tail": "VBench-Long benchmark"
      },
      {
        "head": "autoregressive transformer",
        "relation": "uses_metric",
        "tail": "total score of 84.27"
      },
      {
        "head": "autoregressive transformer",
        "relation": "cites",
        "tail": "distribution matching distillation (DMD)"
      },
      {
        "head": "distribution matching distillation (DMD)",
        "relation": "cites",
        "tail": "diffusion model"
      },
      {
        "head": "distribution matching distillation (DMD)",
        "relation": "cites",
        "tail": "generator"
      },
      {
        "head": "asymmetric distillation strategy",
        "relation": "cites",
        "tail": "causal student model"
      },
      {
        "head": "asymmetric distillation strategy",
        "relation": "cites",
        "tail": "bidirectional teacher"
      },
      {
        "head": "autoregressive transformer",
        "relation": "cites",
        "tail": "KV caching"
      },
      {
        "head": "autoregressive transformer",
        "relation": "cites",
        "tail": "streaming video-to-video translation"
      },
      {
        "head": "autoregressive transformer",
        "relation": "cites",
        "tail": "image-to-video"
      },
      {
        "head": "autoregressive transformer",
        "relation": "cites",
        "tail": "dynamic prompting"
      },
      {
        "head": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment",
        "relation": "proposed_model",
        "tail": "PuLID"
      },
      {
        "head": "PuLID",
        "relation": "author_of",
        "tail": "ToTheBeginning"
      },
      {
        "head": "PuLID",
        "relation": "uses_metric",
        "tail": "ID fidelity"
      },
      {
        "head": "PuLID",
        "relation": "uses_metric",
        "tail": "editability"
      },
      {
        "head": "PuLID",
        "relation": "evaluated_on",
        "tail": "text-to-image generation"
      },
      {
        "head": "PuLID",
        "relation": "baseline_model",
        "tail": "standard diffusion"
      },
      {
        "head": "PuLID",
        "relation": "cites",
        "tail": "Lightning T2I branch"
      },
      {
        "head": "PuLID",
        "relation": "cites",
        "tail": "contrastive alignment loss"
      },
      {
        "head": "PuLID",
        "relation": "cites",
        "tail": "accurate ID loss"
      },
      {
        "head": "PuLID",
        "relation": "cites",
        "tail": "image elements"
      },
      {
        "head": "Denoising Diffusion Probabilistic Models",
        "relation": "proposed_model",
        "tail": "diffusion probabilistic models"
      },
      {
        "head": "Denoising Diffusion Probabilistic Models",
        "relation": "evaluated_on",
        "tail": "CIFAR10"
      },
      {
        "head": "Denoising Diffusion Probabilistic Models",
        "relation": "evaluated_on",
        "tail": "LSUN"
      },
      {
        "head": "Denoising Diffusion Probabilistic Models",
        "relation": "uses_metric",
        "tail": "Inception score"
      },
      {
        "head": "Denoising Diffusion Probabilistic Models",
        "relation": "uses_metric",
        "tail": "FID score"
      },
      {
        "head": "Denoising Diffusion Probabilistic Models",
        "relation": "cites",
        "tail": "ProgressiveGAN"
      },
      {
        "head": "HunyuanVideo: A Systematic Framework For Large Video Generative Models",
        "relation": "proposed_model",
        "tail": "HunyuanVideo"
      },
      {
        "head": "HunyuanVideo: A Systematic Framework For Large Video Generative Models",
        "relation": "evaluated_on",
        "tail": "Runway Gen-3"
      },
      {
        "head": "HunyuanVideo: A Systematic Framework For Large Video Generative Models",
        "relation": "evaluated_on",
        "tail": "Luma 1.6"
      },
      {
        "head": "Tencent",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency",
        "relation": "proposed_model",
        "tail": "Loopy"
      },
      {
        "head": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency",
        "relation": "proposed_model",
        "tail": "audio-only conditioned video diffusion model"
      },
      {
        "head": "Loopy",
        "relation": "uses_metric",
        "tail": "long-term motion information"
      },
      {
        "head": "Loopy",
        "relation": "uses_metric",
        "tail": "audio-portrait movement correlation"
      },
      {
        "head": "Loopy",
        "relation": "evaluated_on",
        "tail": "various scenarios"
      },
      {
        "head": "Loopy",
        "relation": "baseline_model",
        "tail": "audio-driven portrait diffusion models"
      },
      {
        "head": "Loopy",
        "relation": "cites",
        "tail": "diffusion-based video generation techniques"
      },
      {
        "head": "Loopy",
        "relation": "cites",
        "tail": "audio-conditioned human video generation"
      },
      {
        "head": "audio-only conditioned video diffusion model",
        "relation": "author_of",
        "tail": "inter- and intra-clip temporal module"
      },
      {
        "head": "audio-only conditioned video diffusion model",
        "relation": "author_of",
        "tail": "audio-to-latents module"
      },
      {
        "head": "audio-driven portrait diffusion models",
        "relation": "uses_metric",
        "tail": "spatial motion templates"
      },
      {
        "head": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models",
        "relation": "proposed_model",
        "tail": "OmniHuman"
      },
      {
        "head": "OmniHuman",
        "relation": "baseline_model",
        "tail": "existing end-to-end audio-driven methods"
      },
      {
        "head": "OmniHuman",
        "relation": "evaluated_on",
        "tail": "large general video generation models"
      },
      {
        "head": "OmniHuman",
        "relation": "cites",
        "tail": "Diffusion Transformer-based framework"
      },
      {
        "head": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation",
        "relation": "proposed_model",
        "tail": "Hallo2"
      },
      {
        "head": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation",
        "relation": "baseline_model",
        "tail": "Hallo"
      },
      {
        "head": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation",
        "relation": "evaluated_on",
        "tail": "HDTF"
      },
      {
        "head": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation",
        "relation": "evaluated_on",
        "tail": "CelebV"
      },
      {
        "head": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation",
        "relation": "evaluated_on",
        "tail": "Wild"
      },
      {
        "head": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation",
        "relation": "cites",
        "tail": "Hallo"
      },
      {
        "head": "EchoMimicV2",
        "relation": "proposed_model",
        "tail": "Audio-Pose Dynamic Harmonization strategy"
      },
      {
        "head": "Audio-Pose Dynamic Harmonization strategy",
        "relation": "uses_metric",
        "tail": "Pose Sampling"
      },
      {
        "head": "Audio-Pose Dynamic Harmonization strategy",
        "relation": "uses_metric",
        "tail": "Audio Diffusion"
      },
      {
        "head": "EchoMimicV2",
        "relation": "uses_metric",
        "tail": "Head Partial Attention"
      },
      {
        "head": "EchoMimicV2",
        "relation": "uses_metric",
        "tail": "Phase-specific Denoising Loss"
      },
      {
        "head": "EchoMimicV2",
        "relation": "evaluated_on",
        "tail": "novel benchmark for evaluating the effectiveness of half-body human animation"
      },
      {
        "head": "EchoMimicV2",
        "relation": "cites",
        "tail": "Recent work on human animation"
      },
      {
        "head": "Towards Striking, Simplified, and Semi-Body Human Animation",
        "relation": "author_of",
        "tail": "EchoMimicV2"
      },
      {
        "head": "Feature Pyramid Networks for Object Detection",
        "relation": "proposed_model",
        "tail": "Feature Pyramid Network (FPN)"
      },
      {
        "head": "Feature Pyramid Networks for Object Detection",
        "relation": "baseline_model",
        "tail": "Faster R-CNN"
      },
      {
        "head": "Feature Pyramid Networks for Object Detection",
        "relation": "evaluated_on",
        "tail": "COCO detection benchmark"
      },
      {
        "head": "Feature Pyramid Networks for Object Detection",
        "relation": "cites",
        "tail": "COCO 2016 challenge winners"
      },
      {
        "head": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation",
        "relation": "proposed_model",
        "tail": "RNN Encoder-Decoder"
      },
      {
        "head": "RNN Encoder-Decoder",
        "relation": "evaluated_on",
        "tail": "statistical machine translation system"
      },
      {
        "head": "RNN Encoder-Decoder",
        "relation": "uses_metric",
        "tail": "log-linear model"
      },
      {
        "head": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving",
        "relation": "proposed_model",
        "tail": "DriveMLM"
      },
      {
        "head": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving",
        "relation": "baseline_model",
        "tail": "Large language models (LLMs)"
      },
      {
        "head": "DriveMLM",
        "relation": "evaluated_on",
        "tail": "CARLA Town05 Long"
      },
      {
        "head": "DriveMLM",
        "relation": "uses_metric",
        "tail": "improvements of 3.2 and 4.7 points"
      },
      {
        "head": "DriveMLM",
        "relation": "cites",
        "tail": "Autopilot"
      },
      {
        "head": "DriveMLM",
        "relation": "cites",
        "tail": "Apollo"
      },
      {
        "head": "DriveMLM",
        "relation": "author_of",
        "tail": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving"
      },
      {
        "head": "DriveMLM",
        "relation": "proposed_model",
        "tail": "multimodal LLM (MLLM)"
      },
      {
        "head": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability",
        "relation": "proposed_model",
        "tail": "Vista"
      },
      {
        "head": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability",
        "relation": "evaluated_on",
        "tail": "multiple datasets"
      },
      {
        "head": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability",
        "relation": "uses_metric",
        "tail": "FID"
      },
      {
        "head": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability",
        "relation": "uses_metric",
        "tail": "FVD"
      },
      {
        "head": "Vista",
        "relation": "baseline_model",
        "tail": "general-purpose video generator"
      },
      {
        "head": "Vista",
        "relation": "baseline_model",
        "tail": "driving world model"
      },
      {
        "head": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving",
        "relation": "proposed_model",
        "tail": "DiffusionDrive"
      },
      {
        "head": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving",
        "relation": "baseline_model",
        "tail": "vanilla diffusion policy"
      },
      {
        "head": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving",
        "relation": "evaluated_on",
        "tail": "NAVSIM dataset"
      },
      {
        "head": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving",
        "relation": "uses_metric",
        "tail": "PDMS"
      },
      {
        "head": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving",
        "relation": "cites",
        "tail": "ResNet-34"
      },
      {
        "head": "How Far is Video Generation from World Model: A Physical Law Perspective",
        "relation": "author_of",
        "tail": "OpenAI"
      },
      {
        "head": "How Far is Video Generation from World Model: A Physical Law Perspective",
        "relation": "proposed_model",
        "tail": "diffusion-based video generation models"
      },
      {
        "head": "How Far is Video Generation from World Model: A Physical Law Perspective",
        "relation": "evaluated_on",
        "tail": "2D simulation testbed for object movement and collisions"
      },
      {
        "head": "How Far is Video Generation from World Model: A Physical Law Perspective",
        "relation": "cites",
        "tail": "Sora"
      },
      {
        "head": "EMMA: End-to-End Multimodal Model for Autonomous Driving",
        "relation": "proposed_model",
        "tail": "EMMA"
      },
      {
        "head": "EMMA: End-to-End Multimodal Model for Autonomous Driving",
        "relation": "evaluated_on",
        "tail": "nuScenes"
      },
      {
        "head": "EMMA: End-to-End Multimodal Model for Autonomous Driving",
        "relation": "evaluated_on",
        "tail": "Waymo Open Motion Dataset (WOMD)"
      },
      {
        "head": "EMMA: End-to-End Multimodal Model for Autonomous Driving",
        "relation": "evaluated_on",
        "tail": "Waymo Open Dataset (WOD)"
      },
      {
        "head": "EMMA",
        "relation": "baseline_model",
        "tail": "Gemini"
      },
      {
        "head": "Language Models are Few-Shot Learners",
        "relation": "proposed_model",
        "tail": "GPT-3"
      },
      {
        "head": "GPT-3",
        "relation": "evaluated_on",
        "tail": "NLP tasks and benchmarks"
      },
      {
        "head": "GPT-3",
        "relation": "evaluated_on",
        "tail": "translation, question-answering, and cloze tasks"
      },
      {
        "head": "GPT-3",
        "relation": "evaluated_on",
        "tail": "unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic"
      },
      {
        "head": "GPT-3",
        "relation": "evaluated_on",
        "tail": "news articles"
      },
      {
        "head": "InternVL 2.5",
        "relation": "proposed_model",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "InternVL 2.5",
        "relation": "baseline_model",
        "tail": "InternVL 2.0"
      },
      {
        "head": "InternVL 2.5",
        "relation": "evaluated_on",
        "tail": "MMMU benchmark"
      },
      {
        "head": "InternVL 2.5",
        "relation": "cites",
        "tail": "GPT-4o"
      },
      {
        "head": "InternVL 2.5",
        "relation": "cites",
        "tail": "Claude-3.5-Sonnet"
      },
      {
        "head": "InternVL3",
        "relation": "proposed_model",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "InternVL3",
        "relation": "evaluated_on",
        "tail": "MMMU benchmark"
      },
      {
        "head": "InternVL3-78B",
        "relation": "evaluated_on",
        "tail": "MMMU benchmark"
      },
      {
        "head": "InternVL3",
        "relation": "cites",
        "tail": "ChatGPT-4o"
      },
      {
        "head": "InternVL3",
        "relation": "cites",
        "tail": "Claude 3.5 Sonnet"
      },
      {
        "head": "InternVL3",
        "relation": "cites",
        "tail": "Gemini 2.5 Pro"
      },
      {
        "head": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models",
        "relation": "uses_metric",
        "tail": "multi-modal benchmarks"
      },
      {
        "head": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models",
        "relation": "evaluated_on",
        "tail": "large multi-modality models"
      },
      {
        "head": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models",
        "relation": "proposed_model",
        "tail": "OpenVLM Leaderboard"
      },
      {
        "head": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models",
        "relation": "baseline_model",
        "tail": "large multi-modality models"
      },
      {
        "head": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step",
        "relation": "proposed_model",
        "tail": "LLaVA-CoT"
      },
      {
        "head": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step",
        "relation": "baseline_model",
        "tail": "Gemini-1.5-pro"
      },
      {
        "head": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step",
        "relation": "baseline_model",
        "tail": "GPT-4o-mini"
      },
      {
        "head": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step",
        "relation": "baseline_model",
        "tail": "Llama-3.2-90B-Vision-Instruct"
      },
      {
        "head": "LLaVA-CoT",
        "relation": "evaluated_on",
        "tail": "multimodal reasoning benchmarks"
      },
      {
        "head": "LLaVA-CoT",
        "relation": "uses_metric",
        "tail": "performance improvement"
      },
      {
        "head": "LLaVA-CoT",
        "relation": "cites",
        "tail": "chain-of-thought prompting"
      },
      {
        "head": "InternVL 3.5",
        "relation": "proposed_model",
        "tail": "Cascade Reinforcement Learning (Cascade RL) framework"
      },
      {
        "head": "InternVL 3.5",
        "relation": "proposed_model",
        "tail": "Visual Resolution Router (ViR)"
      },
      {
        "head": "InternVL 3.5",
        "relation": "proposed_model",
        "tail": "Decoupled Vision-Language Deployment (DvD) strategy"
      },
      {
        "head": "InternVL 3.5",
        "relation": "evaluated_on",
        "tail": "MMMU"
      },
      {
        "head": "InternVL 3.5",
        "relation": "evaluated_on",
        "tail": "MathVista"
      },
      {
        "head": "InternVL 3.5",
        "relation": "cites",
        "tail": "InternVL3"
      },
      {
        "head": "InternVL3.5-241B-A28B",
        "relation": "cites",
        "tail": "GPT-5"
      },
      {
        "head": "Proximal Policy Optimization Algorithms",
        "relation": "proposed_model",
        "tail": "proximal policy optimization (PPO)"
      },
      {
        "head": "proximal policy optimization (PPO)",
        "relation": "baseline_model",
        "tail": "trust region policy optimization (TRPO)"
      },
      {
        "head": "proximal policy optimization (PPO)",
        "relation": "evaluated_on",
        "tail": "simulated robotic locomotion"
      },
      {
        "head": "proximal policy optimization (PPO)",
        "relation": "evaluated_on",
        "tail": "Atari game playing"
      },
      {
        "head": "Flow-GRPO",
        "relation": "proposed_model",
        "tail": "Flow-GRPO: Training Flow Matching Models via Online RL"
      },
      {
        "head": "Flow-GRPO",
        "relation": "uses_metric",
        "tail": "GenEval"
      },
      {
        "head": "Flow-GRPO",
        "relation": "evaluated_on",
        "tail": "text-to-image tasks"
      },
      {
        "head": "Flow-GRPO",
        "relation": "evaluated_on",
        "tail": "compositional generation"
      },
      {
        "head": "Flow-GRPO",
        "relation": "evaluated_on",
        "tail": "visual text rendering"
      },
      {
        "head": "Flow-GRPO",
        "relation": "evaluated_on",
        "tail": "human preference alignment"
      },
      {
        "head": "Flow-GRPO",
        "relation": "baseline_model",
        "tail": "SD3.5-M"
      },
      {
        "head": "Flow-GRPO",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "Flow-GRPO",
        "relation": "cites",
        "tail": "online policy gradient reinforcement learning (RL)"
      },
      {
        "head": "Flow-GRPO",
        "relation": "cites",
        "tail": "flow matching models"
      },
      {
        "head": "Flow-GRPO",
        "relation": "cites",
        "tail": "ODE-to-SDE conversion"
      },
      {
        "head": "Flow-GRPO",
        "relation": "cites",
        "tail": "Denoising Reduction strategy"
      },
      {
        "head": "ODE-to-SDE conversion",
        "relation": "cites",
        "tail": "Ordinary Differential Equation (ODE)"
      },
      {
        "head": "ODE-to-SDE conversion",
        "relation": "cites",
        "tail": "Stochastic Differential Equation (SDE)"
      },
      {
        "head": "DanceGRPO",
        "relation": "proposed_model",
        "tail": "Group Relative Policy Optimization (GRPO)"
      },
      {
        "head": "DanceGRPO",
        "relation": "baseline_model",
        "tail": "DDPO"
      },
      {
        "head": "DanceGRPO",
        "relation": "baseline_model",
        "tail": "DPOK"
      },
      {
        "head": "DanceGRPO",
        "relation": "evaluated_on",
        "tail": "HPS-v2.1"
      },
      {
        "head": "DanceGRPO",
        "relation": "evaluated_on",
        "tail": "CLIP Score"
      },
      {
        "head": "DanceGRPO",
        "relation": "evaluated_on",
        "tail": "VideoAlign"
      },
      {
        "head": "DanceGRPO",
        "relation": "evaluated_on",
        "tail": "GenEval"
      },
      {
        "head": "DanceGRPO",
        "relation": "uses_metric",
        "tail": "HPS-v2.1"
      },
      {
        "head": "DanceGRPO",
        "relation": "uses_metric",
        "tail": "CLIP Score"
      },
      {
        "head": "DanceGRPO",
        "relation": "uses_metric",
        "tail": "VideoAlign"
      },
      {
        "head": "DanceGRPO",
        "relation": "uses_metric",
        "tail": "GenEval"
      },
      {
        "head": "DanceGRPO",
        "relation": "cites",
        "tail": "Reinforcement Learning from Human Feedback (RLHF)"
      },
      {
        "head": "DanceGRPO: Unleashing GRPO on Visual Generation",
        "relation": "author_of",
        "tail": "DanceGRPO"
      },
      {
        "head": "Seedance 1.0",
        "relation": "proposed_model",
        "tail": "diffusion modeling"
      },
      {
        "head": "Seedance 1.0",
        "relation": "baseline_model",
        "tail": "video generation models"
      },
      {
        "head": "Seedance 1.0",
        "relation": "uses_metric",
        "tail": "precision and meaningful video captioning"
      },
      {
        "head": "Seedance 1.0",
        "relation": "uses_metric",
        "tail": "multi-dimensional reward mechanisms"
      },
      {
        "head": "Seedance 1.0",
        "relation": "evaluated_on",
        "tail": "multi-source data curation"
      },
      {
        "head": "Seedance 1.0",
        "relation": "evaluated_on",
        "tail": "NVIDIA-L20"
      },
      {
        "head": "efficient architecture design",
        "relation": "proposed_model",
        "tail": "proposed training paradigm"
      },
      {
        "head": "proposed training paradigm",
        "relation": "proposed_model",
        "tail": "multi-shot generation"
      },
      {
        "head": "proposed training paradigm",
        "relation": "proposed_model",
        "tail": "text-to-video"
      },
      {
        "head": "proposed training paradigm",
        "relation": "proposed_model",
        "tail": "image-to-video"
      },
      {
        "head": "fine-grained supervised fine-tuning",
        "relation": "proposed_model",
        "tail": "video-specific RLHF"
      },
      {
        "head": "multi-stage distillation strategies",
        "relation": "proposed_model",
        "tail": "system-level optimizations"
      },
      {
        "head": "SkyReels-V2: Infinite-length Film Generative Model",
        "relation": "proposed_model",
        "tail": "SkyReels-V2"
      },
      {
        "head": "SkyReels-V2",
        "relation": "uses_metric",
        "tail": "Multi-modal Large Language Model (MLLM)"
      },
      {
        "head": "SkyReels-V2",
        "relation": "uses_metric",
        "tail": "Multi-stage Pretraining"
      },
      {
        "head": "SkyReels-V2",
        "relation": "uses_metric",
        "tail": "Reinforcement Learning"
      },
      {
        "head": "SkyReels-V2",
        "relation": "uses_metric",
        "tail": "Diffusion Forcing Framework"
      },
      {
        "head": "SkyReels-V2",
        "relation": "baseline_model",
        "tail": "Supervised Fine-Tuning (SFT)"
      },
      {
        "head": "SkyReels-V2",
        "relation": "evaluated_on",
        "tail": "Motion-specific Reinforcement Learning (RL) training"
      },
      {
        "head": "SkyReels-V2",
        "relation": "evaluated_on",
        "tail": "diffusion forcing framework with non-decreasing noise schedules"
      },
      {
        "head": "SkyReels-V2",
        "relation": "author_of",
        "tail": "SkyCaptioner-V1"
      },
      {
        "head": "Unified Reward Model for Multimodal Understanding and Generation",
        "relation": "proposed_model",
        "tail": "UnifiedReward"
      },
      {
        "head": "UnifiedReward",
        "relation": "evaluated_on",
        "tail": "large-scale human preference dataset"
      },
      {
        "head": "UnifiedReward",
        "relation": "uses_metric",
        "tail": "pairwise ranking"
      },
      {
        "head": "UnifiedReward",
        "relation": "uses_metric",
        "tail": "pointwise scoring"
      },
      {
        "head": "UnifiedReward",
        "relation": "cites",
        "tail": "Direct Preference Optimization (DPO)"
      },
      {
        "head": "Jacob Devlin",
        "relation": "author_of",
        "tail": "BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
        "relation": "author_of",
        "tail": "Stanford Question Answering Dataset (SQuAD)"
      },
      {
        "head": "logistic regression model",
        "relation": "evaluated_on",
        "tail": "Stanford Question Answering Dataset (SQuAD)"
      },
      {
        "head": "logistic regression model",
        "relation": "uses_metric",
        "tail": "F1 score"
      },
      {
        "head": "simple baseline",
        "relation": "evaluated_on",
        "tail": "Stanford Question Answering Dataset (SQuAD)"
      },
      {
        "head": "Neural Machine Translation of Rare Words with Subword Units",
        "relation": "proposed_model",
        "tail": "subword models"
      },
      {
        "head": "Neural Machine Translation of Rare Words with Subword Units",
        "relation": "baseline_model",
        "tail": "back-off dictionary baseline"
      },
      {
        "head": "subword models",
        "relation": "evaluated_on",
        "tail": "WMT 15 translation tasks English-German and English-Russian"
      },
      {
        "head": "subword models",
        "relation": "uses_metric",
        "tail": "BLEU"
      },
      {
        "head": "This paper",
        "relation": "proposed_model",
        "tail": "SentencePiece"
      },
      {
        "head": "SentencePiece",
        "relation": "evaluated_on",
        "tail": "English-Japanese machine translation"
      },
      {
        "head": "SentencePiece",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "This paper",
        "relation": "cites",
        "tail": "Neural Machine Translation"
      },
      {
        "head": "GPT-4 Technical Report",
        "relation": "author_of",
        "tail": "GPT-4"
      },
      {
        "head": "GPT-4",
        "relation": "proposed_model",
        "tail": "Transformer-based model"
      },
      {
        "head": "GPT-4",
        "relation": "evaluated_on",
        "tail": "bar exam"
      },
      {
        "head": "GPT-4",
        "relation": "evaluated_on",
        "tail": "professional and academic benchmarks"
      },
      {
        "head": "LLaMA: Open and Efficient Foundation Language Models",
        "relation": "proposed_model",
        "tail": "LLaMA"
      },
      {
        "head": "LLaMA",
        "relation": "evaluated_on",
        "tail": "GPT-3"
      },
      {
        "head": "LLaMA",
        "relation": "evaluated_on",
        "tail": "Chinchilla-70B"
      },
      {
        "head": "LLaMA",
        "relation": "evaluated_on",
        "tail": "PaLM-540B"
      },
      {
        "head": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
        "relation": "proposed_model",
        "tail": "chain of thought"
      },
      {
        "head": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
        "relation": "evaluated_on",
        "tail": "GSM8K benchmark"
      },
      {
        "head": "chain of thought",
        "relation": "baseline_model",
        "tail": "GPT-3"
      },
      {
        "head": "chain of thought",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "Training Verifiers to Solve Math Word Problems",
        "relation": "author_of",
        "tail": "Person"
      },
      {
        "head": "Training Verifiers to Solve Math Word Problems",
        "relation": "proposed_model",
        "tail": "verifiers"
      },
      {
        "head": "Training Verifiers to Solve Math Word Problems",
        "relation": "baseline_model",
        "tail": "finetuning baseline"
      },
      {
        "head": "Training Verifiers to Solve Math Word Problems",
        "relation": "evaluated_on",
        "tail": "GSM8K"
      },
      {
        "head": "verifiers",
        "relation": "uses_metric",
        "tail": "performance"
      },
      {
        "head": "transformer models",
        "relation": "evaluated_on",
        "tail": "GSM8K"
      },
      {
        "head": "Mamba",
        "relation": "proposed_model",
        "tail": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"
      },
      {
        "head": "Mamba",
        "relation": "baseline_model",
        "tail": "Transformer"
      },
      {
        "head": "Mamba",
        "relation": "evaluated_on",
        "tail": "language"
      },
      {
        "head": "Mamba",
        "relation": "evaluated_on",
        "tail": "audio"
      },
      {
        "head": "Mamba",
        "relation": "evaluated_on",
        "tail": "genomics"
      },
      {
        "head": "Mamba",
        "relation": "cites",
        "tail": "structured state space models (SSMs)"
      },
      {
        "head": "Mamba",
        "relation": "cites",
        "tail": "linear attention"
      },
      {
        "head": "Mamba",
        "relation": "cites",
        "tail": "gated convolution"
      },
      {
        "head": "Mamba",
        "relation": "cites",
        "tail": "recurrent models"
      },
      {
        "head": "Mamba-3B",
        "relation": "proposed_model",
        "tail": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"
      },
      {
        "head": "SGDR: Stochastic Gradient Descent with Warm Restarts",
        "relation": "proposed_model",
        "tail": "stochastic gradient descent"
      },
      {
        "head": "SGDR: Stochastic Gradient Descent with Warm Restarts",
        "relation": "evaluated_on",
        "tail": "CIFAR-10"
      },
      {
        "head": "SGDR: Stochastic Gradient Descent with Warm Restarts",
        "relation": "evaluated_on",
        "tail": "CIFAR-100"
      },
      {
        "head": "SGDR: Stochastic Gradient Descent with Warm Restarts",
        "relation": "evaluated_on",
        "tail": "EEG recordings"
      },
      {
        "head": "SGDR: Stochastic Gradient Descent with Warm Restarts",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "loshchil",
        "relation": "author_of",
        "tail": "SGDR: Stochastic Gradient Descent with Warm Restarts"
      },
      {
        "head": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
        "relation": "proposed_model",
        "tail": "Sparsely-Gated Mixture-of-Experts layer (MoE)"
      },
      {
        "head": "Sparsely-Gated Mixture-of-Experts layer (MoE)",
        "relation": "evaluated_on",
        "tail": "language modeling"
      },
      {
        "head": "Sparsely-Gated Mixture-of-Experts layer (MoE)",
        "relation": "evaluated_on",
        "tail": "machine translation"
      },
      {
        "head": "Sparsely-Gated Mixture-of-Experts layer (MoE)",
        "relation": "uses_metric",
        "tail": "large language modeling benchmarks"
      },
      {
        "head": "Sparsely-Gated Mixture-of-Experts layer (MoE)",
        "relation": "uses_metric",
        "tail": "machine translation benchmarks"
      },
      {
        "head": "Sparsely-Gated Mixture-of-Experts layer (MoE)",
        "relation": "baseline_model",
        "tail": "LSTM layers"
      },
      {
        "head": "Pointer Sentinel Mixture Models",
        "relation": "proposed_model",
        "tail": "pointer sentinel mixture architecture"
      },
      {
        "head": "pointer sentinel mixture architecture",
        "relation": "proposed_model",
        "tail": "pointer sentinel-LSTM model"
      },
      {
        "head": "pointer sentinel-LSTM model",
        "relation": "evaluated_on",
        "tail": "Penn Treebank"
      },
      {
        "head": "Pointer Sentinel Mixture Models",
        "relation": "author_of",
        "tail": "pointer sentinel mixture architecture"
      },
      {
        "head": "Pointer Sentinel Mixture Models",
        "relation": "author_of",
        "tail": "pointer sentinel-LSTM model"
      },
      {
        "head": "Pointer Sentinel Mixture Models",
        "relation": "author_of",
        "tail": "WikiText corpus"
      },
      {
        "head": "Measuring Massive Multitask Language Understanding",
        "relation": "proposed_model",
        "tail": "GPT-3"
      },
      {
        "head": "Measuring Massive Multitask Language Understanding",
        "relation": "evaluated_on",
        "tail": "GPT-3"
      },
      {
        "head": "Let's Verify Step by Step",
        "relation": "evaluated_on",
        "tail": "MATH dataset"
      },
      {
        "head": "Let's Verify Step by Step",
        "relation": "proposed_model",
        "tail": "process-supervised model"
      },
      {
        "head": "Let's Verify Step by Step",
        "relation": "uses_metric",
        "tail": "78% of problems from a representative subset of the MATH test set"
      },
      {
        "head": "process-supervised model",
        "relation": "evaluated_on",
        "tail": "MATH dataset"
      },
      {
        "head": "reward model",
        "relation": "evaluated_on",
        "tail": "PRM800K"
      },
      {
        "head": "Let's Verify Step by Step",
        "relation": "cites",
        "tail": "Recent work"
      },
      {
        "head": "We",
        "relation": "author_of",
        "tail": "GPQA"
      },
      {
        "head": "GPQA",
        "relation": "evaluated_on",
        "tail": "GPT-4"
      },
      {
        "head": "GPT-4",
        "relation": "baseline_model",
        "tail": "GPQA"
      },
      {
        "head": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
        "relation": "proposed_model",
        "tail": "GShard"
      },
      {
        "head": "GShard",
        "relation": "uses_metric",
        "tail": "600 billion parameters"
      },
      {
        "head": "GShard",
        "relation": "evaluated_on",
        "tail": "multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts"
      },
      {
        "head": "GShard",
        "relation": "evaluated_on",
        "tail": "TPU v3 accelerators"
      },
      {
        "head": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
        "relation": "cites",
        "tail": "prior art"
      },
      {
        "head": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation",
        "relation": "proposed_model",
        "tail": "R-CNN"
      },
      {
        "head": "R-CNN",
        "relation": "evaluated_on",
        "tail": "PASCAL VOC dataset"
      },
      {
        "head": "R-CNN",
        "relation": "evaluated_on",
        "tail": "ILSVRC2013 detection dataset"
      },
      {
        "head": "R-CNN",
        "relation": "uses_metric",
        "tail": "mean average precision (mAP)"
      },
      {
        "head": "R-CNN",
        "relation": "baseline_model",
        "tail": "OverFeat"
      },
      {
        "head": "MuMu-LLaMA: Multi-modal music understanding and generation via large language models",
        "relation": "proposed_model",
        "tail": "MuMu-LLaMA"
      },
      {
        "head": "MuMu-LLaMA: Multi-modal music understanding and generation via large language models",
        "relation": "uses_metric",
        "tail": "multi-modal music understanding"
      },
      {
        "head": "MuMu-LLaMA: Multi-modal music understanding and generation via large language models",
        "relation": "uses_metric",
        "tail": "multi-modal music generation"
      },
      {
        "head": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy",
        "relation": "proposed_model",
        "tail": "supertype-subtype concept hierarchy"
      },
      {
        "head": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy",
        "relation": "proposed_model",
        "tail": "group-wise suppression method"
      },
      {
        "head": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy",
        "relation": "proposed_model",
        "tail": "Supertype-Preserving Low-Rank Adaptation (SuPLoRA)"
      },
      {
        "head": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy",
        "relation": "evaluated_on",
        "tail": "benchmark"
      },
      {
        "head": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy",
        "relation": "cites",
        "tail": "concept erasure approaches"
      },
      {
        "head": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy",
        "relation": "cites",
        "tail": "diffusion models"
      },
      {
        "head": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy",
        "relation": "uses_metric",
        "tail": "standard diffusion regularization"
      },
      {
        "head": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism",
        "relation": "proposed_model",
        "tail": "Multi-modal Chain and Global Attention Network (MCGA-Net)"
      },
      {
        "head": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism",
        "relation": "proposed_model",
        "tail": "DCGAN-based data augmentation strategy"
      },
      {
        "head": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism",
        "relation": "proposed_model",
        "tail": "Multi-modal Chain Feature Fusion (MCFF)"
      },
      {
        "head": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism",
        "relation": "proposed_model",
        "tail": "Global Attention Mechanism (GAM)"
      },
      {
        "head": "Multi-modal Chain and Global Attention Network (MCGA-Net)",
        "relation": "uses_metric",
        "tail": "Precision"
      },
      {
        "head": "Multi-modal Chain and Global Attention Network (MCGA-Net)",
        "relation": "uses_metric",
        "tail": "Recall"
      },
      {
        "head": "Multi-modal Chain and Global Attention Network (MCGA-Net)",
        "relation": "uses_metric",
        "tail": "mAP@50"
      },
      {
        "head": "Multi-modal Chain and Global Attention Network (MCGA-Net)",
        "relation": "evaluated_on",
        "tail": "GPR images"
      },
      {
        "head": "Multi-modal Chain and Global Attention Network (MCGA-Net)",
        "relation": "baseline_model",
        "tail": "other models"
      },
      {
        "head": "Multi-modal Chain and Global Attention Network (MCGA-Net)",
        "relation": "cites",
        "tail": "MS COCO"
      },
      {
        "head": "Qwen3-VL-Embedding",
        "relation": "proposed_model",
        "tail": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking"
      },
      {
        "head": "Qwen3-VL-Reranker",
        "relation": "proposed_model",
        "tail": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking"
      },
      {
        "head": "Qwen3-VL-Embedding",
        "relation": "evaluated_on",
        "tail": "MMEB-V2"
      },
      {
        "head": "Qwen3-VL-Embedding",
        "relation": "uses_metric",
        "tail": "overall score"
      },
      {
        "head": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness",
        "relation": "proposed_model",
        "tail": "FPSMark"
      },
      {
        "head": "FPSMark",
        "relation": "uses_metric",
        "tail": "extraction accuracy"
      },
      {
        "head": "FPSMark",
        "relation": "evaluated_on",
        "tail": "partial capture percentages"
      },
      {
        "head": "FPSMark",
        "relation": "baseline_model",
        "tail": "existing methods"
      },
      {
        "head": "FPSMark",
        "relation": "uses_metric",
        "tail": "robustness"
      },
      {
        "head": "FPSMark",
        "relation": "cites",
        "tail": "existing methods"
      },
      {
        "head": "Pattern Recognition and Machine Learning",
        "relation": "cites",
        "tail": "Probability Distributions"
      },
      {
        "head": "Pattern Recognition and Machine Learning",
        "relation": "cites",
        "tail": "Linear Models for Regression"
      },
      {
        "head": "Pattern Recognition and Machine Learning",
        "relation": "cites",
        "tail": "Linear Models for Classification"
      },
      {
        "head": "Pattern Recognition and Machine Learning",
        "relation": "cites",
        "tail": "Neural Networks"
      },
      {
        "head": "Pattern Recognition and Machine Learning",
        "relation": "cites",
        "tail": "Kernel Methods"
      },
      {
        "head": "Pattern Recognition and Machine Learning",
        "relation": "cites",
        "tail": "Sparse Kernel Machines"
      },
      {
        "head": "Pattern Recognition and Machine Learning",
        "relation": "cites",
        "tail": "Graphical Models"
      },
      {
        "head": "Pattern Recognition and Machine Learning",
        "relation": "cites",
        "tail": "Mixture Models and EM"
      },
      {
        "head": "Pattern Recognition and Machine Learning",
        "relation": "cites",
        "tail": "Approximate Inference"
      },
      {
        "head": "Pattern Recognition and Machine Learning",
        "relation": "cites",
        "tail": "Sampling Methods"
      },
      {
        "head": "Pattern Recognition and Machine Learning",
        "relation": "cites",
        "tail": "Continuous Latent Variables"
      },
      {
        "head": "Pattern Recognition and Machine Learning",
        "relation": "cites",
        "tail": "Sequential Data"
      },
      {
        "head": "Pattern Recognition and Machine Learning",
        "relation": "cites",
        "tail": "Combining Models"
      },
      {
        "head": "Automatic Image-Based Plant Disease Severity Estimation Using Deep Learning",
        "relation": "proposed_model",
        "tail": "deep VGG16 model"
      },
      {
        "head": "Automatic Image-Based Plant Disease Severity Estimation Using Deep Learning",
        "relation": "evaluated_on",
        "tail": "PlantVillage dataset"
      },
      {
        "head": "Automatic Image-Based Plant Disease Severity Estimation Using Deep Learning",
        "relation": "uses_metric",
        "tail": "overall accuracy"
      },
      {
        "head": "deep VGG16 model",
        "relation": "baseline_model",
        "tail": "deep convolutional neural networks"
      },
      {
        "head": "PlantVillage dataset",
        "relation": "author_of",
        "tail": "botanists"
      },
      {
        "head": "Plant identification using deep neural networks via optimization of transfer learning parameters",
        "relation": "proposed_model",
        "tail": "deep neural networks"
      },
      {
        "head": "Plant identification using deep neural networks via optimization of transfer learning parameters",
        "relation": "evaluated_on",
        "tail": "transfer learning parameters"
      },
      {
        "head": "Deep Learning for Plant Identification in Natural Environment",
        "relation": "proposed_model",
        "tail": "A 26-layer deep learning model consisting of 8 residual building blocks"
      },
      {
        "head": "A 26-layer deep learning model consisting of 8 residual building blocks",
        "relation": "evaluated_on",
        "tail": "BJFU100 dataset"
      },
      {
        "head": "Deep Learning for Plant Identification in Natural Environment",
        "relation": "author_of",
        "tail": "Plant image identification"
      },
      {
        "head": "Going deeper in the automated identification of Herbarium specimens",
        "relation": "proposed_model",
        "tail": "Deep Learning"
      },
      {
        "head": "Going deeper in the automated identification of Herbarium specimens",
        "relation": "evaluated_on",
        "tail": "herbarium images"
      },
      {
        "head": "Going deeper in the automated identification of Herbarium specimens",
        "relation": "evaluated_on",
        "tail": "photos of plants in the field"
      },
      {
        "head": "Going deeper in the automated identification of Herbarium specimens",
        "relation": "evaluated_on",
        "tail": "big dataset with thousands of species from herbaria"
      },
      {
        "head": "Going deeper in the automated identification of Herbarium specimens",
        "relation": "evaluated_on",
        "tail": "different datasets from different herbaria"
      },
      {
        "head": "Deep Learning",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "Deep Learning",
        "relation": "proposed_model",
        "tail": "automated system"
      },
      {
        "head": "Densely Connected Convolutional Networks",
        "relation": "proposed_model",
        "tail": "Dense Convolutional Network (DenseNet)"
      },
      {
        "head": "Dense Convolutional Network (DenseNet)",
        "relation": "evaluated_on",
        "tail": "CIFAR-10"
      },
      {
        "head": "Dense Convolutional Network (DenseNet)",
        "relation": "evaluated_on",
        "tail": "CIFAR-100"
      },
      {
        "head": "Dense Convolutional Network (DenseNet)",
        "relation": "evaluated_on",
        "tail": "SVHN"
      },
      {
        "head": "Dense Convolutional Network (DenseNet)",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "MobileNetV2: Inverted Residuals and Linear Bottlenecks",
        "relation": "proposed_model",
        "tail": "MobileNetV2"
      },
      {
        "head": "MobileNetV2: Inverted Residuals and Linear Bottlenecks",
        "relation": "proposed_model",
        "tail": "SSDLite"
      },
      {
        "head": "MobileNetV2: Inverted Residuals and Linear Bottlenecks",
        "relation": "proposed_model",
        "tail": "Mobile DeepLabv3"
      },
      {
        "head": "MobileNetV2",
        "relation": "evaluated_on",
        "tail": "Imagenet"
      },
      {
        "head": "MobileNetV2",
        "relation": "evaluated_on",
        "tail": "COCO"
      },
      {
        "head": "MobileNetV2",
        "relation": "evaluated_on",
        "tail": "VOC"
      },
      {
        "head": "MobileNetV2",
        "relation": "uses_metric",
        "tail": "multiply-adds (MAdd)"
      },
      {
        "head": "Communication-Efficient Learning of Deep Networks from Decentralized Data",
        "relation": "proposed_model",
        "tail": "Federated Learning"
      },
      {
        "head": "Communication-Efficient Learning of Deep Networks from Decentralized Data",
        "relation": "evaluated_on",
        "tail": "five different model architectures"
      },
      {
        "head": "Communication-Efficient Learning of Deep Networks from Decentralized Data",
        "relation": "evaluated_on",
        "tail": "four datasets"
      },
      {
        "head": "Federated Learning",
        "relation": "baseline_model",
        "tail": "synchronized stochastic gradient descent"
      },
      {
        "head": "A Comprehensive Survey on Transfer Learning",
        "relation": "evaluated_on",
        "tail": "Amazon Reviews"
      },
      {
        "head": "A Comprehensive Survey on Transfer Learning",
        "relation": "evaluated_on",
        "tail": "Reuters-21578"
      },
      {
        "head": "A Comprehensive Survey on Transfer Learning",
        "relation": "evaluated_on",
        "tail": "Office-31"
      },
      {
        "head": "Privacy Preserved and Decentralized Smartphone Recommendation System",
        "relation": "proposed_model",
        "tail": "federated deep neural network (FDNN)"
      },
      {
        "head": "Privacy Preserved and Decentralized Smartphone Recommendation System",
        "relation": "evaluated_on",
        "tail": "Flipkart dataset"
      },
      {
        "head": "federated deep neural network (FDNN)",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "Privacy Preserved and Decentralized Smartphone Recommendation System",
        "relation": "author_of",
        "tail": "We"
      },
      {
        "head": "federated deep neural network (FDNN)",
        "relation": "uses_metric",
        "tail": "Term Frequency-Inverse Document Frequency (TF-IDF)"
      },
      {
        "head": "federated deep neural network (FDNN)",
        "relation": "baseline_model",
        "tail": "federated learning"
      },
      {
        "head": "A Survey on Heterogeneity Taxonomy, Security and Privacy Preservation in the Integration of IoT, Wireless Sensor Networks and Federated Learning",
        "relation": "author_of",
        "tail": "Federated learning"
      },
      {
        "head": "A Survey on Heterogeneity Taxonomy, Security and Privacy Preservation in the Integration of IoT, Wireless Sensor Networks and Federated Learning",
        "relation": "cites",
        "tail": "Federated learning"
      },
      {
        "head": "A Survey on Heterogeneity Taxonomy, Security and Privacy Preservation in the Integration of IoT, Wireless Sensor Networks and Federated Learning",
        "relation": "cites",
        "tail": "Internet of Things"
      },
      {
        "head": "A Survey on Heterogeneity Taxonomy, Security and Privacy Preservation in the Integration of IoT, Wireless Sensor Networks and Federated Learning",
        "relation": "cites",
        "tail": "Wireless Sensor Networks"
      },
      {
        "head": "Federated learning",
        "relation": "evaluated_on",
        "tail": "Internet of Things"
      },
      {
        "head": "Federated learning",
        "relation": "evaluated_on",
        "tail": "Wireless Sensor Networks"
      },
      {
        "head": "A privacy-preserving federated learning with a secure collaborative for malware detection models using Internet of Things resources",
        "relation": "proposed_model",
        "tail": "privacy-preserving federated learning"
      },
      {
        "head": "privacy-preserving federated learning",
        "relation": "evaluated_on",
        "tail": "Internet of Things resources"
      },
      {
        "head": "privacy-preserving federated learning",
        "relation": "uses_metric",
        "tail": "malware detection models"
      },
      {
        "head": "Federated Learning Architectures: A Performance Evaluation With Crop Yield Prediction Application",
        "relation": "proposed_model",
        "tail": "centralized federated learning framework"
      },
      {
        "head": "Federated Learning Architectures: A Performance Evaluation With Crop Yield Prediction Application",
        "relation": "proposed_model",
        "tail": "decentralized federated learning framework"
      },
      {
        "head": "centralized federated learning framework",
        "relation": "uses_metric",
        "tail": "prediction accuracy"
      },
      {
        "head": "centralized federated learning framework",
        "relation": "uses_metric",
        "tail": "precision"
      },
      {
        "head": "centralized federated learning framework",
        "relation": "uses_metric",
        "tail": "recall"
      },
      {
        "head": "centralized federated learning framework",
        "relation": "uses_metric",
        "tail": "F1-Score"
      },
      {
        "head": "centralized federated learning framework",
        "relation": "uses_metric",
        "tail": "training time"
      },
      {
        "head": "decentralized federated learning framework",
        "relation": "uses_metric",
        "tail": "prediction accuracy"
      },
      {
        "head": "decentralized federated learning framework",
        "relation": "uses_metric",
        "tail": "precision"
      },
      {
        "head": "decentralized federated learning framework",
        "relation": "uses_metric",
        "tail": "recall"
      },
      {
        "head": "decentralized federated learning framework",
        "relation": "uses_metric",
        "tail": "F1-Score"
      },
      {
        "head": "decentralized federated learning framework",
        "relation": "uses_metric",
        "tail": "training time"
      },
      {
        "head": "centralized federated learning framework",
        "relation": "evaluated_on",
        "tail": "crop yield prediction"
      },
      {
        "head": "decentralized federated learning framework",
        "relation": "evaluated_on",
        "tail": "crop yield prediction"
      },
      {
        "head": "centralized federated learning framework",
        "relation": "baseline_model",
        "tail": "cloud-only framework"
      },
      {
        "head": "centralized federated learning framework",
        "relation": "cites",
        "tail": "Long Short-Term Memory Network"
      },
      {
        "head": "decentralized federated learning framework",
        "relation": "cites",
        "tail": "Long Short-Term Memory Network"
      },
      {
        "head": "Segment Anything",
        "relation": "proposed_model",
        "tail": "Segment Anything Model (SAM)"
      },
      {
        "head": "Segment Anything",
        "relation": "evaluated_on",
        "tail": "numerous tasks"
      },
      {
        "head": "Segment Anything",
        "relation": "cites",
        "tail": "prior fully supervised results"
      },
      {
        "head": "Segment Anything",
        "relation": "author_of",
        "tail": "Segment Anything"
      },
      {
        "head": "Visual Instruction Tuning",
        "relation": "proposed_model",
        "tail": "LLaVA: Large Language and Vision Assistant"
      },
      {
        "head": "LLaVA: Large Language and Vision Assistant",
        "relation": "evaluated_on",
        "tail": "Science QA"
      },
      {
        "head": "LLaVA: Large Language and Vision Assistant",
        "relation": "evaluated_on",
        "tail": "GPT-4 generated visual instruction tuning data"
      },
      {
        "head": "LLaVA: Large Language and Vision Assistant",
        "relation": "uses_metric",
        "tail": "85.1% relative score"
      },
      {
        "head": "LLaVA: Large Language and Vision Assistant",
        "relation": "uses_metric",
        "tail": "92.53% accuracy"
      },
      {
        "head": "Visual Instruction Tuning",
        "relation": "cites",
        "tail": "GPT-4"
      },
      {
        "head": "LLaVA: Large Language and Vision Assistant",
        "relation": "baseline_model",
        "tail": "GPT-4"
      },
      {
        "head": "Improved Baselines with Visual Instruction Tuning",
        "relation": "proposed_model",
        "tail": "LLaVA"
      },
      {
        "head": "Improved Baselines with Visual Instruction Tuning",
        "relation": "baseline_model",
        "tail": "LLaVA"
      },
      {
        "head": "Improved Baselines with Visual Instruction Tuning",
        "relation": "evaluated_on",
        "tail": "11 benchmarks"
      },
      {
        "head": "Improved Baselines with Visual Instruction Tuning",
        "relation": "uses_metric",
        "tail": "state-of-the-art"
      },
      {
        "head": "Improved Baselines with Visual Instruction Tuning",
        "relation": "cites",
        "tail": "LLaVA"
      },
      {
        "head": "Improved Baselines with Visual Instruction Tuning",
        "relation": "author_of",
        "tail": "Person"
      },
      {
        "head": "LLaVA",
        "relation": "uses_metric",
        "tail": "data-efficient"
      },
      {
        "head": "LLaVA",
        "relation": "cites",
        "tail": "CLIP-ViT-L-336px"
      },
      {
        "head": "LLaVA",
        "relation": "cites",
        "tail": "MLP projection"
      },
      {
        "head": "LLaVA",
        "relation": "evaluated_on",
        "tail": "academic-task-oriented VQA data"
      },
      {
        "head": "DepictQA-Wild",
        "relation": "proposed_model",
        "tail": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset"
      },
      {
        "head": "DepictQA-Wild",
        "relation": "evaluated_on",
        "tail": "DQ-495K"
      },
      {
        "head": "DepictQA-Wild",
        "relation": "baseline_model",
        "tail": "traditional score-based methods"
      },
      {
        "head": "DepictQA-Wild",
        "relation": "baseline_model",
        "tail": "prior VLM-based IQA models"
      },
      {
        "head": "DepictQA-Wild",
        "relation": "baseline_model",
        "tail": "GPT-4V"
      },
      {
        "head": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset",
        "relation": "cites",
        "tail": "Vision Language Models (VLMs)"
      },
      {
        "head": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset",
        "relation": "cites",
        "tail": "VLM-based Image Quality Assessment (IQA)"
      },
      {
        "head": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs",
        "relation": "proposed_model",
        "tail": "Chain-of-Focus (CoF) method"
      },
      {
        "head": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs",
        "relation": "author_of",
        "tail": "Chain-of-Focus (CoF) method"
      },
      {
        "head": "Chain-of-Focus (CoF) method",
        "relation": "evaluated_on",
        "tail": "V* benchmark"
      },
      {
        "head": "Chain-of-Focus (CoF) method",
        "relation": "uses_metric",
        "tail": "V* benchmark"
      },
      {
        "head": "Chain-of-Focus (CoF) method",
        "relation": "baseline_model",
        "tail": "Vision language models (VLMs)"
      },
      {
        "head": "Chain-of-Focus (CoF) method",
        "relation": "cites",
        "tail": "Vision language models (VLMs)"
      },
      {
        "head": "Chain-of-Focus (CoF) method",
        "relation": "uses_metric",
        "tail": "MM-CoF dataset"
      },
      {
        "head": "Qwen2.5-VL model",
        "relation": "evaluated_on",
        "tail": "V* benchmark"
      },
      {
        "head": "Qwen2.5-VL model",
        "relation": "uses_metric",
        "tail": "MM-CoF dataset"
      },
      {
        "head": "3DThinker",
        "relation": "proposed_model",
        "tail": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views"
      },
      {
        "head": "3DThinker",
        "relation": "baseline_model",
        "tail": "vision-language models (VLMs)"
      },
      {
        "head": "3DThinker",
        "relation": "baseline_model",
        "tail": "topological cognitive maps"
      },
      {
        "head": "3DThinker",
        "relation": "uses_metric",
        "tail": "VGGT"
      },
      {
        "head": "zhangquanchen",
        "relation": "author_of",
        "tail": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views"
      },
      {
        "head": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning",
        "relation": "proposed_model",
        "tail": "UniME-V2"
      },
      {
        "head": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning",
        "relation": "proposed_model",
        "tail": "UniME-V2-Reranker"
      },
      {
        "head": "UniME-V2",
        "relation": "uses_metric",
        "tail": "MLLM-as-a-Judge mechanism"
      },
      {
        "head": "UniME-V2",
        "relation": "evaluated_on",
        "tail": "MMEB benchmark"
      },
      {
        "head": "UniME-V2-Reranker",
        "relation": "evaluated_on",
        "tail": "MMEB benchmark"
      },
      {
        "head": "OpenMMReasoner",
        "relation": "proposed_model",
        "tail": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe"
      },
      {
        "head": "OpenMMReasoner",
        "relation": "baseline_model",
        "tail": "Qwen2.5-VL-7B-Instruct"
      },
      {
        "head": "OpenMMReasoner",
        "relation": "evaluated_on",
        "tail": "nine multimodal reasoning benchmarks"
      },
      {
        "head": "OpenMMReasoner",
        "relation": "uses_metric",
        "tail": "11.6% improvement"
      },
      {
        "head": "OpenMMReasoner",
        "relation": "cites",
        "tail": "Qwen2.5-VL-7B-Instruct"
      },
      {
        "head": "OpenMMReasoner",
        "relation": "author_of",
        "tail": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe"
      },
      {
        "head": "Vision-Language-Action (VLA) Models: Concepts, Progress, Applications and Challenges",
        "relation": "author_of",
        "tail": "Applied-AI-Research-Lab"
      },
      {
        "head": "Vision-Language-Action (VLA) Models: Concepts, Progress, Applications and Challenges",
        "relation": "cites",
        "tail": "over 80 VLA models"
      },
      {
        "head": "Vision-Language-Action (VLA) Models: Concepts, Progress, Applications and Challenges",
        "relation": "proposed_model",
        "tail": "Vision-Language-Action (VLA) models"
      },
      {
        "head": "Vision-Language-Action (VLA) Models: Concepts, Progress, Applications and Challenges",
        "relation": "proposed_model",
        "tail": "vision-language models (VLMs)"
      },
      {
        "head": "Vision-Language-Action (VLA) Models: Concepts, Progress, Applications and Challenges",
        "relation": "proposed_model",
        "tail": "agentic AI"
      },
      {
        "head": "Vision-Language-Action (VLA) Models: Concepts, Progress, Applications and Challenges",
        "relation": "proposed_model",
        "tail": "socially aligned, adaptive, and general-purpose embodied agents"
      },
      {
        "head": "Vision-Language-Action (VLA) models",
        "relation": "evaluated_on",
        "tail": "autonomous vehicles"
      },
      {
        "head": "Vision-Language-Action (VLA) models",
        "relation": "evaluated_on",
        "tail": "medical and industrial robotics"
      },
      {
        "head": "Vision-Language-Action (VLA) models",
        "relation": "evaluated_on",
        "tail": "precision agriculture"
      },
      {
        "head": "Vision-Language-Action (VLA) models",
        "relation": "evaluated_on",
        "tail": "humanoid robotics"
      },
      {
        "head": "Vision-Language-Action (VLA) models",
        "relation": "evaluated_on",
        "tail": "augmented reality"
      },
      {
        "head": "Vision-Language-Action (VLA) models",
        "relation": "evaluated_on",
        "tail": "intelligent, real-world robotics"
      },
      {
        "head": "Vision-Language-Action (VLA) models",
        "relation": "evaluated_on",
        "tail": "artificial general intelligence"
      },
      {
        "head": "Vision-Language-Action (VLA) models",
        "relation": "uses_metric",
        "tail": "architectural innovations"
      },
      {
        "head": "Vision-Language-Action (VLA) models",
        "relation": "uses_metric",
        "tail": "efficient training strategies"
      },
      {
        "head": "Vision-Language-Action (VLA) models",
        "relation": "uses_metric",
        "tail": "real-time inference accelerations"
      },
      {
        "head": "Vision-Language-Action (VLA) models",
        "relation": "uses_metric",
        "tail": "agentic adaptation"
      },
      {
        "head": "Vision-Language-Action (VLA) models",
        "relation": "uses_metric",
        "tail": "cross-embodiment planning"
      },
      {
        "head": "Vision-Language-Action (VLA) models",
        "relation": "baseline_model",
        "tail": "cross-modal learning architectures"
      },
      {
        "head": "Vision-Language-Action (VLA) models",
        "relation": "cites",
        "tail": "generalist agents"
      },
      {
        "head": "Vision-Language-Action (VLA) models",
        "relation": "cites",
        "tail": "action planners"
      },
      {
        "head": "Vision-Language-Action (VLA) models",
        "relation": "cites",
        "tail": "hierarchical controllers"
      },
      {
        "head": "Leave No Observation Behind: Real-time Correction for VLA Action Chunks",
        "relation": "proposed_model",
        "tail": "Asynchronous Action Chunk Correction (A2C2)"
      },
      {
        "head": "Asynchronous Action Chunk Correction (A2C2)",
        "relation": "baseline_model",
        "tail": "Real Time Chunking (RTC)"
      },
      {
        "head": "Asynchronous Action Chunk Correction (A2C2)",
        "relation": "evaluated_on",
        "tail": "dynamic Kinetix task suite"
      },
      {
        "head": "Asynchronous Action Chunk Correction (A2C2)",
        "relation": "evaluated_on",
        "tail": "LIBERO Spatial"
      },
      {
        "head": "Asynchronous Action Chunk Correction (A2C2)",
        "relation": "uses_metric",
        "tail": "success rate"
      },
      {
        "head": "Latent-CoT-Drive (LCDrive)",
        "relation": "proposed_model",
        "tail": "Latent Chain-of-Thought World Modeling for End-to-End Driving"
      },
      {
        "head": "Latent-CoT-Drive (LCDrive)",
        "relation": "evaluated_on",
        "tail": "large-scale end-to-end driving benchmark"
      },
      {
        "head": "Latent-CoT-Drive (LCDrive)",
        "relation": "uses_metric",
        "tail": "trajectory quality"
      },
      {
        "head": "Latent-CoT-Drive (LCDrive)",
        "relation": "baseline_model",
        "tail": "non-reasoning baselines"
      },
      {
        "head": "Latent-CoT-Drive (LCDrive)",
        "relation": "baseline_model",
        "tail": "text-reasoning baselines"
      },
      {
        "head": "Latent Chain-of-Thought World Modeling for End-to-End Driving",
        "relation": "cites",
        "tail": "Vision-Language-Action (VLA) models"
      },
      {
        "head": "Latent Chain-of-Thought World Modeling for End-to-End Driving",
        "relation": "cites",
        "tail": "chain-of-thought (CoT) reasoning"
      },
      {
        "head": "HyperVLA",
        "relation": "proposed_model",
        "tail": "Vision-Language-Action (VLA) models"
      },
      {
        "head": "HyperVLA",
        "relation": "baseline_model",
        "tail": "OpenVLA"
      },
      {
        "head": "HyperVLA",
        "relation": "evaluated_on",
        "tail": "zero-shot generalization"
      },
      {
        "head": "HyperVLA",
        "relation": "evaluated_on",
        "tail": "few-shot adaptation"
      },
      {
        "head": "HyperVLA",
        "relation": "uses_metric",
        "tail": "success rate"
      },
      {
        "head": "HyperVLA",
        "relation": "uses_metric",
        "tail": "inference costs"
      },
      {
        "head": "HyperVLA",
        "relation": "uses_metric",
        "tail": "activated parameters"
      },
      {
        "head": "HyperVLA",
        "relation": "uses_metric",
        "tail": "inference speed"
      },
      {
        "head": "HyperVLA",
        "relation": "cites",
        "tail": "OpenVLA"
      },
      {
        "head": "HyperVLA",
        "relation": "author_of",
        "tail": "HyperVLA"
      },
      {
        "head": "Token Expand-and-Merge-VLA (TEAM-VLA)",
        "relation": "proposed_model",
        "tail": "Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models"
      },
      {
        "head": "Token Expand-and-Merge-VLA (TEAM-VLA)",
        "relation": "baseline_model",
        "tail": "Vision-Language-Action (VLA) models"
      },
      {
        "head": "Token Expand-and-Merge-VLA (TEAM-VLA)",
        "relation": "evaluated_on",
        "tail": "LIBERO benchmark"
      },
      {
        "head": "TransSIL: A Silhouette Cue-Aware Image Classification Framework for Bird Ecological Monitoring Systems",
        "relation": "proposed_model",
        "tail": "TransSIL"
      },
      {
        "head": "TransSIL",
        "relation": "evaluated_on",
        "tail": "CUB200-2011"
      },
      {
        "head": "TransSIL",
        "relation": "evaluated_on",
        "tail": "NABirds"
      },
      {
        "head": "TransSIL",
        "relation": "uses_metric",
        "tail": "fine-grained bird image classification (FBIC)"
      },
      {
        "head": "TransSIL",
        "relation": "baseline_model",
        "tail": "bird ecological intelligent detection system"
      },
      {
        "head": "CBRFormer: rendering technology-based transformer for refinement segmentation of bridge crack images",
        "relation": "evaluated_on",
        "tail": "bridge crack images"
      },
      {
        "head": "A Fast and Compact Threshold Switch-Based Cellular Nonlinear Network Cell",
        "relation": "proposed_model",
        "tail": "threshold switch (TS) model"
      },
      {
        "head": "A Fast and Compact Threshold Switch-Based Cellular Nonlinear Network Cell",
        "relation": "baseline_model",
        "tail": "standard isolated CNN cell"
      },
      {
        "head": "A Fast and Compact Threshold Switch-Based Cellular Nonlinear Network Cell",
        "relation": "evaluated_on",
        "tail": "image processing tasks"
      },
      {
        "head": "DrugCLIP",
        "relation": "proposed_model",
        "tail": "Deep contrastive learning enables genome-wide virtual screening"
      },
      {
        "head": "DrugCLIP",
        "relation": "evaluated_on",
        "tail": "norepinephrine transporter"
      },
      {
        "head": "DrugCLIP",
        "relation": "evaluated_on",
        "tail": "thyroid hormone receptor interactor 12"
      },
      {
        "head": "DrugCLIP",
        "relation": "uses_metric",
        "tail": "hit rate"
      },
      {
        "head": "DrugCLIP",
        "relation": "baseline_model",
        "tail": "docking"
      },
      {
        "head": "DrugCLIP",
        "relation": "cites",
        "tail": "AlphaFold2"
      },
      {
        "head": "We",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening"
      },
      {
        "head": "Deep contrastive learning enables genome-wide virtual screening",
        "relation": "cites",
        "tail": "AlphaFold2"
      },
      {
        "head": "LLaVA-based semantic feature modulation diffusion model for underwater image enhancement",
        "relation": "proposed_model",
        "tail": "LLaVA"
      },
      {
        "head": "LLaVA-based semantic feature modulation diffusion model for underwater image enhancement",
        "relation": "evaluated_on",
        "tail": "underwater image enhancement"
      },
      {
        "head": "3DGS-Drag",
        "relation": "proposed_model",
        "tail": "Dragging Gaussians for Intuitive Point-Based 3D Editing"
      },
      {
        "head": "3DGS-Drag",
        "relation": "uses_metric",
        "tail": "state-of-the-art performance"
      },
      {
        "head": "3DGS-Drag",
        "relation": "evaluated_on",
        "tail": "various scenes"
      },
      {
        "head": "3DGS-Drag",
        "relation": "author_of",
        "tail": "Dragging Gaussians for Intuitive Point-Based 3D Editing"
      },
      {
        "head": "LILaC: Late Interacting in Layered Component Graph for Open-domain Multimodal Multihop Retrieval",
        "relation": "proposed_model",
        "tail": "LILaC"
      },
      {
        "head": "LILaC: Late Interacting in Layered Component Graph for Open-domain Multimodal Multihop Retrieval",
        "relation": "proposed_model",
        "tail": "layered component graph"
      },
      {
        "head": "LILaC: Late Interacting in Layered Component Graph for Open-domain Multimodal Multihop Retrieval",
        "relation": "proposed_model",
        "tail": "late-interaction-based subgraph retrieval method"
      },
      {
        "head": "LILaC",
        "relation": "evaluated_on",
        "tail": "five benchmarks"
      },
      {
        "head": "Bidirectional Normalizing Flow (BiFlow)",
        "relation": "proposed_model",
        "tail": "Bidirectional Normalizing Flow: From Data to Noise and Back"
      },
      {
        "head": "Bidirectional Normalizing Flow (BiFlow)",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "Bidirectional Normalizing Flow: From Data to Noise and Back",
        "relation": "cites",
        "tail": "TARFlow"
      },
      {
        "head": "Bidirectional Normalizing Flow: From Data to Noise and Back",
        "relation": "cites",
        "tail": "Normalizing Flows (NFs)"
      },
      {
        "head": "One-step Latent-free Image Generation with Pixel Mean Flows",
        "relation": "proposed_model",
        "tail": "pixel MeanFlow"
      },
      {
        "head": "One-step Latent-free Image Generation with Pixel Mean Flows",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "Meta Flow Maps enable scalable reward alignment",
        "relation": "proposed_model",
        "tail": "Meta Flow Maps"
      },
      {
        "head": "Meta Flow Maps",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "Meta Flow Maps",
        "relation": "baseline_model",
        "tail": "Best-of-1000"
      },
      {
        "head": "Meta Flow Maps",
        "relation": "cites",
        "tail": "consistency models"
      },
      {
        "head": "Meta Flow Maps",
        "relation": "cites",
        "tail": "flow maps"
      },
      {
        "head": "Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective",
        "relation": "proposed_model",
        "tail": "Sequential Flow Matching"
      },
      {
        "head": "Sequential Flow Matching",
        "relation": "evaluated_on",
        "tail": "forecasting, decision-making and state estimation tasks"
      },
      {
        "head": "Sequential Flow Matching",
        "relation": "uses_metric",
        "tail": "performance competitive with full-step diffusion"
      },
      {
        "head": "Sequential Flow Matching",
        "relation": "cites",
        "tail": "Bayesian filtering"
      },
      {
        "head": "Sequential Flow Matching",
        "relation": "cites",
        "tail": "diffusion and flow-matching models"
      },
      {
        "head": "Drifting Models",
        "relation": "proposed_model",
        "tail": "Generative Modeling via Drifting"
      },
      {
        "head": "Generative Modeling via Drifting",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "Generative Modeling via Drifting",
        "relation": "uses_metric",
        "tail": "FID"
      },
      {
        "head": "Generative Modeling via Drifting",
        "relation": "cites",
        "tail": "diffusion models"
      },
      {
        "head": "Generative Modeling via Drifting",
        "relation": "cites",
        "tail": "flow-based models"
      },
      {
        "head": "A Simple Framework for Contrastive Learning of Visual Representations",
        "relation": "proposed_model",
        "tail": "SimCLR"
      },
      {
        "head": "SimCLR",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "SimCLR",
        "relation": "baseline_model",
        "tail": "ResNet-50"
      },
      {
        "head": "SimCLR",
        "relation": "baseline_model",
        "tail": "AlexNet"
      },
      {
        "head": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning",
        "relation": "proposed_model",
        "tail": "Directional Decoupling Alignment (D²-Align)"
      },
      {
        "head": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning",
        "relation": "proposed_model",
        "tail": "DivGenBench"
      },
      {
        "head": "Directional Decoupling Alignment (D²-Align)",
        "relation": "evaluated_on",
        "tail": "DivGenBench"
      },
      {
        "head": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning",
        "relation": "cites",
        "tail": "Reinforcement Learning from Human Feedback"
      },
      {
        "head": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning",
        "relation": "cites",
        "tail": "Preference Mode Collapse (PMC)"
      },
      {
        "head": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints",
        "relation": "proposed_model",
        "tail": "ImagerySearch"
      },
      {
        "head": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints",
        "relation": "evaluated_on",
        "tail": "LDT-Bench"
      },
      {
        "head": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints",
        "relation": "evaluated_on",
        "tail": "VBench"
      },
      {
        "head": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints",
        "relation": "cites",
        "tail": "VBench"
      },
      {
        "head": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders",
        "relation": "proposed_model",
        "tail": "Representation Autoencoders (RAEs)"
      },
      {
        "head": "Representation Autoencoders (RAEs)",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "Representation Autoencoders (RAEs)",
        "relation": "baseline_model",
        "tail": "FLUX VAE"
      },
      {
        "head": "Representation Autoencoders (RAEs)",
        "relation": "baseline_model",
        "tail": "VAEs"
      },
      {
        "head": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders",
        "relation": "cites",
        "tail": "SigLIP-2"
      },
      {
        "head": "Simulating the Visual World with Artificial Intelligence: A Roadmap",
        "relation": "proposed_model",
        "tail": "video foundation models"
      },
      {
        "head": "video foundation models",
        "relation": "proposed_model",
        "tail": "implicit world model"
      },
      {
        "head": "video foundation models",
        "relation": "proposed_model",
        "tail": "video renderer"
      },
      {
        "head": "world model",
        "relation": "proposed_model",
        "tail": "video generation model"
      },
      {
        "head": "Simulating the Visual World with Artificial Intelligence: A Roadmap",
        "relation": "evaluated_on",
        "tail": "robotics"
      },
      {
        "head": "Simulating the Visual World with Artificial Intelligence: A Roadmap",
        "relation": "evaluated_on",
        "tail": "autonomous driving"
      },
      {
        "head": "Simulating the Visual World with Artificial Intelligence: A Roadmap",
        "relation": "evaluated_on",
        "tail": "interactive gaming"
      },
      {
        "head": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks",
        "relation": "proposed_model",
        "tail": "unified design specification for world models"
      },
      {
        "head": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks",
        "relation": "cites",
        "tail": "World models"
      },
      {
        "head": "World models",
        "relation": "evaluated_on",
        "tail": "visual prediction"
      },
      {
        "head": "World models",
        "relation": "evaluated_on",
        "tail": "3D estimation"
      },
      {
        "head": "World models",
        "relation": "evaluated_on",
        "tail": "symbol grounding"
      },
      {
        "head": "unified design specification for world models",
        "relation": "uses_metric",
        "tail": "interaction"
      },
      {
        "head": "unified design specification for world models",
        "relation": "uses_metric",
        "tail": "perception"
      },
      {
        "head": "unified design specification for world models",
        "relation": "uses_metric",
        "tail": "symbolic reasoning"
      },
      {
        "head": "unified design specification for world models",
        "relation": "uses_metric",
        "tail": "spatial representation"
      },
      {
        "head": "RecTok: Reconstruction Distillation along Rectified Flow",
        "relation": "proposed_model",
        "tail": "RecTok"
      },
      {
        "head": "RecTok",
        "relation": "author_of",
        "tail": "Shi Qingyu"
      },
      {
        "head": "RecTok",
        "relation": "evaluated_on",
        "tail": "gFID-50K"
      },
      {
        "head": "RecTok",
        "relation": "uses_metric",
        "tail": "gFID-50K"
      },
      {
        "head": "RecTok",
        "relation": "baseline_model",
        "tail": "visual tokenizers"
      },
      {
        "head": "RecTok",
        "relation": "cites",
        "tail": "diffusion models"
      },
      {
        "head": "RecTok",
        "relation": "cites",
        "tail": "vision foundation models"
      },
      {
        "head": "RecTok",
        "relation": "cites",
        "tail": "flow matching"
      },
      {
        "head": "RecTok",
        "relation": "cites",
        "tail": "diffusion transformers"
      },
      {
        "head": "RecTok",
        "relation": "cites",
        "tail": "VFMs"
      },
      {
        "head": "RePack then Refine",
        "relation": "proposed_model",
        "tail": "RePack-DiT-XL/1"
      },
      {
        "head": "RePack then Refine",
        "relation": "proposed_model",
        "tail": "RePack module"
      },
      {
        "head": "RePack then Refine",
        "relation": "proposed_model",
        "tail": "Latent-Guided Refiner"
      },
      {
        "head": "RePack then Refine",
        "relation": "baseline_model",
        "tail": "Latent Diffusion Models"
      },
      {
        "head": "RePack then Refine",
        "relation": "evaluated_on",
        "tail": "ImageNet-1K"
      },
      {
        "head": "RePack then Refine",
        "relation": "uses_metric",
        "tail": "FID"
      },
      {
        "head": "RePack then Refine",
        "relation": "cites",
        "tail": "Vision Foundation Models"
      },
      {
        "head": "RePack then Refine",
        "relation": "cites",
        "tail": "Diffusion Transformers"
      },
      {
        "head": "RePack module",
        "relation": "cites",
        "tail": "Vision Foundation Models"
      },
      {
        "head": "Latent-Guided Refiner",
        "relation": "cites",
        "tail": "RePack module"
      },
      {
        "head": "Diffusion Models Beat GANs on Image Synthesis",
        "relation": "proposed_model",
        "tail": "diffusion models"
      },
      {
        "head": "Diffusion Models Beat GANs on Image Synthesis",
        "relation": "baseline_model",
        "tail": "GANs"
      },
      {
        "head": "Diffusion Models Beat GANs on Image Synthesis",
        "relation": "evaluated_on",
        "tail": "ImageNet 128×128"
      },
      {
        "head": "Diffusion Models Beat GANs on Image Synthesis",
        "relation": "evaluated_on",
        "tail": "ImageNet 256×256"
      },
      {
        "head": "Diffusion Models Beat GANs on Image Synthesis",
        "relation": "evaluated_on",
        "tail": "ImageNet 512×512"
      },
      {
        "head": "Diffusion Models Beat GANs on Image Synthesis",
        "relation": "uses_metric",
        "tail": "FID"
      },
      {
        "head": "diffusion models",
        "relation": "cites",
        "tail": "BigGAN-deep"
      },
      {
        "head": "diffusion models",
        "relation": "uses_metric",
        "tail": "FID"
      },
      {
        "head": "classifier guidance",
        "relation": "cites",
        "tail": "upsampling diffusion models"
      },
      {
        "head": "DINOv2: Learning Robust Visual Features without Supervision",
        "relation": "proposed_model",
        "tail": "ViT model (Dosovitskiy et al., 2020)"
      },
      {
        "head": "DINOv2: Learning Robust Visual Features without Supervision",
        "relation": "baseline_model",
        "tail": "OpenCLIP (Ilharco et al., 2021)"
      },
      {
        "head": "DINOv2: Learning Robust Visual Features without Supervision",
        "relation": "evaluated_on",
        "tail": "OpenCLIP (Ilharco et al., 2021)"
      },
      {
        "head": "Dosovitskiy et al.",
        "relation": "author_of",
        "tail": "ViT model (Dosovitskiy et al., 2020)"
      },
      {
        "head": "Ilharco et al.",
        "relation": "author_of",
        "tail": "OpenCLIP (Ilharco et al., 2021)"
      },
      {
        "head": "DINOv2: Learning Robust Visual Features without Supervision",
        "relation": "cites",
        "tail": "ViT model (Dosovitskiy et al., 2020)"
      },
      {
        "head": "DINOv2: Learning Robust Visual Features without Supervision",
        "relation": "cites",
        "tail": "OpenCLIP (Ilharco et al., 2021)"
      },
      {
        "head": "PixelGen",
        "relation": "proposed_model",
        "tail": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss"
      },
      {
        "head": "PixelGen",
        "relation": "baseline_model",
        "tail": "latent diffusion"
      },
      {
        "head": "PixelGen",
        "relation": "evaluated_on",
        "tail": "ImageNet-256"
      },
      {
        "head": "PixelGen",
        "relation": "evaluated_on",
        "tail": "GenEval"
      },
      {
        "head": "PixelGen",
        "relation": "uses_metric",
        "tail": "FID"
      },
      {
        "head": "PixelGen",
        "relation": "uses_metric",
        "tail": "GenEval score"
      },
      {
        "head": "PixelGen",
        "relation": "cites",
        "tail": "LPIPS"
      },
      {
        "head": "PixelGen",
        "relation": "cites",
        "tail": "DINO"
      },
      {
        "head": "Zehong-Ma",
        "relation": "author_of",
        "tail": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss"
      },
      {
        "head": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
        "relation": "author_of",
        "tail": "Person"
      },
      {
        "head": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
        "relation": "cites",
        "tail": "GPT-4o"
      },
      {
        "head": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
        "relation": "cites",
        "tail": "multimodal understanding models"
      },
      {
        "head": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
        "relation": "cites",
        "tail": "image generation models"
      },
      {
        "head": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
        "relation": "cites",
        "tail": "autoregressive-based architectures"
      },
      {
        "head": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
        "relation": "cites",
        "tail": "diffusion-based models"
      },
      {
        "head": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
        "relation": "cites",
        "tail": "unified frameworks"
      },
      {
        "head": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
        "relation": "cites",
        "tail": "diffusion-based"
      },
      {
        "head": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
        "relation": "cites",
        "tail": "autoregressive-based"
      },
      {
        "head": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
        "relation": "cites",
        "tail": "hybrid approaches"
      },
      {
        "head": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
        "relation": "cites",
        "tail": "datasets and benchmarks"
      },
      {
        "head": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
        "relation": "cites",
        "tail": "GitHub"
      },
      {
        "head": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery",
        "relation": "author_of",
        "tail": "Person"
      },
      {
        "head": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery",
        "relation": "proposed_model",
        "tail": "MentisOculi"
      },
      {
        "head": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery",
        "relation": "evaluated_on",
        "tail": "Frontier models"
      },
      {
        "head": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery",
        "relation": "evaluated_on",
        "tail": "multimodal large language models (MLLMs)"
      },
      {
        "head": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery",
        "relation": "evaluated_on",
        "tail": "unified multimodal models (UMMs)"
      },
      {
        "head": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery",
        "relation": "cites",
        "tail": "Frontier models"
      },
      {
        "head": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery",
        "relation": "cites",
        "tail": "multimodal large language models (MLLMs)"
      },
      {
        "head": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery",
        "relation": "cites",
        "tail": "unified multimodal models (UMMs)"
      },
      {
        "head": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation",
        "relation": "proposed_model",
        "tail": "OpenVision 3"
      },
      {
        "head": "OpenVision 3",
        "relation": "uses_metric",
        "tail": "SeedBench"
      },
      {
        "head": "OpenVision 3",
        "relation": "uses_metric",
        "tail": "POPE"
      },
      {
        "head": "OpenVision 3",
        "relation": "uses_metric",
        "tail": "ImageNet"
      },
      {
        "head": "OpenVision 3",
        "relation": "baseline_model",
        "tail": "CLIP"
      },
      {
        "head": "OpenVision 3",
        "relation": "evaluated_on",
        "tail": "LLaVA-1.5"
      },
      {
        "head": "OpenVision 3",
        "relation": "evaluated_on",
        "tail": "RAE"
      },
      {
        "head": "OpenVision 3",
        "relation": "cites",
        "tail": "VAE"
      },
      {
        "head": "OpenVision 3",
        "relation": "cites",
        "tail": "ViT"
      },
      {
        "head": "OpenVision 3",
        "relation": "cites",
        "tail": "ViT-VAE"
      },
      {
        "head": "Distributed Representations of Words and Phrases and their Compositionality",
        "relation": "proposed_model",
        "tail": "continuous Skip-gram model"
      },
      {
        "head": "Distributed Representations of Words and Phrases and their Compositionality",
        "relation": "proposed_model",
        "tail": "subsampling of the frequent words"
      },
      {
        "head": "Distributed Representations of Words and Phrases and their Compositionality",
        "relation": "proposed_model",
        "tail": "negative sampling"
      },
      {
        "head": "Distributed Representations of Words and Phrases and their Compositionality",
        "relation": "proposed_model",
        "tail": "simple method for finding phrases in text"
      },
      {
        "head": "Distributed Representations of Words and Phrases and their Compositionality",
        "relation": "baseline_model",
        "tail": "hierarchical softmax"
      },
      {
        "head": "GloVe: Global Vectors for Word Representation",
        "relation": "proposed_model",
        "tail": "global logbilinear regression model"
      },
      {
        "head": "global logbilinear regression model",
        "relation": "baseline_model",
        "tail": "global matrix factorization"
      },
      {
        "head": "global logbilinear regression model",
        "relation": "baseline_model",
        "tail": "local context window methods"
      },
      {
        "head": "global logbilinear regression model",
        "relation": "evaluated_on",
        "tail": "word analogy task"
      },
      {
        "head": "global logbilinear regression model",
        "relation": "evaluated_on",
        "tail": "similarity tasks"
      },
      {
        "head": "global logbilinear regression model",
        "relation": "evaluated_on",
        "tail": "named entity recognition"
      },
      {
        "head": "global logbilinear regression model",
        "relation": "uses_metric",
        "tail": "word-word cooccurrence matrix"
      },
      {
        "head": "Deep Contextualized Word Representations",
        "relation": "proposed_model",
        "tail": "deep bidirectional language model"
      },
      {
        "head": "Deep Contextualized Word Representations",
        "relation": "evaluated_on",
        "tail": "question answering"
      },
      {
        "head": "Deep Contextualized Word Representations",
        "relation": "evaluated_on",
        "tail": "textual entailment"
      },
      {
        "head": "Deep Contextualized Word Representations",
        "relation": "evaluated_on",
        "tail": "sentiment analysis"
      },
      {
        "head": "We",
        "relation": "author_of",
        "tail": "Deep Contextualized Word Representations"
      },
      {
        "head": "Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis",
        "relation": "proposed_model",
        "tail": "Modern models for common NLP tasks"
      },
      {
        "head": "Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis",
        "relation": "evaluated_on",
        "tail": "2018 Twitter data spanning 51 U.S. regions and 99 countries"
      },
      {
        "head": "Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis",
        "relation": "evaluated_on",
        "tail": "18 international and 5 U.S.-based statistical gender gaps"
      },
      {
        "head": "SegMamba-V2",
        "relation": "proposed_model",
        "tail": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation"
      },
      {
        "head": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation",
        "relation": "author_of",
        "tail": "ge-xing"
      },
      {
        "head": "SegMamba-V2",
        "relation": "evaluated_on",
        "tail": "CRC-2000"
      },
      {
        "head": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation",
        "relation": "cites",
        "tail": "Transformer"
      },
      {
        "head": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation",
        "relation": "cites",
        "tail": "Mamba"
      },
      {
        "head": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation",
        "relation": "cites",
        "tail": "State Space Model (SSM)"
      },
      {
        "head": "Generative Classifiers Avoid Shortcut Solutions",
        "relation": "proposed_model",
        "tail": "diffusion-based generative classifiers"
      },
      {
        "head": "Generative Classifiers Avoid Shortcut Solutions",
        "relation": "proposed_model",
        "tail": "autoregressive generative classifiers"
      },
      {
        "head": "Generative Classifiers Avoid Shortcut Solutions",
        "relation": "evaluated_on",
        "tail": "five standard image and text distribution shift benchmarks"
      },
      {
        "head": "Generative Classifiers Avoid Shortcut Solutions",
        "relation": "evaluated_on",
        "tail": "medical datasets"
      },
      {
        "head": "Generative Classifiers Avoid Shortcut Solutions",
        "relation": "evaluated_on",
        "tail": "satellite datasets"
      },
      {
        "head": "Generative Classifiers Avoid Shortcut Solutions",
        "relation": "evaluated_on",
        "tail": "Gaussian toy setting"
      },
      {
        "head": "Edge Large AI Model Agent-Empowered Cognitive Multimodal Semantic Communication",
        "relation": "proposed_model",
        "tail": "Edge Large AI Model Agent"
      },
      {
        "head": "Edge Large AI Model Agent-Empowered Cognitive Multimodal Semantic Communication",
        "relation": "proposed_model",
        "tail": "Cognitive Multimodal Semantic Communication"
      },
      {
        "head": "Language Models are Unsupervised Multitask Learners",
        "relation": "proposed_model",
        "tail": "GPT-2"
      },
      {
        "head": "The Llama 3 Herd of Models",
        "relation": "proposed_model",
        "tail": "Llama 3"
      },
      {
        "head": "The Llama 3 Herd of Models",
        "relation": "evaluated_on",
        "tail": "Llama 3"
      },
      {
        "head": "Llama 3",
        "relation": "uses_metric",
        "tail": "GPT-4"
      },
      {
        "head": "The Llama 3 Herd of Models",
        "relation": "cites",
        "tail": "GPT-4"
      },
      {
        "head": "A Survey on Diffusion Language Models",
        "relation": "author_of",
        "tail": "VILA-Lab"
      },
      {
        "head": "A Survey on Diffusion Language Models",
        "relation": "cites",
        "tail": "Diffusion Language Models"
      },
      {
        "head": "A Survey on Diffusion Language Models",
        "relation": "cites",
        "tail": "autoregressive paradigm"
      },
      {
        "head": "A Survey on Diffusion Language Models",
        "relation": "cites",
        "tail": "masked language models"
      },
      {
        "head": "A Survey on Diffusion Language Models",
        "relation": "proposed_model",
        "tail": "Diffusion Language Models"
      },
      {
        "head": "Diffusion Language Models",
        "relation": "baseline_model",
        "tail": "autoregressive paradigm"
      },
      {
        "head": "VILA-Lab",
        "relation": "author_of",
        "tail": "Awesome-DLMs"
      },
      {
        "head": "Training Optimal Large Diffusion Language Models",
        "relation": "proposed_model",
        "tail": "Quokka"
      },
      {
        "head": "Quokka",
        "relation": "cites",
        "tail": "Chinchilla"
      },
      {
        "head": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models",
        "relation": "proposed_model",
        "tail": "IGPO (Inpainting Guided Policy Optimization)"
      },
      {
        "head": "IGPO (Inpainting Guided Policy Optimization)",
        "relation": "baseline_model",
        "tail": "GRPO"
      },
      {
        "head": "IGPO (Inpainting Guided Policy Optimization)",
        "relation": "evaluated_on",
        "tail": "GSM8K"
      },
      {
        "head": "IGPO (Inpainting Guided Policy Optimization)",
        "relation": "evaluated_on",
        "tail": "Math500"
      },
      {
        "head": "IGPO (Inpainting Guided Policy Optimization)",
        "relation": "evaluated_on",
        "tail": "AMC"
      },
      {
        "head": "IGPO (Inpainting Guided Policy Optimization)",
        "relation": "uses_metric",
        "tail": "sample efficiency"
      },
      {
        "head": "masked diffusion large language models (dLLMs)",
        "relation": "cites",
        "tail": "autoregressive LLMs"
      },
      {
        "head": "IGPO (Inpainting Guided Policy Optimization)",
        "relation": "cites",
        "tail": "reinforcement learning"
      },
      {
        "head": "IGPO (Inpainting Guided Policy Optimization)",
        "relation": "cites",
        "tail": "supervised fine-tuning"
      },
      {
        "head": "IGPO (Inpainting Guided Policy Optimization)",
        "relation": "cites",
        "tail": "entropy-based filtering"
      },
      {
        "head": "Encoder-Decoder Diffusion Language Models for Efficient Training and Inference",
        "relation": "proposed_model",
        "tail": "encoder-decoder architecture"
      },
      {
        "head": "Encoder-Decoder Diffusion Language Models for Efficient Training and Inference",
        "relation": "proposed_model",
        "tail": "Efficient Encoder-Decoder Diffusion (E2D2)"
      },
      {
        "head": "Encoder-Decoder Diffusion Language Models for Efficient Training and Inference",
        "relation": "baseline_model",
        "tail": "discrete diffusion models"
      },
      {
        "head": "Encoder-Decoder Diffusion Language Models for Efficient Training and Inference",
        "relation": "baseline_model",
        "tail": "autoregressive approaches"
      },
      {
        "head": "Encoder-Decoder Diffusion Language Models for Efficient Training and Inference",
        "relation": "evaluated_on",
        "tail": "summarization"
      },
      {
        "head": "Encoder-Decoder Diffusion Language Models for Efficient Training and Inference",
        "relation": "evaluated_on",
        "tail": "translation"
      },
      {
        "head": "Encoder-Decoder Diffusion Language Models for Efficient Training and Inference",
        "relation": "evaluated_on",
        "tail": "mathematical reasoning"
      },
      {
        "head": "Efficient Encoder-Decoder Diffusion (E2D2)",
        "relation": "uses_metric",
        "tail": "generation quality"
      },
      {
        "head": "Efficient Encoder-Decoder Diffusion (E2D2)",
        "relation": "uses_metric",
        "tail": "inference throughput"
      },
      {
        "head": "encoder-decoder architecture",
        "relation": "cites",
        "tail": "block diffusion models"
      },
      {
        "head": "Stable-DiffCoder",
        "relation": "proposed_model",
        "tail": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model"
      },
      {
        "head": "Stable-DiffCoder",
        "relation": "baseline_model",
        "tail": "Seed-Coder"
      },
      {
        "head": "Stable-DiffCoder",
        "relation": "evaluated_on",
        "tail": "code benchmarks"
      },
      {
        "head": "Stable-DiffCoder",
        "relation": "uses_metric",
        "tail": "code benchmarks"
      },
      {
        "head": "Stable-DiffCoder",
        "relation": "cites",
        "tail": "Seed-Coder"
      },
      {
        "head": "Stable-DiffCoder",
        "relation": "author_of",
        "tail": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model"
      },
      {
        "head": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications",
        "relation": "author_of",
        "tail": "Large Language Model (LLM)-based agents"
      },
      {
        "head": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications",
        "relation": "proposed_model",
        "tail": "graph-based agent memory"
      },
      {
        "head": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications",
        "relation": "evaluated_on",
        "tail": "open-sourced libraries and benchmarks"
      },
      {
        "head": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications",
        "relation": "cites",
        "tail": "research papers"
      },
      {
        "head": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications",
        "relation": "cites",
        "tail": "open-source data"
      },
      {
        "head": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications",
        "relation": "cites",
        "tail": "projects"
      },
      {
        "head": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications",
        "relation": "uses_metric",
        "tail": "open-sourced libraries and benchmarks"
      },
      {
        "head": "graph-based agent memory",
        "relation": "baseline_model",
        "tail": "self-evolving agent memory"
      },
      {
        "head": "open-sourced libraries and benchmarks",
        "relation": "evaluated_on",
        "tail": "self-evolving agent memory"
      },
      {
        "head": "https://github.com/DEEP-PolyU/Awesome-GraphMemory",
        "relation": "cites",
        "tail": "research papers"
      },
      {
        "head": "https://github.com/DEEP-PolyU/Awesome-GraphMemory",
        "relation": "cites",
        "tail": "open-source data"
      },
      {
        "head": "https://github.com/DEEP-PolyU/Awesome-GraphMemory",
        "relation": "cites",
        "tail": "projects"
      },
      {
        "head": "Empirical-MCTS",
        "relation": "proposed_model",
        "tail": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search"
      },
      {
        "head": "Empirical-MCTS",
        "relation": "baseline_model",
        "tail": "Monte Carlo Tree Search (MCTS)"
      },
      {
        "head": "Empirical-MCTS",
        "relation": "evaluated_on",
        "tail": "AIME25"
      },
      {
        "head": "Empirical-MCTS",
        "relation": "evaluated_on",
        "tail": "ARC-AGI-2"
      },
      {
        "head": "Empirical-MCTS",
        "relation": "evaluated_on",
        "tail": "MathArena Apex"
      },
      {
        "head": "Empirical-MCTS",
        "relation": "uses_metric",
        "tail": "complex reasoning benchmarks"
      },
      {
        "head": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search",
        "relation": "cites",
        "tail": "Monte Carlo Tree Search (MCTS)"
      },
      {
        "head": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search",
        "relation": "cites",
        "tail": "Large Language Models (LLMs)"
      },
      {
        "head": "Empirical-MCTS",
        "relation": "author_of",
        "tail": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search"
      },
      {
        "head": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking",
        "relation": "proposed_model",
        "tail": "TAME"
      },
      {
        "head": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking",
        "relation": "evaluated_on",
        "tail": "Trust-Memevo benchmark"
      },
      {
        "head": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking",
        "relation": "cites",
        "tail": "Agent Memory Misevolution"
      },
      {
        "head": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents",
        "relation": "proposed_model",
        "tail": "ProcMEM"
      },
      {
        "head": "ProcMEM",
        "relation": "uses_metric",
        "tail": "reuse rates"
      },
      {
        "head": "ProcMEM",
        "relation": "uses_metric",
        "tail": "performance gains"
      },
      {
        "head": "ProcMEM",
        "relation": "uses_metric",
        "tail": "memory compression"
      },
      {
        "head": "ProcMEM",
        "relation": "evaluated_on",
        "tail": "in-domain scenarios"
      },
      {
        "head": "ProcMEM",
        "relation": "evaluated_on",
        "tail": "cross-task scenarios"
      },
      {
        "head": "ProcMEM",
        "relation": "evaluated_on",
        "tail": "cross-agent scenarios"
      },
      {
        "head": "ProcMEM",
        "relation": "baseline_model",
        "tail": "on-the-fly reasoning"
      },
      {
        "head": "ProcMEM",
        "relation": "cites",
        "tail": "LLM-driven agents"
      },
      {
        "head": "Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting",
        "relation": "proposed_model",
        "tail": "agentic time series forecasting (ATSF)"
      },
      {
        "head": "agentic time series forecasting (ATSF)",
        "relation": "author_of",
        "tail": "Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting"
      },
      {
        "head": "agentic time series forecasting (ATSF)",
        "relation": "cites",
        "tail": "workflow-based design"
      },
      {
        "head": "agentic time series forecasting (ATSF)",
        "relation": "cites",
        "tail": "agentic reinforcement learning"
      },
      {
        "head": "agentic time series forecasting (ATSF)",
        "relation": "cites",
        "tail": "hybrid agentic workflow paradigm"
      },
      {
        "head": "Training language models to follow instructions with human feedback",
        "relation": "proposed_model",
        "tail": "InstructGPT"
      },
      {
        "head": "Training language models to follow instructions with human feedback",
        "relation": "baseline_model",
        "tail": "GPT-3"
      },
      {
        "head": "InstructGPT",
        "relation": "evaluated_on",
        "tail": "public NLP datasets"
      },
      {
        "head": "InstructGPT",
        "relation": "uses_metric",
        "tail": "truthfulness"
      },
      {
        "head": "InstructGPT",
        "relation": "uses_metric",
        "tail": "toxic output generation"
      },
      {
        "head": "Training language models to follow instructions with human feedback",
        "relation": "cites",
        "tail": "GPT-3"
      },
      {
        "head": "Training language models to follow instructions with human feedback",
        "relation": "author_of",
        "tail": "InstructGPT"
      },
      {
        "head": "PaLM: Scaling Language Modeling with Pathways",
        "relation": "proposed_model",
        "tail": "Pathways Language Model PaLM"
      },
      {
        "head": "Pathways Language Model PaLM",
        "relation": "evaluated_on",
        "tail": "BIG-bench"
      },
      {
        "head": "Pathways Language Model PaLM",
        "relation": "uses_metric",
        "tail": "few-shot learning results"
      },
      {
        "head": "Pathways Language Model PaLM",
        "relation": "baseline_model",
        "tail": "Transformer language model"
      },
      {
        "head": "Pathways Language Model PaLM",
        "relation": "evaluated_on",
        "tail": "language understanding and generation benchmarks"
      },
      {
        "head": "Pathways Language Model PaLM",
        "relation": "evaluated_on",
        "tail": "multilingual tasks and source code generation benchmarks"
      },
      {
        "head": "ReAct: Synergizing Reasoning and Acting in Language Models",
        "relation": "proposed_model",
        "tail": "ReAct"
      },
      {
        "head": "ReAct",
        "relation": "evaluated_on",
        "tail": "HotpotQA"
      },
      {
        "head": "ReAct",
        "relation": "evaluated_on",
        "tail": "Fever"
      },
      {
        "head": "ReAct",
        "relation": "evaluated_on",
        "tail": "ALFWorld"
      },
      {
        "head": "ReAct",
        "relation": "evaluated_on",
        "tail": "WebShop"
      },
      {
        "head": "ReAct",
        "relation": "uses_metric",
        "tail": "absolute success rate"
      },
      {
        "head": "ReAct",
        "relation": "cites",
        "tail": "large language models (LLMs)"
      },
      {
        "head": "ReAct",
        "relation": "cites",
        "tail": "chain-of-thought prompting"
      },
      {
        "head": "ReAct",
        "relation": "cites",
        "tail": "imitation and reinforcement learning methods"
      },
      {
        "head": "ReAct",
        "relation": "baseline_model",
        "tail": "chain-of-thought prompting"
      },
      {
        "head": "ReAct",
        "relation": "baseline_model",
        "tail": "imitation and reinforcement learning methods"
      },
      {
        "head": "From System 1 to System 2: A Survey of Reasoning Large Language Models",
        "relation": "author_of",
        "tail": "zzli2022"
      },
      {
        "head": "From System 1 to System 2: A Survey of Reasoning Large Language Models",
        "relation": "cites",
        "tail": "OpenAI's o1/o3"
      },
      {
        "head": "From System 1 to System 2: A Survey of Reasoning Large Language Models",
        "relation": "cites",
        "tail": "DeepSeek's R1"
      },
      {
        "head": "From System 1 to System 2: A Survey of Reasoning Large Language Models",
        "relation": "evaluated_on",
        "tail": "Awesome-Slow-Reason-System"
      },
      {
        "head": "Deep Research Agents: A Systematic Examination And Roadmap",
        "relation": "proposed_model",
        "tail": "Deep Research (DR) agents"
      },
      {
        "head": "Deep Research Agents: A Systematic Examination And Roadmap",
        "relation": "proposed_model",
        "tail": "taxonomy"
      },
      {
        "head": "Deep Research Agents: A Systematic Examination And Roadmap",
        "relation": "evaluated_on",
        "tail": "benchmarks"
      },
      {
        "head": "Deep Research (DR) agents",
        "relation": "uses_metric",
        "tail": "benchmarks"
      },
      {
        "head": "Deep Research Agents: A Systematic Examination And Roadmap",
        "relation": "cites",
        "tail": "Large Language Models (LLMs)"
      },
      {
        "head": "Deep Research Agents: A Systematic Examination And Roadmap",
        "relation": "cites",
        "tail": "Model Context Protocols (MCPs)"
      },
      {
        "head": "Deep Research Agents: A Systematic Examination And Roadmap",
        "relation": "cites",
        "tail": "repository of DR agent research"
      },
      {
        "head": "Table-R1: Inference-Time Scaling for Table Reasoning",
        "relation": "proposed_model",
        "tail": "Table-R1-SFT"
      },
      {
        "head": "Table-R1: Inference-Time Scaling for Table Reasoning",
        "relation": "proposed_model",
        "tail": "Table-R1-Zero"
      },
      {
        "head": "Table-R1: Inference-Time Scaling for Table Reasoning",
        "relation": "baseline_model",
        "tail": "GPT-4.1"
      },
      {
        "head": "Table-R1: Inference-Time Scaling for Table Reasoning",
        "relation": "baseline_model",
        "tail": "DeepSeek-R1"
      },
      {
        "head": "Table-R1: Inference-Time Scaling for Table Reasoning",
        "relation": "evaluated_on",
        "tail": "short-form QA"
      },
      {
        "head": "Table-R1: Inference-Time Scaling for Table Reasoning",
        "relation": "evaluated_on",
        "tail": "fact verification"
      },
      {
        "head": "Table-R1: Inference-Time Scaling for Table Reasoning",
        "relation": "evaluated_on",
        "tail": "free-form QA"
      },
      {
        "head": "Table-R1: Inference-Time Scaling for Table Reasoning",
        "relation": "uses_metric",
        "tail": "verifiable reward functions"
      },
      {
        "head": "Table-R1: Inference-Time Scaling for Table Reasoning",
        "relation": "cites",
        "tail": "DeepSeek-R1"
      },
      {
        "head": "Table-R1: Inference-Time Scaling for Table Reasoning",
        "relation": "cites",
        "tail": "GRPO algorithm"
      },
      {
        "head": "Table-R1: Inference-Time Scaling for Table Reasoning",
        "relation": "author_of",
        "tail": "Table-R1: Inference-Time Scaling for Table Reasoning"
      },
      {
        "head": "Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis",
        "relation": "proposed_model",
        "tail": "Mind2Report"
      },
      {
        "head": "Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis",
        "relation": "evaluated_on",
        "tail": "QRC-Eval"
      },
      {
        "head": "Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis",
        "relation": "baseline_model",
        "tail": "OpenAI deep research agents"
      },
      {
        "head": "Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis",
        "relation": "baseline_model",
        "tail": "Gemini deep research agents"
      },
      {
        "head": "Mind2Report",
        "relation": "uses_metric",
        "tail": "QRC-Eval"
      },
      {
        "head": "PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware Sequence-Level Policy Optimization",
        "relation": "proposed_model",
        "tail": "PaperScout"
      },
      {
        "head": "PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware Sequence-Level Policy Optimization",
        "relation": "proposed_model",
        "tail": "Proximal Sequence Policy Optimization (PSPO)"
      },
      {
        "head": "PaperScout",
        "relation": "evaluated_on",
        "tail": "synthetic and real-world benchmarks"
      },
      {
        "head": "PaperScout",
        "relation": "uses_metric",
        "tail": "recall"
      },
      {
        "head": "PaperScout",
        "relation": "uses_metric",
        "tail": "relevance"
      },
      {
        "head": "PaperScout",
        "relation": "baseline_model",
        "tail": "workflow-driven baselines"
      },
      {
        "head": "PaperScout",
        "relation": "baseline_model",
        "tail": "RL baselines"
      },
      {
        "head": "GGL-Net",
        "relation": "proposed_model",
        "tail": "Gradient-Guided Learning Network for Infrared Small Target Detection"
      },
      {
        "head": "GGL-Net",
        "relation": "uses_metric",
        "tail": "NUAA-SIRST dataset"
      },
      {
        "head": "GGL-Net",
        "relation": "uses_metric",
        "tail": "NUDT-SIRST dataset"
      },
      {
        "head": "GGL-Net",
        "relation": "evaluated_on",
        "tail": "NUAA-SIRST dataset"
      },
      {
        "head": "GGL-Net",
        "relation": "evaluated_on",
        "tail": "NUDT-SIRST dataset"
      },
      {
        "head": "GGL-Net",
        "relation": "cites",
        "tail": "gradient supplementary module (GSM)"
      },
      {
        "head": "GGL-Net",
        "relation": "cites",
        "tail": "two-way guidance fusion module (TGFM)"
      },
      {
        "head": "YuChuang1205",
        "relation": "author_of",
        "tail": "Gradient-Guided Learning Network for Infrared Small Target Detection"
      },
      {
        "head": "A Retrieval-Augmented Generation Approach to Extracting Algorithmic Logic from Neural Networks",
        "relation": "proposed_model",
        "tail": "NN-RAG"
      },
      {
        "head": "NN-RAG",
        "relation": "evaluated_on",
        "tail": "19 major repositories"
      },
      {
        "head": "NN-RAG",
        "relation": "evaluated_on",
        "tail": "LEMUR dataset"
      },
      {
        "head": "NN-RAG",
        "relation": "uses_metric",
        "tail": "structural uniqueness"
      },
      {
        "head": "CenterMamba-SAM",
        "relation": "proposed_model",
        "tail": "CenterMamba encoder"
      },
      {
        "head": "CenterMamba-SAM",
        "relation": "proposed_model",
        "tail": "memory-driven structural prompt generator"
      },
      {
        "head": "CenterMamba-SAM",
        "relation": "proposed_model",
        "tail": "memory-augmented multi-scale decoder"
      },
      {
        "head": "CenterMamba-SAM",
        "relation": "evaluated_on",
        "tail": "brain lesion segmentation"
      },
      {
        "head": "CenterMamba-SAM",
        "relation": "evaluated_on",
        "tail": "public benchmarks"
      },
      {
        "head": "IMobileTransformer",
        "relation": "proposed_model",
        "tail": "rice disease identification"
      },
      {
        "head": "RF-DETR",
        "relation": "proposed_model",
        "tail": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers"
      },
      {
        "head": "RF-DETR",
        "relation": "evaluated_on",
        "tail": "COCO"
      },
      {
        "head": "RF-DETR",
        "relation": "evaluated_on",
        "tail": "Roboflow100-VL"
      },
      {
        "head": "RF-DETR",
        "relation": "baseline_model",
        "tail": "D-FINE"
      },
      {
        "head": "RF-DETR",
        "relation": "baseline_model",
        "tail": "GroundingDINO"
      },
      {
        "head": "RF-DETR",
        "relation": "cites",
        "tail": "vision-language model (VLM)"
      },
      {
        "head": "RF-DETR",
        "relation": "cites",
        "tail": "DETRs"
      },
      {
        "head": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images",
        "relation": "evaluated_on",
        "tail": "winter wheat"
      },
      {
        "head": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images",
        "relation": "evaluated_on",
        "tail": "winter rye"
      },
      {
        "head": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images",
        "relation": "uses_metric",
        "tail": "UAV-based RGB images"
      },
      {
        "head": "Deep learning models for efficient geotechnical predictions: reducing training effort and data requirements with transfer learning",
        "relation": "proposed_model",
        "tail": "transfer learning"
      },
      {
        "head": "Image quality assessment: from error visibility to structural similarity",
        "relation": "evaluated_on",
        "tail": "error visibility"
      },
      {
        "head": "Image quality assessment: from error visibility to structural similarity",
        "relation": "evaluated_on",
        "tail": "structural similarity"
      },
      {
        "head": "FreeOrbit4D",
        "relation": "proposed_model",
        "tail": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction"
      },
      {
        "head": "FreeOrbit4D",
        "relation": "uses_metric",
        "tail": "monocular video"
      },
      {
        "head": "FreeOrbit4D",
        "relation": "evaluated_on",
        "tail": "monocular video"
      },
      {
        "head": "FreeOrbit4D",
        "relation": "baseline_model",
        "tail": "diffusion-based methods"
      },
      {
        "head": "FreeOrbit4D",
        "relation": "cites",
        "tail": "diffusion-based methods"
      },
      {
        "head": "FreeOrbit4D",
        "relation": "uses_metric",
        "tail": "geometry-complete 4D proxy"
      },
      {
        "head": "FreeOrbit4D",
        "relation": "author_of",
        "tail": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction"
      },
      {
        "head": "object-centric multi-view diffusion model",
        "relation": "proposed_model",
        "tail": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction"
      },
      {
        "head": "conditional video diffusion model",
        "relation": "proposed_model",
        "tail": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction"
      },
      {
        "head": "FreeOrbit4D",
        "relation": "cites",
        "tail": "edit propagation"
      },
      {
        "head": "FreeOrbit4D",
        "relation": "cites",
        "tail": "4D data generation"
      },
      {
        "head": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos",
        "relation": "proposed_model",
        "tail": "NeoVerse"
      },
      {
        "head": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos",
        "relation": "author_of",
        "tail": "Person"
      },
      {
        "head": "NeoVerse",
        "relation": "evaluated_on",
        "tail": "standard reconstruction and generation benchmarks"
      },
      {
        "head": "NeoVerse",
        "relation": "uses_metric",
        "tail": "state-of-the-art performance"
      },
      {
        "head": "NeoVerse",
        "relation": "baseline_model",
        "tail": "4D world modeling methods"
      },
      {
        "head": "NeoVerse",
        "relation": "cites",
        "tail": "4D world modeling methods"
      },
      {
        "head": "Scalable Diffusion Models with Transformers",
        "relation": "proposed_model",
        "tail": "Diffusion Transformers (DiTs)"
      },
      {
        "head": "Diffusion Transformers (DiTs)",
        "relation": "baseline_model",
        "tail": "U-Net"
      },
      {
        "head": "Diffusion Transformers (DiTs)",
        "relation": "evaluated_on",
        "tail": "ImageNet 512x512"
      },
      {
        "head": "Diffusion Transformers (DiTs)",
        "relation": "evaluated_on",
        "tail": "ImageNet 256x256"
      },
      {
        "head": "Diffusion Transformers (DiTs)",
        "relation": "uses_metric",
        "tail": "FID"
      },
      {
        "head": "Diffusion Transformers (DiTs)",
        "relation": "uses_metric",
        "tail": "Gflops"
      },
      {
        "head": "DiT-XL/2",
        "relation": "evaluated_on",
        "tail": "ImageNet 512x512"
      },
      {
        "head": "DiT-XL/2",
        "relation": "evaluated_on",
        "tail": "ImageNet 256x256"
      },
      {
        "head": "RoFormer: Enhanced Transformer with Rotary Position Embedding",
        "relation": "proposed_model",
        "tail": "RoPE"
      },
      {
        "head": "RoFormer: Enhanced Transformer with Rotary Position Embedding",
        "relation": "evaluated_on",
        "tail": "long text classification benchmark datasets"
      },
      {
        "head": "RoFormer: Enhanced Transformer with Rotary Position Embedding",
        "relation": "cites",
        "tail": "transformer"
      },
      {
        "head": "RoFormer",
        "relation": "uses_metric",
        "tail": "long text classification benchmark datasets"
      },
      {
        "head": "The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey",
        "relation": "author_of",
        "tail": "LMD0311"
      },
      {
        "head": "The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey",
        "relation": "cites",
        "tail": "Driving World Model (DWM)"
      },
      {
        "head": "The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey",
        "relation": "cites",
        "tail": "autonomous driving (AD)"
      },
      {
        "head": "Driving World Model (DWM)",
        "relation": "proposed_model",
        "tail": "autonomous driving (AD)"
      },
      {
        "head": "Driving World Model (DWM)",
        "relation": "evaluated_on",
        "tail": "mainstream simulators"
      },
      {
        "head": "Driving World Model (DWM)",
        "relation": "evaluated_on",
        "tail": "high-impact datasets"
      },
      {
        "head": "Driving World Model (DWM)",
        "relation": "uses_metric",
        "tail": "various metrics"
      },
      {
        "head": "Driving World Model (DWM)",
        "relation": "baseline_model",
        "tail": "representative approaches"
      },
      {
        "head": "representative approaches",
        "relation": "evaluated_on",
        "tail": "generating tasks"
      },
      {
        "head": "representative approaches",
        "relation": "evaluated_on",
        "tail": "driving tasks"
      },
      {
        "head": "current research",
        "relation": "cites",
        "tail": "future directions"
      },
      {
        "head": "Vision meets robotics: The KITTI dataset",
        "relation": "evaluated_on",
        "tail": "KITTI dataset"
      },
      {
        "head": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations",
        "relation": "proposed_model",
        "tail": "FlexMap"
      },
      {
        "head": "FlexMap",
        "relation": "uses_metric",
        "tail": "HD maps"
      },
      {
        "head": "FlexMap",
        "relation": "evaluated_on",
        "tail": "autonomous driving systems"
      },
      {
        "head": "FlexMap",
        "relation": "baseline_model",
        "tail": "geometry-aware foundation model"
      },
      {
        "head": "FlexMap",
        "relation": "author_of",
        "tail": "spatial-temporal enhancement module"
      },
      {
        "head": "FlexMap",
        "relation": "author_of",
        "tail": "camera-aware decoder"
      },
      {
        "head": "We",
        "relation": "author_of",
        "tail": "Universal Feed-Forward Metric 3D Reconstruction"
      },
      {
        "head": "Universal Feed-Forward Metric 3D Reconstruction",
        "relation": "proposed_model",
        "tail": "MapAnything"
      },
      {
        "head": "MapAnything",
        "relation": "uses_metric",
        "tail": "metric 3D scene geometry"
      },
      {
        "head": "MapAnything",
        "relation": "evaluated_on",
        "tail": "diverse datasets"
      },
      {
        "head": "Seedream 4.0",
        "relation": "proposed_model",
        "tail": "diffusion transformer"
      },
      {
        "head": "Seedream 4.0",
        "relation": "proposed_model",
        "tail": "VAE"
      },
      {
        "head": "Seedream 4.0",
        "relation": "uses_metric",
        "tail": "inference time"
      },
      {
        "head": "Seedream 4.0",
        "relation": "evaluated_on",
        "tail": "text-to-image synthesis"
      },
      {
        "head": "Seedream 4.0",
        "relation": "evaluated_on",
        "tail": "image editing"
      },
      {
        "head": "Seedream 4.0",
        "relation": "baseline_model",
        "tail": "traditional T2I systems"
      },
      {
        "head": "Seedream 4.0",
        "relation": "cites",
        "tail": "Seedream 4.5"
      },
      {
        "head": "We",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Seedream 4.0",
        "relation": "uses_metric",
        "tail": "state-of-the-art results"
      },
      {
        "head": "Seedream 4.0",
        "relation": "proposed_model",
        "tail": "VLM model"
      },
      {
        "head": "Seedream 4.0",
        "relation": "proposed_model",
        "tail": "adversarial distillation"
      },
      {
        "head": "Seedream 4.0",
        "relation": "proposed_model",
        "tail": "distribution matching"
      },
      {
        "head": "Seedream 4.0",
        "relation": "proposed_model",
        "tail": "quantization"
      },
      {
        "head": "Seedream 4.0",
        "relation": "proposed_model",
        "tail": "speculative decoding"
      },
      {
        "head": "Seedream 4.0",
        "relation": "evaluated_on",
        "tail": "multi-image composition"
      },
      {
        "head": "Seedream 4.0",
        "relation": "cites",
        "tail": "Volcano Engine"
      },
      {
        "head": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation",
        "relation": "proposed_model",
        "tail": "Echo-4o"
      },
      {
        "head": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation",
        "relation": "proposed_model",
        "tail": "Echo-4o-Image"
      },
      {
        "head": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation",
        "relation": "proposed_model",
        "tail": "GenEval++"
      },
      {
        "head": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation",
        "relation": "proposed_model",
        "tail": "Imagine-Bench"
      },
      {
        "head": "Echo-4o",
        "relation": "baseline_model",
        "tail": "Bagel"
      },
      {
        "head": "Echo-4o",
        "relation": "evaluated_on",
        "tail": "GenEval++"
      },
      {
        "head": "Echo-4o",
        "relation": "evaluated_on",
        "tail": "Imagine-Bench"
      },
      {
        "head": "Echo-4o-Image",
        "relation": "evaluated_on",
        "tail": "GenEval++"
      },
      {
        "head": "Echo-4o-Image",
        "relation": "evaluated_on",
        "tail": "Imagine-Bench"
      },
      {
        "head": "Echo-4o-Image",
        "relation": "uses_metric",
        "tail": "GenEval++"
      },
      {
        "head": "Echo-4o-Image",
        "relation": "uses_metric",
        "tail": "Imagine-Bench"
      },
      {
        "head": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation",
        "relation": "cites",
        "tail": "GPT-4o"
      },
      {
        "head": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation",
        "relation": "cites",
        "tail": "Bagel"
      },
      {
        "head": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation",
        "relation": "cites",
        "tail": "OmniGen2"
      },
      {
        "head": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation",
        "relation": "cites",
        "tail": "BLIP3-o"
      },
      {
        "head": "Lumina-DiMOO",
        "relation": "proposed_model",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Lumina-DiMOO",
        "relation": "baseline_model",
        "tail": "autoregressive (AR) or hybrid AR-Diffusion paradigms"
      },
      {
        "head": "Lumina-DiMOO",
        "relation": "evaluated_on",
        "tail": "multiple benchmarks"
      },
      {
        "head": "Lumina-DiMOO",
        "relation": "uses_metric",
        "tail": "sampling efficiency"
      },
      {
        "head": "Lumina-DiMOO",
        "relation": "cites",
        "tail": "autoregressive (AR) or hybrid AR-Diffusion paradigms"
      },
      {
        "head": "Lumina-DiMOO",
        "relation": "author_of",
        "tail": "code and checkpoints"
      },
      {
        "head": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale",
        "relation": "proposed_model",
        "tail": "NextStep-1"
      },
      {
        "head": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale",
        "relation": "baseline_model",
        "tail": "autoregressive models"
      },
      {
        "head": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale",
        "relation": "baseline_model",
        "tail": "diffusion models"
      },
      {
        "head": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale",
        "relation": "baseline_model",
        "tail": "vector quantization (VQ)"
      },
      {
        "head": "NextStep-1",
        "relation": "uses_metric",
        "tail": "text-to-image generation tasks"
      },
      {
        "head": "NextStep-1",
        "relation": "evaluated_on",
        "tail": "text-to-image generation tasks"
      },
      {
        "head": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
        "relation": "proposed_model",
        "tail": "RoBERTa"
      },
      {
        "head": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
        "relation": "baseline_model",
        "tail": "BERT"
      },
      {
        "head": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
        "relation": "evaluated_on",
        "tail": "GLUE"
      },
      {
        "head": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
        "relation": "evaluated_on",
        "tail": "RACE"
      },
      {
        "head": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
        "relation": "evaluated_on",
        "tail": "SQuAD"
      },
      {
        "head": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
        "relation": "cites",
        "tail": "BERT"
      },
      {
        "head": "Devlin et al., 2019",
        "relation": "author_of",
        "tail": "BERT"
      },
      {
        "head": "DeepSeek-R1",
        "relation": "proposed_model",
        "tail": "reinforcement learning (RL)"
      },
      {
        "head": "DeepSeek-R1",
        "relation": "baseline_model",
        "tail": "large language models (LLMs)"
      },
      {
        "head": "DeepSeek-R1",
        "relation": "evaluated_on",
        "tail": "mathematics"
      },
      {
        "head": "DeepSeek-R1",
        "relation": "evaluated_on",
        "tail": "coding competitions"
      },
      {
        "head": "DeepSeek-R1",
        "relation": "evaluated_on",
        "tail": "STEM fields"
      },
      {
        "head": "DeepSeek-R1",
        "relation": "uses_metric",
        "tail": "reasoning patterns"
      },
      {
        "head": "DeepSeek-R1",
        "relation": "cites",
        "tail": "chain-of-thought prompting"
      },
      {
        "head": "DeepSeek-R1",
        "relation": "cites",
        "tail": "supervised learning"
      },
      {
        "head": "reinforcement learning (RL)",
        "relation": "author_of",
        "tail": "DeepSeek-R1"
      },
      {
        "head": "large language models (LLMs)",
        "relation": "author_of",
        "tail": "DeepSeek-R1"
      },
      {
        "head": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning",
        "relation": "proposed_model",
        "tail": "Search-R1"
      },
      {
        "head": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning",
        "relation": "evaluated_on",
        "tail": "seven question-answering datasets"
      },
      {
        "head": "Search-R1",
        "relation": "baseline_model",
        "tail": "Qwen2.5-7B"
      },
      {
        "head": "Search-R1",
        "relation": "baseline_model",
        "tail": "Qwen2.5-3B"
      },
      {
        "head": "PeterGriffinJin",
        "relation": "author_of",
        "tail": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning"
      },
      {
        "head": "Med-PaLM 2",
        "relation": "proposed_model",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Med-PaLM 2",
        "relation": "baseline_model",
        "tail": "Med-PaLM"
      },
      {
        "head": "Med-PaLM 2",
        "relation": "evaluated_on",
        "tail": "MedQA"
      },
      {
        "head": "Med-PaLM 2",
        "relation": "evaluated_on",
        "tail": "MedMCQA"
      },
      {
        "head": "Med-PaLM 2",
        "relation": "evaluated_on",
        "tail": "PubMedQA"
      },
      {
        "head": "Med-PaLM 2",
        "relation": "evaluated_on",
        "tail": "MMLU clinical topics"
      },
      {
        "head": "Toward expert-level medical question answering with large language models",
        "relation": "cites",
        "tail": "Med-PaLM"
      },
      {
        "head": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training",
        "relation": "author_of",
        "tail": "Person"
      },
      {
        "head": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training",
        "relation": "proposed_model",
        "tail": "Supervised fine-tuning (SFT)"
      },
      {
        "head": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training",
        "relation": "proposed_model",
        "tail": "reinforcement learning (RL)"
      },
      {
        "head": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training",
        "relation": "evaluated_on",
        "tail": "GeneralPoints"
      },
      {
        "head": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training",
        "relation": "evaluated_on",
        "tail": "V-IRL"
      },
      {
        "head": "Supervised fine-tuning (SFT)",
        "relation": "uses_metric",
        "tail": "GeneralPoints"
      },
      {
        "head": "reinforcement learning (RL)",
        "relation": "uses_metric",
        "tail": "GeneralPoints"
      },
      {
        "head": "Supervised fine-tuning (SFT)",
        "relation": "uses_metric",
        "tail": "V-IRL"
      },
      {
        "head": "reinforcement learning (RL)",
        "relation": "uses_metric",
        "tail": "V-IRL"
      },
      {
        "head": "reinforcement learning (RL)",
        "relation": "baseline_model",
        "tail": "outcome-based reward"
      },
      {
        "head": "EquiCSP",
        "relation": "proposed_model",
        "tail": "Equivariant Diffusion for Crystal Structure Prediction"
      },
      {
        "head": "EquiCSP",
        "relation": "evaluated_on",
        "tail": "Crystal Structure Prediction"
      },
      {
        "head": "Equivariant Diffusion for Crystal Structure Prediction",
        "relation": "cites",
        "tail": "symmetry-aware deep learning models"
      },
      {
        "head": "Equivariant Diffusion for Crystal Structure Prediction",
        "relation": "cites",
        "tail": "diffusion models"
      },
      {
        "head": "EquiCSP",
        "relation": "baseline_model",
        "tail": "existing models"
      },
      {
        "head": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling",
        "relation": "proposed_model",
        "tail": "WorldPlay"
      },
      {
        "head": "WorldPlay",
        "relation": "uses_metric",
        "tail": "24 FPS"
      },
      {
        "head": "WorldPlay",
        "relation": "uses_metric",
        "tail": "720p video"
      },
      {
        "head": "WorldPlay",
        "relation": "proposed_model",
        "tail": "Dual Action Representation"
      },
      {
        "head": "WorldPlay",
        "relation": "proposed_model",
        "tail": "Reconstituted Context Memory"
      },
      {
        "head": "WorldPlay",
        "relation": "proposed_model",
        "tail": "Context Forcing"
      },
      {
        "head": "WorldPlay",
        "relation": "evaluated_on",
        "tail": "existing techniques"
      },
      {
        "head": "Context Forcing",
        "relation": "proposed_model",
        "tail": "memory-aware model"
      },
      {
        "head": "Context Forcing",
        "relation": "cites",
        "tail": "teacher"
      },
      {
        "head": "Context Forcing",
        "relation": "cites",
        "tail": "student"
      },
      {
        "head": "Native and Compact Structured Latents for 3D Generation",
        "relation": "proposed_model",
        "tail": "O-Voxel"
      },
      {
        "head": "Native and Compact Structured Latents for 3D Generation",
        "relation": "proposed_model",
        "tail": "Sparse Compression VAE"
      },
      {
        "head": "Native and Compact Structured Latents for 3D Generation",
        "relation": "proposed_model",
        "tail": "flow-matching models"
      },
      {
        "head": "Native and Compact Structured Latents for 3D Generation",
        "relation": "evaluated_on",
        "tail": "public 3D asset datasets"
      },
      {
        "head": "Sparse Compression VAE",
        "relation": "uses_metric",
        "tail": "high spatial compression rate"
      },
      {
        "head": "Sparse Compression VAE",
        "relation": "uses_metric",
        "tail": "compact latent space"
      },
      {
        "head": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?",
        "relation": "proposed_model",
        "tail": "JEPA-WMs"
      },
      {
        "head": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?",
        "relation": "evaluated_on",
        "tail": "simulated environments"
      },
      {
        "head": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?",
        "relation": "evaluated_on",
        "tail": "real-world robotic data"
      },
      {
        "head": "JEPA-WMs",
        "relation": "baseline_model",
        "tail": "DINO-WM"
      },
      {
        "head": "JEPA-WMs",
        "relation": "baseline_model",
        "tail": "V-JEPA-2-AC"
      },
      {
        "head": "Progressive Learned Image Transmission for Semantic Communication Using Hierarchical VAE",
        "relation": "proposed_model",
        "tail": "PLIT"
      },
      {
        "head": "PLIT",
        "relation": "uses_metric",
        "tail": "rate-distortion performance"
      },
      {
        "head": "PLIT",
        "relation": "evaluated_on",
        "tail": "channel noise"
      },
      {
        "head": "We",
        "relation": "author_of",
        "tail": "Progressive Learned Image Transmission for Semantic Communication Using Hierarchical VAE"
      },
      {
        "head": "PLIT",
        "relation": "cites",
        "tail": "previous works"
      },
      {
        "head": "STORM: Search-Guided Generative World Models for Robotic Manipulation",
        "relation": "proposed_model",
        "tail": "STORM"
      },
      {
        "head": "STORM: Search-Guided Generative World Models for Robotic Manipulation",
        "relation": "proposed_model",
        "tail": "Search-Guided Generative World Models"
      },
      {
        "head": "STORM",
        "relation": "evaluated_on",
        "tail": "SimplerEnv"
      },
      {
        "head": "STORM",
        "relation": "uses_metric",
        "tail": "Frechet Video Distance"
      },
      {
        "head": "STORM",
        "relation": "baseline_model",
        "tail": "CogACT"
      },
      {
        "head": "STORM",
        "relation": "cites",
        "tail": "Vision-Language-Action (VLA) models"
      },
      {
        "head": "STORM",
        "relation": "cites",
        "tail": "Monte Carlo Tree Search (MCTS)"
      },
      {
        "head": "Evolutionary Optimization of Model Merging Recipes",
        "relation": "proposed_model",
        "tail": "evolutionary approach"
      },
      {
        "head": "evolutionary approach",
        "relation": "uses_metric",
        "tail": "Japanese LLM benchmarks"
      },
      {
        "head": "evolutionary approach",
        "relation": "evaluated_on",
        "tail": "Japanese LLM benchmarks"
      },
      {
        "head": "evolutionary approach",
        "relation": "proposed_model",
        "tail": "Japanese LLM with Math reasoning capabilities"
      },
      {
        "head": "evolutionary approach",
        "relation": "proposed_model",
        "tail": "culturally-aware Japanese VLM"
      },
      {
        "head": "Japanese Math LLM",
        "relation": "evaluated_on",
        "tail": "Japanese LLM benchmarks"
      },
      {
        "head": "culturally-aware Japanese VLM",
        "relation": "evaluated_on",
        "tail": "Japanese culture-specific content"
      },
      {
        "head": "evolutionary approach",
        "relation": "baseline_model",
        "tail": "Japanese VLMs"
      },
      {
        "head": "evolutionary approach",
        "relation": "cites",
        "tail": "model merging"
      },
      {
        "head": "evolutionary approach",
        "relation": "cites",
        "tail": "open-source models"
      },
      {
        "head": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
        "relation": "proposed_model",
        "tail": "RNN Encoder-Decoder"
      },
      {
        "head": "RNN Encoder-Decoder",
        "relation": "evaluated_on",
        "tail": "statistical machine translation system"
      },
      {
        "head": "statistical machine translation system",
        "relation": "uses_metric",
        "tail": "log-linear model"
      },
      {
        "head": "DriveMLM",
        "relation": "proposed_model",
        "tail": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving"
      },
      {
        "head": "DriveMLM",
        "relation": "baseline_model",
        "tail": "Autopilot"
      },
      {
        "head": "DriveMLM",
        "relation": "baseline_model",
        "tail": "Apollo"
      },
      {
        "head": "DriveMLM",
        "relation": "evaluated_on",
        "tail": "CARLA Town05 Long"
      },
      {
        "head": "DriveMLM",
        "relation": "uses_metric",
        "tail": "CARLA Town05 Long"
      },
      {
        "head": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving",
        "relation": "cites",
        "tail": "Large language models (LLMs)"
      },
      {
        "head": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving",
        "relation": "cites",
        "tail": "Autonomous driving (AD)"
      },
      {
        "head": "DriveMLM",
        "relation": "author_of",
        "tail": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving"
      },
      {
        "head": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models",
        "relation": "uses_metric",
        "tail": "multi-modal benchmarks"
      },
      {
        "head": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models",
        "relation": "evaluated_on",
        "tail": "large multi-modality models"
      },
      {
        "head": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models",
        "relation": "proposed_model",
        "tail": "OpenVLM Leaderboard"
      },
      {
        "head": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models",
        "relation": "baseline_model",
        "tail": "PyTorch"
      },
      {
        "head": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
        "relation": "proposed_model",
        "tail": "chain of thought prompting"
      },
      {
        "head": "chain of thought prompting",
        "relation": "evaluated_on",
        "tail": "GSM8K benchmark"
      },
      {
        "head": "chain of thought prompting",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "chain of thought prompting",
        "relation": "baseline_model",
        "tail": "GPT-3"
      },
      {
        "head": "Rich feature hierarchies for accurate object detection and semantic segmentation",
        "relation": "proposed_model",
        "tail": "R-CNN: Regions with CNN features"
      },
      {
        "head": "Rich feature hierarchies for accurate object detection and semantic segmentation",
        "relation": "evaluated_on",
        "tail": "PASCAL VOC dataset"
      },
      {
        "head": "Rich feature hierarchies for accurate object detection and semantic segmentation",
        "relation": "evaluated_on",
        "tail": "ILSVRC2013 detection dataset"
      },
      {
        "head": "Rich feature hierarchies for accurate object detection and semantic segmentation",
        "relation": "uses_metric",
        "tail": "mean average precision (mAP)"
      },
      {
        "head": "Rich feature hierarchies for accurate object detection and semantic segmentation",
        "relation": "cites",
        "tail": "OverFeat"
      },
      {
        "head": "Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism",
        "relation": "proposed_model",
        "tail": "Multi-modal Chain and Global Attention Network (MCGA-Net)"
      },
      {
        "head": "Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism",
        "relation": "proposed_model",
        "tail": "DCGAN-based data augmentation strategy"
      },
      {
        "head": "Multi-modal Chain and Global Attention Network (MCGA-Net)",
        "relation": "uses_metric",
        "tail": "Precision"
      },
      {
        "head": "Multi-modal Chain and Global Attention Network (MCGA-Net)",
        "relation": "uses_metric",
        "tail": "Recall"
      },
      {
        "head": "Multi-modal Chain and Global Attention Network (MCGA-Net)",
        "relation": "uses_metric",
        "tail": "mAP@50"
      },
      {
        "head": "Multi-modal Chain and Global Attention Network (MCGA-Net)",
        "relation": "evaluated_on",
        "tail": "GPR road hidden defect images"
      },
      {
        "head": "Multi-modal Chain and Global Attention Network (MCGA-Net)",
        "relation": "baseline_model",
        "tail": "other models"
      },
      {
        "head": "Multi-modal Chain and Global Attention Network (MCGA-Net)",
        "relation": "cites",
        "tail": "Ground Penetrating Radar (GPR)"
      },
      {
        "head": "Multi-modal Chain and Global Attention Network (MCGA-Net)",
        "relation": "cites",
        "tail": "MS COCO"
      },
      {
        "head": "Federated Learning Architectures: A Performance Evaluation with Crop Yield Prediction Application",
        "relation": "proposed_model",
        "tail": "centralized federated learning framework"
      },
      {
        "head": "Federated Learning Architectures: A Performance Evaluation with Crop Yield Prediction Application",
        "relation": "proposed_model",
        "tail": "decentralized federated learning framework"
      },
      {
        "head": "centralized federated learning framework",
        "relation": "baseline_model",
        "tail": "cloud-only framework"
      },
      {
        "head": "centralized federated learning framework",
        "relation": "evaluated_on",
        "tail": "prediction accuracy"
      },
      {
        "head": "centralized federated learning framework",
        "relation": "evaluated_on",
        "tail": "precision"
      },
      {
        "head": "centralized federated learning framework",
        "relation": "evaluated_on",
        "tail": "recall"
      },
      {
        "head": "centralized federated learning framework",
        "relation": "evaluated_on",
        "tail": "F1-Score"
      },
      {
        "head": "centralized federated learning framework",
        "relation": "evaluated_on",
        "tail": "training time"
      },
      {
        "head": "decentralized federated learning framework",
        "relation": "evaluated_on",
        "tail": "prediction accuracy"
      },
      {
        "head": "decentralized federated learning framework",
        "relation": "evaluated_on",
        "tail": "precision"
      },
      {
        "head": "decentralized federated learning framework",
        "relation": "evaluated_on",
        "tail": "recall"
      },
      {
        "head": "decentralized federated learning framework",
        "relation": "evaluated_on",
        "tail": "F1-Score"
      },
      {
        "head": "decentralized federated learning framework",
        "relation": "evaluated_on",
        "tail": "training time"
      },
      {
        "head": "centralized federated learning framework",
        "relation": "uses_metric",
        "tail": "prediction accuracy"
      },
      {
        "head": "centralized federated learning framework",
        "relation": "uses_metric",
        "tail": "precision"
      },
      {
        "head": "centralized federated learning framework",
        "relation": "uses_metric",
        "tail": "recall"
      },
      {
        "head": "centralized federated learning framework",
        "relation": "uses_metric",
        "tail": "F1-Score"
      },
      {
        "head": "centralized federated learning framework",
        "relation": "uses_metric",
        "tail": "training time"
      },
      {
        "head": "decentralized federated learning framework",
        "relation": "uses_metric",
        "tail": "prediction accuracy"
      },
      {
        "head": "decentralized federated learning framework",
        "relation": "uses_metric",
        "tail": "precision"
      },
      {
        "head": "decentralized federated learning framework",
        "relation": "uses_metric",
        "tail": "recall"
      },
      {
        "head": "decentralized federated learning framework",
        "relation": "uses_metric",
        "tail": "F1-Score"
      },
      {
        "head": "decentralized federated learning framework",
        "relation": "uses_metric",
        "tail": "training time"
      },
      {
        "head": "DepictQA-Wild",
        "relation": "proposed_model",
        "tail": "Enhancing Descriptive Image Quality Assessment with A Large-scale Multi-modal Dataset"
      },
      {
        "head": "DepictQA-Wild",
        "relation": "baseline_model",
        "tail": "traditional score-based methods"
      },
      {
        "head": "DepictQA-Wild",
        "relation": "baseline_model",
        "tail": "prior VLM-based IQA models"
      },
      {
        "head": "DepictQA-Wild",
        "relation": "baseline_model",
        "tail": "GPT-4V"
      },
      {
        "head": "DepictQA-Wild",
        "relation": "evaluated_on",
        "tail": "distortion identification"
      },
      {
        "head": "DepictQA-Wild",
        "relation": "evaluated_on",
        "tail": "instant rating"
      },
      {
        "head": "DepictQA-Wild",
        "relation": "evaluated_on",
        "tail": "reasoning tasks"
      },
      {
        "head": "DepictQA-Wild",
        "relation": "evaluated_on",
        "tail": "web-downloaded images"
      },
      {
        "head": "DepictQA-Wild",
        "relation": "evaluated_on",
        "tail": "model-processed images"
      },
      {
        "head": "DepictQA-Wild",
        "relation": "uses_metric",
        "tail": "DQ-495K"
      },
      {
        "head": "Enhancing Descriptive Image Quality Assessment with A Large-scale Multi-modal Dataset",
        "relation": "cites",
        "tail": "Vision Language Models (VLMs)"
      },
      {
        "head": "Enhancing Descriptive Image Quality Assessment with A Large-scale Multi-modal Dataset",
        "relation": "cites",
        "tail": "VLM-based Image Quality Assessment (IQA)"
      },
      {
        "head": "Deep contextualized word representations",
        "relation": "proposed_model",
        "tail": "deep bidirectional language model (biLM)"
      },
      {
        "head": "Deep contextualized word representations",
        "relation": "evaluated_on",
        "tail": "question answering"
      },
      {
        "head": "Deep contextualized word representations",
        "relation": "evaluated_on",
        "tail": "textual entailment"
      },
      {
        "head": "Deep contextualized word representations",
        "relation": "evaluated_on",
        "tail": "sentiment analysis"
      },
      {
        "head": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
        "relation": "proposed_model",
        "tail": "reinforcement learning"
      },
      {
        "head": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
        "relation": "baseline_model",
        "tail": "supervised learning"
      },
      {
        "head": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
        "relation": "evaluated_on",
        "tail": "mathematics"
      },
      {
        "head": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
        "relation": "evaluated_on",
        "tail": "coding competitions"
      },
      {
        "head": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
        "relation": "evaluated_on",
        "tail": "STEM fields"
      },
      {
        "head": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
        "relation": "cites",
        "tail": "large language models"
      },
      {
        "head": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
        "relation": "cites",
        "tail": "chain-of-thought prompting"
      }
    ]
  }
}