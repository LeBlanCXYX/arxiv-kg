{
  "paper_metadata": {
    "id": "1706.03762",
    "title": "Attention Is All You Need",
    "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
    "published_date": "2017-06-12",
    "pdf_url": "https://arxiv.org/pdf/1706.03762v7",
    "authors": [
      "Ashish Vaswani",
      "Noam Shazeer",
      "Niki Parmar",
      "Jakob Uszkoreit",
      "Llion Jones",
      "Aidan N. Gomez",
      "Lukasz Kaiser",
      "Illia Polosukhin"
    ]
  },
  "related_papers_count": {
    "references": 5,
    "citations": 5
  },
  "related_papers": [
    {
      "title": "Deep Residual Learning for Image Recognition",
      "arxiv_id": "1512.03385",
      "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.\n  The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "published_date": "2015-12-10",
      "pdf_url": "https://arxiv.org/pdf/1512.03385v1",
      "citation_count": 219322,
      "year": 2015
    },
    {
      "title": "Adam: A Method for Stochastic Optimization",
      "arxiv_id": "1412.6980",
      "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
      "authors": [
        "Diederik P. Kingma",
        "Jimmy Ba"
      ],
      "published_date": "2014-12-22",
      "pdf_url": "https://arxiv.org/pdf/1412.6980v9",
      "citation_count": 162373,
      "year": 2014
    },
    {
      "title": "Long Short-Term Memory",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Sepp Hochreiter",
        "J. Schmidhuber"
      ],
      "published_date": "1997",
      "pdf_url": "",
      "citation_count": 100213,
      "year": 1997
    },
    {
      "title": "Dropout: a simple way to prevent neural networks from overfitting",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Nitish Srivastava",
        "Geoffrey E. Hinton",
        "A. Krizhevsky",
        "I. Sutskever",
        "R. Salakhutdinov"
      ],
      "published_date": "2014",
      "pdf_url": "",
      "citation_count": 42312,
      "year": 2014
    },
    {
      "title": "Rethinking the Inception Architecture for Computer Vision",
      "arxiv_id": "1512.00567",
      "abstract": "Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the validation set.",
      "authors": [
        "Christian Szegedy",
        "Vincent Vanhoucke",
        "Sergey Ioffe",
        "Jonathon Shlens",
        "Zbigniew Wojna"
      ],
      "published_date": "2015-12-02",
      "pdf_url": "https://arxiv.org/pdf/1512.00567v3",
      "citation_count": 30057,
      "year": 2015
    },
    {
      "title": "A comprehensive review of recommender systems: Transitioning from theory to practice",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Shaina Raza",
        "Mizanur Rahman",
        "Safiullah Kamawal",
        "Armin Toroghi",
        "Ananya Raval",
        "F. Navah",
        "Amirmohammad Kazemeini"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 7,
      "year": 2026
    },
    {
      "title": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
      "arxiv_id": "",
      "abstract": "Autonomous driving relies on robust models trained on high-quality, large-scale multi-view driving videos. While world models offer a cost-effective solution for generating realistic driving videos, they struggle to maintain instance-level temporal consistency and spatial geometric fidelity. To address these challenges, we propose InstaDrive, a novel framework that enhances driving video realism through two key advancements: (1) Instance Flow Guider, which extracts and propagates instance features across frames to enforce temporal consistency, preserving instance identity over time. (2) Spatial Geometric Aligner, which improves spatial reasoning, ensures precise instance positioning, and explicitly models occlusion hierarchies. By incorporating these instance-aware mechanisms, InstaDrive achieves state-of-the-art video generation quality and enhances downstream autonomous driving tasks on the nuScenes dataset. Additionally, we utilize CARLA's autopilot to procedurally and stochastically simulate rare but safety-critical driving scenarios across diverse maps and regions, enabling rigorous safety evaluation for autonomous systems. Our project page is https://shanpoyang654.github.io/InstaDrive/page.html.",
      "authors": [
        "Zhuoran Yang",
        "Xi Guo",
        "Chenjing Ding",
        "Chiyu Wang",
        "Wei Wu",
        "Yanyong Zhang"
      ],
      "published_date": "2026-02-03",
      "pdf_url": "https://arxiv.org/pdf/2602.03242v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Zisheng Wang",
        "Junjie Chen",
        "Chisen Wang",
        "Cong Peng",
        "Jianping Xuan",
        "Tielin Shi",
        "Ming J. Zuo"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 4,
      "year": 2026
    },
    {
      "title": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Jinghuan Zhang",
        "Wang Chen",
        "Jian Zhang"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 3,
      "year": 2026
    },
    {
      "title": "DiffusionEngine: Diffusion model is scalable data engine for object detection",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Manlin Zhang",
        "Jie Wu",
        "Yuxi Ren",
        "Jiahong Yang",
        "Ming Li",
        "Andy J. Ma"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 2,
      "year": 2026
    },
    {
      "title": "ImageNet classification with deep convolutional neural networks",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "A. Krizhevsky",
        "I. Sutskever",
        "Geoffrey E. Hinton"
      ],
      "published_date": "2012",
      "pdf_url": "",
      "citation_count": 126487,
      "year": 2012
    },
    {
      "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "arxiv_id": "1409.1556",
      "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.",
      "authors": [
        "Karen Simonyan",
        "Andrew Zisserman"
      ],
      "published_date": "2014-09-04",
      "pdf_url": "https://arxiv.org/pdf/1409.1556v6",
      "citation_count": 108906,
      "year": 2014
    },
    {
      "title": "Et al",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "P. Cochat",
        "L. Vaucoret",
        "J. Sarles"
      ],
      "published_date": "2008",
      "pdf_url": "",
      "citation_count": 74031,
      "year": 2008
    },
    {
      "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
      "arxiv_id": "1506.01497",
      "abstract": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.",
      "authors": [
        "Shaoqing Ren",
        "Kaiming He",
        "Ross Girshick",
        "Jian Sun"
      ],
      "published_date": "2015-06-04",
      "pdf_url": "https://arxiv.org/pdf/1506.01497v3",
      "citation_count": 70038,
      "year": 2015
    },
    {
      "title": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Yuqi Cheng",
        "Yunkang Cao",
        "Haiming Yao",
        "Wei Luo",
        "Cheng Jiang",
        "Hui Zhang",
        "Weiming Shen"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 6,
      "year": 2026
    },
    {
      "title": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning",
      "arxiv_id": null,
      "abstract": "Federated learning (FL) has emerged as a promising solution to enable distributed learning without sharing sensitive data. However, FL is vulnerable to data poisoning attacks, where malicious clients inject malicious data during training to compromise the global model. Existing FL defenses suffer from the assumptions of independent and identically distributed (IID) model updates, asymptotic optimal error rate bounds, and strong convexity in the optimization problem. Hence, we propose a novel framework called Federated Learning Optimal Transport (FLOT) that leverages the Wasserstein barycentric technique to obtain a global model from a set of locally trained non-IID models on client devices. In addition, we introduce a loss function-based rejection (LFR) mechanism to suppress malicious updates and a dynamic weighting scheme to optimize the Wasserstein barycentric aggregation function. We provide the theoretical proof of the Byzantine resilience and convergence of FLOT to highlight its efficacy. We evaluate FLOT on four benchmark datasets: GTSRB, KBTS, CIFAR10, and EMNIST. The experimental results underscore the practical significance of FLOT as an effective defense mechanism against data poisoning attacks in FL while maintaining high accuracy and scalability. Also, we observe that FLOT serves as a robust client selection technique under no attack, which demonstrates its effectiveness.",
      "authors": [
        "Naveen Kumar Srinivasa",
        "Ajeet Rao Chalamala",
        "Kumar Singh",
        "Ieee Krishna Mohan Senior Member",
        "K. Naveen",
        "Srinivasa Rao",
        "Ajeet Kumar Singh"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 5,
      "year": 2026
    },
    {
      "title": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID",
      "arxiv_id": null,
      "abstract": "Person re-identification (Re-ID) can recognize users based on their clothing, body shape, and other information without the need for clear facial images, and is widely applied in the field of intelligent security. Traditional Re-ID systems mainly rely on high-definition RGB cameras, but the deployment of large-scale high-definition RGB cameras indoors has caused serious privacy and ethical concerns. Recently, wireless-based Re-ID systems (Wi-Fi, RFID, millimeter-wave radar, etc.) have shown promising prospects, but the limited sensing resolution hinders their practical deployment. In this paper, we propose WarmGait, a Re-ID system based on thermal array sensors, which can achieve high-precision Re-ID at low cost and minimize the invasion of user privacy. However, using thermal arrays for Re-ID still faces two major challenges. The first is the low and unclear texture resolution of images caused by low-cost infrared devices. The second is that existing gait recognition methods require maintaining the sequential constraint of gait images, which reduces the flexibility of gait recognition or Re-ID. To address these two challenges, we first designed an edge module inspired by Taylor Finite Difference (TFD) to aggregate image edge information to help improve the resolution of infrared devices. Then, we considered gait as a collection of gait profiles and extracted features from the frame level and collection level for recognition, breaking through the limitations of the number and order of input images. After extensive experimental evaluation, our model can achieve an average recognition accuracy of 87.3% in various scenarios, demonstrating the potential of WarmGait in Re-ID.",
      "authors": [
        "Hongbo Jiang",
        "Lei Ye",
        "Jingyang Hu",
        "Xiaotian Chen",
        "Siyu Chen",
        "Wei Zhang",
        "Kehua Yang"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 2,
      "year": 2026
    },
    {
      "title": "Noise Perception Self-Supervised Learning for Unsupervised Person Re-Identification",
      "arxiv_id": null,
      "abstract": "Most existing unsupervised person re-identification (ReID) methods utilize a transfer learning paradigm that requires independent source annotations. Although recent Unsupervised Domain Adaptation techniques can achieve promising results, they still suffer from the issues of source domain variance and privacy due to the access of raw source data. In this paper, we propose a novel Noise Perception Self-Supervised Learning (NPSSL) paradigm based on the idea that visual tracking would provide useful spatio-temporal localized constraints for improving ReID model learning. Apart from using visual similarity, we fully exploit spatial-temporal motion consistency to assist the person tracklet formulation, complement with visual cues to enhance re-association in crowded scenes. To further alleviate the noise raised by multi-person tracking, we introduce a Noise Perception Self-Paced Learning method to learn from the most confident examples progressively. Specifically, a cluster-level filter and a sample-level filter are devised to jointly exploit the pseudo label during the training process. Extensive experiments on Duke dataset demonstrate the superiority of the NPSSL model over a wide range of unsupervised learning methods and the competitiveness of this paradigm with unsupervised transfer learning methods.",
      "authors": [
        "Jingya Wang",
        "Jianfeng Wen",
        "Weiping Ding",
        "Chunlin Yu",
        "Xiatian Zhu",
        "Zhiyong Wang"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 2,
      "year": 2026
    },
    {
      "title": "Auto-Encoding Variational Bayes",
      "arxiv_id": "1312.6114",
      "abstract": "How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.",
      "authors": [
        "Diederik P Kingma",
        "Max Welling"
      ],
      "published_date": "2013-12-20",
      "pdf_url": "https://arxiv.org/pdf/1312.6114v11",
      "citation_count": 17203,
      "year": 2013
    },
    {
      "title": "Supporting Online Material for Reducing the Dimensionality of Data with Neural Networks",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Geoffrey E. Hinton",
        "R. Salakhutdinov"
      ],
      "published_date": "2006",
      "pdf_url": "",
      "citation_count": 11470,
      "year": 2006
    },
    {
      "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "John C. Duchi",
        "Elad Hazan",
        "Y. Singer"
      ],
      "published_date": "2011",
      "pdf_url": "",
      "citation_count": 11099,
      "year": 2011
    },
    {
      "title": "Speech recognition with deep recurrent neural networks",
      "arxiv_id": "1303.5778",
      "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \\emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.",
      "authors": [
        "Alex Graves",
        "Abdel-rahman Mohamed",
        "Geoffrey Hinton"
      ],
      "published_date": "2013-03-22",
      "pdf_url": "https://arxiv.org/pdf/1303.5778v1",
      "citation_count": 8846,
      "year": 2013
    },
    {
      "title": "Weakly Supervised Image Dehazing via Physics-Based Decomposition",
      "arxiv_id": null,
      "abstract": "Recent weakly supervised image dehazing (WSID) works have succeeded to improve models’ generalization ability to real scene dehazing by using generative adversarial network (GAN) for unpaired image training. However, it is still difficult for current WSID methods to train one effective dehazing model for various scenes since 1) they always result in residual haze due to insufficient generalization to the feature distribution of real scenes, and 2) they are prone to cause distortions like color shifts, artifacts or halos etc, owing to embedding manual prior or threshold hypothesis for image reconstruction. To solve above problems, in this paper, we propose a novel WSID model via physics-based decomposition (PBD), which estimates atmospheric light, scattering coefficient and scene depth of real haze input to effectively capture the illumination information and haze distribution to recover a preliminary dehazed image by minimizing reconstruction loss. With this constraint, we subtly design a discrete wavelet discriminator (DWD) to effectively improve the generalization to real scene from both spatial and frequency aspect under the supervision of unpaired real clear image. Our PBD is a purely data-driven model freeing from any manual setting or partially correct prior, thus simultaneously ensuring the realness and visibility of dehazed images. Experiments on seven benchmarks verified the strong generalization ability of our PBD, which achieves SOTA dehazing performance with realistic details. Code will be published at https://github.com/NianWang-HJJGCDX/PBD",
      "authors": [
        "Nian Wang",
        "Zhigao Cui",
        "Yanzhao Su",
        "Yunwei Lan",
        "Yuanliang Xue",
        "Cong Zhang",
        "Aihua Li"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 10,
      "year": 2026
    },
    {
      "title": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer",
      "arxiv_id": null,
      "abstract": "As Adam optimizer’s learning rate decay hyperparameter has recently been deprecated, this journal article focuses not only on providing an alternate optimizer but also on comparing the performance of the said optimizer, AdamW, with the Adam optimizer using a face mask detection model. This study experiments with different weight decay values and finds that a weight decay of 0.00009 with the AdamW optimizer consistently achieves a 98% accuracy rate. Aside from that, this study also discusses the differences between Adam with L2-regularization and AdamW on how the weight decay is decoupled from the Adam optimizer’s gradient-based update that impacts the performance of AdamW. Overall, the study provides insights to those new to AdamW and looking for a starting point in optimizing deep learning models.",
      "authors": [
        "Leong Kah Meng",
        "Ho Hooi Yi",
        "Ng Bo Wei",
        "Lim Jia Xin",
        "Zailan Arabee Abdul Salam"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 8,
      "year": 2026
    },
    {
      "title": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
      "arxiv_id": "",
      "abstract": "While Mixture-of-Experts (MoE) scales capacity via conditional computation, Transformers lack a native primitive for knowledge lookup, forcing them to inefficiently simulate retrieval through computation. To address this, we introduce conditional memory as a complementary sparsity axis, instantiated via Engram, a module that modernizes classic $N$-gram embedding for O(1) lookup. By formulating the Sparsity Allocation problem, we uncover a U-shaped scaling law that optimizes the trade-off between neural computation (MoE) and static memory (Engram). Guided by this law, we scale Engram to 27B parameters, achieving superior performance over a strictly iso-parameter and iso-FLOPs MoE baseline. Most notably, while the memory module is expected to aid knowledge retrieval (e.g., MMLU +3.4; CMMLU +4.0), we observe even larger gains in general reasoning (e.g., BBH +5.0; ARC-Challenge +3.7) and code/math domains~(HumanEval +3.0; MATH +2.4). Mechanistic analyses reveal that Engram relieves the backbone's early layers from static reconstruction, effectively deepening the network for complex reasoning. Furthermore, by delegating local dependencies to lookups, it frees up attention capacity for global context, substantially boosting long-context retrieval (e.g., Multi-Query NIAH: 84.2 to 97.0). Finally, Engram establishes infrastructure-aware efficiency: its deterministic addressing enables runtime prefetching from host memory, incurring negligible overhead. We envision conditional memory as an indispensable modeling primitive for next-generation sparse models.",
      "authors": [
        "Xin Cheng",
        "Wangding Zeng",
        "Damai Dai",
        "Qinyu Chen",
        "Bingxuan Wang",
        "Zhenda Xie",
        "Kezhao Huang",
        "Xingkai Yu",
        "Zhewen Hao",
        "Yukun Li",
        "Han Zhang",
        "Huishuai Zhang",
        "Dongyan Zhao",
        "Wenfeng Liang"
      ],
      "published_date": "2026-01-12",
      "pdf_url": "https://arxiv.org/pdf/2601.07372v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction",
      "arxiv_id": null,
      "abstract": "Abstract. The past few years have witnessed the rise of neural networks (NNs) applications for hydrological time series modeling. By virtue of their capabilities, NN models can achieve unprecedented levels of performance when learning how to solve increasingly complex rainfall-runoff processes via data, making them pivotal for the development of computational hydrologic tasks such as flood predictions. The NN models should, to be considered practical, provide a probabilistic understanding of the model mechanisms and predictions and hints on what could perturb the model. In this paper, we developed two NN models, i.e., Neural Hierarchical Interpolation for Time Series Forecasting (N-HiTS) and Network-Based Expansion Analysis for Interpretable Time Series Forecasting (N-BEATS) with a probabilistic multi-quantile objective and benchmarked them with long short-term memory (LSTM) for flood prediction across two headwater streams in Georgia and North Carolina, USA. To generate a probabilistic prediction, a Multi-Quantile Loss was used to assess the 95th percentile prediction uncertainty (95 PPU) of multiple flooding events. Extensive experiments demonstrated the advantages of hierarchical interpolation and interpretable architecture, where both N-HiTS and N-BEATS provided an average accuracy improvement of ∼ 5 % over the LSTM benchmarking model. On a variety of flooding events, both N-HiTS and N-BEATS demonstrated significant performance improvements over the LSTM benchmark and showcased their probabilistic predictions by specifying a likelihood objective.",
      "authors": [
        "Mostafa Saberian",
        "Vidya Samadi",
        "Ioana Popescu"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 5,
      "year": 2026
    },
    {
      "title": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Husheng Fang",
        "Shunlin Liang",
        "Wenyuan Li",
        "Yongzhe Chen",
        "Han Ma",
        "Jianglei Xu",
        "Yichuan Ma",
        "Tao He",
        "Feng Tian",
        "Fengjiao Zhang",
        "Hui Liang"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 3,
      "year": 2026
    },
    {
      "title": "Going deeper with convolutions",
      "arxiv_id": "1409.4842",
      "abstract": "We propose a deep convolutional neural network architecture codenamed \"Inception\", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.",
      "authors": [
        "Christian Szegedy",
        "Wei Liu",
        "Yangqing Jia",
        "Pierre Sermanet",
        "Scott Reed",
        "Dragomir Anguelov",
        "Dumitru Erhan",
        "Vincent Vanhoucke",
        "Andrew Rabinovich"
      ],
      "published_date": "2014-09-17",
      "pdf_url": "https://arxiv.org/pdf/1409.4842v1",
      "citation_count": 46399,
      "year": 2014
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "arxiv_id": "1502.03167",
      "abstract": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.",
      "authors": [
        "Sergey Ioffe",
        "Christian Szegedy"
      ],
      "published_date": "2015-02-11",
      "pdf_url": "https://arxiv.org/pdf/1502.03167v3",
      "citation_count": 45908,
      "year": 2015
    },
    {
      "title": "ImageNet Large Scale Visual Recognition Challenge",
      "arxiv_id": "1409.0575",
      "abstract": "The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions.\n  This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.",
      "authors": [
        "Olga Russakovsky",
        "Jia Deng",
        "Hao Su",
        "Jonathan Krause",
        "Sanjeev Satheesh",
        "Sean Ma",
        "Zhiheng Huang",
        "Andrej Karpathy",
        "Aditya Khosla",
        "Michael Bernstein",
        "Alexander C. Berg",
        "Li Fei-Fei"
      ],
      "published_date": "2014-09-01",
      "pdf_url": "https://arxiv.org/pdf/1409.0575v3",
      "citation_count": 41735,
      "year": 2014
    },
    {
      "title": "Diffusion Transformers with Representation Autoencoders",
      "arxiv_id": "",
      "abstract": "Latent generative modeling, where a pretrained autoencoder maps pixels into a latent space for the diffusion process, has become the standard strategy for Diffusion Transformers (DiT); however, the autoencoder component has barely evolved. Most DiTs continue to rely on the original VAE encoder, which introduces several limitations: outdated backbones that compromise architectural simplicity, low-dimensional latent spaces that restrict information capacity, and weak representations that result from purely reconstruction-based training and ultimately limit generative quality. In this work, we explore replacing the VAE with pretrained representation encoders (e.g., DINO, SigLIP, MAE) paired with trained decoders, forming what we term Representation Autoencoders (RAEs). These models provide both high-quality reconstructions and semantically rich latent spaces, while allowing for a scalable transformer-based architecture. Since these latent spaces are typically high-dimensional, a key challenge is enabling diffusion transformers to operate effectively within them. We analyze the sources of this difficulty, propose theoretically motivated solutions, and validate them empirically. Our approach achieves faster convergence without auxiliary representation alignment losses. Using a DiT variant equipped with a lightweight, wide DDT head, we achieve strong image generation results on ImageNet: 1.51 FID at 256x256 (no guidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers clear advantages and should be the new default for diffusion transformer training.",
      "authors": [
        "Boyang Zheng",
        "Nanye Ma",
        "Shengbang Tong",
        "Saining Xie"
      ],
      "published_date": "2025-10-13",
      "pdf_url": "https://arxiv.org/pdf/2510.11690v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Scaling Large Language Models for Next-Generation Single-Cell Analysis",
      "arxiv_id": null,
      "abstract": "Single-cell RNA sequencing has transformed our understanding of cellular diversity, yet current single-cell foundation models (scFMs) remain limited in their scalability, flexibility across diverse tasks, and ability to natively integrate textual information. In this work, we build upon the Cell2Sentence (C2S) framework, which represents scRNA-seq profiles as textual “cell sentences,” to train Large Language Models (LLMs) on a corpus comprising over one billion tokens of transcriptomic data, biological text, and metadata. Scaling the model to 27 billion parameters yields consistent improvements in predictive and generative capabilities and supports advanced downstream tasks that require synthesis of information across multi-cellular contexts. Targeted fine-tuning with modern reinforcement learning techniques produces strong performance in perturbation response prediction, natural language interpretation, and complex biological reasoning. This predictive strength enabled a dual-context virtual screen that nominated the kinase inhibitor silmitasertib (CX-4945) as a candidate for context-selective upregulation of antigen presentation. Experimental assessment in human cell models unseen during training supported this prediction, demonstrating that C2S-Scale can effectively guide the discovery of context-conditioned biology. C2S-Scale unifies transcriptomic and textual data at unprecedented scales, surpassing both specialized single-cell models and general-purpose LLMs to provide a platform for next-generation single-cell analysis and the development of “virtual cells.”",
      "authors": [
        "S. Rizvi",
        "Daniel Levine",
        "Aakash Patel",
        "Shiyang Zhang",
        "Eric Wang",
        "Curtis Jamison Perry",
        "Ivan Vrkic",
        "Nicole Mayerli Constante",
        "Zirui Fu",
        "Sizhuang He",
        "David Zhang",
        "Cerise Tang",
        "Zhuoyang Lyu",
        "Rayyan Y Darji",
        "Chang Li",
        "Emily Sun",
        "David Jeong",
        "Lawrence Zhao",
        "Jennifer Kwan",
        "David Braun",
        "Brian Hafler",
        "Hattie Chung",
        "R. M. Dhodapkar",
        "Paul F. Jaeger",
        "Bryan Perozzi",
        "Jeffrey Ishizuka",
        "Shekoofeh Azizi",
        "D. van Dijk"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 17,
      "year": 2025
    },
    {
      "title": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights",
      "arxiv_id": null,
      "abstract": "Transferable adversarial images raise critical security concerns for computer vision systems in real-world, black-box attack scenarios. Although many transfer attacks have been proposed, existing research lacks a systematic and comprehensive evaluation. In this paper, we systemize transfer attacks into five categories around the general machine learning pipeline and provide the first comprehensive evaluation, with 23 representative attacks against 11 representative defenses, including the recent, transfer-oriented defense and the real-world Google Cloud Vision. In particular, we identify two main problems of existing evaluations: (1) for attack transferability, lack of intra-category analyses with fair hyperparameter settings, and (2) for attack stealthiness, lack of diverse measures. Our evaluation results validate that these problems have indeed caused misleading conclusions and missing points, and addressing them leads to new, <italic>consensus-challenging</italic> insights, such as (1) an early attack, DI, even outperforms all similar follow-up ones, (2) the state-of-the-art (white-box) defense, DiffPure, is even vulnerable to (black-box) transfer attacks, and (3) even under the same <inline-formula><tex-math notation=\"LaTeX\">$L_{p}$</tex-math><alternatives><mml:math><mml:msub><mml:mi>L</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math><inline-graphic xlink:href=\"zhao-ieq1-3610085.gif\"/></alternatives></inline-formula> constraint, different attacks yield dramatically different stealthiness results regarding diverse imperceptibility metrics, finer-grained measures, and a user study. We hope that our analyses will serve as guidance on properly evaluating transferable adversarial images and advance the design of attacks and defenses.",
      "authors": [
        "Zhengyu Zhao",
        "Hanwei Zhang",
        "Renjue Li",
        "R. Sicre",
        "L. Amsaleg",
        "Michael Backes",
        "Qi Li",
        "Chao Shen"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 13,
      "year": 2025
    },
    {
      "title": "HybridVisionNet: An advanced hybrid deep learning framework for automated multi-class ocular disease diagnosis using fundus imaging",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Şafak Kılıç"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 12,
      "year": 2025
    },
    {
      "title": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot",
      "arxiv_id": null,
      "abstract": "To solve the labor shortage, robots have dramatically changed the world by combining powerful deep learning (DL) technology. Certainly, DL technology has become the key point of the widespread robot application. Efficient DL models depend on high-quality datasets and optimized architectures. However, some open datasets contain anomalous data that degrade model performance. Moreover, complex structures and high computational costs limit the adoption of DL models. This study proposes a dataset purification-based lightweight DL model construction strategy to solve these challenges. Initially, a dataset purification method is developed to filter out the anomaly data in a newly created dataset, which utilizes a lightweight cross-scale DL model OGNet to detect the anomaly data to achieve dataset purification. Subsequently, a highly efficient lightweight OGNet-based object detection (OD) model family, YOLO-OG, is presented to train the purified dataset. To evaluate the proposal, the strategy is implemented on the Empty-dish Recycling Robot. Experiments show that OGNet achieves excellent accuracy with only 0.68 M parameters and 0.35 GFLOPs. On purification Dish-10 dataset, the mean Average Precision (mAP) of YOLO-OG increases a maximum of 4.28% than original Dish-10 dataset. Meanwhile, YOLO-OG outperforms other advanced OD models, achieving the best accuracy of 99.20% mAP and the smallest 1.60 M parameters. YOLO-OG also reaches 99.86% mAP on the Dish-20 open dataset. On three other open datasets, YOLO-OG also shows excellent performance and surpasses most of the other OD models, which confirms the strong generalization ability of YOLO-OG.",
      "authors": [
        "Yifei Ge",
        "Zhuo Li",
        "Xuebin Yue",
        "Hengyi Li",
        "Lin Meng"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 10,
      "year": 2025
    },
    {
      "title": "Attention is All you Need",
      "arxiv_id": "1706.03762",
      "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan N. Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "published_date": "2017-06-12",
      "pdf_url": "https://arxiv.org/pdf/1706.03762v7",
      "citation_count": 164494,
      "year": 2017
    },
    {
      "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
      "arxiv_id": "1910.10683",
      "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",
      "authors": [
        "Colin Raffel",
        "Noam Shazeer",
        "Adam Roberts",
        "Katherine Lee",
        "Sharan Narang",
        "Michael Matena",
        "Yanqi Zhou",
        "Wei Li",
        "Peter J. Liu"
      ],
      "published_date": "2019-10-23",
      "pdf_url": "https://arxiv.org/pdf/1910.10683v4",
      "citation_count": 24173,
      "year": 2019
    },
    {
      "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "M. Heusel",
        "Hubert Ramsauer",
        "Thomas Unterthiner",
        "Bernhard Nessler",
        "Sepp Hochreiter"
      ],
      "published_date": "2017",
      "pdf_url": "",
      "citation_count": 16741,
      "year": 2017
    },
    {
      "title": "nuScenes: A Multimodal Dataset for Autonomous Driving",
      "arxiv_id": "1903.11027",
      "abstract": "Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image based benchmark datasets have driven development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. In this work we present nuTonomy scenes (nuScenes), the first dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. nuScenes comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. We define novel 3D detection and tracking metrics. We also provide careful dataset analysis as well as baselines for lidar and image based detection and tracking. Data, development kit and more information are available online.",
      "authors": [
        "Holger Caesar",
        "Varun Bankiti",
        "Alex H. Lang",
        "Sourabh Vora",
        "Venice Erin Liong",
        "Qiang Xu",
        "Anush Krishnan",
        "Yu Pan",
        "Giancarlo Baldan",
        "Oscar Beijbom"
      ],
      "published_date": "2019-03-26",
      "pdf_url": "https://arxiv.org/pdf/1903.11027v5",
      "citation_count": 7334,
      "year": 2019
    },
    {
      "title": "CARLA: An Open Urban Driving Simulator",
      "arxiv_id": "1711.03938",
      "abstract": "We introduce CARLA, an open-source simulator for autonomous driving research. CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems. In addition to open-source code and protocols, CARLA provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. The simulation platform supports flexible specification of sensor suites and environmental conditions. We use CARLA to study the performance of three approaches to autonomous driving: a classic modular pipeline, an end-to-end model trained via imitation learning, and an end-to-end model trained via reinforcement learning. The approaches are evaluated in controlled scenarios of increasing difficulty, and their performance is examined via metrics provided by CARLA, illustrating the platform's utility for autonomous driving research. The supplementary video can be viewed at https://youtu.be/Hp8Dz-Zek2E",
      "authors": [
        "Alexey Dosovitskiy",
        "German Ros",
        "Felipe Codevilla",
        "Antonio Lopez",
        "Vladlen Koltun"
      ],
      "published_date": "2017-11-10",
      "pdf_url": "https://arxiv.org/pdf/1711.03938v1",
      "citation_count": 6234,
      "year": 2017
    },
    {
      "title": "Sora as a World Model? A Complete Survey on Text-to-Video Generation",
      "arxiv_id": "2403.05131",
      "abstract": "The evolution of video generation from text, from animating MNIST to simulating the world with Sora, has progressed at a breakneck speed. Here, we systematically discuss how far text-to-video generation technology supports essential requirements in world modeling. We curate 250+ studies on text-based video synthesis and world modeling. We then observe that recent models increasingly support spatial, action, and strategic intelligences in world modeling through adherence to completeness, consistency, invention, as well as human interaction and control. We conclude that text-to-video generation is adept at world modeling, although homework in several aspects, such as the diversity-consistency trade-offs, remains to be addressed.",
      "authors": [
        "Fachrina Dewi Puspitasari",
        "Chaoning Zhang",
        "Joseph Cho",
        "Adnan Haider",
        "Noor Ul Eman",
        "Omer Amin",
        "Alexis Mankowski",
        "Muhammad Umair",
        "Jingyao Zheng",
        "Sheng Zheng",
        "Lik-Hang Lee",
        "Caiyan Qin",
        "Tae-Ho Kim",
        "Choong Seon Hong",
        "Yang Yang",
        "Heng Tao Shen"
      ],
      "published_date": "2024-03-08",
      "pdf_url": "https://arxiv.org/pdf/2403.05131v3",
      "citation_count": 66,
      "year": 2024
    },
    {
      "title": "OmniNWM: Omniscient Driving Navigation World Models",
      "arxiv_id": "",
      "abstract": "Autonomous driving world models are expected to work effectively across three core dimensions: state, action, and reward. Existing models, however, are typically restricted to limited state modalities, short video sequences, imprecise action control, and a lack of reward awareness. In this paper, we introduce OmniNWM, an omniscient panoramic navigation world model that addresses all three dimensions within a unified framework. For state, OmniNWM jointly generates panoramic videos of RGB, semantics, metric depth, and 3D occupancy. A flexible forcing strategy enables high-quality long-horizon auto-regressive generation. For action, we introduce a normalized panoramic Plucker ray-map representation that encodes input trajectories into pixel-level signals, enabling highly precise and generalizable control over panoramic video generation. Regarding reward, we move beyond learning reward functions with external image-based models: instead, we leverage the generated 3D occupancy to directly define rule-based dense rewards for driving compliance and safety. Extensive experiments demonstrate that OmniNWM achieves state-of-the-art performance in video generation, control accuracy, and long-horizon stability, while providing a reliable closed-loop evaluation framework through occupancy-grounded rewards. Project page is available at https://arlo0o.github.io/OmniNWM/.",
      "authors": [
        "Bohan Li",
        "Zhuang Ma",
        "Dalong Du",
        "Baorui Peng",
        "Zhujin Liang",
        "Zhenqiang Liu",
        "Chao Ma",
        "Yueming Jin",
        "Hao Zhao",
        "Wenjun Zeng",
        "Xin Jin"
      ],
      "published_date": "2025-10-21",
      "pdf_url": "https://arxiv.org/pdf/2510.18313v4",
      "citation_count": null,
      "year": null
    },
    {
      "title": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask",
      "arxiv_id": "",
      "abstract": "Autonomous driving relies on robust models trained on large-scale, high-quality multi-view driving videos. Although world models provide a cost-effective solution for generating realistic driving data, they often suffer from identity drift, where the same object changes its appearance or category across frames due to the absence of instance-level temporal constraints. We introduce ConsisDrive, an identity-preserving driving world model designed to enforce temporal consistency at the instance level. Our framework incorporates two key components: (1) Instance-Masked Attention, which applies instance identity masks and trajectory masks within attention blocks to ensure that visual tokens interact only with their corresponding instance features across spatial and temporal dimensions, thereby preserving object identity consistency; and (2) Instance-Masked Loss, which adaptively emphasizes foreground regions with probabilistic instance masking, reducing background noise while maintaining overall scene fidelity. By integrating these mechanisms, ConsisDrive achieves state-of-the-art driving video generation quality and demonstrates significant improvements in downstream autonomous driving tasks on the nuScenes dataset. Our project page is https://shanpoyang654.github.io/ConsisDrive/page.html.",
      "authors": [
        "Zhuoran Yang",
        "Yanyong Zhang"
      ],
      "published_date": "2026-02-03",
      "pdf_url": "https://arxiv.org/pdf/2602.03213v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving",
      "arxiv_id": "",
      "abstract": "World models have demonstrated significant promise for data synthesis in autonomous driving. However, existing methods predominantly concentrate on single-modality generation, typically focusing on either multi-camera video or LiDAR sequence synthesis. In this paper, we propose UniDriveDreamer, a single-stage unified multimodal world model for autonomous driving, which directly generates multimodal future observations without relying on intermediate representations or cascaded modules. Our framework introduces a LiDAR-specific variational autoencoder (VAE) designed to encode input LiDAR sequences, alongside a video VAE for multi-camera images. To ensure cross-modal compatibility and training stability, we propose Unified Latent Anchoring (ULA), which explicitly aligns the latent distributions of the two modalities. The aligned features are fused and processed by a diffusion transformer that jointly models their geometric correspondence and temporal evolution. Additionally, structured scene layout information is projected per modality as a conditioning signal to guide the synthesis. Extensive experiments demonstrate that UniDriveDreamer outperforms previous state-of-the-art methods in both video and LiDAR generation, while also yielding measurable improvements in downstream",
      "authors": [
        "Guosheng Zhao",
        "Yaozeng Wang",
        "Xiaofeng Wang",
        "Zheng Zhu",
        "Tingdong Yu",
        "Guan Huang",
        "Yongchen Zai",
        "Ji Jiao",
        "Changliang Xue",
        "Xiaole Wang",
        "Zhen Yang",
        "Futang Zhu",
        "Xingang Wang"
      ],
      "published_date": "2026-02-02",
      "pdf_url": "https://arxiv.org/pdf/2602.02002v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
      "arxiv_id": "",
      "abstract": "Recent video diffusion models generate photorealistic, temporally coherent videos, yet they fall short as reliable world models for autonomous driving, where structured motion and physically consistent interactions are essential. Adapting these generalist video models to driving domains has shown promise but typically requires massive domain-specific data and costly fine-tuning. We propose an efficient adaptation framework that converts generalist video diffusion models into controllable driving world models with minimal supervision. The key idea is to decouple motion learning from appearance synthesis. First, the model is adapted to predict structured motion in a simplified form: videos of skeletonized agents and scene elements, focusing learning on physical and social plausibility. Then, the same backbone is reused to synthesize realistic RGB videos conditioned on these motion sequences, effectively \"dressing\" the motion with texture and lighting. This two-stage process mirrors a reasoning-rendering paradigm: first infer dynamics, then render appearance. Our experiments show this decoupled approach is exceptionally efficient: adapting SVD, we match prior SOTA models with less than 6% of their compute. Scaling to LTX, our MAD-LTX model outperforms all open-source competitors, and supports a comprehensive suite of text, ego, and object controls. Project page: https://vita-epfl.github.io/MAD-World-Model/",
      "authors": [
        "Ahmad Rahimi",
        "Valentin Gerard",
        "Eloi Zablocki",
        "Matthieu Cord",
        "Alexandre Alahi"
      ],
      "published_date": "2026-01-14",
      "pdf_url": "https://arxiv.org/pdf/2601.09452v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "I and J",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "W. Marsden"
      ],
      "published_date": "2012",
      "pdf_url": "",
      "citation_count": 168818,
      "year": 2012
    },
    {
      "title": "ImageNet: A large-scale hierarchical image database",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Jia Deng",
        "Wei Dong",
        "R. Socher",
        "Li-Jia Li",
        "K. Li",
        "Li Fei-Fei"
      ],
      "published_date": "2009",
      "pdf_url": "",
      "citation_count": 70609,
      "year": 2009
    },
    {
      "title": "A and V",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "R. Stephenson"
      ],
      "published_date": "1962",
      "pdf_url": "",
      "citation_count": 51407,
      "year": 1962
    },
    {
      "title": "What matters for Representation Alignment: Global Information or Spatial Structure?",
      "arxiv_id": "",
      "abstract": "Representation alignment (REPA) guides generative training by distilling representations from a strong, pretrained vision encoder to intermediate diffusion features. We investigate a fundamental question: what aspect of the target representation matters for generation, its \\textit{global} \\revision{semantic} information (e.g., measured by ImageNet-1K accuracy) or its spatial structure (i.e. pairwise cosine similarity between patch tokens)? Prevalent wisdom holds that stronger global semantic performance leads to better generation as a target representation. To study this, we first perform a large-scale empirical analysis across 27 different vision encoders and different model scales. The results are surprising; spatial structure, rather than global performance, drives the generation performance of a target representation. To further study this, we introduce two straightforward modifications, which specifically accentuate the transfer of \\emph{spatial} information. We replace the standard MLP projection layer in REPA with a simple convolution layer and introduce a spatial normalization layer for the external representation. Surprisingly, our simple method (implemented in $<$4 lines of code), termed iREPA, consistently improves convergence speed of REPA, across a diverse set of vision encoders, model sizes, and training variants (such as REPA, REPA-E, Meanflow, JiT etc). %, etc. Our work motivates revisiting the fundamental working mechanism of representational alignment and how it can be leveraged for improved training of generative models. The code and project page are available at https://end2end-diffusion.github.io/irepa",
      "authors": [
        "Jaskirat Singh",
        "Xingjian Leng",
        "Zongze Wu",
        "Liang Zheng",
        "Richard Zhang",
        "Eli Shechtman",
        "Saining Xie"
      ],
      "published_date": "2025-12-11",
      "pdf_url": "https://arxiv.org/pdf/2512.10794v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "SCB-DETR: Multiscale Deformable Transformers for Occlusion-Resilient Student Learning Behavior Detection in Smart Classroom",
      "arxiv_id": null,
      "abstract": "The integration of artificial intelligence (AI) into the modern educational system is rapidly evolving, particularly in monitoring student behavior in classrooms—a task traditionally dependent on manual observation. This conventional method is notably inefficient, prompting a shift toward more advanced solutions such as computer vision. However, existing target detection models face significant challenges such as occlusion, blurring, and scale disparity, which are exacerbated by the dynamic and complex nature of classroom settings. Furthermore, these models must adeptly handle multiple target detection. To overcome these obstacles, we introduce the student classroom behavior detection with multiscale deformable transformers (SCB-DETR), an innovative approach that utilizes large convolutional kernels for upstream feature extraction, and multiscale feature fusion. This technique significantly improves the detection capabilities for multiscale and occluded targets, offering a robust solution for analyzing student behavior. SCB-DETR establishes an end-to-end system that simplifies the detection process and consistently outperforms other deep learning methods. Employing our custom student classroom behavior (SCBehavior) dataset, SCB-DETR achieves a mean Average Precision (mAP) of 0.626, which is a 1.5% improvement over the baseline model’s mAP and a 6% increase in AP50. These results demonstrate SCB-DETR’s superior performance in handling the uneven distribution of student behaviors and ensuring precise detection in dynamic classroom environments. The source code of this study is publicly available at https://github.com/CCNUZFW/SCB-DETR.",
      "authors": [
        "Zhifeng Wang",
        "Minghui Wang",
        "Chunyan Zeng",
        "Longlong Li"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 7,
      "year": 2025
    },
    {
      "title": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination",
      "arxiv_id": null,
      "abstract": "This article introduces a robust deep feature ultrasound image-based visual servoing (UIBVS) technique for an ultrasound robot focusing on automatic cardiac examination. To this end, a convolutional neural network named ultrasound-cardiac-feature-net (UCF-Net) is developed, which is trained in a supervised manner to process ultrasound images and generate a set of six image features referred to as deep ultrasound image features. To enhance the robustness of UCF-Net against the variables that affect the ultrasound image quality, such as interaction normal force, scan depth, dynamic range, power, and gain, several datasets with different sets of parameters are gathered for training. Deep ultrasound image features enable an eye-in-hand robot to interact with the human body through UIBVS. To implement UIBVS, a filtered integral quasi-super-twisting algorithm (FIQSTA) is synthesized as the primary controller. Interaction force control is also considered within a hybrid vision/force control framework, providing compliance with the body and increasing the safety of the interaction. The proof of the robustness and stability of FIQSTA is also investigated. Experimental results on a cardiac phantom for four main views, i.e., parasternal short axis, parasternal long axis, subcostal, and apical four chambers views, and a trajectory passing through the main views demonstrate the feasibility of the proposed method for cardiac examination and the superior performance of the main controller to other well-known methods, including proportional (P) controller, sliding mode controller, super-twisting algorithm (STA), and integral quasi-STA.",
      "authors": [
        "Ehsan Zakeri",
        "Amanda Spilkin",
        "Hanae Elmekki",
        "Antonela Zanuttini",
        "L. Kadem",
        "Jamal Bentahar",
        "Wen-Fang Xie",
        "Philippe Pibarot"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 5,
      "year": 2025
    },
    {
      "title": "Bio-Inspired Fine-Tuning for Selective Transfer Learning in Image Classification",
      "arxiv_id": "2601.11235",
      "abstract": "Deep learning has significantly advanced image analysis across diverse domains but often depends on large, annotated datasets for success. Transfer learning addresses this challenge by utilizing pre-trained models to tackle new tasks with limited labeled data. However, discrepancies between source and target domains can hinder effective transfer learning. We introduce BioTune, a novel adaptive fine-tuning technique utilizing evolutionary optimization. BioTune enhances transfer learning by optimally choosing which layers to freeze and adjusting learning rates for unfrozen layers. Through extensive evaluation on nine image classification datasets, spanning natural and specialized domains such as medical imaging, BioTune demonstrates superior accuracy and efficiency over state-of-the-art fine-tuning methods, including AutoRGN and LoRA, highlighting its adaptability to various data characteristics and distribution changes. Additionally, BioTune consistently achieves top performance across four different CNN architectures, underscoring its flexibility. Ablation studies provide valuable insights into the impact of BioTune's key components on overall performance. The source code is available at https://github.com/davilac/BioTune.",
      "authors": [
        "Ana Davila",
        "Jacinto Colan",
        "Yasuhisa Hasegawa"
      ],
      "published_date": "2026-01-16",
      "pdf_url": "https://arxiv.org/pdf/2601.11235v1",
      "citation_count": 3,
      "year": 2026
    },
    {
      "title": "Hand Sign Language Detection Using Deep Learning",
      "arxiv_id": "2601.08262",
      "abstract": "Hand gesture recognition is an important aspect of human-computer interaction. It forms the basis of sign language for the visually impaired people. This work proposes a novel hand gesture recognizing system for the differently-abled persons. The model uses a convolutional neural network, known as VGG-16 net, for building a trained model on a widely used image dataset by employing Python and Keras libraries. Furthermore, the result is validated by the NUS dataset, consisting of 10 classes of hand gestures, fed to the model as the validation set. Afterwards, a testing dataset of 10 classes is built by employing Google's open source Application Programming Interface (API) that captures different gestures of human hand and the efficacy is then measured by carrying out experiments. The experimental results show that by combining a transfer learning mechanism together with the image data augmentation, the VGG-16 net produced around 98% accuracy.",
      "authors": [
        "Subham Sharma",
        "Sharmila Subudhi"
      ],
      "published_date": "2026-01-13",
      "pdf_url": "https://arxiv.org/pdf/2601.08262v1",
      "citation_count": 3,
      "year": 2026
    },
    {
      "title": "and as an in",
      "arxiv_id": null,
      "abstract": "",
      "authors": [],
      "published_date": "0",
      "pdf_url": "",
      "citation_count": 79439,
      "year": 0
    },
    {
      "title": "Microsoft COCO: Common Objects in Context",
      "arxiv_id": "1405.0312",
      "abstract": "We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.",
      "authors": [
        "Tsung-Yi Lin",
        "Michael Maire",
        "Serge Belongie",
        "Lubomir Bourdev",
        "Ross Girshick",
        "James Hays",
        "Pietro Perona",
        "Deva Ramanan",
        "C. Lawrence Zitnick",
        "Piotr Dollár"
      ],
      "published_date": "2014-05-01",
      "pdf_url": "https://arxiv.org/pdf/1405.0312v3",
      "citation_count": 50009,
      "year": 2014
    },
    {
      "title": "FANet: Frequency-Aware Attention-Based Tiny-Object Detection in Remote Sensing Images",
      "arxiv_id": null,
      "abstract": "In recent years, deep learning-based remote sensing object detection has achieved remarkable progress, yet the detection of tiny objects remains a significant challenge. Tiny objects in remote sensing images typically occupy only a few pixels, resulting in low contrast, poor resolution, and high sensitivity to localization errors. Their diverse scales and appearances, combined with complex backgrounds and severe class imbalance, further complicate the detection tasks. Conventional spatial feature extraction methods often struggle to capture the discriminative characteristics of tiny objects, especially in the presence of noise and occlusion. To address these challenges, we propose a frequency-aware attention-based tiny-object detection network with two plug-and-play modules that leverage frequency-domain information to enhance the targets. Specifically, we introduce a Multi-Scale Frequency Feature Enhancement Module (MSFFEM) to adaptively highlight the contour and texture details of tiny objects while suppressing background noise. Additionally, a Channel Attention-based RoI Enhancement Module (CAREM) is proposed to selectively emphasize high-frequency responses within RoI features, further improving object localization and classification. Furthermore, to mitigate sample imbalance, we employ multi-directional flip sample augmentation and redundancy filtering strategies, which significantly boost detection performance for few-shot categories. Extensive experiments on public object detection datasets, i.e., AI-TOD, VisDrone2019, and DOTA-v1.5, demonstrate that the proposed FANet consistently improves detection performance for tiny objects, outperforming existing methods and providing new insights into the integration of frequency-domain analysis and attention mechanisms for robust tiny-object detection in remote sensing applications.",
      "authors": [
        "Zixiao Wen",
        "Peifeng Li",
        "Yuhan Liu",
        "Jingming Chen",
        "Xiantai Xiang",
        "Yuan Li",
        "Huixian Wang",
        "Yongchao Zhao",
        "Guangyao Zhou"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 4,
      "year": 2025
    },
    {
      "title": "UAV-based multimodal object detection via feature enhancement and dynamic gated fusion",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Yu Gu",
        "Weili Chen",
        "Dongliang Peng"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 2,
      "year": 2026
    },
    {
      "title": "An empirical analysis of deep learning methods for small object detection from satellite imagery",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Xiaohui Yuan",
        "Aniv Chakravarty",
        "Elinor M. Lichtenberg",
        "Lichuan Gu",
        "Zhenchun Wei",
        "Tian Chen"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 2,
      "year": 2026
    },
    {
      "title": "Representation Learning: A Review and New Perspectives",
      "arxiv_id": "",
      "abstract": "The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.",
      "authors": [
        "Yoshua Bengio",
        "Aaron Courville",
        "Pascal Vincent"
      ],
      "published_date": "2012-06-24",
      "pdf_url": "https://arxiv.org/pdf/1206.5538v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Pascal Vincent",
        "H. Larochelle",
        "Isabelle Lajoie",
        "Yoshua Bengio",
        "Pierre-Antoine Manzagol"
      ],
      "published_date": "2010",
      "pdf_url": "",
      "citation_count": 7440,
      "year": 2010
    },
    {
      "title": "In Advances in Neural Information Processing Systems",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "D. Touretzky",
        "M. C. Mozer",
        "M. E. Hasselmo",
        "RegressionChristopher",
        "I. K.",
        "WilliamsNeural",
        "GroupAston",
        "UniversityBirmingham"
      ],
      "published_date": "1996",
      "pdf_url": "",
      "citation_count": 6307,
      "year": 1996
    },
    {
      "title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Danilo Jimenez Rezende",
        "S. Mohamed",
        "Daan Wierstra"
      ],
      "published_date": "2014",
      "pdf_url": "",
      "citation_count": 5526,
      "year": 2014
    },
    {
      "title": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation",
      "arxiv_id": "",
      "abstract": "We propose a diffusion distillation method that achieves new state-of-the-art in one-step/few-step 1024px text-to-image generation based on SDXL. Our method combines progressive and adversarial distillation to achieve a balance between quality and mode coverage. In this paper, we discuss the theoretical analysis, discriminator design, model formulation, and training techniques. We open-source our distilled SDXL-Lightning models both as LoRA and full UNet weights.",
      "authors": [
        "Shanchuan Lin",
        "Anran Wang",
        "Xiao Yang"
      ],
      "published_date": "2024-02-21",
      "pdf_url": "https://arxiv.org/pdf/2402.13929v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions",
      "arxiv_id": "",
      "abstract": "The area of portrait image animation, propelled by audio input, has witnessed notable progress in the generation of lifelike and dynamic portraits. Conventional methods are limited to utilizing either audios or facial key points to drive images into videos, while they can yield satisfactory results, certain issues exist. For instance, methods driven solely by audios can be unstable at times due to the relatively weaker audio signal, while methods driven exclusively by facial key points, although more stable in driving, can result in unnatural outcomes due to the excessive control of key point information. In addressing the previously mentioned challenges, in this paper, we introduce a novel approach which we named EchoMimic. EchoMimic is concurrently trained using both audios and facial landmarks. Through the implementation of a novel training strategy, EchoMimic is capable of generating portrait videos not only by audios and facial landmarks individually, but also by a combination of both audios and selected facial landmarks. EchoMimic has been comprehensively compared with alternative algorithms across various public datasets and our collected dataset, showcasing superior performance in both quantitative and qualitative evaluations. Additional visualization and access to the source code can be located on the EchoMimic project page.",
      "authors": [
        "Zhiyuan Chen",
        "Jiajiong Cao",
        "Zhiquan Chen",
        "Yuming Li",
        "Chenguang Ma"
      ],
      "published_date": "2024-07-11",
      "pdf_url": "https://arxiv.org/pdf/2407.08136v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "GenAD: Generative End-to-End Autonomous Driving",
      "arxiv_id": "",
      "abstract": "Directly producing planning results from raw sensors has been a long-desired solution for autonomous driving and has attracted increasing attention recently. Most existing end-to-end autonomous driving methods factorize this problem into perception, motion prediction, and planning. However, we argue that the conventional progressive pipeline still cannot comprehensively model the entire traffic evolution process, e.g., the future interaction between the ego car and other traffic participants and the structural trajectory prior. In this paper, we explore a new paradigm for end-to-end autonomous driving, where the key is to predict how the ego car and the surroundings evolve given past scenes. We propose GenAD, a generative framework that casts autonomous driving into a generative modeling problem. We propose an instance-centric scene tokenizer that first transforms the surrounding scenes into map-aware instance tokens. We then employ a variational autoencoder to learn the future trajectory distribution in a structural latent space for trajectory prior modeling. We further adopt a temporal model to capture the agent and ego movements in the latent space to generate more effective future trajectories. GenAD finally simultaneously performs motion prediction and planning by sampling distributions in the learned structural latent space conditioned on the instance tokens and using the learned temporal model to generate futures. Extensive experiments on the widely used nuScenes benchmark show that the proposed GenAD achieves state-of-the-art performance on vision-centric end-to-end autonomous driving with high efficiency. Code: https://github.com/wzzheng/GenAD.",
      "authors": [
        "Wenzhao Zheng",
        "Ruiqi Song",
        "Xianda Guo",
        "Chenming Zhang",
        "Long Chen"
      ],
      "published_date": "2024-02-18",
      "pdf_url": "https://arxiv.org/pdf/2402.11502v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model",
      "arxiv_id": "",
      "abstract": "Current Multimodal Large Language Models (MLLMs) typically integrate a pre-trained LLM with another pre-trained vision transformer through a connector, such as an MLP, endowing the LLM with visual capabilities. However, the misalignment between two embedding strategies in MLLMs -- the structural textual embeddings based on an embedding look-up table and the continuous embeddings generated directly by the vision encoder -- makes challenges for a more seamless fusion of visual and textual information. We propose Ovis, a novel MLLM architecture designed to structurally align visual and textual embeddings. Ovis integrates an additional learnable visual embedding table into the visual encoder's process. To capture rich visual semantics, each image patch indexes the visual embedding table multiple times, resulting in a final visual embedding that is a probabilistic combination of the indexed embeddings. This structural approach mirrors the method used for generating textual embeddings. Empirical evaluations on various multimodal benchmarks show that Ovis outperforms open-source MLLMs of similar parameter scales and even surpasses the proprietary model Qwen-VL-Plus overall. These results highlight the potential of Ovis' structured visual representation for advancing MLLM architectural design and promoting more effective multimodal learning. Code, datasets, and models are available at https://github.com/AIDC-AI/Ovis.",
      "authors": [
        "Shiyin Lu",
        "Yang Li",
        "Qing-Guo Chen",
        "Zhao Xu",
        "Weihua Luo",
        "Kaifu Zhang",
        "Han-Jia Ye"
      ],
      "published_date": "2024-05-31",
      "pdf_url": "https://arxiv.org/pdf/2405.20797v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Improving Video Generation with Human Feedback",
      "arxiv_id": "",
      "abstract": "Video generation has achieved significant advances through rectified flow techniques, but issues like unsmooth motion and misalignment between videos and prompts persist. In this work, we develop a systematic pipeline that harnesses human feedback to mitigate these problems and refine the video generation model. Specifically, we begin by constructing a large-scale human preference dataset focused on modern video generation models, incorporating pairwise annotations across multi-dimensions. We then introduce VideoReward, a multi-dimensional video reward model, and examine how annotations and various design choices impact its rewarding efficacy. From a unified reinforcement learning perspective aimed at maximizing reward with KL regularization, we introduce three alignment algorithms for flow-based models. These include two training-time strategies: direct preference optimization for flow (Flow-DPO) and reward weighted regression for flow (Flow-RWR), and an inference-time technique, Flow-NRG, which applies reward guidance directly to noisy videos. Experimental results indicate that VideoReward significantly outperforms existing reward models, and Flow-DPO demonstrates superior performance compared to both Flow-RWR and supervised fine-tuning methods. Additionally, Flow-NRG lets users assign custom weights to multiple objectives during inference, meeting personalized video quality needs.",
      "authors": [
        "Jie Liu",
        "Gongye Liu",
        "Jiajun Liang",
        "Ziyang Yuan",
        "Xiaokun Liu",
        "Mingwu Zheng",
        "Xiele Wu",
        "Qiulin Wang",
        "Menghan Xia",
        "Xintao Wang",
        "Xiaohong Liu",
        "Fei Yang",
        "Pengfei Wan",
        "Di Zhang",
        "Kun Gai",
        "Yujiu Yang",
        "Wanli Ouyang"
      ],
      "published_date": "2025-01-23",
      "pdf_url": "https://arxiv.org/pdf/2501.13918v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Speech Recognition with Deep Recurrent Neural Networks",
      "arxiv_id": "",
      "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \\emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.",
      "authors": [
        "Alex Graves",
        "Abdel-rahman Mohamed",
        "Geoffrey Hinton"
      ],
      "published_date": "2013-03-22",
      "pdf_url": "https://arxiv.org/pdf/1303.5778v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Learning representations by back-propagating errors",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "D. Rumelhart",
        "Geoffrey E. Hinton",
        "Ronald J. Williams"
      ],
      "published_date": "1986",
      "pdf_url": "",
      "citation_count": 30026,
      "year": 1986
    },
    {
      "title": "Bidirectional recurrent neural networks",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "M. Schuster",
        "K. Paliwal"
      ],
      "published_date": "1997",
      "pdf_url": "",
      "citation_count": 9317,
      "year": 1997
    },
    {
      "title": "Connectionist Temporal Classiﬁcation: Labelling Unsegmented Sequence Data with Recurrent Neural Networks",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Alex Graves",
        "Santiago Fern´andez",
        "Faustino J. Gomez",
        "J¨urgen Schmidhuber"
      ],
      "published_date": "0",
      "pdf_url": "",
      "citation_count": 4899,
      "year": 0
    },
    {
      "title": "2005 Special Issue: Framewise phoneme classification with bidirectional LSTM and other neural network architectures",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Alex Graves",
        "J. Schmidhuber"
      ],
      "published_date": "2005",
      "pdf_url": "",
      "citation_count": 4758,
      "year": 2005
    },
    {
      "title": "A high-performance neuroprosthesis for speech decoding and avatar control",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Sean L. Metzger",
        "K. T. Littlejohn",
        "Alexander B. Silva",
        "D. Moses",
        "Margaret P. Seaton",
        "Ran Wang",
        "Maximilian E. Dougherty",
        "Jessie R. Liu",
        "Peter Wu",
        "M. Berger",
        "Inga Zhuravleva",
        "A. Tu-Chan",
        "K. Ganguly",
        "G. Anumanchipalli",
        "Edward F. Chang"
      ],
      "published_date": "2023",
      "pdf_url": "",
      "citation_count": 391,
      "year": 2023
    },
    {
      "title": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses",
      "arxiv_id": null,
      "abstract": "The latest progress of emerging smart flexible sensing systems driven by brain-inspired artificial intelligence (AI) from both the algorithm (machine learning) and the framework (artificial synapses) level is reviewed. New enabling features such as powerful data analysis and intelligent decision-making resulting from the fusion of AI technology with flexible sensors are discussed. Promising application prospects of AI-driven smart flexible sensing systems such as more intelligent monitoring for human activities, more humanoid feeling by artificial sensory organs, and more autonomous action of soft robotics are demonstrated. The latest progress of emerging smart flexible sensing systems driven by brain-inspired artificial intelligence (AI) from both the algorithm (machine learning) and the framework (artificial synapses) level is reviewed. New enabling features such as powerful data analysis and intelligent decision-making resulting from the fusion of AI technology with flexible sensors are discussed. Promising application prospects of AI-driven smart flexible sensing systems such as more intelligent monitoring for human activities, more humanoid feeling by artificial sensory organs, and more autonomous action of soft robotics are demonstrated. The recent wave of the artificial intelligence (AI) revolution has aroused unprecedented interest in the intelligentialize of human society. As an essential component that bridges the physical world and digital signals, flexible sensors are evolving from a single sensing element to a smarter system, which is capable of highly efficient acquisition, analysis, and even perception of vast, multifaceted data. While challenging from a manual perspective, the development of intelligent flexible sensing has been remarkably facilitated owing to the rapid advances of brain-inspired AI innovations from both the algorithm (machine learning) and the framework (artificial synapses) level. This review presents the recent progress of the emerging AI-driven, intelligent flexible sensing systems. The basic concept of machine learning and artificial synapses are introduced. The new enabling features induced by the fusion of AI and flexible sensing are comprehensively reviewed, which significantly advances the applications such as flexible sensory systems, soft/humanoid robotics, and human activity monitoring. As two of the most profound innovations in the twenty-first century, the deep incorporation of flexible sensing and AI technology holds tremendous potential for creating a smarter world for human beings.",
      "authors": [
        "Tianming Sun",
        "Bin Feng",
        "Jinpeng Huo",
        "Yu Xiao",
        "Wengan Wang",
        "Jin Peng",
        "Zehua Li",
        "Chengjie Du",
        "Wenxian Wang",
        "G. Zou",
        "Lei Liu"
      ],
      "published_date": "2023",
      "pdf_url": "",
      "citation_count": 239,
      "year": 2023
    },
    {
      "title": "A high-performance speech neuroprosthesis",
      "arxiv_id": null,
      "abstract": "Speech brain–computer interfaces (BCIs) have the potential to restore rapid communication to people with paralysis by decoding neural activity evoked by attempted speech into text^ 1 , 2 or sound^ 3 , 4 . Early demonstrations, although promising, have not yet achieved accuracies sufficiently high for communication of unconstrained sentences from a large vocabulary^ 1 – 7 . Here we demonstrate a speech-to-text BCI that records spiking activity from intracortical microelectrode arrays. Enabled by these high-resolution recordings, our study participant—who can no longer speak intelligibly owing to amyotrophic lateral sclerosis—achieved a 9.1% word error rate on a 50-word vocabulary (2.7 times fewer errors than the previous state-of-the-art speech BCI^ 2 ) and a 23.8% word error rate on a 125,000-word vocabulary (the first successful demonstration, to our knowledge, of large-vocabulary decoding). Our participant’s attempted speech was decoded  at 62 words per minute, which is 3.4 times as fast as the previous record^ 8 and begins to approach the speed of natural conversation (160 words per minute^ 9 ). Finally, we highlight two aspects of the neural code for speech that are encouraging for speech BCIs: spatially intermixed tuning to speech articulators that makes accurate decoding possible from only a small region of cortex, and a detailed articulatory representation of phonemes that persists years after paralysis. These results show a feasible path forward for restoring rapid communication to people with paralysis who can no longer speak. A speech-to-text brain–computer interface that records spiking activity from intracortical microelectrode arrays enabled an individual who cannot speak intelligibly to achieve 9.1 and 23.8% word error rates on a 50- and 125,000-word vocabulary, respectively.",
      "authors": [
        "Francis R. Willett",
        "Erin M. Kunz",
        "Chaofei Fan",
        "Donald T. Avansino",
        "G. Wilson",
        "Eun Young Choi",
        "Foram B. Kamdar",
        "M. Glasser",
        "L. Hochberg",
        "S. Druckmann",
        "K. Shenoy",
        "J. Henderson"
      ],
      "published_date": "2023",
      "pdf_url": "",
      "citation_count": 224,
      "year": 2023
    },
    {
      "title": "Loss of plasticity in deep continual learning",
      "arxiv_id": null,
      "abstract": "Artificial neural networks, deep-learning methods and the backpropagation algorithm1 form the foundation of modern machine learning and artificial intelligence. These methods are almost always used in two phases, one in which the weights of the network are updated and one in which the weights are held constant while the network is used or evaluated. This contrasts with natural learning and many applications, which require continual learning. It has been unclear whether or not deep learning methods work in continual learning settings. Here we show that they do not—that standard deep-learning methods gradually lose plasticity in continual-learning settings until they learn no better than a shallow network. We show such loss of plasticity using the classic ImageNet dataset and reinforcement-learning problems across a wide range of variations in the network and the learning algorithm. Plasticity is maintained indefinitely only by algorithms that continually inject diversity into the network, such as our continual backpropagation algorithm, a variation of backpropagation in which a small fraction of less-used units are continually and randomly reinitialized. Our results indicate that methods based on gradient descent are not enough—that sustained deep learning requires a random, non-gradient component to maintain variability and plasticity. The pervasive problem of artificial neural networks losing plasticity in continual-learning settings is demonstrated and a simple solution called the continual backpropagation algorithm is described to prevent this issue.",
      "authors": [
        "Shibhansh Dohare",
        "J. F. Hernandez-Garcia",
        "Qingfeng Lan",
        "Parash Rahman",
        "A. Mahmood",
        "R. Sutton"
      ],
      "published_date": "2024",
      "pdf_url": "",
      "citation_count": 222,
      "year": 2024
    },
    {
      "title": "An analog-AI chip for energy-efficient speech recognition and transcription",
      "arxiv_id": null,
      "abstract": "A low-power chip that runs AI models using analog rather than digital computation shows comparable accuracy on speech-recognition tasks but is more than 14 times as energy efficient. Models of artificial intelligence (AI) that have billions of parameters can achieve high accuracy across a range of tasks^ 1 , 2 , but they exacerbate the poor energy efficiency of conventional general-purpose processors, such as graphics processing units or central processing units. Analog in-memory computing (analog-AI)^ 3 – 7 can provide better energy efficiency by performing matrix–vector multiplications in parallel on ‘memory tiles’. However, analog-AI has yet to demonstrate software-equivalent (SW_eq) accuracy on models that require many such tiles and efficient communication of neural-network activations between the tiles. Here we present an analog-AI chip that combines 35 million phase-change memory devices across 34 tiles, massively parallel inter-tile communication and analog, low-power peripheral circuitry that can achieve up to 12.4 tera-operations per second per watt (TOPS/W) chip-sustained performance. We demonstrate fully end-to-end SW_eq accuracy for a small keyword-spotting network and near-SW_eq accuracy on the much larger MLPerf^ 8 recurrent neural-network transducer (RNNT), with more than 45 million weights mapped onto more than 140 million phase-change memory devices across five chips.",
      "authors": [
        "S. Ambrogio",
        "P. Narayanan",
        "A. Okazaki",
        "A. Fasoli",
        "C. Mackin",
        "K. Hosokawa",
        "A. Nomura",
        "Takeo Yasuda",
        "An Chen",
        "A. Friz",
        "M. Ishii",
        "J. Luquin",
        "Y. Kohda",
        "N. Saulnier",
        "K. Brew",
        "Samuel Choi",
        "I. Ok",
        "Timothy Philip",
        "Victor Chan",
        "M. Silvestre",
        "Ishtiaq Ahsan",
        "Vijay Narayanan",
        "H. Tsai",
        "Geoffrey W. Burr"
      ],
      "published_date": "2023",
      "pdf_url": "",
      "citation_count": 184,
      "year": 2023
    },
    {
      "title": "A Mathematical Theory of Communication",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "J. Shin",
        "Sang Joon Kim"
      ],
      "published_date": "2006",
      "pdf_url": "",
      "citation_count": 72176,
      "year": 2006
    },
    {
      "title": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers",
      "arxiv_id": "",
      "abstract": "Tokenization is fundamental to how language models represent and process text, yet the behavior of widely used BPE tokenizers has received far less study than model architectures and training. In this paper, we investigate intermediate merge residues in BPE vocabularies: tokens that are frequent during merge learning so that retained in the final vocabulary, but are mostly further merged and rarely emitted when tokenizing the corpus during tokenizer usage. Such low-frequency tokens not only waste vocabulary capacity but also increase vulnerability to adversarial or atypical inputs. We present a systematic empirical characterization of this phenomenon across commonly used tokenizers and introduce LiteToken, a simple method for removing residue tokens. Because the affected tokens are rarely used, pretrained models can often accommodate the modified tokenizer without additional fine-tuning. Experiments show that LiteToken reduces token fragmentation, reduces parameters, and improves robustness to noisy or misspelled inputs, while preserving overall performance.",
      "authors": [
        "Yike Sun",
        "Haotong Yang",
        "Zhouchen Lin",
        "Muhan Zhang"
      ],
      "published_date": "2026-02-04",
      "pdf_url": "https://arxiv.org/pdf/2602.04706v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling",
      "arxiv_id": "",
      "abstract": "Scaling Large Language Models (LLMs) typically relies on increasing the number of parameters or test-time computations to boost performance. However, these strategies are impractical for edge device deployment due to limited RAM and NPU resources. Despite hardware constraints, deploying performant LLM on edge devices such as smartphone remains crucial for user experience. To address this, we propose MeKi (Memory-based Expert Knowledge Injection), a novel system that scales LLM capacity via storage space rather than FLOPs. MeKi equips each Transformer layer with token-level memory experts that injects pre-stored semantic knowledge into the generation process. To bridge the gap between training capacity and inference efficiency, we employ a re-parameterization strategy to fold parameter matrices used during training into a compact static lookup table. By offloading the knowledge to ROM, MeKi decouples model capacity from computational cost, introducing zero inference latency overhead. Extensive experiments demonstrate that MeKi significantly outperforms dense LLM baselines with identical inference speed, validating the effectiveness of memory-based scaling paradigm for on-device LLMs. Project homepage is at https://github.com/ningding-o/MeKi.",
      "authors": [
        "Ning Ding",
        "Fangcheng Liu",
        "Kyungrae Kim",
        "Linji Hao",
        "Kyeng-Hun Lee",
        "Hyeonmok Ko",
        "Yehui Tang"
      ],
      "published_date": "2026-02-03",
      "pdf_url": "https://arxiv.org/pdf/2602.03359v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram",
      "arxiv_id": "",
      "abstract": "Current genomic foundation models (GFMs) rely on extensive neural computation to implicitly approximate conserved biological motifs from single-nucleotide inputs. We propose Gengram, a conditional memory module that introduces an explicit and highly efficient lookup primitive for multi-base motifs via a genomic-specific hashing scheme, establishing genomic \"syntax\". Integrated into the backbone of state-of-the-art GFMs, Gengram achieves substantial gains (up to 14%) across several functional genomics tasks. The module demonstrates robust architectural generalization, while further inspection of Gengram's latent space reveals the emergence of meaningful representations that align closely with fundamental biological knowledge. By establishing structured motif memory as a modeling primitive, Gengram simultaneously boosts empirical performance and mechanistic interpretability, providing a scalable and biology-aligned pathway for the next generation of GFMs. The code is available at https://github.com/zhejianglab/Genos, and the model checkpoint is available at https://huggingface.co/ZhejiangLab/Gengram.",
      "authors": [
        "Huinan Xu",
        "Xuyang Feng",
        "Junhong Chen",
        "Junchen Liu",
        "Kaiwen Deng",
        "Kai Ding",
        "Shengning Long",
        "Jiaxue Shuai",
        "Zhaorong Li",
        "Shiping Liu",
        "Guirong Xue",
        "Zhan Xiao"
      ],
      "published_date": "2026-01-29",
      "pdf_url": "https://arxiv.org/pdf/2601.22203v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "L$^3$: Large Lookup Layers",
      "arxiv_id": "",
      "abstract": "Modern sparse language models typically achieve sparsity through Mixture-of-Experts (MoE) layers, which dynamically route tokens to dense MLP \"experts.\" However, dynamic hard routing has a number of drawbacks, such as potentially poor hardware efficiency and needing auxiliary losses for stable training. In contrast, the tokenizer embedding table, which is natively sparse, largely avoids these issues by selecting a single embedding per token at the cost of not having contextual information. In this work, we introduce the Large Lookup Layer (L$^3$), which unlocks a new axis of sparsity by generalizing embedding tables to model decoder layers. L$^3$ layers use static token-based routing to aggregate a set of learned embeddings per token in a context-dependent way, allowing the model to efficiently balance memory and compute by caching information in embeddings. L$^3$ has two main components: (1) a systems-friendly architecture that allows for fast training and CPU-offloaded inference with no overhead, and (2) an information-theoretic embedding allocation algorithm that effectively balances speed and quality. We empirically test L$^3$ by training transformers with up to 2.6B active parameters and find that L$^3$ strongly outperforms both dense models and iso-sparse MoEs in both language modeling and downstream tasks.",
      "authors": [
        "Albert Tseng",
        "Christopher De Sa"
      ],
      "published_date": "2026-01-29",
      "pdf_url": "https://arxiv.org/pdf/2601.21461v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Scaling Embeddings Outperforms Scaling Experts in Language Models",
      "arxiv_id": "",
      "abstract": "While Mixture-of-Experts (MoE) architectures have become the standard for sparsity scaling in large language models, they increasingly face diminishing returns and system-level bottlenecks. In this work, we explore embedding scaling as a potent, orthogonal dimension for scaling sparsity. Through a comprehensive analysis and experiments, we identify specific regimes where embedding scaling achieves a superior Pareto frontier compared to expert scaling. We systematically characterize the critical architectural factors governing this efficacy -- ranging from parameter budgeting to the interplay with model width and depth. Moreover, by integrating tailored system optimizations and speculative decoding, we effectively convert this sparsity into tangible inference speedups. Guided by these insights, we introduce LongCat-Flash-Lite, a 68.5B parameter model with ~3B activated trained from scratch. Despite allocating over 30B parameters to embeddings, LongCat-Flash-Lite not only surpasses parameter-equivalent MoE baselines but also exhibits exceptional competitiveness against existing models of comparable scale, particularly in agentic and coding domains.",
      "authors": [
        "Hong Liu",
        "Jiaqi Zhang",
        "Chao Wang",
        "Xing Hu",
        "Linkun Lyu",
        "Jiaqi Sun",
        "Xurui Yang",
        "Bo Wang",
        "Fengcun Li",
        "Yulei Qian",
        "Lingtong Si",
        "Yerui Sun",
        "Rumei Li",
        "Peng Pei",
        "Yuchen Xie",
        "Xunliang Cai"
      ],
      "published_date": "2026-01-29",
      "pdf_url": "https://arxiv.org/pdf/2601.21204v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Going Deeper with Convolutions",
      "arxiv_id": "",
      "abstract": "We propose a deep convolutional neural network architecture codenamed \"Inception\", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.",
      "authors": [
        "Christian Szegedy",
        "Wei Liu",
        "Yangqing Jia",
        "Pierre Sermanet",
        "Scott Reed",
        "Dragomir Anguelov",
        "Dumitru Erhan",
        "Vincent Vanhoucke",
        "Andrew Rabinovich"
      ],
      "published_date": "2014-09-17",
      "pdf_url": "https://arxiv.org/pdf/1409.4842v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Gradient-based learning applied to document recognition",
      "arxiv_id": null,
      "abstract": "",
      "authors": [],
      "published_date": "",
      "pdf_url": "",
      "citation_count": 58645,
      "year": 1998
    },
    {
      "title": "Regression Shrinkage and Selection via the Lasso",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "R. Tibshirani"
      ],
      "published_date": "1996",
      "pdf_url": "",
      "citation_count": 50362,
      "year": 1996
    },
    {
      "title": "LifeCLEF Plant Identification Task 2015",
      "arxiv_id": "",
      "abstract": "The LifeCLEF plant identification challenge aims at evaluating plant identification methods and systems at a very large scale, close to the conditions of a real-world biodiversity monitoring scenario. The 2015 evaluation was actually conducted on a set of more than 100K images illustrating 1000 plant species living in West Europe. The main originality of this dataset is that it was built through a large-scale participatory sensing plateform initiated in 2011 and which now involves tens of thousands of contributors. This overview presents more precisely the resources and assessments of the challenge, summarizes the approaches and systems employed by the participating research groups, and provides an analysis of the main outcomes.",
      "authors": [
        "Herve Goeau",
        "Pierre Bonnet",
        "Alexis Joly"
      ],
      "published_date": "2025-09-28",
      "pdf_url": "https://arxiv.org/pdf/2509.23891v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics",
      "arxiv_id": "",
      "abstract": "The Internet of Things (IoT) has recently proliferated in both size and complexity. Using multi-source and heterogeneous IoT data aids in providing efficient data analytics for a variety of prevalent and crucial applications. To address the privacy and security concerns raised by analyzing IoT data locally or in the cloud, distributed data analytics techniques were proposed to collect and analyze data in edge or fog devices. In this context, federated learning has been recommended as an ideal distributed machine/deep learning-based technique for edge/fog computing environments. Additionally, the data analytics results are time-sensitive; they should be generated with minimal latency and high reliability. As a result, reusing efficient architectures validated through a high number of challenging test cases would be advantageous. The work proposed here presents a solution using a microservices-based architecture that allows an IoT application to be structured as a collection of fine-grained, loosely coupled, and reusable entities. The proposed solution uses the promising capabilities of federated learning to provide intelligent microservices that ensure efficient, flexible, and extensible data analytics. This solution aims to deliver cloud calculations to the edge to reduce latency and bandwidth congestion while protecting the privacy of exchanged data. The proposed approach was validated through an IoT-malware detection and classification use case. MaleVis, a publicly available dataset, was used in the experiments to analyze and validate the proposed approach. This dataset included more than 14,000 RGB-converted images, comprising 25 malware classes and one benign class. The results showed that our proposed approach outperformed existing state-of-the-art methods in terms of detection and classification performance, with a 99.24%.",
      "authors": [
        "Safa Ben Atitallah",
        "Maha Driss",
        "Henda Ben Ghezela"
      ],
      "published_date": "2025-10-22",
      "pdf_url": "https://arxiv.org/pdf/2510.20852v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Enhancing pine wilt disease detection with synthetic data and external attention-based transformers",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Sareer Ul Amin",
        "Yonghoon Jung",
        "Muhammad Fayaz",
        "Bumsoo Kim",
        "Sanghyun Seo"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 30,
      "year": 2025
    },
    {
      "title": "A comprehensive review on YOLO versions for object detection",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Ayşe Aybilge Murat",
        "M. S. Kıran"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 27,
      "year": 2025
    },
    {
      "title": "Harnessing large vision and language models in agriculture: a review",
      "arxiv_id": null,
      "abstract": "Introduction Agriculture is a cornerstone of human society but faces significant challenges, including pests, diseases, and the need for increased production efficiency. Large models, encompassing large language models, large vision models, and multimodal large language models, have shown transformative potential in various domains. This review aims to explore the potential applications of these models in agriculture to address existing problems and improve production. Methods We conduct a systematic review of the development trajectories and key capabilities of large models. A bibliometric analysis of literature from Web of Science and arXiv is performed to quantify the current research focus and identify the gap between the potential and the application of large models in the agricultural sector. Results Our analysis confirms that agriculture is an emerging but currently underrepresented field for large model research. Nevertheless, we identify and categorize promising applications, including tailored models for agricultural question-answering, robotic automation, and advanced image analysis from remote sensing and spectral data. These applications demonstrate significant potential to solve complex, nuanced agricultural tasks. Discussion This review culminates in a pragmatic framework to guide the choice between large and traditional models, balancing data availability against deployment constraints. We also highlight critical challenges, including data acquisition, infrastructure barriers, and the significant ethical considerations for responsible deployment. We conclude that while tailored large models are poised to greatly enhance agricultural efficiency and yield, realizing this future requires a concerted effort to overcome the existing technical, infrastructural, and ethical hurdles.",
      "authors": [
        "Hongyan Zhu",
        "Shuai Qin",
        "Min Su",
        "Chengzhi Lin",
        "Anjie Li",
        "Junfeng Gao"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 22,
      "year": 2025
    },
    {
      "title": "Multi-axis vision transformer for medical image segmentation",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Abdul Rehman Khan",
        "Asifullah Khan"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 8,
      "year": 2025
    },
    {
      "title": "A comprehensive review of facial beauty prediction using deep learning techniques",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "D. E. Boukhari",
        "F. Dornaika",
        "A. Chemsa",
        "Abdelmalik Taleb-Ahmed"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 7,
      "year": 2025
    },
    {
      "title": "A systematic comparison of predictive models on the retina",
      "arxiv_id": null,
      "abstract": "Understanding the nonlinear encoding mechanisms of retinal ganglion cells (RGCs) in response to various visual stimuli presents a central challenge in neuroscience, driving the development of increasingly complex predictive models. Here, we systematically evaluate linear-nonlinear (LN) models – applying various regularization techniques – and convolutional neural networks (CNNs) of increasing depth, to predict RGC responses to white noise and natural movies. Our analysis includes publicly available datasets from marmoset and salamander retinas. We demonstrate that LN models, when equipped with appropriate inductive biases, can achieve robust predictive performance on neural responses to both white noise and natural movie stimuli. The optimal inductive biases vary substantially across datasets and stimulus types, indicating that the LN model’s performance is susceptible to these choices. This warrants care when using LN models as baselines: their performance is not fixed, and inappropriate design choices can lead to “unfair” comparisons. However, even in the optimal inductive bias scenario, CNNs consistently outperform LN models across conditions, confirming the advantage derived from their nonlinear representation capacity. Investigating cross-stimulus generalization, we observe that models trained on white noise generalize better to natural movies than vice versa. Notably, LN models exhibit a smaller performance gap between in-domain and cross-domain predictions compared to CNNs, suggesting that the nonlinear processing captured by CNNs is more stimulus-specific. Overall, this study provides valuable benchmarks and methodological insights for neuroscientists designing predictive models of retinal encoding.",
      "authors": [
        "Michaela Vystrčilová",
        "Shashwat Sridhar",
        "Max F. Burg",
        "M. Khani",
        "Dimokratis Karamanlis",
        "H. Schreyer",
        "Varsha Ramakrishna",
        "Steffen Krüppel",
        "Sören J. Zapp",
        "Matthias Mietsch",
        "T. Gollisch",
        "Alexander S. Ecker"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 6,
      "year": 2025
    },
    {
      "title": "Distinctive Image Features from Scale-Invariant Keypoints",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "D. Lowe"
      ],
      "published_date": "2004",
      "pdf_url": "",
      "citation_count": 49301,
      "year": 2004
    },
    {
      "title": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training",
      "arxiv_id": "",
      "abstract": "We present LLaVA-OneVision-1.5, a novel family of Large Multimodal Models (LMMs) that achieve state-of-the-art performance with significantly reduced computational and financial costs. Different from the existing works, LLaVA-OneVision-1.5 provides an open, efficient, and reproducible framework for building high-quality vision-language models entirely from scratch. The LLaVA-OneVision-1.5 release comprises three primary components: (1) Large-Scale Curated Datasets: We construct an 85M concept-balanced pretraining dataset LLaVA-OneVision-1.5-Mid-Traning and a meticulously curated 22M instruction dataset LLaVA-OneVision-1.5-Instruct. (2) Efficient Training Framework: We develop a complete end-to-end efficient training framework leveraging an offline parallel data packing strategy to facilitate the training of LLaVA-OneVision-1.5 within a $16,000 budget. (3) State-of-the-art Performance: Experimental results demonstrate that LLaVA-OneVision-1.5 yields exceptionally competitive performance across a broad range of downstream tasks. Specifically, LLaVA-OneVision-1.5-8B outperforms Qwen2.5-VL-7B on 18 of 27 benchmarks, and LLaVA-OneVision-1.5-4B surpasses Qwen2.5-VL-3B on all 27 benchmarks. (4) RL-based Post-training: We unlock the model's latent potential through a lightweight RL stage, effectively eliciting robust chain-of-thought reasoning to significantly boost performance on complex multimodal reasoning tasks.",
      "authors": [
        "Xiang An",
        "Yin Xie",
        "Kaicheng Yang",
        "Wenkang Zhang",
        "Xiuwei Zhao",
        "Zheng Cheng",
        "Yirui Wang",
        "Songcen Xu",
        "Changrui Chen",
        "Didi Zhu",
        "Chunsheng Wu",
        "Huajie Tan",
        "Chunyuan Li",
        "Jing Yang",
        "Jie Yu",
        "Xiyao Wang",
        "Bin Qin",
        "Yumeng Wang",
        "Zizhen Yan",
        "Ziyong Feng",
        "Ziwei Liu",
        "Bo Li",
        "Jiankang Deng"
      ],
      "published_date": "2025-09-28",
      "pdf_url": "https://arxiv.org/pdf/2509.23661v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications",
      "arxiv_id": "",
      "abstract": "Amid growing efforts to leverage advances in large language models (LLMs) and vision-language models (VLMs) for robotics, Vision-Language-Action (VLA) models have recently gained significant attention. By unifying vision, language, and action data at scale, which have traditionally been studied separately, VLA models aim to learn policies that generalise across diverse tasks, objects, embodiments, and environments. This generalisation capability is expected to enable robots to solve novel downstream tasks with minimal or no additional task-specific data, facilitating more flexible and scalable real-world deployment. Unlike previous surveys that focus narrowly on action representations or high-level model architectures, this work offers a comprehensive, full-stack review, integrating both software and hardware components of VLA systems. In particular, this paper provides a systematic review of VLAs, covering their strategy and architectural transition, architectures and building blocks, modality-specific processing techniques, and learning paradigms. In addition, to support the deployment of VLAs in real-world robotic applications, we also review commonly used robot platforms, data collection strategies, publicly available datasets, data augmentation methods, and evaluation benchmarks. Throughout this comprehensive survey, this paper aims to offer practical guidance for the robotics community in applying VLAs to real-world robotic systems. All references categorized by training approach, evaluation method, modality, and dataset are available in the table on our project website: https://vla-survey.github.io .",
      "authors": [
        "Kento Kawaharazuka",
        "Jihoon Oh",
        "Jun Yamada",
        "Ingmar Posner",
        "Yuke Zhu"
      ],
      "published_date": "2025-10-08",
      "pdf_url": "https://arxiv.org/pdf/2510.07077v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Aligning machine and human visual representations across abstraction levels",
      "arxiv_id": null,
      "abstract": "Deep neural networks have achieved success across a wide range of applications, including as models of human behaviour and neural representations in vision tasks1,2. However, neural network training and human learning differ in fundamental ways, and neural networks often fail to generalize as robustly as humans do3,4, raising questions regarding the similarity of their underlying representations. We need to determine what is missing for modern learning systems to exhibit more human-aligned behaviour. Here we highlight a key misalignment between vision models and humans: whereas human conceptual knowledge is hierarchically organized from fine- to coarse-scale distinctions (for example, ref. 5), model representations do not accurately capture all these levels of abstraction. To address this misalignment, we first train a teacher model to imitate human judgements, then transfer human-aligned structure from its representations to refine the representations of pretrained state-of-the-art vision foundation models via fine-tuning. These human-aligned models more accurately approximate human behaviour and uncertainty across a wide range of similarity tasks, including a dataset of human judgements spanning multiple levels of semantic abstractions. They also perform better on a diverse set of machine learning tasks, increasing generalization and out-of-distribution robustness. Thus, infusing neural networks with additional human knowledge yields a best-of-both-worlds representation that is both more consistent with human cognitive judgements and more practically useful, paving the way towards more robust, interpretable and human-aligned artificial intelligence systems. Aligning foundation models with human judgments enables them to more accurately approximate human behaviour and uncertainty across various levels of visual abstraction, while additionally improving their generalization performance.",
      "authors": [
        "Lukas Muttenthaler",
        "Klaus Greff",
        "Frieda Born",
        "Bernhard Spitzer",
        "Simon Kornblith",
        "M. C. Mozer",
        "Klaus-Robert Muller",
        "Thomas Unterthiner",
        "Andrew Kyle Lampinen"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 26,
      "year": 2025
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "arxiv_id": "2010.11929",
      "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly",
        "Jakob Uszkoreit",
        "Neil Houlsby"
      ],
      "published_date": "2020-10-22",
      "pdf_url": "https://arxiv.org/pdf/2010.11929v2",
      "citation_count": 56546,
      "year": 2020
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "arxiv_id": "2103.00020",
      "abstract": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",
      "authors": [
        "Alec Radford",
        "Jong Wook Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark",
        "Gretchen Krueger",
        "Ilya Sutskever"
      ],
      "published_date": "2021-02-26",
      "pdf_url": "https://arxiv.org/pdf/2103.00020v1",
      "citation_count": 42493,
      "year": 2021
    },
    {
      "title": "GENERATIVE ADVERSARIAL NETS",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Individualized Treat",
        "Jinsung Yoon"
      ],
      "published_date": "2018",
      "pdf_url": "",
      "citation_count": 39386,
      "year": 2018
    },
    {
      "title": "Improved Mean Flows: On the Challenges of Fastforward Generative Models",
      "arxiv_id": "",
      "abstract": "MeanFlow (MF) has recently been established as a framework for one-step generative modeling. However, its ``fastforward'' nature introduces key challenges in both the training objective and the guidance mechanism. First, the original MF's training target depends not only on the underlying ground-truth fields but also on the network itself. To address this issue, we recast the objective as a loss on the instantaneous velocity $v$, re-parameterized by a network that predicts the average velocity $u$. Our reformulation yields a more standard regression problem and improves the training stability. Second, the original MF fixes the classifier-free guidance scale during training, which sacrifices flexibility. We tackle this issue by formulating guidance as explicit conditioning variables, thereby retaining flexibility at test time. The diverse conditions are processed through in-context conditioning, which reduces model size and benefits performance. Overall, our $\\textbf{improved MeanFlow}$ ($\\textbf{iMF}$) method, trained entirely from scratch, achieves $\\textbf{1.72}$ FID with a single function evaluation (1-NFE) on ImageNet 256$\\times$256. iMF substantially outperforms prior methods of this kind and closes the gap with multi-step methods while using no distillation. We hope our work will further advance fastforward generative modeling as a stand-alone paradigm.",
      "authors": [
        "Zhengyang Geng",
        "Yiyang Lu",
        "Zongze Wu",
        "Eli Shechtman",
        "J. Zico Kolter",
        "Kaiming He"
      ],
      "published_date": "2025-12-01",
      "pdf_url": "https://arxiv.org/pdf/2512.02012v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training",
      "arxiv_id": "",
      "abstract": "Pixel-space generative models are often more difficult to train and generally underperform compared to their latent-space counterparts, leaving a persistent performance and efficiency gap. In this paper, we introduce a novel two-stage training framework that closes this gap for pixel-space diffusion and consistency models. In the first stage, we pre-train encoders to capture meaningful semantics from clean images while aligning them with points along the same deterministic sampling trajectory, which evolves points from the prior to the data distribution. In the second stage, we integrate the encoder with a randomly initialized decoder and fine-tune the complete model end-to-end for both diffusion and consistency models. Our framework achieves state-of-the-art (SOTA) performance on ImageNet. Specifically, our diffusion model reaches an FID of 1.58 on ImageNet-256 and 2.35 on ImageNet-512 with 75 number of function evaluations (NFE) surpassing prior pixel-space methods and VAE-based counterparts by a large margin in both generation quality and training efficiency. In a direct comparison, our model significantly outperforms DiT while using only around 30\\% of its training compute.",
      "authors": [
        "Jiachen Lei",
        "Keli Liu",
        "Julius Berner",
        "Haiming Yu",
        "Hongkai Zheng",
        "Jiahong Wu",
        "Xiangxiang Chu"
      ],
      "published_date": "2025-10-14",
      "pdf_url": "https://arxiv.org/pdf/2510.12586v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
      "arxiv_id": "2512.11749",
      "abstract": "Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain. By leveraging a standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 on GenEval and 85.78 on DPG-Bench. This performance validates the intrinsic representational power of VFMs for generative tasks. We fully open-source the project, including the autoencoder and generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation.",
      "authors": [
        "Minglei Shi",
        "Haolin Wang",
        "Borui Zhang",
        "Wenzhao Zheng",
        "Bohan Zeng",
        "Ziyang Yuan",
        "Xiaoshi Wu",
        "Yuanxing Zhang",
        "Huan Yang",
        "Xintao Wang",
        "Pengfei Wan",
        "Kun Gai",
        "Jie Zhou",
        "Jiwen Lu"
      ],
      "published_date": "2025-12-12",
      "pdf_url": "https://arxiv.org/pdf/2512.11749v1",
      "citation_count": 5,
      "year": 2025
    },
    {
      "title": "PixelDiT: Pixel Diffusion Transformers for Image Generation",
      "arxiv_id": "",
      "abstract": "Latent-space modeling has been the standard for Diffusion Transformers (DiTs). However, it relies on a two-stage pipeline where the pretrained autoencoder introduces lossy reconstruction, leading to error accumulation while hindering joint optimization. To address these issues, we propose PixelDiT, a single-stage, end-to-end model that eliminates the need for the autoencoder and learns the diffusion process directly in the pixel space. PixelDiT adopts a fully transformer-based architecture shaped by a dual-level design: a patch-level DiT that captures global semantics and a pixel-level DiT that refines texture details, enabling efficient training of a pixel-space diffusion model while preserving fine details. Our analysis reveals that effective pixel-level token modeling is essential to the success of pixel diffusion. PixelDiT achieves 1.61 FID on ImageNet 256x256, surpassing existing pixel generative models by a large margin. We further extend PixelDiT to text-to-image generation and pretrain it at the 1024x1024 resolution in pixel space. It achieves 0.74 on GenEval and 83.5 on DPG-bench, approaching the best latent diffusion models.",
      "authors": [
        "Yongsheng Yu",
        "Wei Xiong",
        "Weili Nie",
        "Yichen Sheng",
        "Shiqiu Liu",
        "Jiebo Luo"
      ],
      "published_date": "2025-11-25",
      "pdf_url": "https://arxiv.org/pdf/2511.20645v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models",
      "arxiv_id": "",
      "abstract": "Unified multimodal models (UMMs) aim to jointly perform multimodal understanding and generation within a single framework. We present TUNA, a native UMM that builds a unified continuous visual representation by cascading a VAE encoder with a representation encoder. This unified representation space allows end-to-end processing of images and videos for both understanding and generation tasks. Compared to prior UMMs with decoupled representations, TUNA's unified visual space avoids representation format mismatches introduced by separate encoders, outperforming decoupled alternatives in both understanding and generation. Moreover, we observe that stronger pretrained representation encoders consistently yield better performance across all multimodal tasks, highlighting the importance of the representation encoder. Finally, in this unified setting, jointly training on both understanding and generation data allows the two tasks to benefit from each other rather than interfere. Our extensive experiments on multimodal understanding and generation benchmarks show that TUNA achieves state-of-the-art results in image and video understanding, image and video generation, and image editing, demonstrating the effectiveness and scalability of its unified representation design.",
      "authors": [
        "Zhiheng Liu",
        "Weiming Ren",
        "Haozhe Liu",
        "Zijian Zhou",
        "Shoufa Chen",
        "Haonan Qiu",
        "Xiaoke Huang",
        "Zhaochong An",
        "Fanny Yang",
        "Aditya Patel",
        "Viktar Atliha",
        "Tony Ng",
        "Xiao Han",
        "Chuyan Zhu",
        "Chenyang Zhang",
        "Ding Liu",
        "Juan-Manuel Perez-Rua",
        "Sen He",
        "Jürgen Schmidhuber",
        "Wenhu Chen",
        "Ping Luo",
        "Wei Liu",
        "Tao Xiang",
        "Jonas Schult",
        "Yuren Cong"
      ],
      "published_date": "2025-12-01",
      "pdf_url": "https://arxiv.org/pdf/2512.02014v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "arxiv_id": "1810.04805",
      "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\n  BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "published_date": "2018-10-11",
      "pdf_url": "https://arxiv.org/pdf/1810.04805v2",
      "citation_count": 109563,
      "year": 2019
    },
    {
      "title": "A comprehensive review of object detection with traditional and deep learning methods",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Vrushali Pagire",
        "M. Chavali",
        "Ashish Kale"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 19,
      "year": 2025
    },
    {
      "title": "Diffusion Language Models are Super Data Learners",
      "arxiv_id": "",
      "abstract": "Under strictly controlled pre-training settings, we observe a Crossover: when unique data is limited, diffusion language models (DLMs) consistently surpass autoregressive (AR) models by training for more epochs. The crossover shifts later with more or higher-quality data, earlier with larger models, and persists across dense and sparse architectures. We attribute the gains to three compounding factors: (1) any-order modeling, (2) super-dense compute from iterative bidirectional denoising, and (3) built-in Monte Carlo augmentation; input or parameter noise improves AR under data constraint but cannot close the gap. At scale, a 1.7B DLM trained with a ~1.5T-token compute budget on 10B unique Python tokens overtakes an AR coder trained with strictly matched settings. In addition, a 1B-parameter DLM achieves > 56% accuracy on HellaSwag and > 33% on MMLU using only 1B tokens, without any special tricks, just by repeating standard pre-training data. We also show that rising validation cross-entropy does not imply degraded downstream performance in this regime.",
      "authors": [
        "Jinjie Ni",
        "Qian Liu",
        "Longxu Dou",
        "Chao Du",
        "Zili Wang",
        "Hang Yan",
        "Tianyu Pang",
        "Michael Qizhe Shieh"
      ],
      "published_date": "2025-11-05",
      "pdf_url": "https://arxiv.org/pdf/2511.03276v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas",
      "arxiv_id": "",
      "abstract": "Diffusion Language Models (DLMs) present a compelling alternative to autoregressive models, offering flexible, any-order infilling without specialized prompting design. However, their practical utility is blocked by a critical limitation: the requirement of a fixed-length masked sequence for generation. This constraint severely degrades code infilling performance when the predefined mask size mismatches the ideal completion length. To address this, we propose DreamOn, a novel diffusion framework that enables dynamic, variable-length generation. DreamOn augments the diffusion process with two length control states, allowing the model to autonomously expand or contract the output length based solely on its own predictions. We integrate this mechanism into existing DLMs with minimal modifications to the training objective and no architectural changes. Built upon Dream-Coder-7B and DiffuCoder-7B, DreamOn achieves infilling performance on par with state-of-the-art autoregressive models on HumanEval-Infilling and SantaCoder-FIM and matches oracle performance achieved with ground-truth length. Our work removes a fundamental barrier to the practical deployment of DLMs, significantly advancing their flexibility and applicability for variable-length generation. Our code is available at https://github.com/DreamLM/DreamOn.",
      "authors": [
        "Zirui Wu",
        "Lin Zheng",
        "Zhihui Xie",
        "Jiacheng Ye",
        "Jiahui Gao",
        "Shansan Gong",
        "Yansong Feng",
        "Zhenguo Li",
        "Wei Bi",
        "Guorui Zhou",
        "Lingpeng Kong"
      ],
      "published_date": "2026-02-01",
      "pdf_url": "https://arxiv.org/pdf/2602.01326v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "FLEX: Continuous Agent Evolution via Forward Learning from Experience",
      "arxiv_id": "",
      "abstract": "Autonomous agents driven by Large Language Models (LLMs) have revolutionized reasoning and problem-solving but remain static after training, unable to grow with experience as intelligent beings do during deployment. We introduce Forward Learning with EXperience (FLEX), a gradient-free learning paradigm that enables LLM agents to continuously evolve through accumulated experience. Specifically, FLEX cultivates scalable and inheritable evolution by constructing a structured experience library through continual reflection on successes and failures during interaction with the environment. FLEX delivers substantial improvements on mathematical reasoning, chemical retrosynthesis, and protein fitness prediction (up to 23% on AIME25, 10% on USPTO50k, and 14% on ProteinGym). We further identify a clear scaling law of experiential growth and the phenomenon of experience inheritance across agents, marking a step toward scalable and inheritable continuous agent evolution. Project Page: https://flex-gensi-thuair.github.io.",
      "authors": [
        "Zhicheng Cai",
        "Xinyuan Guo",
        "Yu Pei",
        "Jiangtao Feng",
        "Jinsong Su",
        "Jiangjie Chen",
        "Ya-Qin Zhang",
        "Wei-Ying Ma",
        "Mingxuan Wang",
        "Hao Zhou"
      ],
      "published_date": "2025-11-09",
      "pdf_url": "https://arxiv.org/pdf/2511.06449v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning",
      "arxiv_id": "",
      "abstract": "Large Language Models (LLMs) are increasingly being explored for building Agents capable of active environmental interaction (e.g., via tool use) to solve complex problems. Reinforcement Learning (RL) is considered a key technology with significant potential for training such Agents; however, the effective application of RL to LLM Agents is still in its nascent stages and faces considerable challenges. Currently, this emerging field lacks in-depth exploration into RL approaches specifically tailored for the LLM Agent context, alongside a scarcity of flexible and easily extensible training frameworks designed for this purpose. To help advance this area, this paper first revisits and clarifies Reinforcement Learning methodologies for LLM Agents by systematically extending the Markov Decision Process (MDP) framework to comprehensively define the key components of an LLM Agent. Secondly, we introduce Agent-R1, a modular, flexible, and user-friendly training framework for RL-based LLM Agents, designed for straightforward adaptation across diverse task scenarios and interactive environments. We conducted experiments on Multihop QA benchmark tasks, providing initial validation for the effectiveness of our proposed methods and framework.",
      "authors": [
        "Mingyue Cheng",
        "Jie Ouyang",
        "Shuo Yu",
        "Ruiran Yan",
        "Yucong Luo",
        "Zirui Liu",
        "Daoyu Wang",
        "Qi Liu",
        "Enhong Chen"
      ],
      "published_date": "2025-11-18",
      "pdf_url": "https://arxiv.org/pdf/2511.14460v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "nuScenes: A multimodal dataset for autonomous driving",
      "arxiv_id": "",
      "abstract": "Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image based benchmark datasets have driven development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. In this work we present nuTonomy scenes (nuScenes), the first dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. nuScenes comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. We define novel 3D detection and tracking metrics. We also provide careful dataset analysis as well as baselines for lidar and image based detection and tracking. Data, development kit and more information are available online.",
      "authors": [
        "Holger Caesar",
        "Varun Bankiti",
        "Alex H. Lang",
        "Sourabh Vora",
        "Venice Erin Liong",
        "Qiang Xu",
        "Anush Krishnan",
        "Yu Pan",
        "Giancarlo Baldan",
        "Oscar Beijbom"
      ],
      "published_date": "2019-03-26",
      "pdf_url": "https://arxiv.org/pdf/1903.11027v5",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Histograms of oriented gradients for human detection",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Navneet Dalal",
        "B. Triggs"
      ],
      "published_date": "2005",
      "pdf_url": "",
      "citation_count": 35230,
      "year": 2005
    },
    {
      "title": "Controllable Video Generation: A Survey",
      "arxiv_id": "",
      "abstract": "With the rapid development of AI-generated content (AIGC), video generation has emerged as one of its most dynamic and impactful subfields. In particular, the advancement of video generation foundation models has led to growing demand for controllable video generation methods that can more accurately reflect user intent. Most existing foundation models are designed for text-to-video generation, where text prompts alone are often insufficient to express complex, multi-modal, and fine-grained user requirements. This limitation makes it challenging for users to generate videos with precise control using current models. To address this issue, recent research has explored the integration of additional non-textual conditions, such as camera motion, depth maps, and human pose, to extend pretrained video generation models and enable more controllable video synthesis. These approaches aim to enhance the flexibility and practical applicability of AIGC-driven video generation systems. In this survey, we provide a systematic review of controllable video generation, covering both theoretical foundations and recent advances in the field. We begin by introducing the key concepts and commonly used open-source video generation models. We then focus on control mechanisms in video diffusion models, analyzing how different types of conditions can be incorporated into the denoising process to guide generation. Finally, we categorize existing methods based on the types of control signals they leverage, including single-condition generation, multi-condition generation, and universal controllable generation. For a complete list of the literature on controllable video generation reviewed, please visit our curated repository at https://github.com/mayuelala/Awesome-Controllable-Video-Generation.",
      "authors": [
        "Yue Ma",
        "Kunyu Feng",
        "Zhongyuan Hu",
        "Xinyu Wang",
        "Yucheng Wang",
        "Mingzhe Zheng",
        "Bingyuan Wang",
        "Qinghe Wang",
        "Xuanhua He",
        "Hongfa Wang",
        "Chenyang Zhu",
        "Hongyu Liu",
        "Yingqing He",
        "Zeyu Wang",
        "Zhifeng Li",
        "Xiu Li",
        "Sirui Han",
        "Yike Guo",
        "Wei Liu",
        "Dan Xu",
        "Linfeng Zhang",
        "Qifeng Chen"
      ],
      "published_date": "2025-07-22",
      "pdf_url": "https://arxiv.org/pdf/2507.16869v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Adversarial Attacks on Autonomous Driving Systems in the Physical World: A Survey",
      "arxiv_id": null,
      "abstract": "Autonomous Driving Systems (ADS) represent a revolutionary advancement in transportation and offer unprecedented safety and convenience. Real-world physical attacks are emphasized because Autonomous Driving Systems (ADS) depend heavily on sensors and perception modules to detect and interpret their surroundings, making security a critical concern. Defenders usually have the upper hand in the digital sphere while they are challenged in the physical world because attackers have greater flexibility for covert operations. A comprehensive analysis is essential for understanding attack trends, evolution, and defense directions. This paper provides a survey of state-of-the-art physical attacks that threaten ADS perception. A novel multi-label classification method is introduced to categorize these attacks along four main dimensions. Visualization and analysis of the classification enhance the understanding of these multidimensional threats. Five research directions for future exploration are also proposed.",
      "authors": [
        "Lijun Chi",
        "M. Msahli",
        "Qingjie Zhang",
        "Han Qiu",
        "Tianwei Zhang",
        "Gérard Memmi",
        "Meikang Qiu"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 20,
      "year": 2025
    },
    {
      "title": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail",
      "arxiv_id": "2511.00088",
      "abstract": "End-to-end architectures trained via imitation learning have advanced autonomous driving by scaling model size and data, yet performance remains brittle in safety-critical long-tail scenarios where supervision is sparse and causal understanding is limited. We introduce Alpamayo-R1 (AR1), a vision-language-action model (VLA) that integrates Chain of Causation reasoning with trajectory planning for complex driving scenarios. Our approach features three key innovations: (1) the Chain of Causation (CoC) dataset, built through a hybrid auto-labeling and human-in-the-loop pipeline producing decision-grounded, causally linked reasoning traces aligned with driving behaviors; (2) a modular VLA architecture combining Cosmos-Reason, a vision-language model pre-trained for Physical AI, with a diffusion-based trajectory decoder that generates dynamically feasible trajectories in real time; (3) a multi-stage training strategy using supervised fine-tuning to elicit reasoning and reinforcement learning (RL) to enforce reasoning-action consistency and optimize reasoning quality. AR1 achieves up to a 12% improvement in planning accuracy on challenging cases compared to a trajectory-only baseline, with a 35% reduction in close encounter rate in closed-loop simulation. RL post-training improves reasoning quality by 45% and reasoning-action consistency by 37%. Model scaling from 0.5B to 7B parameters shows consistent improvements. On-vehicle road tests confirm real-time performance (99 ms latency) and successful urban deployment. By bridging interpretable reasoning with precise control, AR1 demonstrates a practical path towards Level 4 autonomous driving. Model weights are available at https://huggingface.co/nvidia/Alpamayo-R1-10B with inference code at https://github.com/NVlabs/alpamayo.",
      "authors": [
        "NVIDIA",
        ":",
        "Yan Wang",
        "Wenjie Luo",
        "Junjie Bai",
        "Yulong Cao",
        "Tong Che",
        "Ke Chen",
        "Yuxiao Chen",
        "Jenna Diamond",
        "Yifan Ding",
        "Wenhao Ding",
        "Liang Feng",
        "Greg Heinrich",
        "Jack Huang",
        "Peter Karkus",
        "Boyi Li",
        "Pinyi Li",
        "Tsung-Yi Lin",
        "Dongran Liu",
        "Ming-Yu Liu",
        "Langechuan Liu",
        "Zhijian Liu",
        "Jason Lu",
        "Yunxiang Mao",
        "Pavlo Molchanov",
        "Lindsey Pavao",
        "Zhenghao Peng",
        "Mike Ranzinger",
        "Ed Schmerling",
        "Shida Shen",
        "Yunfei Shi",
        "Sarah Tariq",
        "Ran Tian",
        "Tilman Wekel",
        "Xinshuo Weng",
        "Tianjun Xiao",
        "Eric Yang",
        "Xiaodong Yang",
        "Yurong You",
        "Xiaohui Zeng",
        "Wenyuan Zhang",
        "Boris Ivanovic",
        "Marco Pavone"
      ],
      "published_date": "2025-10-30",
      "pdf_url": "https://arxiv.org/pdf/2511.00088v2",
      "citation_count": 18,
      "year": 2025
    },
    {
      "title": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes",
      "arxiv_id": "",
      "abstract": "Understanding 3D spatial relationships remains a major limitation of current Vision-Language Models (VLMs). Prior work has addressed this issue by creating spatial question-answering (QA) datasets based on single images or indoor videos. However, real-world embodied AI agents such as robots and self-driving cars typically rely on ego-centric, multi-view observations. To this end, we introduce Ego3D-Bench, a new benchmark designed to evaluate the spatial reasoning abilities of VLMs using ego-centric, multi-view outdoor data. Ego3D-Bench comprises over 8,600 QA pairs, created with significant involvement from human annotators to ensure quality and diversity. We benchmark 16 SOTA VLMs, including GPT-4o, Gemini1.5-Pro, InternVL3, and Qwen2.5-VL. Our results reveal a notable performance gap between human level scores and VLM performance, highlighting that current VLMs still fall short of human level spatial understanding. To bridge this gap, we propose Ego3D-VLM, a post-training framework that enhances 3D spatial reasoning of VLMs. Ego3D-VLM generates cognitive map based on estimated global 3D coordinates, resulting in 12% average improvement on multi-choice QA and 56% average improvement on absolute distance estimation. Ego3D-VLM is modular and can be integrated with any existing VLM. Together, Ego3D-Bench and Ego3D-VLM offer valuable tools for advancing toward human level spatial understanding in real-world, multi-view environments.",
      "authors": [
        "Mohsen Gholami",
        "Ahmad Rezaei",
        "Zhou Weimin",
        "Sitong Mao",
        "Shunbo Zhou",
        "Yong Zhang",
        "Mohammad Akbari"
      ],
      "published_date": "2025-09-08",
      "pdf_url": "https://arxiv.org/pdf/2509.06266v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Detect Anything via Next Point Prediction",
      "arxiv_id": "",
      "abstract": "Object detection has long been dominated by traditional coordinate regression-based models, such as YOLO, DETR, and Grounding DINO. Although recent efforts have attempted to leverage MLLMs to tackle this task, they face challenges like low recall rate, duplicate predictions, coordinate misalignment, etc. In this work, we bridge this gap and propose Rex-Omni, a 3B-scale MLLM that achieves state-of-the-art object perception performance. On benchmarks like COCO and LVIS, Rex-Omni attains performance comparable to or exceeding regression-based models (e.g., DINO, Grounding DINO) in a zero-shot setting. This is enabled by three key designs: 1) Task Formulation: we use special tokens to represent quantized coordinates from 0 to 999, reducing the model's learning difficulty and improving token efficiency for coordinate prediction; 2) Data Engines: we construct multiple data engines to generate high-quality grounding, referring, and pointing data, providing semantically rich supervision for training; \\3) Training Pipelines: we employ a two-stage training process, combining supervised fine-tuning on 22 million data with GRPO-based reinforcement post-training. This RL post-training leverages geometry-aware rewards to effectively bridge the discrete-to-continuous coordinate prediction gap, improve box accuracy, and mitigate undesirable behaviors like duplicate predictions that stem from the teacher-guided nature of the initial SFT stage. Beyond conventional detection, Rex-Omni's inherent language understanding enables versatile capabilities such as object referring, pointing, visual prompting, GUI grounding, spatial referring, OCR and key-pointing, all systematically evaluated on dedicated benchmarks. We believe that Rex-Omni paves the way for more versatile and language-aware visual perception systems.",
      "authors": [
        "Qing Jiang",
        "Junan Huo",
        "Xingyu Chen",
        "Yuda Xiong",
        "Zhaoyang Zeng",
        "Yihao Chen",
        "Tianhe Ren",
        "Junzhi Yu",
        "Lei Zhang"
      ],
      "published_date": "2025-10-14",
      "pdf_url": "https://arxiv.org/pdf/2510.12798v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Human-level control through deep reinforcement learning",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Volodymyr Mnih",
        "K. Kavukcuoglu",
        "David Silver",
        "Andrei A. Rusu",
        "J. Veness",
        "Marc G. Bellemare",
        "Alex Graves",
        "Martin A. Riedmiller",
        "A. Fidjeland",
        "Georg Ostrovski",
        "Stig Petersen",
        "Charlie Beattie",
        "Amir Sadik",
        "Ioannis Antonoglou",
        "Helen King",
        "D. Kumaran",
        "Daan Wierstra",
        "S. Legg",
        "D. Hassabis"
      ],
      "published_date": "2015",
      "pdf_url": "",
      "citation_count": 30344,
      "year": 2015
    },
    {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "arxiv_id": "",
      "abstract": "We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.",
      "authors": [
        "Volodymyr Mnih",
        "Adrià Puigdomènech Badia",
        "Mehdi Mirza",
        "Alex Graves",
        "Timothy P. Lillicrap",
        "Tim Harley",
        "David Silver",
        "Koray Kavukcuoglu"
      ],
      "published_date": "2016-02-04",
      "pdf_url": "https://arxiv.org/pdf/1602.01783v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation",
      "arxiv_id": "1611.06612",
      "abstract": "Recently, very deep convolutional neural networks (CNNs) have shown outstanding performance in object recognition and have also been the first choice for dense classification problems such as semantic segmentation. However, repeated subsampling operations like pooling or convolution striding in deep CNNs lead to a significant decrease in the initial image resolution. Here, we present RefineNet, a generic multi-path refinement network that explicitly exploits all the information available along the down-sampling process to enable high-resolution prediction using long-range residual connections. In this way, the deeper layers that capture high-level semantic features can be directly refined using fine-grained features from earlier convolutions. The individual components of RefineNet employ residual connections following the identity mapping mindset, which allows for effective end-to-end training. Further, we introduce chained residual pooling, which captures rich background context in an efficient manner. We carry out comprehensive experiments and set new state-of-the-art results on seven public datasets. In particular, we achieve an intersection-over-union score of 83.4 on the challenging PASCAL VOC 2012 dataset, which is the best reported result to date.",
      "authors": [
        "Guosheng Lin",
        "Anton Milan",
        "Chunhua Shen",
        "Ian Reid"
      ],
      "published_date": "2016-11-20",
      "pdf_url": "https://arxiv.org/pdf/1611.06612v3",
      "citation_count": 3054,
      "year": 2016
    },
    {
      "title": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving",
      "arxiv_id": "",
      "abstract": "Vision-Language Models (VLMs) have demonstrated significant potential for end-to-end autonomous driving. However, the field still lacks a practical platform that enables dynamic model updates, rapid validation, fair comparison, and intuitive performance assessment. To that end, we introduce LightEMMA, a Lightweight End-to-End Multimodal Model for Autonomous driving. LightEMMA provides a unified, VLM-based autonomous driving framework without ad hoc customizations, enabling easy integration with evolving state-of-the-art commercial and open-source models. We construct twelve autonomous driving agents using various VLMs and evaluate their performance on the challenging nuScenes prediction task, comprehensively assessing computational metrics and providing critical insights. Illustrative examples show that, although VLMs exhibit strong scenario interpretation capabilities, their practical performance in autonomous driving tasks remains a concern. Additionally, increased model complexity and extended reasoning do not necessarily lead to better performance, emphasizing the need for further improvements and task-specific designs. The code is available at https://github.com/michigan-traffic-lab/LightEMMA.",
      "authors": [
        "Zhijie Qiao",
        "Haowei Li",
        "Zhong Cao",
        "Henry X. Liu"
      ],
      "published_date": "2025-05-01",
      "pdf_url": "https://arxiv.org/pdf/2505.00284v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving",
      "arxiv_id": "",
      "abstract": "Recent studies have explored leveraging the world knowledge and cognitive capabilities of Vision-Language Models (VLMs) to address the long-tail problem in end-to-end autonomous driving. However, existing methods typically formulate trajectory planning as a language modeling task, where physical actions are output in the language space, potentially leading to issues such as format-violating outputs, infeasible actions, and slow inference speeds. In this paper, we propose ReCogDrive, a novel Reinforced Cognitive framework for end-to-end autonomous Driving, unifying driving understanding and planning by integrating an autoregressive model with a diffusion planner. First, to instill human driving cognition into the VLM, we introduce a hierarchical data pipeline that mimics the sequential cognitive process of human drivers through three stages: generation, refinement, and quality control. Building on this cognitive foundation, we then address the language-action mismatch by injecting the VLM's learned driving priors into a diffusion planner to efficiently generate continuous and stable trajectories. Furthermore, to enhance driving safety and reduce collisions, we introduce a Diffusion Group Relative Policy Optimization (DiffGRPO) stage, reinforcing the planner for enhanced safety and comfort. Extensive experiments on the NAVSIM and Bench2Drive benchmarks demonstrate that ReCogDrive achieves state-of-the-art performance. Additionally, qualitative results across diverse driving scenarios and DriveBench highlight the model's scene comprehension. All code, model weights, and datasets will be made publicly available to facilitate subsequent research.",
      "authors": [
        "Yongkang Li",
        "Kaixin Xiong",
        "Xiangyu Guo",
        "Fang Li",
        "Sixu Yan",
        "Gangwei Xu",
        "Lijun Zhou",
        "Long Chen",
        "Haiyang Sun",
        "Bing Wang",
        "Kun Ma",
        "Guang Chen",
        "Hangjun Ye",
        "Wenyu Liu",
        "Xinggang Wang"
      ],
      "published_date": "2025-06-09",
      "pdf_url": "https://arxiv.org/pdf/2506.08052v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Pseudo-Simulation for Autonomous Driving",
      "arxiv_id": "",
      "abstract": "Existing evaluation paradigms for Autonomous Vehicles (AVs) face critical limitations. Real-world evaluation is often challenging due to safety concerns and a lack of reproducibility, whereas closed-loop simulation can face insufficient realism or high computational costs. Open-loop evaluation, while being efficient and data-driven, relies on metrics that generally overlook compounding errors. In this paper, we propose pseudo-simulation, a novel paradigm that addresses these limitations. Pseudo-simulation operates on real datasets, similar to open-loop evaluation, but augments them with synthetic observations generated prior to evaluation using 3D Gaussian Splatting. Our key idea is to approximate potential future states the AV might encounter by generating a diverse set of observations that vary in position, heading, and speed. Our method then assigns a higher importance to synthetic observations that best match the AV's likely behavior using a novel proximity-based weighting scheme. This enables evaluating error recovery and the mitigation of causal confusion, as in closed-loop benchmarks, without requiring sequential interactive simulation. We show that pseudo-simulation is better correlated with closed-loop simulations ($R^2=0.8$) than the best existing open-loop approach ($R^2=0.7$). We also establish a public leaderboard for the community to benchmark new methodologies with pseudo-simulation. Our code is available at https://github.com/autonomousvision/navsim.",
      "authors": [
        "Wei Cao",
        "Marcel Hallgarten",
        "Tianyu Li",
        "Daniel Dauner",
        "Xunjiang Gu",
        "Caojun Wang",
        "Yakov Miron",
        "Marco Aiello",
        "Hongyang Li",
        "Igor Gilitschenski",
        "Boris Ivanovic",
        "Marco Pavone",
        "Andreas Geiger",
        "Kashyap Chitta"
      ],
      "published_date": "2025-06-04",
      "pdf_url": "https://arxiv.org/pdf/2506.04218v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving",
      "arxiv_id": "",
      "abstract": "End-to-end autonomous driving (E2E-AD) demands effective processing of multi-view sensory data and robust handling of diverse and complex driving scenarios, particularly rare maneuvers such as aggressive turns. Recent success of Mixture-of-Experts (MoE) architecture in Large Language Models (LLMs) demonstrates that specialization of parameters enables strong scalability. In this work, we propose DriveMoE, a novel MoE-based E2E-AD framework, with a Scene-Specialized Vision MoE and a Skill-Specialized Action MoE. DriveMoE is built upon our $π_0$ Vision-Language-Action (VLA) baseline (originally from the embodied AI field), called Drive-$π_0$. Specifically, we add Vision MoE to Drive-$π_0$ by training a router to select relevant cameras according to the driving context dynamically. This design mirrors human driving cognition, where drivers selectively attend to crucial visual cues rather than exhaustively processing all visual information. In addition, we add Action MoE by training another router to activate specialized expert modules for different driving behaviors. Through explicit behavioral specialization, DriveMoE is able to handle diverse scenarios without suffering from modes averaging like existing models. In Bench2Drive closed-loop evaluation experiments, DriveMoE achieves state-of-the-art (SOTA) performance, demonstrating the effectiveness of combining vision and action MoE in autonomous driving tasks. We will release our code and models of DriveMoE and Drive-$π_0$.",
      "authors": [
        "Zhenjie Yang",
        "Yilin Chai",
        "Xiaosong Jia",
        "Qifeng Li",
        "Yuqian Shao",
        "Xuekai Zhu",
        "Haisheng Su",
        "Junchi Yan"
      ],
      "published_date": "2025-05-22",
      "pdf_url": "https://arxiv.org/pdf/2505.16278v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "A Survey on Vision-Language-Action Models for Autonomous Driving",
      "arxiv_id": "",
      "abstract": "The rapid progress of multimodal large language models (MLLM) has paved the way for Vision-Language-Action (VLA) paradigms, which integrate visual perception, natural language understanding, and control within a single policy. Researchers in autonomous driving are actively adapting these methods to the vehicle domain. Such models promise autonomous vehicles that can interpret high-level instructions, reason about complex traffic scenes, and make their own decisions. However, the literature remains fragmented and is rapidly expanding. This survey offers the first comprehensive overview of VLA for Autonomous Driving (VLA4AD). We (i) formalize the architectural building blocks shared across recent work, (ii) trace the evolution from early explainer to reasoning-centric VLA models, and (iii) compare over 20 representative models according to VLA's progress in the autonomous driving domain. We also consolidate existing datasets and benchmarks, highlighting protocols that jointly measure driving safety, accuracy, and explanation quality. Finally, we detail open challenges - robustness, real-time efficiency, and formal verification - and outline future directions of VLA4AD. This survey provides a concise yet complete reference for advancing interpretable socially aligned autonomous vehicles. Github repo is available at \\href{https://github.com/JohnsonJiang1996/Awesome-VLA4AD}{SicongJiang/Awesome-VLA4AD}.",
      "authors": [
        "Sicong Jiang",
        "Zilin Huang",
        "Kangan Qian",
        "Ziang Luo",
        "Tianze Zhu",
        "Yang Zhong",
        "Yihong Tang",
        "Menglin Kong",
        "Yunlong Wang",
        "Siwen Jiao",
        "Hao Ye",
        "Zihao Sheng",
        "Xin Zhao",
        "Tuopu Wen",
        "Zheng Fu",
        "Sikai Chen",
        "Kun Jiang",
        "Diange Yang",
        "Seongjin Choi",
        "Lijun Sun"
      ],
      "published_date": "2025-06-30",
      "pdf_url": "https://arxiv.org/pdf/2506.24044v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis",
      "arxiv_id": null,
      "abstract": "The Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) statement, published in 2009, was designed to help systematic reviewers transparently report why the review was done, what the authors did, and what they found. Over the past decade, advances in systematic review methodology and terminology have necessitated an update to the guideline. The PRISMA 2020 statement replaces the 2009 statement and includes new reporting guidance that reflects advances in methods to identify, select, appraise, and synthesise studies. The structure and presentation of the items have been modified to facilitate implementation. In this article, we present the PRISMA 2020 27-item checklist, an expanded checklist that details reporting recommendations for each item, the PRISMA 2020 abstract checklist, and the revised flow diagrams for original and updated reviews.",
      "authors": [
        "M. Page",
        "J. McKenzie",
        "P. Bossuyt",
        "I. Boutron",
        "T. Hoffmann",
        "C. Mulrow",
        "Larissa Shamseer",
        "J. Tetzlaff",
        "E. Akl",
        "S. Brennan",
        "R. Chou",
        "Julie May Glanville",
        "J. Grimshaw",
        "A. Hrõbjartsson",
        "M. Lalu",
        "Tianjing Li",
        "E. Loder",
        "E. Mayo-Wilson",
        "Steve McDonald",
        "L. McGuinness",
        "L. Stewart",
        "James Thomas",
        "A. Tricco",
        "V. Welch",
        "P. Whiting",
        "D. Moher"
      ],
      "published_date": "2020",
      "pdf_url": "",
      "citation_count": 92171,
      "year": 2020
    },
    {
      "title": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond",
      "arxiv_id": "",
      "abstract": "General world models represent a crucial pathway toward achieving Artificial General Intelligence (AGI), serving as the cornerstone for various applications ranging from virtual environments to decision-making systems. Recently, the emergence of the Sora model has attained significant attention due to its remarkable simulation capabilities, which exhibits an incipient comprehension of physical laws. In this survey, we embark on a comprehensive exploration of the latest advancements in world models. Our analysis navigates through the forefront of generative methodologies in video generation, where world models stand as pivotal constructs facilitating the synthesis of highly realistic visual content. Additionally, we scrutinize the burgeoning field of autonomous-driving world models, meticulously delineating their indispensable role in reshaping transportation and urban mobility. Furthermore, we delve into the intricacies inherent in world models deployed within autonomous agents, shedding light on their profound significance in enabling intelligent interactions within dynamic environmental contexts. At last, we examine challenges and limitations of world models, and discuss their potential future directions. We hope this survey can serve as a foundational reference for the research community and inspire continued innovation. This survey will be regularly updated at: https://github.com/GigaAI-research/General-World-Models-Survey.",
      "authors": [
        "Zheng Zhu",
        "Xiaofeng Wang",
        "Wangbo Zhao",
        "Chen Min",
        "Bohan Li",
        "Nianchen Deng",
        "Min Dou",
        "Yuqi Wang",
        "Botian Shi",
        "Kai Wang",
        "Chi Zhang",
        "Yang You",
        "Zhaoxiang Zhang",
        "Dawei Zhao",
        "Liang Xiao",
        "Jian Zhao",
        "Jiwen Lu",
        "Guan Huang"
      ],
      "published_date": "2024-05-06",
      "pdf_url": "https://arxiv.org/pdf/2405.03520v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Understanding World or Predicting Future? A Comprehensive Survey of World Models",
      "arxiv_id": "",
      "abstract": "The concept of world models has garnered significant attention due to advancements in multimodal large language models such as GPT-4 and video generation models such as Sora, which are central to the pursuit of artificial general intelligence. This survey offers a comprehensive review of the literature on world models. Generally, world models are regarded as tools for either understanding the present state of the world or predicting its future dynamics. This review presents a systematic categorization of world models, emphasizing two primary functions: (1) constructing internal representations to understand the mechanisms of the world, and (2) predicting future states to simulate and guide decision-making. Initially, we examine the current progress in these two categories. We then explore the application of world models in key domains, including generative games, autonomous driving, robotics, and social simulacra, with a focus on how each domain utilizes these aspects. Finally, we outline key challenges and provide insights into potential future research directions. We summarize the representative papers along with their code repositories in https://github.com/tsinghua-fib-lab/World-Model.",
      "authors": [
        "Jingtao Ding",
        "Yunke Zhang",
        "Yu Shang",
        "Jie Feng",
        "Yuheng Zhang",
        "Zefang Zong",
        "Yuan Yuan",
        "Hongyuan Su",
        "Nian Li",
        "Jinghua Piao",
        "Yucheng Deng",
        "Nicholas Sukiennik",
        "Chen Gao",
        "Fengli Xu",
        "Yong Li"
      ],
      "published_date": "2024-11-21",
      "pdf_url": "https://arxiv.org/pdf/2411.14499v4",
      "citation_count": null,
      "year": null
    },
    {
      "title": "A Survey of Multimodal Learning: Methods, Applications, and Future",
      "arxiv_id": null,
      "abstract": "The multimodal interplay of the five fundamental senses—Sight, Hearing, Smell, Taste, and Touch—provides humans with superior environmental perception and learning skills. Adapted from the human perceptual system, multimodal machine learning tries to incorporate different forms of input, such as image, audio, and text, and determine their fundamental connections through joint modeling. As one of the future development forms of artificial intelligence, it is necessary to summarize the progress of multimodal machine learning. In this article, we start with the form of a multimodal combination and provide a comprehensive survey of the emerging subject of multimodal machine learning, covering representative research approaches, the most recent advancements, and their applications. Specifically, this article analyzes the relationship between different modalities in detail and sorts out the key issues in multimodal research from the application scenarios. Besides, we thoroughly reviewed state-of-the-art methods and datasets covered in multimodal learning research. We then identify the substantial challenges and potential developing directions in this field. Finally, given the comprehensive nature of this survey, both modality-specific and task-specific researchers can benefit from this survey and advance the field.",
      "authors": [
        "Yuan Yuan",
        "Zhaojian Li",
        "Bin Zhao"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 50,
      "year": 2025
    },
    {
      "title": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey",
      "arxiv_id": "",
      "abstract": "The rapid evolution of multimodal foundation models has led to significant advancements in cross-modal understanding and generation across diverse modalities, including text, images, audio, and video. However, these models remain susceptible to jailbreak attacks, which can bypass built-in safety mechanisms and induce the production of potentially harmful content. Consequently, understanding the methods of jailbreak attacks and existing defense mechanisms is essential to ensure the safe deployment of multimodal generative models in real-world scenarios, particularly in security-sensitive applications. To provide comprehensive insight into this topic, this survey reviews jailbreak and defense in multimodal generative models. First, given the generalized lifecycle of multimodal jailbreak, we systematically explore attacks and corresponding defense strategies across four levels: input, encoder, generator, and output. Based on this analysis, we present a detailed taxonomy of attack methods, defense mechanisms, and evaluation frameworks specific to multimodal generative models. Additionally, we cover a wide range of input-output configurations, including modalities such as Any-to-Text, Any-to-Vision, and Any-to-Any within generative systems. Finally, we highlight current research challenges and propose potential directions for future research. The open-source repository corresponding to this work can be found at https://github.com/liuxuannan/Awesome-Multimodal-Jailbreak.",
      "authors": [
        "Xuannan Liu",
        "Xing Cui",
        "Peipei Li",
        "Zekun Li",
        "Huaibo Huang",
        "Shuhan Xia",
        "Miaoxuan Zhang",
        "Yueying Zou",
        "Ran He"
      ],
      "published_date": "2024-11-14",
      "pdf_url": "https://arxiv.org/pdf/2411.09259v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Effects of Generative AI in Tourism Industry",
      "arxiv_id": null,
      "abstract": "In the dynamic and evolving tourism industry, engaging with stakeholders is essential for fostering innovation and improving service quality. However, tourism companies often struggle to meet expectations for customer satisfaction through interactivity and real-time feedback. While new digital technologies can address the challenge of providing personalized travel experiences, they can also increase the workload for travel agencies due to the maintenance and updates required to keep travel details current. Intelligent chatbots and other generative artificial intelligence (GAI) tools can help mitigate these obstacles by transforming tourism and travel-related services, offering interactive guidance for both tourism companies and travelers. In this study, we explore and compare the main characteristics of existing responsive AI instruments applicable in tourism and hospitality scenarios. Then, we propose a new theoretical framework for decision making in the tourism industry, integrating GAI technologies to enable agencies to create and manage itineraries, and tourists to interact online with these innovative instruments. The advantages of the proposed framework are as follows: (1) providing a comprehensive understanding of the transformative potential of new generation AI tools in tourism and facilitating their effective implementation; (2) offering a holistic methodology to enhance the tourist experience; (3) unifying the applications of contemporary AI instruments in tourism activities and paving the way for their further development. The study contributes to the expanding literature on tourism modernization and offers recommendations for industry practitioners, consumers, and local, regional, and national tourism bodies to adopt a more user-centric approach to enhancing travel services.",
      "authors": [
        "Galina Ilieva",
        "Tania Yankova",
        "Stanislava Klisarova-Belcheva"
      ],
      "published_date": "2024",
      "pdf_url": "",
      "citation_count": 25,
      "year": 2024
    },
    {
      "title": "Squeeze-and-Excitation Networks",
      "arxiv_id": "",
      "abstract": "The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the \"Squeeze-and-Excitation\" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of ~25%. Models and code are available at https://github.com/hujie-frank/SENet.",
      "authors": [
        "Jie Hu",
        "Li Shen",
        "Samuel Albanie",
        "Gang Sun",
        "Enhua Wu"
      ],
      "published_date": "2017-09-05",
      "pdf_url": "https://arxiv.org/pdf/1709.01507v4",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Decoupled Weight Decay Regularization",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "I. Loshchilov",
        "F. Hutter"
      ],
      "published_date": "2017",
      "pdf_url": "",
      "citation_count": 30257,
      "year": 2017
    },
    {
      "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
      "arxiv_id": "",
      "abstract": "Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.\n  To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.",
      "authors": [
        "Mingxing Tan",
        "Quoc V. Le"
      ],
      "published_date": "2019-05-28",
      "pdf_url": "https://arxiv.org/pdf/1905.11946v5",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control",
      "arxiv_id": "",
      "abstract": "Recent advances in illumination control extend image-based methods to video, yet still facing a trade-off between lighting fidelity and temporal consistency. Moving beyond relighting, a key step toward generative modeling of real-world scenes is the joint control of camera trajectory and illumination, since visual dynamics are inherently shaped by both geometry and lighting. To this end, we present Light-X, a video generation framework that enables controllable rendering from monocular videos with both viewpoint and illumination control. 1) We propose a disentangled design that decouples geometry and lighting signals: geometry and motion are captured via dynamic point clouds projected along user-defined camera trajectories, while illumination cues are provided by a relit frame consistently projected into the same geometry. These explicit, fine-grained cues enable effective disentanglement and guide high-quality illumination. 2) To address the lack of paired multi-view and multi-illumination videos, we introduce Light-Syn, a degradation-based pipeline with inverse-mapping that synthesizes training pairs from in-the-wild monocular footage. This strategy yields a dataset covering static, dynamic, and AI-generated scenes, ensuring robust training. Extensive experiments show that Light-X outperforms baseline methods in joint camera-illumination control and surpasses prior video relighting methods under both text- and background-conditioned settings.",
      "authors": [
        "Tianqi Liu",
        "Zhaoxi Chen",
        "Zihao Huang",
        "Shaocong Xu",
        "Saining Zhang",
        "Chongjie Ye",
        "Bohan Li",
        "Zhiguo Cao",
        "Wei Li",
        "Hao Zhao",
        "Ziwei Liu"
      ],
      "published_date": "2025-12-04",
      "pdf_url": "https://arxiv.org/pdf/2512.05115v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World",
      "arxiv_id": "",
      "abstract": "World models have become crucial for autonomous driving, as they learn how scenarios evolve over time to address the long-tail challenges of the real world. However, current approaches relegate world models to limited roles: they operate within ostensibly unified architectures that still keep world prediction and motion planning as decoupled processes. To bridge this gap, we propose DriveLaW, a novel paradigm that unifies video generation and motion planning. By directly injecting the latent representation from its video generator into the planner, DriveLaW ensures inherent consistency between high-fidelity future generation and reliable trajectory planning. Specifically, DriveLaW consists of two core components: DriveLaW-Video, our powerful world model that generates high-fidelity forecasting with expressive latent representations, and DriveLaW-Act, a diffusion planner that generates consistent and reliable trajectories from the latent of DriveLaW-Video, with both components optimized by a three-stage progressive training strategy. The power of our unified paradigm is demonstrated by new state-of-the-art results across both tasks. DriveLaW not only advances video prediction significantly, surpassing best-performing work by 33.3% in FID and 1.8% in FVD, but also achieves a new record on the NAVSIM planning benchmark.",
      "authors": [
        "Tianze Xia",
        "Yongkang Li",
        "Lijun Zhou",
        "Jingfeng Yao",
        "Kaixin Xiong",
        "Haiyang Sun",
        "Bing Wang",
        "Kun Ma",
        "Guang Chen",
        "Hangjun Ye",
        "Wenyu Liu",
        "Xinggang Wang"
      ],
      "published_date": "2025-12-29",
      "pdf_url": "https://arxiv.org/pdf/2512.23421v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "DVGT: Driving Visual Geometry Transformer",
      "arxiv_id": "",
      "abstract": "Perceiving and reconstructing 3D scene geometry from visual inputs is crucial for autonomous driving. However, there still lacks a driving-targeted dense geometry perception model that can adapt to different scenarios and camera configurations. To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed multi-view visual inputs. We first extract visual features for each image using a DINO backbone, and employ alternating intra-view local attention, cross-view spatial attention, and cross-frame temporal attention to infer geometric relations across images. We then use multiple heads to decode a global point map in the ego coordinate of the first frame and the ego poses for each frame. Unlike conventional methods that rely on precise camera parameters, DVGT is free of explicit 3D geometric priors, enabling flexible processing of arbitrary camera configurations. DVGT directly predicts metric-scaled geometry from image sequences, eliminating the need for post-alignment with external sensors. Trained on a large mixture of driving datasets including nuScenes, OpenScene, Waymo, KITTI, and DDAD, DVGT significantly outperforms existing models on various scenarios. Code is available at https://github.com/wzzheng/DVGT.",
      "authors": [
        "Sicheng Zuo",
        "Zixun Xie",
        "Wenzhao Zheng",
        "Shaoqing Xu",
        "Fang Li",
        "Shengyin Jiang",
        "Long Chen",
        "Zhi-Xin Yang",
        "Jiwen Lu"
      ],
      "published_date": "2025-12-18",
      "pdf_url": "https://arxiv.org/pdf/2512.16919v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Adding Conditional Control to Text-to-Image Diffusion Models",
      "arxiv_id": "",
      "abstract": "We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with \"zero convolutions\" (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, eg, edges, depth, segmentation, human pose, etc, with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small (<50k) and large (>1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models.",
      "authors": [
        "Lvmin Zhang",
        "Anyi Rao",
        "Maneesh Agrawala"
      ],
      "published_date": "2023-02-10",
      "pdf_url": "https://arxiv.org/pdf/2302.05543v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Scaling Instruction-Finetuned Language Models",
      "arxiv_id": "",
      "abstract": "Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.",
      "authors": [
        "Hyung Won Chung",
        "Le Hou",
        "Shayne Longpre",
        "Barret Zoph",
        "Yi Tay",
        "William Fedus",
        "Yunxuan Li",
        "Xuezhi Wang",
        "Mostafa Dehghani",
        "Siddhartha Brahma",
        "Albert Webson",
        "Shixiang Shane Gu",
        "Zhuyun Dai",
        "Mirac Suzgun",
        "Xinyun Chen",
        "Aakanksha Chowdhery",
        "Alex Castro-Ros",
        "Marie Pellat",
        "Kevin Robinson",
        "Dasha Valter",
        "Sharan Narang",
        "Gaurav Mishra",
        "Adams Yu",
        "Vincent Zhao",
        "Yanping Huang",
        "Andrew Dai",
        "Hongkun Yu",
        "Slav Petrov",
        "Ed H. Chi",
        "Jeff Dean",
        "Jacob Devlin",
        "Adam Roberts",
        "Denny Zhou",
        "Quoc V. Le",
        "Jason Wei"
      ],
      "published_date": "2022-10-20",
      "pdf_url": "https://arxiv.org/pdf/2210.11416v5",
      "citation_count": null,
      "year": null
    },
    {
      "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
      "arxiv_id": "",
      "abstract": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .",
      "authors": [
        "Robin Rombach",
        "Andreas Blattmann",
        "Dominik Lorenz",
        "Patrick Esser",
        "Björn Ommer"
      ],
      "published_date": "2021-12-20",
      "pdf_url": "https://arxiv.org/pdf/2112.10752v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "AUTO-ENCODING VARIATIONAL BAYES",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Romain Lopez",
        "Pierre Boyeau",
        "N. Yosef",
        "Michael I. Jordan",
        "J. Regier"
      ],
      "published_date": "2020",
      "pdf_url": "",
      "citation_count": 21125,
      "year": 2020
    },
    {
      "title": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
      "arxiv_id": "",
      "abstract": "While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called \"perceptual losses\"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.",
      "authors": [
        "Richard Zhang",
        "Phillip Isola",
        "Alexei A. Efros",
        "Eli Shechtman",
        "Oliver Wang"
      ],
      "published_date": "2018-01-11",
      "pdf_url": "https://arxiv.org/pdf/1801.03924v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset",
      "arxiv_id": "",
      "abstract": "The research community has increasing interest in autonomous driving research, despite the resource intensity of obtaining representative real world data. Existing self-driving datasets are limited in the scale and variation of the environments they capture, even though generalization within and between operating regions is crucial to the overall viability of the technology. In an effort to help align the research community's contributions with real-world self-driving problems, we introduce a new large scale, high quality, diverse dataset. Our new dataset consists of 1150 scenes that each span 20 seconds, consisting of well synchronized and calibrated high quality LiDAR and camera data captured across a range of urban and suburban geographies. It is 15x more diverse than the largest camera+LiDAR dataset available based on our proposed diversity metric. We exhaustively annotated this data with 2D (camera image) and 3D (LiDAR) bounding boxes, with consistent identifiers across frames. Finally, we provide strong baselines for 2D as well as 3D detection and tracking tasks. We further study the effects of dataset size and generalization across geographies on 3D detection methods. Find data, code and more up-to-date information at http://www.waymo.com/open.",
      "authors": [
        "Pei Sun",
        "Henrik Kretzschmar",
        "Xerxes Dotiwalla",
        "Aurelien Chouard",
        "Vijaysai Patnaik",
        "Paul Tsui",
        "James Guo",
        "Yin Zhou",
        "Yuning Chai",
        "Benjamin Caine",
        "Vijay Vasudevan",
        "Wei Han",
        "Jiquan Ngiam",
        "Hang Zhao",
        "Aleksei Timofeev",
        "Scott Ettinger",
        "Maxim Krivokon",
        "Amy Gao",
        "Aditya Joshi",
        "Sheng Zhao",
        "Shuyang Cheng",
        "Yu Zhang",
        "Jonathon Shlens",
        "Zhifeng Chen",
        "Dragomir Anguelov"
      ],
      "published_date": "2019-12-10",
      "pdf_url": "https://arxiv.org/pdf/1912.04838v7",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Generative Adversarial Networks",
      "arxiv_id": "",
      "abstract": "Generative Adversarial Networks (GANs) are very popular frameworks for generating high-quality data, and are immensely used in both the academia and industry in many domains. Arguably, their most substantial impact has been in the area of computer vision, where they achieve state-of-the-art image generation. This chapter gives an introduction to GANs, by discussing their principle mechanism and presenting some of their inherent problems during training and evaluation. We focus on these three issues: (1) mode collapse, (2) vanishing gradients, and (3) generation of low-quality images. We then list some architecture-variant and loss-variant GANs that remedy the above challenges. Lastly, we present two utilization examples of GANs for real-world applications: Data augmentation and face images generation.",
      "authors": [
        "Gilad Cohen",
        "Raja Giryes"
      ],
      "published_date": "2022-03-01",
      "pdf_url": "https://arxiv.org/pdf/2203.00667v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Generalization of Diffusion Models Arises with a Balanced Representation Space",
      "arxiv_id": "",
      "abstract": "Diffusion models excel at generating high-quality, diverse samples, yet they risk memorizing training data when overfit to the training objective. We analyze the distinctions between memorization and generalization in diffusion models through the lens of representation learning. By investigating a two-layer ReLU denoising autoencoder (DAE), we prove that (i) memorization corresponds to the model storing raw training samples in the learned weights for encoding and decoding, yielding localized \"spiky\" representations, whereas (ii) generalization arises when the model captures local data statistics, producing \"balanced\" representations. Furthermore, we validate these theoretical findings on real-world unconditional and text-to-image diffusion models, demonstrating that the same representation structures emerge in deep generative models with significant practical implications. Building on these insights, we propose a representation-based method for detecting memorization and a training-free editing technique that allows precise control via representation steering. Together, our results highlight that learning good representations is central to novel and meaningful generative modeling.",
      "authors": [
        "Zekai Zhang",
        "Xiao Li",
        "Xiang Li",
        "Lianghe Shi",
        "Meng Wu",
        "Molei Tao",
        "Qing Qu"
      ],
      "published_date": "2025-12-24",
      "pdf_url": "https://arxiv.org/pdf/2512.20963v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Stable Velocity: A Variance Perspective on Flow Matching",
      "arxiv_id": "",
      "abstract": "While flow matching is elegant, its reliance on single-sample conditional velocities leads to high-variance training targets that destabilize optimization and slow convergence. By explicitly characterizing this variance, we identify 1) a high-variance regime near the prior, where optimization is challenging, and 2) a low-variance regime near the data distribution, where conditional and marginal velocities nearly coincide. Leveraging this insight, we propose Stable Velocity, a unified framework that improves both training and sampling. For training, we introduce Stable Velocity Matching (StableVM), an unbiased variance-reduction objective, along with Variance-Aware Representation Alignment (VA-REPA), which adaptively strengthen auxiliary supervision in the low-variance regime. For inference, we show that dynamics in the low-variance regime admit closed-form simplifications, enabling Stable Velocity Sampling (StableVS), a finetuning-free acceleration. Extensive experiments on ImageNet $256\\times256$ and large pretrained text-to-image and text-to-video models, including SD3.5, Flux, Qwen-Image, and Wan2.2, demonstrate consistent improvements in training efficiency and more than $2\\times$ faster sampling within the low-variance regime without degrading sample quality. Our code is available at https://github.com/linYDTHU/StableVelocity.",
      "authors": [
        "Donglin Yang",
        "Yongxing Zhang",
        "Xin Yu",
        "Liang Hou",
        "Xin Tao",
        "Pengfei Wan",
        "Xiaojuan Qi",
        "Renjie Liao"
      ],
      "published_date": "2026-02-05",
      "pdf_url": "https://arxiv.org/pdf/2602.05435v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Laminating Representation Autoencoders for Efficient Diffusion",
      "arxiv_id": "",
      "abstract": "Recent work has shown that diffusion models can generate high-quality images by operating directly on SSL patch features rather than pixel-space latents. However, the dense patch grids from encoders like DINOv2 contain significant redundancy, making diffusion needlessly expensive. We introduce FlatDINO, a variational autoencoder that compresses this representation into a one-dimensional sequence of just 32 continuous tokens -an 8x reduction in sequence length and 48x compression in total dimensionality. On ImageNet 256x256, a DiT-XL trained on FlatDINO latents achieves a gFID of 1.80 with classifier-free guidance while requiring 8x fewer FLOPs per forward pass and up to 4.5x fewer FLOPs per training step compared to diffusion on uncompressed DINOv2 features. These are preliminary results and this work is in progress.",
      "authors": [
        "Ramón Calvo-González",
        "François Fleuret"
      ],
      "published_date": "2026-02-04",
      "pdf_url": "https://arxiv.org/pdf/2602.04873v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Adaptive 1D Video Diffusion Autoencoder",
      "arxiv_id": "",
      "abstract": "Recent video generation models largely rely on video autoencoders that compress pixel-space videos into latent representations. However, existing video autoencoders suffer from three major limitations: (1) fixed-rate compression that wastes tokens on simple videos, (2) inflexible CNN architectures that prevent variable-length latent modeling, and (3) deterministic decoders that struggle to recover appropriate details from compressed latents. To address these issues, we propose One-Dimensional Diffusion Video Autoencoder (One-DVA), a transformer-based framework for adaptive 1D encoding and diffusion-based decoding. The encoder employs query-based vision transformers to extract spatiotemporal features and produce latent representations, while a variable-length dropout mechanism dynamically adjusts the latent length. The decoder is a pixel-space diffusion transformer that reconstructs videos with the latents as input conditions. With a two-stage training strategy, One-DVA achieves performance comparable to 3D-CNN VAEs on reconstruction metrics at identical compression ratios. More importantly, it supports adaptive compression and thus can achieve higher compression ratios. To better support downstream latent generation, we further regularize the One-DVA latent distribution for generative modeling and fine-tune its decoder to mitigate artifacts caused by the generation process.",
      "authors": [
        "Yao Teng",
        "Minxuan Lin",
        "Xian Liu",
        "Shuai Wang",
        "Xiao Yang",
        "Xihui Liu"
      ],
      "published_date": "2026-02-04",
      "pdf_url": "https://arxiv.org/pdf/2602.04220v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Test-Time Conditioning with Representation-Aligned Visual Features",
      "arxiv_id": "",
      "abstract": "While representation alignment with self-supervised models has been shown to improve diffusion model training, its potential for enhancing inference-time conditioning remains largely unexplored. We introduce Representation-Aligned Guidance (REPA-G), a framework that leverages these aligned representations, with rich semantic properties, to enable test-time conditioning from features in generation. By optimizing a similarity objective (the potential) at inference, we steer the denoising process toward a conditioned representation extracted from a pre-trained feature extractor. Our method provides versatile control at multiple scales, ranging from fine-grained texture matching via single patches to broad semantic guidance using global image feature tokens. We further extend this to multi-concept composition, allowing for the faithful combination of distinct concepts. REPA-G operates entirely at inference time, offering a flexible and precise alternative to often ambiguous text prompts or coarse class labels. We theoretically justify how this guidance enables sampling from the potential-induced tilted distribution. Quantitative results on ImageNet and COCO demonstrate that our approach achieves high-quality, diverse generations. Code is available at https://github.com/valeoai/REPA-G.",
      "authors": [
        "Nicolas Sereyjol-Garros",
        "Ellington Kirby",
        "Victor Letzelter",
        "Victor Besnier",
        "Nermin Samet"
      ],
      "published_date": "2026-02-03",
      "pdf_url": "https://arxiv.org/pdf/2602.03753v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Bio-inspired fine-tuning for selective transfer learning in image classification",
      "arxiv_id": "",
      "abstract": "Deep learning has significantly advanced image analysis across diverse domains but often depends on large, annotated datasets for success. Transfer learning addresses this challenge by utilizing pre-trained models to tackle new tasks with limited labeled data. However, discrepancies between source and target domains can hinder effective transfer learning. We introduce BioTune, a novel adaptive fine-tuning technique utilizing evolutionary optimization. BioTune enhances transfer learning by optimally choosing which layers to freeze and adjusting learning rates for unfrozen layers. Through extensive evaluation on nine image classification datasets, spanning natural and specialized domains such as medical imaging, BioTune demonstrates superior accuracy and efficiency over state-of-the-art fine-tuning methods, including AutoRGN and LoRA, highlighting its adaptability to various data characteristics and distribution changes. Additionally, BioTune consistently achieves top performance across four different CNN architectures, underscoring its flexibility. Ablation studies provide valuable insights into the impact of BioTune's key components on overall performance. The source code is available at https://github.com/davilac/BioTune.",
      "authors": [
        "Ana Davila",
        "Jacinto Colan",
        "Yasuhisa Hasegawa"
      ],
      "published_date": "2026-01-16",
      "pdf_url": "https://arxiv.org/pdf/2601.11235v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Densely Connected Convolutional Networks",
      "arxiv_id": "",
      "abstract": "Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .",
      "authors": [
        "Gao Huang",
        "Zhuang Liu",
        "Laurens van der Maaten",
        "Kilian Q. Weinberger"
      ],
      "published_date": "2016-08-25",
      "pdf_url": "https://arxiv.org/pdf/1608.06993v5",
      "citation_count": null,
      "year": null
    },
    {
      "title": "An extratropical cyclone center location method on satellite images based on transfer learning",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Zhongyu Zeng",
        "Xuan Peng"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 0,
      "year": 2026
    },
    {
      "title": "An Advanced Convolutional Neural Network Architecture Utilizing Transfer Learning for Melanoma Detection",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "J. B. Awotunde",
        "Korede Israel Adeyanju",
        "Kehinde Elisha Akerele",
        "Oluwatobi Akinlade",
        "S. Folorunso",
        "S. Ajagbe"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 0,
      "year": 2025
    },
    {
      "title": "Adaptive Transfer Learning for Surgical Tool Presence Detection in Laparoscopic Videos Through Gradual Freezing Fine‐Tuning",
      "arxiv_id": "2510.15372",
      "abstract": "Minimally invasive surgery can benefit significantly from automated surgical tool detection, enabling advanced analysis and assistance. However, the limited availability of annotated data in surgical settings poses a challenge for training robust deep learning models. This paper introduces a novel staged adaptive fine-tuning approach consisting of two steps: a linear probing stage to condition additional classification layers on a pre-trained CNN-based architecture and a gradual freezing stage to dynamically reduce the fine-tunable layers, aiming to regulate adaptation to the surgical domain. This strategy reduces network complexity and improves efficiency, requiring only a single training loop and eliminating the need for multiple iterations. We validated our method on the Cholec80 dataset, employing CNN architectures (ResNet-50 and DenseNet-121) pre-trained on ImageNet for detecting surgical tools in cholecystectomy endoscopic videos. Our results demonstrate that our method improves detection performance compared to existing approaches and established fine-tuning techniques, achieving a mean average precision (mAP) of 96.4%. To assess its broader applicability, the generalizability of the fine-tuning strategy was further confirmed on the CATARACTS dataset, a distinct domain of minimally invasive ophthalmic surgery. These findings suggest that gradual freezing fine-tuning is a promising technique for improving tool presence detection in diverse surgical procedures and may have broader applications in general image classification tasks.",
      "authors": [
        "Ana Davila",
        "Jacinto Colan",
        "Yasuhisa Hasegawa"
      ],
      "published_date": "2025-10-17",
      "pdf_url": "https://arxiv.org/pdf/2510.15372v1",
      "citation_count": 0,
      "year": 2025
    },
    {
      "title": "VGG Induced Deep Hand Sign Language Detection",
      "arxiv_id": "",
      "abstract": "Hand gesture recognition is an important aspect of human-computer interaction. It forms the basis of sign language for the visually impaired people. This work proposes a novel hand gesture recognizing system for the differently-abled persons. The model uses a convolutional neural network, known as VGG-16 net, for building a trained model on a widely used image dataset by employing Python and Keras libraries. Furthermore, the result is validated by the NUS dataset, consisting of 10 classes of hand gestures, fed to the model as the validation set. Afterwards, a testing dataset of 10 classes is built by employing Google's open source Application Programming Interface (API) that captures different gestures of human hand and the efficacy is then measured by carrying out experiments. The experimental results show that by combining a transfer learning mechanism together with the image data augmentation, the VGG-16 net produced around 98% accuracy.",
      "authors": [
        "Subham Sharma",
        "Sharmila Subudhi"
      ],
      "published_date": "2026-01-13",
      "pdf_url": "https://arxiv.org/pdf/2601.08262v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "MediaPipe: A Framework for Building Perception Pipelines",
      "arxiv_id": "",
      "abstract": "Building applications that perceive the world around them is challenging. A developer needs to (a) select and develop corresponding machine learning algorithms and models, (b) build a series of prototypes and demos, (c) balance resource consumption against the quality of the solutions, and finally (d) identify and mitigate problematic cases. The MediaPipe framework addresses all of these challenges. A developer can use MediaPipe to build prototypes by combining existing perception components, to advance them to polished cross-platform applications and measure system performance and resource consumption on target platforms. We show that these features enable a developer to focus on the algorithm or model development and use MediaPipe as an environment for iteratively improving their application with results reproducible across different devices and platforms. MediaPipe will be open-sourced at https://github.com/google/mediapipe.",
      "authors": [
        "Camillo Lugaresi",
        "Jiuqiang Tang",
        "Hadon Nash",
        "Chris McClanahan",
        "Esha Uboweja",
        "Michael Hays",
        "Fan Zhang",
        "Chuo-Ling Chang",
        "Ming Guang Yong",
        "Juhyun Lee",
        "Wan-Teh Chang",
        "Wei Hua",
        "Manfred Georg",
        "Matthias Grundmann"
      ],
      "published_date": "2019-06-14",
      "pdf_url": "https://arxiv.org/pdf/1906.08172v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Hand Pose Estimation and Hand Shape Classification Using Multi-layered Randomized Decision Forests",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Cem Keskin",
        "Mustafa Furkan Kıraç",
        "Yunus Emre Kara",
        "L. Akarun"
      ],
      "published_date": "2012",
      "pdf_url": "",
      "citation_count": 322,
      "year": 2012
    },
    {
      "title": "A robust static hand gesture recognition system using geometry based normalizations and Krawtchouk moments",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "S. P. Priyal",
        "P. Bora"
      ],
      "published_date": "2013",
      "pdf_url": "",
      "citation_count": 133,
      "year": 2013
    },
    {
      "title": "Hand signal classification system for sign language communication in Virtual Reality",
      "arxiv_id": null,
      "abstract": "Sign language is an essential form of communication for people with hearing disabilities. However, effective communication traditionally required the use of trained translators. With recent advances in AI and virtual reality, technology seems to provide a viable means of facilitating communication with sign-language users. This paper describes a solution developed around the detection and classification of the user’s hand signals and shows its integration within a system meant to facilitate communication with hearing impaired individuals. The system relies on the capabilities of a virtual reality headset to capture the user’s hand signals and then employs a machine learning model in order to correctly classify them. Within this paper, we discuss some of the relevant technical aspects of the system as well as provide insight into its observed capabilities.",
      "authors": [
        "Octavian Dudas",
        "C. Nandra",
        "C. Mocan",
        "D. Gorgan"
      ],
      "published_date": "2023",
      "pdf_url": "",
      "citation_count": 1,
      "year": 2023
    },
    {
      "title": "Real-Time Static Hand Sign Recognition System using MediaPipe and Fully Connected Neural Network",
      "arxiv_id": null,
      "abstract": "About 5% of the world's population faces the challenges of being deaf and mute, according to the World Health Organization. This highlights the urgent need for communication technologies to connect individuals with speech and hearing impairments to the wider community, where Sign Language Recognition (SLR) is a pioneer. However, the development of sign language recognition systems is often challenged by the scarce availability of suitable datasets, as each spoken language has its own distinct sign language. Additionally, the high computational power required to process this data complicates the progress, making it hard to reach the masses. This paper presents a real-time static sign recognition system that not only creates robust and customizable datasets but also preprocesses the data, and recognizes static hand signs, making it adaptable to any static sign language. Currently based off the American Sign Language (ASL), the model uses MediaPipe and a fully connected neural network (FCNN) that is trained and tested on 10 signs with 200 images per sign and later with 500 images per sign to include diversity. The proposed system demonstrates high accuracy and fast recognition, offering a practical solution for enhancing communication for speech and hearing-impaired individuals in real time.",
      "authors": [
        "Avinash Dhiran",
        "Anurag Kumbhare",
        "Achal Patil",
        "Mrugank Vichare",
        "Dhananjay Patel"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 0,
      "year": 2025
    },
    {
      "title": "Real-Time Gesture Recognition to Aid Communication in Children with Motor Impairments",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Saransh Mishra",
        "Pavan Nair",
        "Pushpalatha M",
        "Poornima S"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 0,
      "year": 2025
    },
    {
      "title": "Learning Multiple Layers of Features from Tiny Images",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "A. Krizhevsky"
      ],
      "published_date": "2009",
      "pdf_url": "",
      "citation_count": 40339,
      "year": 2009
    },
    {
      "title": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation",
      "arxiv_id": "1311.2524",
      "abstract": "Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012---achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.",
      "authors": [
        "Ross Girshick",
        "Jeff Donahue",
        "Trevor Darrell",
        "Jitendra Malik"
      ],
      "published_date": "2013-11-11",
      "pdf_url": "https://arxiv.org/pdf/1311.2524v5",
      "citation_count": 28347,
      "year": 2013
    },
    {
      "title": "MuMu-LLaMA: Multi-modal music understanding and generation via large language models",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Shansong Liu",
        "Atin Sakkeer Hussain",
        "Qilong Wu",
        "Chenshuo Sun",
        "Ying Shan"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 12,
      "year": 2025
    },
    {
      "title": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy",
      "arxiv_id": "",
      "abstract": "The success of diffusion models has raised concerns about the generation of unsafe or harmful content, prompting concept erasure approaches that fine-tune modules to suppress specific concepts while preserving general generative capabilities. However, as the number of erased concepts grows, these methods often become inefficient and ineffective, since each concept requires a separate set of fine-tuned parameters and may degrade the overall generation quality. In this work, we propose a supertype-subtype concept hierarchy that organizes erased concepts into a parent-child structure. Each erased concept is treated as a child node, and semantically related concepts (e.g., macaw, and bald eagle) are grouped under a shared parent node, referred to as a supertype concept (e.g., bird). Rather than erasing concepts individually, we introduce an effective and efficient group-wise suppression method, where semantically similar concepts are grouped and erased jointly by sharing a single set of learnable parameters. During the erasure phase, standard diffusion regularization is applied to preserve denoising process in unmasked regions. To mitigate the degradation of supertype generation caused by excessive erasure of semantically related subtypes, we propose a novel method called Supertype-Preserving Low-Rank Adaptation (SuPLoRA), which encodes the supertype concept information in the frozen down-projection matrix and updates only the up-projection matrix during erasure. Theoretical analysis demonstrates the effectiveness of SuPLoRA in mitigating generation performance degradation. We construct a more challenging benchmark that requires simultaneous erasure of concepts across diverse domains, including celebrities, objects, and pornographic content.",
      "authors": [
        "Jiahang Tu",
        "Ye Li",
        "Yiming Wu",
        "Hanbin Zhao",
        "Chao Zhang",
        "Hui Qian"
      ],
      "published_date": "2026-01-06",
      "pdf_url": "https://arxiv.org/pdf/2601.03305v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism",
      "arxiv_id": "2512.21452",
      "abstract": "Ground Penetrating Radar (GPR) has emerged as a pivotal tool for non-destructive evaluation of subsurface road defects. However, conventional GPR image interpretation remains heavily reliant on subjective expertise, introducing inefficiencies and inaccuracies. This study introduces a comprehensive framework to address these limitations: (1) A DCGAN-based data augmentation strategy synthesizes high-fidelity GPR images to mitigate data scarcity while preserving defect morphology under complex backgrounds; (2) A novel Multi-modal Chain and Global Attention Network (MCGA-Net) is proposed, integrating Multi-modal Chain Feature Fusion (MCFF) for hierarchical multi-scale defect representation and Global Attention Mechanism (GAM) for context-aware feature enhancement; (3) MS COCO transfer learning fine-tunes the backbone network, accelerating convergence and improving generalization. Ablation and comparison experiments validate the framework's efficacy. MCGA-Net achieves Precision (92.8%), Recall (92.5%), and mAP@50 (95.9%). In the detection of Gaussian noise, weak signals and small targets, MCGA-Net maintains robustness and outperforms other models. This work establishes a new paradigm for automated GPR-based defect detection, balancing computational efficiency with high accuracy in complex subsurface environments.",
      "authors": [
        "Haotian Lv",
        "Yuhui Zhang",
        "Jiangbo Dai",
        "Hanli Wu",
        "Jiaji Wang",
        "Dawei Wang"
      ],
      "published_date": "2025-12-25",
      "pdf_url": "https://arxiv.org/pdf/2512.21452v1",
      "citation_count": 6,
      "year": 2025
    },
    {
      "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
      "arxiv_id": "",
      "abstract": "In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in $\\textbf{2B}$ and $\\textbf{8B}$ parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of $\\textbf{77.8}$ on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.",
      "authors": [
        "Mingxin Li",
        "Yanzhao Zhang",
        "Dingkun Long",
        "Keqin Chen",
        "Sibo Song",
        "Shuai Bai",
        "Zhibo Yang",
        "Pengjun Xie",
        "An Yang",
        "Dayiheng Liu",
        "Jingren Zhou",
        "Junyang Lin"
      ],
      "published_date": "2026-01-08",
      "pdf_url": "https://arxiv.org/pdf/2601.04720v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness",
      "arxiv_id": null,
      "abstract": "Screen-shooting watermarking is an effective means of protecting screen content from unauthorized capture and illegal dissemination. However, existing methods are primarily designed for full-image capture, making them ineffective for partial screen-shooting prevalent in real-world scenarios. To address this limitation, we propose <inline-formula> <tex-math notation=\"LaTeX\">$\\textsf {FPSMark}$ </tex-math></inline-formula>, a flexible watermarking method tailored for partial screen-shooting that embeds consistent watermarks in multiple uniformly distributed cover blocks. Specifically, considering that robustness requirements vary according to the layout of each image, we model the mathematical relationship between the watermark block count and robustness, proving the flexibility of <inline-formula> <tex-math notation=\"LaTeX\">$\\textsf {FPSMark}$ </tex-math></inline-formula> in ensuring partial screen-shooting robustness. Moreover, partial screen-shooting disrupts watermark synchronization, posing challenges for precise watermark localization. To overcome this, we design an intrinsic signal localization network optimized with a hybrid loss. The localization network exploits the inherent distinctions between the watermark and non-watermark features, while the hybrid loss constrains the network at three dimensions: pixel-level, region-level, and sample-level. Experimental results demonstrate the superiority of <inline-formula> <tex-math notation=\"LaTeX\">$\\textsf {FPSMark}$ </tex-math></inline-formula>, showing robust performance across partial capture percentages. Its extraction accuracy exceeds 98% even with only half of the image captured, and it achieves 82% accuracy at a 40% capture ratio, whereas existing methods achieve only around 50% under the same conditions.",
      "authors": [
        "Mingyue Chen",
        "Xin Liao",
        "Han Fang",
        "Jinlin Guo",
        "Yanxiang Chen",
        "Xiaoshuai Wu"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 5,
      "year": 2025
    },
    {
      "title": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "A. Dempster",
        "N. Laird",
        "D. Rubin"
      ],
      "published_date": "1977",
      "pdf_url": "",
      "citation_count": 53643,
      "year": 1977
    },
    {
      "title": "Visualizing Data using t-SNE",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "L. Maaten",
        "Geoffrey E. Hinton"
      ],
      "published_date": "2008",
      "pdf_url": "",
      "citation_count": 47140,
      "year": 2008
    },
    {
      "title": "LLM Social Simulations Are a Promising Research Method",
      "arxiv_id": "",
      "abstract": "Accurate and verifiable large language model (LLM) simulations of human research subjects promise an accessible data source for understanding human behavior and training new AI systems. However, results to date have been limited, and few social scientists have adopted this method. In this position paper, we argue that the promise of LLM social simulations can be achieved by addressing five tractable challenges. We ground our argument in a review of empirical comparisons between LLMs and human research subjects, commentaries on the topic, and related work. We identify promising directions, including context-rich prompting and fine-tuning with social science datasets. We believe that LLM social simulations can already be used for pilot and exploratory studies, and more widespread use may soon be possible with rapidly advancing LLM capabilities. Researchers should prioritize developing conceptual models and iterative evaluations to make the best use of new AI systems.",
      "authors": [
        "Jacy Reese Anthis",
        "Ryan Liu",
        "Sean M. Richardson",
        "Austin C. Kozlowski",
        "Bernard Koch",
        "James Evans",
        "Erik Brynjolfsson",
        "Michael Bernstein"
      ],
      "published_date": "2025-04-03",
      "pdf_url": "https://arxiv.org/pdf/2504.02234v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Federated Contrastive Learning With Feature-Based Distillation for Human Activity Recognition",
      "arxiv_id": null,
      "abstract": "This article proposes a federated contrastive learning with feature-based distillation (FCLFD) framework tailored for human activity recognition (HAR). The FCLFD system integrates a central server with multiple mobile users to address a diverse range of HAR challenges. The framework encompasses two pivotal elements: a contrastive student–teacher (CST) architecture with feature-based distillation and an average weight scheme (AWS). The CST framework facilitates the transfer of comprehensive knowledge from a teacher model to a student model through feature-based distillation and contrastive learning, with both models sharing an identical architecture. Each participating user periodically uploads the weights of its student model to the central server, where the AWS deployed on the server calculates the average weights based on contributions from all connected users. The aggregated weights are then redistributed to each user, who updates their teacher model accordingly. Experimental evaluations demonstrate that when 50 users are connected, the proposed FCLFD scheme obtains the highest $F_{1}$ values of 89.01 and 94.19, outperforming several state-of-the-art federated learning algorithms on the wireless sensor data mining (WISDM) and PAMAP2 datasets.",
      "authors": [
        "Zhiwen Xiao",
        "Huagang Tong"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 47,
      "year": 2025
    },
    {
      "title": "Diffuse and Disperse: Image Generation with Representation Regularization",
      "arxiv_id": "",
      "abstract": "The development of diffusion-based generative models over the past decade has largely proceeded independently of progress in representation learning. These diffusion models typically rely on regression-based objectives and generally lack explicit regularization. In this work, we propose \\textit{Dispersive Loss}, a simple plug-and-play regularizer that effectively improves diffusion-based generative models. Our loss function encourages internal representations to disperse in the hidden space, analogous to contrastive self-supervised learning, with the key distinction that it requires no positive sample pairs and therefore does not interfere with the sampling process used for regression. Compared to the recent method of representation alignment (REPA), our approach is self-contained and minimalist, requiring no pre-training, no additional parameters, and no external data. We evaluate Dispersive Loss on the ImageNet dataset across a range of models and report consistent improvements over widely used and strong baselines. We hope our work will help bridge the gap between generative modeling and representation learning.",
      "authors": [
        "Runqian Wang",
        "Kaiming He"
      ],
      "published_date": "2025-06-10",
      "pdf_url": "https://arxiv.org/pdf/2506.09027v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Deep Autoencoder Neural Networks: A Comprehensive Review and New Perspectives",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Ibomoiye Domor Mienye",
        "Theo G. Swart"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 40,
      "year": 2025
    },
    {
      "title": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems",
      "arxiv_id": "",
      "abstract": "As scaling large language models faces prohibitive costs, multi-agent systems emerge as a promising alternative, though challenged by static knowledge assumptions and coordination inefficiencies. We introduces Knowledge-Aware Bayesian Bandits (KABB), a novel framework that enhances multi-agent system coordination through semantic understanding and dynamic adaptation. The framework features three key innovations: a three-dimensional knowledge distance model for deep semantic understanding, a dual-adaptation mechanism for continuous expert optimization, and a knowledge-aware Thompson Sampling strategy for efficient expert selection. Extensive evaluation demonstrates KABB achieves an optimal cost-performance balance, maintaining high performance while keeping computational demands relatively low in multi-agent coordination.",
      "authors": [
        "Jusheng Zhang",
        "Zimeng Huang",
        "Yijia Fan",
        "Ningyuan Liu",
        "Mingyan Li",
        "Zhuojie Yang",
        "Jiawei Yao",
        "Jian Wang",
        "Keze Wang"
      ],
      "published_date": "2025-02-11",
      "pdf_url": "https://arxiv.org/pdf/2502.07350v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
      "arxiv_id": "",
      "abstract": "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .",
      "authors": [
        "Olaf Ronneberger",
        "Philipp Fischer",
        "Thomas Brox"
      ],
      "published_date": "2015-05-18",
      "pdf_url": "https://arxiv.org/pdf/1505.04597v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Improved Distribution Matching Distillation for Fast Image Synthesis",
      "arxiv_id": "",
      "abstract": "Recent approaches have shown promises distilling diffusion models into efficient one-step generators. Among them, Distribution Matching Distillation (DMD) produces one-step generators that match their teacher in distribution, without enforcing a one-to-one correspondence with the sampling trajectories of their teachers. However, to ensure stable training, DMD requires an additional regression loss computed using a large set of noise-image pairs generated by the teacher with many steps of a deterministic sampler. This is costly for large-scale text-to-image synthesis and limits the student's quality, tying it too closely to the teacher's original sampling paths. We introduce DMD2, a set of techniques that lift this limitation and improve DMD training. First, we eliminate the regression loss and the need for expensive dataset construction. We show that the resulting instability is due to the fake critic not estimating the distribution of generated samples accurately and propose a two time-scale update rule as a remedy. Second, we integrate a GAN loss into the distillation procedure, discriminating between generated samples and real images. This lets us train the student model on real data, mitigating the imperfect real score estimation from the teacher model, and enhancing quality. Lastly, we modify the training procedure to enable multi-step sampling. We identify and address the training-inference input mismatch problem in this setting, by simulating inference-time generator samples during training time. Taken together, our improvements set new benchmarks in one-step image generation, with FID scores of 1.28 on ImageNet-64x64 and 8.35 on zero-shot COCO 2014, surpassing the original teacher despite a 500X reduction in inference cost. Further, we show our approach can generate megapixel images by distilling SDXL, demonstrating exceptional visual quality among few-step methods.",
      "authors": [
        "Tianwei Yin",
        "Michaël Gharbi",
        "Taesung Park",
        "Richard Zhang",
        "Eli Shechtman",
        "Fredo Durand",
        "William T. Freeman"
      ],
      "published_date": "2024-05-23",
      "pdf_url": "https://arxiv.org/pdf/2405.14867v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation",
      "arxiv_id": "",
      "abstract": "Diffusion models are the main driver of progress in image and video synthesis, but suffer from slow inference speed. Distillation methods, like the recently introduced adversarial diffusion distillation (ADD) aim to shift the model from many-shot to single-step inference, albeit at the cost of expensive and difficult optimization due to its reliance on a fixed pretrained DINOv2 discriminator. We introduce Latent Adversarial Diffusion Distillation (LADD), a novel distillation approach overcoming the limitations of ADD. In contrast to pixel-based ADD, LADD utilizes generative features from pretrained latent diffusion models. This approach simplifies training and enhances performance, enabling high-resolution multi-aspect ratio image synthesis. We apply LADD to Stable Diffusion 3 (8B) to obtain SD3-Turbo, a fast model that matches the performance of state-of-the-art text-to-image generators using only four unguided sampling steps. Moreover, we systematically investigate its scaling behavior and demonstrate LADD's effectiveness in various applications such as image editing and inpainting.",
      "authors": [
        "Axel Sauer",
        "Frederic Boesel",
        "Tim Dockhorn",
        "Andreas Blattmann",
        "Patrick Esser",
        "Robin Rombach"
      ],
      "published_date": "2024-03-18",
      "pdf_url": "https://arxiv.org/pdf/2403.12015v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Evolutionary optimization of model merging recipes",
      "arxiv_id": "2403.13187",
      "abstract": "Large language models (LLMs) have become increasingly capable, but their development often requires substantial computational resources. While model merging has emerged as a cost-effective promising approach for creating new models by combining existing ones, it currently relies on human intuition and domain knowledge, limiting its potential. Here, we propose an evolutionary approach that overcomes this limitation by automatically discovering effective combinations of diverse open-source models, harnessing their collective intelligence without requiring extensive additional training data or compute. Our approach operates in both parameter space and data flow space, allowing for optimization beyond just the weights of the individual models. This approach even facilitates cross-domain merging, generating models like a Japanese LLM with Math reasoning capabilities. Surprisingly, our Japanese Math LLM achieved state-of-the-art performance on a variety of established Japanese LLM benchmarks, even surpassing models with significantly more parameters, despite not being explicitly trained for such tasks. Furthermore, a culturally-aware Japanese VLM generated through our approach demonstrates its effectiveness in describing Japanese culture-specific content, outperforming previous Japanese VLMs. This work not only contributes new state-of-the-art models back to the open-source community, but also introduces a new paradigm for automated model composition, paving the way for exploring alternative, efficient approaches to foundation model development.",
      "authors": [
        "Takuya Akiba",
        "Makoto Shing",
        "Yujin Tang",
        "Qi Sun",
        "David Ha"
      ],
      "published_date": "2024-03-19",
      "pdf_url": "https://arxiv.org/pdf/2403.13187v2",
      "citation_count": 184,
      "year": 2024
    },
    {
      "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
      "arxiv_id": "",
      "abstract": "Current video diffusion models achieve impressive generation quality but struggle in interactive applications due to bidirectional attention dependencies. The generation of a single frame requires the model to process the entire sequence, including the future. We address this limitation by adapting a pretrained bidirectional diffusion transformer to an autoregressive transformer that generates frames on-the-fly. To further reduce latency, we extend distribution matching distillation (DMD) to videos, distilling 50-step diffusion model into a 4-step generator. To enable stable and high-quality distillation, we introduce a student initialization scheme based on teacher's ODE trajectories, as well as an asymmetric distillation strategy that supervises a causal student model with a bidirectional teacher. This approach effectively mitigates error accumulation in autoregressive generation, allowing long-duration video synthesis despite training on short clips. Our model achieves a total score of 84.27 on the VBench-Long benchmark, surpassing all previous video generation models. It enables fast streaming generation of high-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our approach also enables streaming video-to-video translation, image-to-video, and dynamic prompting in a zero-shot manner.",
      "authors": [
        "Tianwei Yin",
        "Qiang Zhang",
        "Richard Zhang",
        "William T. Freeman",
        "Fredo Durand",
        "Eli Shechtman",
        "Xun Huang"
      ],
      "published_date": "2024-12-10",
      "pdf_url": "https://arxiv.org/pdf/2412.07772v4",
      "citation_count": null,
      "year": null
    },
    {
      "title": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment",
      "arxiv_id": "",
      "abstract": "We propose Pure and Lightning ID customization (PuLID), a novel tuning-free ID customization method for text-to-image generation. By incorporating a Lightning T2I branch with a standard diffusion one, PuLID introduces both contrastive alignment loss and accurate ID loss, minimizing disruption to the original model and ensuring high ID fidelity. Experiments show that PuLID achieves superior performance in both ID fidelity and editability. Another attractive property of PuLID is that the image elements (e.g., background, lighting, composition, and style) before and after the ID insertion are kept as consistent as possible. Codes and models are available at https://github.com/ToTheBeginning/PuLID",
      "authors": [
        "Zinan Guo",
        "Yanze Wu",
        "Zhuowei Chen",
        "Lang Chen",
        "Peng Zhang",
        "Qian He"
      ],
      "published_date": "2024-04-24",
      "pdf_url": "https://arxiv.org/pdf/2404.16022v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Denoising Diffusion Probabilistic Models",
      "arxiv_id": "",
      "abstract": "We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion",
      "authors": [
        "Jonathan Ho",
        "Ajay Jain",
        "Pieter Abbeel"
      ],
      "published_date": "2020-06-19",
      "pdf_url": "https://arxiv.org/pdf/2006.11239v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "HunyuanVideo: A Systematic Framework For Large Video Generative Models",
      "arxiv_id": "",
      "abstract": "Recent advancements in video generation have significantly impacted daily life for both individuals and industries. However, the leading video generation models remain closed-source, resulting in a notable performance gap between industry capabilities and those available to the public. In this report, we introduce HunyuanVideo, an innovative open-source video foundation model that demonstrates performance in video generation comparable to, or even surpassing, that of leading closed-source models. HunyuanVideo encompasses a comprehensive framework that integrates several key elements, including data curation, advanced architectural design, progressive model scaling and training, and an efficient infrastructure tailored for large-scale model training and inference. As a result, we successfully trained a video generative model with over 13 billion parameters, making it the largest among all open-source models. We conducted extensive experiments and implemented a series of targeted designs to ensure high visual quality, motion dynamics, text-video alignment, and advanced filming techniques. According to evaluations by professionals, HunyuanVideo outperforms previous state-of-the-art models, including Runway Gen-3, Luma 1.6, and three top-performing Chinese video generative models. By releasing the code for the foundation model and its applications, we aim to bridge the gap between closed-source and open-source communities. This initiative will empower individuals within the community to experiment with their ideas, fostering a more dynamic and vibrant video generation ecosystem. The code is publicly available at https://github.com/Tencent/HunyuanVideo.",
      "authors": [
        "Weijie Kong",
        "Qi Tian",
        "Zijian Zhang",
        "Rox Min",
        "Zuozhuo Dai",
        "Jin Zhou",
        "Jiangfeng Xiong",
        "Xin Li",
        "Bo Wu",
        "Jianwei Zhang",
        "Kathrina Wu",
        "Qin Lin",
        "Junkun Yuan",
        "Yanxin Long",
        "Aladdin Wang",
        "Andong Wang",
        "Changlin Li",
        "Duojun Huang",
        "Fang Yang",
        "Hao Tan",
        "Hongmei Wang",
        "Jacob Song",
        "Jiawang Bai",
        "Jianbing Wu",
        "Jinbao Xue",
        "Joey Wang",
        "Kai Wang",
        "Mengyang Liu",
        "Pengyu Li",
        "Shuai Li",
        "Weiyan Wang",
        "Wenqing Yu",
        "Xinchi Deng",
        "Yang Li",
        "Yi Chen",
        "Yutao Cui",
        "Yuanbo Peng",
        "Zhentao Yu",
        "Zhiyu He",
        "Zhiyong Xu",
        "Zixiang Zhou",
        "Zunnan Xu",
        "Yangyu Tao",
        "Qinglin Lu",
        "Songtao Liu",
        "Dax Zhou",
        "Hongfa Wang",
        "Yong Yang",
        "Di Wang",
        "Yuhong Liu",
        "Jie Jiang",
        "Caesar Zhong"
      ],
      "published_date": "2024-12-03",
      "pdf_url": "https://arxiv.org/pdf/2412.03603v6",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency",
      "arxiv_id": "",
      "abstract": "With the introduction of diffusion-based video generation techniques, audio-conditioned human video generation has recently achieved significant breakthroughs in both the naturalness of motion and the synthesis of portrait details. Due to the limited control of audio signals in driving human motion, existing methods often add auxiliary spatial signals to stabilize movements, which may compromise the naturalness and freedom of motion. In this paper, we propose an end-to-end audio-only conditioned video diffusion model named Loopy. Specifically, we designed an inter- and intra-clip temporal module and an audio-to-latents module, enabling the model to leverage long-term motion information from the data to learn natural motion patterns and improving audio-portrait movement correlation. This method removes the need for manually specified spatial motion templates used in existing methods to constrain motion during inference. Extensive experiments show that Loopy outperforms recent audio-driven portrait diffusion models, delivering more lifelike and high-quality results across various scenarios.",
      "authors": [
        "Jianwen Jiang",
        "Chao Liang",
        "Jiaqi Yang",
        "Gaojie Lin",
        "Tianyun Zhong",
        "Yanbo Zheng"
      ],
      "published_date": "2024-09-04",
      "pdf_url": "https://arxiv.org/pdf/2409.02634v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models",
      "arxiv_id": "",
      "abstract": "End-to-end human animation, such as audio-driven talking human generation, has undergone notable advancements in the recent few years. However, existing methods still struggle to scale up as large general video generation models, limiting their potential in real applications. In this paper, we propose OmniHuman, a Diffusion Transformer-based framework that scales up data by mixing motion-related conditions into the training phase. To this end, we introduce two training principles for these mixed conditions, along with the corresponding model architecture and inference strategy. These designs enable OmniHuman to fully leverage data-driven motion generation, ultimately achieving highly realistic human video generation. More importantly, OmniHuman supports various portrait contents (face close-up, portrait, half-body, full-body), supports both talking and singing, handles human-object interactions and challenging body poses, and accommodates different image styles. Compared to existing end-to-end audio-driven methods, OmniHuman not only produces more realistic videos, but also offers greater flexibility in inputs. It also supports multiple driving modalities (audio-driven, video-driven and combined driving signals). Video samples are provided on the ttfamily project page (https://omnihuman-lab.github.io)",
      "authors": [
        "Gaojie Lin",
        "Jianwen Jiang",
        "Jiaqi Yang",
        "Zerong Zheng",
        "Chao Liang"
      ],
      "published_date": "2025-02-03",
      "pdf_url": "https://arxiv.org/pdf/2502.01061v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation",
      "arxiv_id": "",
      "abstract": "Recent advances in latent diffusion-based generative models for portrait image animation, such as Hallo, have achieved impressive results in short-duration video synthesis. In this paper, we present updates to Hallo, introducing several design enhancements to extend its capabilities. First, we extend the method to produce long-duration videos. To address substantial challenges such as appearance drift and temporal artifacts, we investigate augmentation strategies within the image space of conditional motion frames. Specifically, we introduce a patch-drop technique augmented with Gaussian noise to enhance visual consistency and temporal coherence over long duration. Second, we achieve 4K resolution portrait video generation. To accomplish this, we implement vector quantization of latent codes and apply temporal alignment techniques to maintain coherence across the temporal dimension. By integrating a high-quality decoder, we realize visual synthesis at 4K resolution. Third, we incorporate adjustable semantic textual labels for portrait expressions as conditional inputs. This extends beyond traditional audio cues to improve controllability and increase the diversity of the generated content. To the best of our knowledge, Hallo2, proposed in this paper, is the first method to achieve 4K resolution and generate hour-long, audio-driven portrait image animations enhanced with textual prompts. We have conducted extensive experiments to evaluate our method on publicly available datasets, including HDTF, CelebV, and our introduced \"Wild\" dataset. The experimental results demonstrate that our approach achieves state-of-the-art performance in long-duration portrait video animation, successfully generating rich and controllable content at 4K resolution for duration extending up to tens of minutes. Project page https://fudan-generative-vision.github.io/hallo2",
      "authors": [
        "Jiahao Cui",
        "Hui Li",
        "Yao Yao",
        "Hao Zhu",
        "Hanlin Shang",
        "Kaihui Cheng",
        "Hang Zhou",
        "Siyu Zhu",
        "Jingdong Wang"
      ],
      "published_date": "2024-10-10",
      "pdf_url": "https://arxiv.org/pdf/2410.07718v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation",
      "arxiv_id": "",
      "abstract": "Recent work on human animation usually involves audio, pose, or movement maps conditions, thereby achieves vivid animation quality. However, these methods often face practical challenges due to extra control conditions, cumbersome condition injection modules, or limitation to head region driving. Hence, we ask if it is possible to achieve striking half-body human animation while simplifying unnecessary conditions. To this end, we propose a half-body human animation method, dubbed EchoMimicV2, that leverages a novel Audio-Pose Dynamic Harmonization strategy, including Pose Sampling and Audio Diffusion, to enhance half-body details, facial and gestural expressiveness, and meanwhile reduce conditions redundancy. To compensate for the scarcity of half-body data, we utilize Head Partial Attention to seamlessly accommodate headshot data into our training framework, which can be omitted during inference, providing a free lunch for animation. Furthermore, we design the Phase-specific Denoising Loss to guide motion, detail, and low-level quality for animation in specific phases, respectively. Besides, we also present a novel benchmark for evaluating the effectiveness of half-body human animation. Extensive experiments and analyses demonstrate that EchoMimicV2 surpasses existing methods in both quantitative and qualitative evaluations.",
      "authors": [
        "Rang Meng",
        "Xingyu Zhang",
        "Yuming Li",
        "Chenguang Ma"
      ],
      "published_date": "2024-11-15",
      "pdf_url": "https://arxiv.org/pdf/2411.10061v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Feature Pyramid Networks for Object Detection",
      "arxiv_id": "",
      "abstract": "Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.",
      "authors": [
        "Tsung-Yi Lin",
        "Piotr Dollár",
        "Ross Girshick",
        "Kaiming He",
        "Bharath Hariharan",
        "Serge Belongie"
      ],
      "published_date": "2016-12-09",
      "pdf_url": "https://arxiv.org/pdf/1612.03144v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation",
      "arxiv_id": "1406.1078",
      "abstract": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
      "authors": [
        "Kyunghyun Cho",
        "Bart van Merrienboer",
        "Caglar Gulcehre",
        "Dzmitry Bahdanau",
        "Fethi Bougares",
        "Holger Schwenk",
        "Yoshua Bengio"
      ],
      "published_date": "2014-06-03",
      "pdf_url": "https://arxiv.org/pdf/1406.1078v3",
      "citation_count": 25480,
      "year": 2014
    },
    {
      "title": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving",
      "arxiv_id": "2312.09245",
      "abstract": "Large language models (LLMs) have opened up new possibilities for intelligent agents, endowing them with human-like thinking and cognitive abilities. In this work, we delve into the potential of large language models (LLMs) in autonomous driving (AD). We introduce DriveMLM, an LLM-based AD framework that can perform close-loop autonomous driving in realistic simulators. To this end, (1) we bridge the gap between the language decisions and the vehicle control commands by standardizing the decision states according to the off-the-shelf motion planning module. (2) We employ a multimodal LLM (MLLM) to model the behavior planning module of a module AD system, which uses driving rules, user commands, and inputs from various sensors (e.g., camera, lidar) as input and makes driving decisions and provide explanations; This model can plug-and-play in existing AD systems such as Autopilot and Apollo for close-loop driving. (3) We design an effective data engine to collect a dataset that includes decision state and corresponding explanation annotation for model training and evaluation. We conduct extensive experiments and show that replacing the decision-making modules of the Autopilot and Apollo with DriveMLM resulted in significant improvements of 3.2 and 4.7 points on the CARLA Town05 Long respectively, demonstrating the effectiveness of our model. We hope this work can serve as a baseline for autonomous driving with LLMs.",
      "authors": [
        "Erfei Cui",
        "Wenhai Wang",
        "Zhiqi Li",
        "Jiangwei Xie",
        "Haoming Zou",
        "Hanming Deng",
        "Gen Luo",
        "Lewei Lu",
        "Xizhou Zhu",
        "Jifeng Dai"
      ],
      "published_date": "2023-12-14",
      "pdf_url": "https://arxiv.org/pdf/2312.09245v3",
      "citation_count": 220,
      "year": 2023
    },
    {
      "title": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability",
      "arxiv_id": "",
      "abstract": "World models can foresee the outcomes of different actions, which is of paramount importance for autonomous driving. Nevertheless, existing driving world models still have limitations in generalization to unseen environments, prediction fidelity of critical details, and action controllability for flexible application. In this paper, we present Vista, a generalizable driving world model with high fidelity and versatile controllability. Based on a systematic diagnosis of existing methods, we introduce several key ingredients to address these limitations. To accurately predict real-world dynamics at high resolution, we propose two novel losses to promote the learning of moving instances and structural information. We also devise an effective latent replacement approach to inject historical frames as priors for coherent long-horizon rollouts. For action controllability, we incorporate a versatile set of controls from high-level intentions (command, goal point) to low-level maneuvers (trajectory, angle, and speed) through an efficient learning strategy. After large-scale training, the capabilities of Vista can seamlessly generalize to different scenarios. Extensive experiments on multiple datasets show that Vista outperforms the most advanced general-purpose video generator in over 70% of comparisons and surpasses the best-performing driving world model by 55% in FID and 27% in FVD. Moreover, for the first time, we utilize the capacity of Vista itself to establish a generalizable reward for real-world action evaluation without accessing the ground truth actions.",
      "authors": [
        "Shenyuan Gao",
        "Jiazhi Yang",
        "Li Chen",
        "Kashyap Chitta",
        "Yihang Qiu",
        "Andreas Geiger",
        "Jun Zhang",
        "Hongyang Li"
      ],
      "published_date": "2024-05-27",
      "pdf_url": "https://arxiv.org/pdf/2405.17398v5",
      "citation_count": null,
      "year": null
    },
    {
      "title": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving",
      "arxiv_id": "",
      "abstract": "Recently, the diffusion model has emerged as a powerful generative technique for robotic policy learning, capable of modeling multi-mode action distributions. Leveraging its capability for end-to-end autonomous driving is a promising direction. However, the numerous denoising steps in the robotic diffusion policy and the more dynamic, open-world nature of traffic scenes pose substantial challenges for generating diverse driving actions at a real-time speed. To address these challenges, we propose a novel truncated diffusion policy that incorporates prior multi-mode anchors and truncates the diffusion schedule, enabling the model to learn denoising from anchored Gaussian distribution to the multi-mode driving action distribution. Additionally, we design an efficient cascade diffusion decoder for enhanced interaction with conditional scene context. The proposed model, DiffusionDrive, demonstrates 10$\\times$ reduction in denoising steps compared to vanilla diffusion policy, delivering superior diversity and quality in just 2 steps. On the planning-oriented NAVSIM dataset, with the aligned ResNet-34 backbone, DiffusionDrive achieves 88.1 PDMS without bells and whistles, setting a new record, while running at a real-time speed of 45 FPS on an NVIDIA 4090. Qualitative results on challenging scenarios further confirm that DiffusionDrive can robustly generate diverse plausible driving actions. Code and model will be available at https://github.com/hustvl/DiffusionDrive.",
      "authors": [
        "Bencheng Liao",
        "Shaoyu Chen",
        "Haoran Yin",
        "Bo Jiang",
        "Cheng Wang",
        "Sixu Yan",
        "Xinbang Zhang",
        "Xiangyu Li",
        "Ying Zhang",
        "Qian Zhang",
        "Xinggang Wang"
      ],
      "published_date": "2024-11-22",
      "pdf_url": "https://arxiv.org/pdf/2411.15139v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "How Far is Video Generation from World Model: A Physical Law Perspective",
      "arxiv_id": "",
      "abstract": "OpenAI's Sora highlights the potential of video generation for developing world models that adhere to fundamental physical laws. However, the ability of video generation models to discover such laws purely from visual data without human priors can be questioned. A world model learning the true law should give predictions robust to nuances and correctly extrapolate on unseen scenarios. In this work, we evaluate across three key scenarios: in-distribution, out-of-distribution, and combinatorial generalization. We developed a 2D simulation testbed for object movement and collisions to generate videos deterministically governed by one or more classical mechanics laws. This provides an unlimited supply of data for large-scale experimentation and enables quantitative evaluation of whether the generated videos adhere to physical laws. We trained diffusion-based video generation models to predict object movements based on initial frames. Our scaling experiments show perfect generalization within the distribution, measurable scaling behavior for combinatorial generalization, but failure in out-of-distribution scenarios. Further experiments reveal two key insights about the generalization mechanisms of these models: (1) the models fail to abstract general physical rules and instead exhibit \"case-based\" generalization behavior, i.e., mimicking the closest training example; (2) when generalizing to new cases, models are observed to prioritize different factors when referencing training data: color > size > velocity > shape. Our study suggests that scaling alone is insufficient for video generation models to uncover fundamental physical laws, despite its role in Sora's broader success. See our project page at https://phyworld.github.io",
      "authors": [
        "Bingyi Kang",
        "Yang Yue",
        "Rui Lu",
        "Zhijie Lin",
        "Yang Zhao",
        "Kaixin Wang",
        "Gao Huang",
        "Jiashi Feng"
      ],
      "published_date": "2024-11-04",
      "pdf_url": "https://arxiv.org/pdf/2411.02385v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "EMMA: End-to-End Multimodal Model for Autonomous Driving",
      "arxiv_id": "",
      "abstract": "We introduce EMMA, an End-to-end Multimodal Model for Autonomous driving. Built upon a multi-modal large language model foundation like Gemini, EMMA directly maps raw camera sensor data into various driving-specific outputs, including planner trajectories, perception objects, and road graph elements. EMMA maximizes the utility of world knowledge from the pre-trained large language models, by representing all non-sensor inputs (e.g. navigation instructions and ego vehicle status) and outputs (e.g. trajectories and 3D locations) as natural language text. This approach allows EMMA to jointly process various driving tasks in a unified language space, and generate the outputs for each task using task-specific prompts. Empirically, we demonstrate EMMA's effectiveness by achieving state-of-the-art performance in motion planning on nuScenes as well as competitive results on the Waymo Open Motion Dataset (WOMD). EMMA also yields competitive results for camera-primary 3D object detection on the Waymo Open Dataset (WOD). We show that co-training EMMA with planner trajectories, object detection, and road graph tasks yields improvements across all three domains, highlighting EMMA's potential as a generalist model for autonomous driving applications. We hope that our results will inspire research to further evolve the state of the art in autonomous driving model architectures.",
      "authors": [
        "Jyh-Jing Hwang",
        "Runsheng Xu",
        "Hubert Lin",
        "Wei-Chih Hung",
        "Jingwei Ji",
        "Kristy Choi",
        "Di Huang",
        "Tong He",
        "Paul Covington",
        "Benjamin Sapp",
        "Yin Zhou",
        "James Guo",
        "Dragomir Anguelov",
        "Mingxing Tan"
      ],
      "published_date": "2024-10-30",
      "pdf_url": "https://arxiv.org/pdf/2410.23262v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Language Models are Few-Shot Learners",
      "arxiv_id": "",
      "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
      "authors": [
        "Tom B. Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell",
        "Sandhini Agarwal",
        "Ariel Herbert-Voss",
        "Gretchen Krueger",
        "Tom Henighan",
        "Rewon Child",
        "Aditya Ramesh",
        "Daniel M. Ziegler",
        "Jeffrey Wu",
        "Clemens Winter",
        "Christopher Hesse",
        "Mark Chen",
        "Eric Sigler",
        "Mateusz Litwin",
        "Scott Gray",
        "Benjamin Chess",
        "Jack Clark",
        "Christopher Berner",
        "Sam McCandlish",
        "Alec Radford",
        "Ilya Sutskever",
        "Dario Amodei"
      ],
      "published_date": "2020-05-28",
      "pdf_url": "https://arxiv.org/pdf/2005.14165v4",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling",
      "arxiv_id": "",
      "abstract": "We introduce InternVL 2.5, an advanced multimodal large language model (MLLM) series that builds upon InternVL 2.0, maintaining its core model architecture while introducing significant enhancements in training and testing strategies as well as data quality. In this work, we delve into the relationship between model scaling and performance, systematically exploring the performance trends in vision encoders, language models, dataset sizes, and test-time configurations. Through extensive evaluations on a wide range of benchmarks, including multi-discipline reasoning, document understanding, multi-image / video understanding, real-world comprehension, multimodal hallucination detection, visual grounding, multilingual capabilities, and pure language processing, InternVL 2.5 exhibits competitive performance, rivaling leading commercial models such as GPT-4o and Claude-3.5-Sonnet. Notably, our model is the first open-source MLLMs to surpass 70% on the MMMU benchmark, achieving a 3.7-point improvement through Chain-of-Thought (CoT) reasoning and showcasing strong potential for test-time scaling. We hope this model contributes to the open-source community by setting new standards for developing and applying multimodal AI systems. HuggingFace demo see https://huggingface.co/spaces/OpenGVLab/InternVL",
      "authors": [
        "Zhe Chen",
        "Weiyun Wang",
        "Yue Cao",
        "Yangzhou Liu",
        "Zhangwei Gao",
        "Erfei Cui",
        "Jinguo Zhu",
        "Shenglong Ye",
        "Hao Tian",
        "Zhaoyang Liu",
        "Lixin Gu",
        "Xuehui Wang",
        "Qingyun Li",
        "Yiming Ren",
        "Zixuan Chen",
        "Jiapeng Luo",
        "Jiahao Wang",
        "Tan Jiang",
        "Bo Wang",
        "Conghui He",
        "Botian Shi",
        "Xingcheng Zhang",
        "Han Lv",
        "Yi Wang",
        "Wenqi Shao",
        "Pei Chu",
        "Zhongying Tu",
        "Tong He",
        "Zhiyong Wu",
        "Huipeng Deng",
        "Jiaye Ge",
        "Kai Chen",
        "Kaipeng Zhang",
        "Limin Wang",
        "Min Dou",
        "Lewei Lu",
        "Xizhou Zhu",
        "Tong Lu",
        "Dahua Lin",
        "Yu Qiao",
        "Jifeng Dai",
        "Wenhai Wang"
      ],
      "published_date": "2024-12-06",
      "pdf_url": "https://arxiv.org/pdf/2412.05271v5",
      "citation_count": null,
      "year": null
    },
    {
      "title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models",
      "arxiv_id": "",
      "abstract": "We introduce InternVL3, a significant advancement in the InternVL series featuring a native multimodal pre-training paradigm. Rather than adapting a text-only large language model (LLM) into a multimodal large language model (MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and linguistic capabilities from both diverse multimodal data and pure-text corpora during a single pre-training stage. This unified training paradigm effectively addresses the complexities and alignment challenges commonly encountered in conventional post-hoc training pipelines for MLLMs. To further improve performance and scalability, InternVL3 incorporates variable visual position encoding (V2PE) to support extended multimodal contexts, employs advanced post-training techniques such as supervised fine-tuning (SFT) and mixed preference optimization (MPO), and adopts test-time scaling strategies alongside an optimized training infrastructure. Extensive empirical evaluations demonstrate that InternVL3 delivers superior performance across a wide range of multi-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the MMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its capabilities remain highly competitive with leading proprietary models, including ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also maintaining strong pure-language proficiency. In pursuit of open-science principles, we will publicly release both the training data and model weights to foster further research and development in next-generation MLLMs.",
      "authors": [
        "Jinguo Zhu",
        "Weiyun Wang",
        "Zhe Chen",
        "Zhaoyang Liu",
        "Shenglong Ye",
        "Lixin Gu",
        "Hao Tian",
        "Yuchen Duan",
        "Weijie Su",
        "Jie Shao",
        "Zhangwei Gao",
        "Erfei Cui",
        "Xuehui Wang",
        "Yue Cao",
        "Yangzhou Liu",
        "Xingguang Wei",
        "Hongjie Zhang",
        "Haomin Wang",
        "Weiye Xu",
        "Hao Li",
        "Jiahao Wang",
        "Nianchen Deng",
        "Songze Li",
        "Yinan He",
        "Tan Jiang",
        "Jiapeng Luo",
        "Yi Wang",
        "Conghui He",
        "Botian Shi",
        "Xingcheng Zhang",
        "Wenqi Shao",
        "Junjun He",
        "Yingtong Xiong",
        "Wenwen Qu",
        "Peng Sun",
        "Penglong Jiao",
        "Han Lv",
        "Lijun Wu",
        "Kaipeng Zhang",
        "Huipeng Deng",
        "Jiaye Ge",
        "Kai Chen",
        "Limin Wang",
        "Min Dou",
        "Lewei Lu",
        "Xizhou Zhu",
        "Tong Lu",
        "Dahua Lin",
        "Yu Qiao",
        "Jifeng Dai",
        "Wenhai Wang"
      ],
      "published_date": "2025-04-14",
      "pdf_url": "https://arxiv.org/pdf/2504.10479v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models",
      "arxiv_id": "2407.11691",
      "abstract": "We present VLMEvalKit: an open-source toolkit for evaluating large multi-modality models based on PyTorch. The toolkit aims to provide a user-friendly and comprehensive framework for researchers and developers to evaluate existing multi-modality models and publish reproducible evaluation results. In VLMEvalKit, we implement over 200+ different large multi-modality models, including both proprietary APIs and open-source models, as well as more than 80 different multi-modal benchmarks. By implementing a single interface, new models can be easily added to the toolkit, while the toolkit automatically handles the remaining workloads, including data preparation, distributed inference, prediction post-processing, and metric calculation. Although the toolkit is currently mainly used for evaluating large vision-language models, its design is compatible with future updates that incorporate additional modalities, such as audio and video. Based on the evaluation results obtained with the toolkit, we host OpenVLM Leaderboard, a comprehensive leaderboard to track the progress of multi-modality learning research. The toolkit is released on https://github.com/open-compass/VLMEvalKit and is actively maintained.",
      "authors": [
        "Haodong Duan",
        "Xinyu Fang",
        "Junming Yang",
        "Xiangyu Zhao",
        "Yuxuan Qiao",
        "Mo Li",
        "Amit Agarwal",
        "Zhe Chen",
        "Lin Chen",
        "Yuan Liu",
        "Yubo Ma",
        "Hailong Sun",
        "Yifan Zhang",
        "Shiyin Lu",
        "Tack Hwa Wong",
        "Weiyun Wang",
        "Peiheng Zhou",
        "Xiaozhe Li",
        "Chaoyou Fu",
        "Junbo Cui",
        "Jixuan Chen",
        "Enxin Song",
        "Song Mao",
        "Shengyuan Ding",
        "Tianhao Liang",
        "Zicheng Zhang",
        "Xiaoyi Dong",
        "Yuhang Zang",
        "Pan Zhang",
        "Jiaqi Wang",
        "Dahua Lin",
        "Kai Chen"
      ],
      "published_date": "2024-07-16",
      "pdf_url": "https://arxiv.org/pdf/2407.11691v4",
      "citation_count": 374,
      "year": 2024
    },
    {
      "title": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step",
      "arxiv_id": "",
      "abstract": "Large language models have demonstrated substantial advancements in reasoning capabilities. However, current Vision-Language Models (VLMs) often struggle to perform systematic and structured reasoning, especially when handling complex visual question-answering tasks. In this work, we introduce LLaVA-CoT, a large VLM designed to conduct autonomous multistage reasoning. Unlike chain-of-thought prompting, LLaVA-CoT independently engages in sequential stages of summarization, visual interpretation, logical reasoning, and conclusion generation. This structured approach enables LLaVA-CoT to achieve marked improvements on reasoning-intensive tasks. To accomplish this, we construct the LLaVA-CoT-100k dataset, integrating samples from various visual question answering sources and providing structured reasoning annotations. Besides, we propose a test-time stage-wise retracing search method (SWIRES), which enables effective and efficient test-time scaling. Remarkably, with only 100k training samples and test-time scaling, LLaVA-CoT not only outperforms its base model by 9.4% on a wide range of multimodal reasoning benchmarks, but also surpasses the performance of larger and even closed-source models, such as Gemini-1.5-pro, GPT-4o-mini, and Llama-3.2-90B-Vision-Instruct. The code, dataset, and pre-trained weights are publicly available at https://github.com/PKU-YuanGroup/LLaVA-CoT.",
      "authors": [
        "Guowei Xu",
        "Peng Jin",
        "Ziang Wu",
        "Hao Li",
        "Yibing Song",
        "Lichao Sun",
        "Li Yuan"
      ],
      "published_date": "2024-11-15",
      "pdf_url": "https://arxiv.org/pdf/2411.10440v6",
      "citation_count": null,
      "year": null
    },
    {
      "title": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency",
      "arxiv_id": "",
      "abstract": "We introduce InternVL 3.5, a new family of open-source multimodal models that significantly advances versatility, reasoning capability, and inference efficiency along the InternVL series. A key innovation is the Cascade Reinforcement Learning (Cascade RL) framework, which enhances reasoning through a two-stage process: offline RL for stable convergence and online RL for refined alignment. This coarse-to-fine training strategy leads to substantial improvements on downstream reasoning tasks, e.g., MMMU and MathVista. To optimize efficiency, we propose a Visual Resolution Router (ViR) that dynamically adjusts the resolution of visual tokens without compromising performance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD) strategy separates the vision encoder and language model across different GPUs, effectively balancing computational load. These contributions collectively enable InternVL3.5 to achieve up to a +16.0\\% gain in overall reasoning performance and a 4.05$\\times$ inference speedup compared to its predecessor, i.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as GUI interaction and embodied agency. Notably, our largest model, i.e., InternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs across general multimodal, reasoning, text, and agentic tasks -- narrowing the performance gap with leading commercial models like GPT-5. All models and code are publicly released.",
      "authors": [
        "Weiyun Wang",
        "Zhangwei Gao",
        "Lixin Gu",
        "Hengjun Pu",
        "Long Cui",
        "Xingguang Wei",
        "Zhaoyang Liu",
        "Linglin Jing",
        "Shenglong Ye",
        "Jie Shao",
        "Zhaokai Wang",
        "Zhe Chen",
        "Hongjie Zhang",
        "Ganlin Yang",
        "Haomin Wang",
        "Qi Wei",
        "Jinhui Yin",
        "Wenhao Li",
        "Erfei Cui",
        "Guanzhou Chen",
        "Zichen Ding",
        "Changyao Tian",
        "Zhenyu Wu",
        "Jingjing Xie",
        "Zehao Li",
        "Bowen Yang",
        "Yuchen Duan",
        "Xuehui Wang",
        "Zhi Hou",
        "Haoran Hao",
        "Tianyi Zhang",
        "Songze Li",
        "Xiangyu Zhao",
        "Haodong Duan",
        "Nianchen Deng",
        "Bin Fu",
        "Yinan He",
        "Yi Wang",
        "Conghui He",
        "Botian Shi",
        "Junjun He",
        "Yingtong Xiong",
        "Han Lv",
        "Lijun Wu",
        "Wenqi Shao",
        "Kaipeng Zhang",
        "Huipeng Deng",
        "Biqing Qi",
        "Jiaye Ge",
        "Qipeng Guo",
        "Wenwei Zhang",
        "Songyang Zhang",
        "Maosong Cao",
        "Junyao Lin",
        "Kexian Tang",
        "Jianfei Gao",
        "Haian Huang",
        "Yuzhe Gu",
        "Chengqi Lyu",
        "Huanze Tang",
        "Rui Wang",
        "Haijun Lv",
        "Wanli Ouyang",
        "Limin Wang",
        "Min Dou",
        "Xizhou Zhu",
        "Tong Lu",
        "Dahua Lin",
        "Jifeng Dai",
        "Weijie Su",
        "Bowen Zhou",
        "Kai Chen",
        "Yu Qiao",
        "Wenhai Wang",
        "Gen Luo"
      ],
      "published_date": "2025-08-25",
      "pdf_url": "https://arxiv.org/pdf/2508.18265v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Proximal Policy Optimization Algorithms",
      "arxiv_id": "",
      "abstract": "We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.",
      "authors": [
        "John Schulman",
        "Filip Wolski",
        "Prafulla Dhariwal",
        "Alec Radford",
        "Oleg Klimov"
      ],
      "published_date": "2017-07-20",
      "pdf_url": "https://arxiv.org/pdf/1707.06347v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Flow-GRPO: Training Flow Matching Models via Online RL",
      "arxiv_id": "",
      "abstract": "We propose Flow-GRPO, the first method to integrate online policy gradient reinforcement learning (RL) into flow matching models. Our approach uses two key strategies: (1) an ODE-to-SDE conversion that transforms a deterministic Ordinary Differential Equation (ODE) into an equivalent Stochastic Differential Equation (SDE) that matches the original model's marginal distribution at all timesteps, enabling statistical sampling for RL exploration; and (2) a Denoising Reduction strategy that reduces training denoising steps while retaining the original number of inference steps, significantly improving sampling efficiency without sacrificing performance. Empirically, Flow-GRPO is effective across multiple text-to-image tasks. For compositional generation, RL-tuned SD3.5-M generates nearly perfect object counts, spatial relations, and fine-grained attributes, increasing GenEval accuracy from $63\\%$ to $95\\%$. In visual text rendering, accuracy improves from $59\\%$ to $92\\%$, greatly enhancing text generation. Flow-GRPO also achieves substantial gains in human preference alignment. Notably, very little reward hacking occurred, meaning rewards did not increase at the cost of appreciable image quality or diversity degradation.",
      "authors": [
        "Jie Liu",
        "Gongye Liu",
        "Jiajun Liang",
        "Yangguang Li",
        "Jiaheng Liu",
        "Xintao Wang",
        "Pengfei Wan",
        "Di Zhang",
        "Wanli Ouyang"
      ],
      "published_date": "2025-05-08",
      "pdf_url": "https://arxiv.org/pdf/2505.05470v5",
      "citation_count": null,
      "year": null
    },
    {
      "title": "DanceGRPO: Unleashing GRPO on Visual Generation",
      "arxiv_id": "",
      "abstract": "Recent advances in generative AI have revolutionized visual content creation, yet aligning model outputs with human preferences remains a critical challenge. While Reinforcement Learning (RL) has emerged as a promising approach for fine-tuning generative models, existing methods like DDPO and DPOK face fundamental limitations - particularly their inability to maintain stable optimization when scaling to large and diverse prompt sets, severely restricting their practical utility. This paper presents DanceGRPO, a framework that addresses these limitations through an innovative adaptation of Group Relative Policy Optimization (GRPO) for visual generation tasks. Our key insight is that GRPO's inherent stability mechanisms uniquely position it to overcome the optimization challenges that plague prior RL-based approaches on visual generation. DanceGRPO establishes several significant advances: First, it demonstrates consistent and stable policy optimization across multiple modern generative paradigms, including both diffusion models and rectified flows. Second, it maintains robust performance when scaling to complex, real-world scenarios encompassing three key tasks and four foundation models. Third, it shows remarkable versatility in optimizing for diverse human preferences as captured by five distinct reward models assessing image/video aesthetics, text-image alignment, video motion quality, and binary feedback. Our comprehensive experiments reveal that DanceGRPO outperforms baseline methods by up to 181\\% across multiple established benchmarks, including HPS-v2.1, CLIP Score, VideoAlign, and GenEval. Our results establish DanceGRPO as a robust and versatile solution for scaling Reinforcement Learning from Human Feedback (RLHF) tasks in visual generation, offering new insights into harmonizing reinforcement learning and visual synthesis.",
      "authors": [
        "Zeyue Xue",
        "Jie Wu",
        "Yu Gao",
        "Fangyuan Kong",
        "Lingting Zhu",
        "Mengzhao Chen",
        "Zhiheng Liu",
        "Wei Liu",
        "Qiushan Guo",
        "Weilin Huang",
        "Ping Luo"
      ],
      "published_date": "2025-05-12",
      "pdf_url": "https://arxiv.org/pdf/2505.07818v4",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Seedance 1.0: Exploring the Boundaries of Video Generation Models",
      "arxiv_id": "",
      "abstract": "Notable breakthroughs in diffusion modeling have propelled rapid improvements in video generation, yet current foundational model still face critical challenges in simultaneously balancing prompt following, motion plausibility, and visual quality. In this report, we introduce Seedance 1.0, a high-performance and inference-efficient video foundation generation model that integrates several core technical improvements: (i) multi-source data curation augmented with precision and meaningful video captioning, enabling comprehensive learning across diverse scenarios; (ii) an efficient architecture design with proposed training paradigm, which allows for natively supporting multi-shot generation and jointly learning of both text-to-video and image-to-video tasks. (iii) carefully-optimized post-training approaches leveraging fine-grained supervised fine-tuning, and video-specific RLHF with multi-dimensional reward mechanisms for comprehensive performance improvements; (iv) excellent model acceleration achieving ~10x inference speedup through multi-stage distillation strategies and system-level optimizations. Seedance 1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds (NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance 1.0 stands out with high-quality and fast video generation having superior spatiotemporal fluidity with structural stability, precise instruction adherence in complex multi-subject contexts, native multi-shot narrative coherence with consistent subject representation.",
      "authors": [
        "Yu Gao",
        "Haoyuan Guo",
        "Tuyen Hoang",
        "Weilin Huang",
        "Lu Jiang",
        "Fangyuan Kong",
        "Huixia Li",
        "Jiashi Li",
        "Liang Li",
        "Xiaojie Li",
        "Xunsong Li",
        "Yifu Li",
        "Shanchuan Lin",
        "Zhijie Lin",
        "Jiawei Liu",
        "Shu Liu",
        "Xiaonan Nie",
        "Zhiwu Qing",
        "Yuxi Ren",
        "Li Sun",
        "Zhi Tian",
        "Rui Wang",
        "Sen Wang",
        "Guoqiang Wei",
        "Guohong Wu",
        "Jie Wu",
        "Ruiqi Xia",
        "Fei Xiao",
        "Xuefeng Xiao",
        "Jiangqiao Yan",
        "Ceyuan Yang",
        "Jianchao Yang",
        "Runkai Yang",
        "Tao Yang",
        "Yihang Yang",
        "Zilyu Ye",
        "Xuejiao Zeng",
        "Yan Zeng",
        "Heng Zhang",
        "Yang Zhao",
        "Xiaozheng Zheng",
        "Peihao Zhu",
        "Jiaxin Zou",
        "Feilong Zuo"
      ],
      "published_date": "2025-06-10",
      "pdf_url": "https://arxiv.org/pdf/2506.09113v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "SkyReels-V2: Infinite-length Film Generative Model",
      "arxiv_id": "",
      "abstract": "Recent advances in video generation have been driven by diffusion models and autoregressive frameworks, yet critical challenges persist in harmonizing prompt adherence, visual quality, motion dynamics, and duration: compromises in motion dynamics to enhance temporal visual quality, constrained video duration (5-10 seconds) to prioritize resolution, and inadequate shot-aware generation stemming from general-purpose MLLMs' inability to interpret cinematic grammar, such as shot composition, actor expressions, and camera motions. These intertwined limitations hinder realistic long-form synthesis and professional film-style generation. To address these limitations, we propose SkyReels-V2, an Infinite-length Film Generative Model, that synergizes Multi-modal Large Language Model (MLLM), Multi-stage Pretraining, Reinforcement Learning, and Diffusion Forcing Framework. Firstly, we design a comprehensive structural representation of video that combines the general descriptions by the Multi-modal LLM and the detailed shot language by sub-expert models. Aided with human annotation, we then train a unified Video Captioner, named SkyCaptioner-V1, to efficiently label the video data. Secondly, we establish progressive-resolution pretraining for the fundamental video generation, followed by a four-stage post-training enhancement: Initial concept-balanced Supervised Fine-Tuning (SFT) improves baseline quality; Motion-specific Reinforcement Learning (RL) training with human-annotated and synthetic distortion data addresses dynamic artifacts; Our diffusion forcing framework with non-decreasing noise schedules enables long-video synthesis in an efficient search space; Final high-quality SFT refines visual fidelity. All the code and models are available at https://github.com/SkyworkAI/SkyReels-V2.",
      "authors": [
        "Guibin Chen",
        "Dixuan Lin",
        "Jiangping Yang",
        "Chunze Lin",
        "Junchen Zhu",
        "Mingyuan Fan",
        "Hao Zhang",
        "Sheng Chen",
        "Zheng Chen",
        "Chengcheng Ma",
        "Weiming Xiong",
        "Wei Wang",
        "Nuo Pang",
        "Kang Kang",
        "Zhiheng Xu",
        "Yuzhe Jin",
        "Yupeng Liang",
        "Yubing Song",
        "Peng Zhao",
        "Boyuan Xu",
        "Di Qiu",
        "Debang Li",
        "Zhengcong Fei",
        "Yang Li",
        "Yahui Zhou"
      ],
      "published_date": "2025-04-17",
      "pdf_url": "https://arxiv.org/pdf/2504.13074v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Unified Reward Model for Multimodal Understanding and Generation",
      "arxiv_id": "",
      "abstract": "Recent advances in human preference alignment have significantly enhanced multimodal generation and understanding. A key approach is training reward models to guide preference optimization. However, existing models are often task-specific, limiting their adaptability across diverse visual applications. We also argue that jointly learning to assess multiple tasks may foster a synergistic effect, where improved image understanding enhances image generation assessment, and refined image evaluation benefits video assessment through better frame analysis. To this end, this paper proposes UnifiedReward, the first unified reward model for multimodal understanding and generation assessment, enabling both pairwise ranking and pointwise scoring, which can be employed for vision model preference alignment. Specifically, (1) we first develop UnifiedReward on our constructed large-scale human preference dataset, including both image and video generation/understanding tasks. (2) Then, it is utilized to automatically construct high-quality preference pair data based on the vision models, fine-gradually filtering their outputs through pair ranking and point sifting. (3) Finally, these data are used for their preference alignment through Direct Preference Optimization (DPO). Experimental results demonstrate that joint learning to assess diverse visual tasks can lead to substantial mutual benefits and we apply our pipeline to both image and video understanding/generation tasks, significantly improving the performance in each domain.",
      "authors": [
        "Yibin Wang",
        "Yuhang Zang",
        "Hao Li",
        "Cheng Jin",
        "Jiaqi Wang"
      ],
      "published_date": "2025-03-07",
      "pdf_url": "https://arxiv.org/pdf/2503.05236v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "GPT-4 Technical Report",
      "arxiv_id": "",
      "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",
      "authors": [
        "OpenAI",
        "Josh Achiam",
        "Steven Adler",
        "Sandhini Agarwal",
        "Lama Ahmad",
        "Ilge Akkaya",
        "Florencia Leoni Aleman",
        "Diogo Almeida",
        "Janko Altenschmidt",
        "Sam Altman",
        "Shyamal Anadkat",
        "Red Avila",
        "Igor Babuschkin",
        "Suchir Balaji",
        "Valerie Balcom",
        "Paul Baltescu",
        "Haiming Bao",
        "Mohammad Bavarian",
        "Jeff Belgum",
        "Irwan Bello",
        "Jake Berdine",
        "Gabriel Bernadett-Shapiro",
        "Christopher Berner",
        "Lenny Bogdonoff",
        "Oleg Boiko",
        "Madelaine Boyd",
        "Anna-Luisa Brakman",
        "Greg Brockman",
        "Tim Brooks",
        "Miles Brundage",
        "Kevin Button",
        "Trevor Cai",
        "Rosie Campbell",
        "Andrew Cann",
        "Brittany Carey",
        "Chelsea Carlson",
        "Rory Carmichael",
        "Brooke Chan",
        "Che Chang",
        "Fotis Chantzis",
        "Derek Chen",
        "Sully Chen",
        "Ruby Chen",
        "Jason Chen",
        "Mark Chen",
        "Ben Chess",
        "Chester Cho",
        "Casey Chu",
        "Hyung Won Chung",
        "Dave Cummings",
        "Jeremiah Currier",
        "Yunxing Dai",
        "Cory Decareaux",
        "Thomas Degry",
        "Noah Deutsch",
        "Damien Deville",
        "Arka Dhar",
        "David Dohan",
        "Steve Dowling",
        "Sheila Dunning",
        "Adrien Ecoffet",
        "Atty Eleti",
        "Tyna Eloundou",
        "David Farhi",
        "Liam Fedus",
        "Niko Felix",
        "Simón Posada Fishman",
        "Juston Forte",
        "Isabella Fulford",
        "Leo Gao",
        "Elie Georges",
        "Christian Gibson",
        "Vik Goel",
        "Tarun Gogineni",
        "Gabriel Goh",
        "Rapha Gontijo-Lopes",
        "Jonathan Gordon",
        "Morgan Grafstein",
        "Scott Gray",
        "Ryan Greene",
        "Joshua Gross",
        "Shixiang Shane Gu",
        "Yufei Guo",
        "Chris Hallacy",
        "Jesse Han",
        "Jeff Harris",
        "Yuchen He",
        "Mike Heaton",
        "Johannes Heidecke",
        "Chris Hesse",
        "Alan Hickey",
        "Wade Hickey",
        "Peter Hoeschele",
        "Brandon Houghton",
        "Kenny Hsu",
        "Shengli Hu",
        "Xin Hu",
        "Joost Huizinga",
        "Shantanu Jain",
        "Shawn Jain",
        "Joanne Jang",
        "Angela Jiang",
        "Roger Jiang",
        "Haozhun Jin",
        "Denny Jin",
        "Shino Jomoto",
        "Billie Jonn",
        "Heewoo Jun",
        "Tomer Kaftan",
        "Łukasz Kaiser",
        "Ali Kamali",
        "Ingmar Kanitscheider",
        "Nitish Shirish Keskar",
        "Tabarak Khan",
        "Logan Kilpatrick",
        "Jong Wook Kim",
        "Christina Kim",
        "Yongjik Kim",
        "Jan Hendrik Kirchner",
        "Jamie Kiros",
        "Matt Knight",
        "Daniel Kokotajlo",
        "Łukasz Kondraciuk",
        "Andrew Kondrich",
        "Aris Konstantinidis",
        "Kyle Kosic",
        "Gretchen Krueger",
        "Vishal Kuo",
        "Michael Lampe",
        "Ikai Lan",
        "Teddy Lee",
        "Jan Leike",
        "Jade Leung",
        "Daniel Levy",
        "Chak Ming Li",
        "Rachel Lim",
        "Molly Lin",
        "Stephanie Lin",
        "Mateusz Litwin",
        "Theresa Lopez",
        "Ryan Lowe",
        "Patricia Lue",
        "Anna Makanju",
        "Kim Malfacini",
        "Sam Manning",
        "Todor Markov",
        "Yaniv Markovski",
        "Bianca Martin",
        "Katie Mayer",
        "Andrew Mayne",
        "Bob McGrew",
        "Scott Mayer McKinney",
        "Christine McLeavey",
        "Paul McMillan",
        "Jake McNeil",
        "David Medina",
        "Aalok Mehta",
        "Jacob Menick",
        "Luke Metz",
        "Andrey Mishchenko",
        "Pamela Mishkin",
        "Vinnie Monaco",
        "Evan Morikawa",
        "Daniel Mossing",
        "Tong Mu",
        "Mira Murati",
        "Oleg Murk",
        "David Mély",
        "Ashvin Nair",
        "Reiichiro Nakano",
        "Rajeev Nayak",
        "Arvind Neelakantan",
        "Richard Ngo",
        "Hyeonwoo Noh",
        "Long Ouyang",
        "Cullen O'Keefe",
        "Jakub Pachocki",
        "Alex Paino",
        "Joe Palermo",
        "Ashley Pantuliano",
        "Giambattista Parascandolo",
        "Joel Parish",
        "Emy Parparita",
        "Alex Passos",
        "Mikhail Pavlov",
        "Andrew Peng",
        "Adam Perelman",
        "Filipe de Avila Belbute Peres",
        "Michael Petrov",
        "Henrique Ponde de Oliveira Pinto",
        "Michael",
        "Pokorny",
        "Michelle Pokrass",
        "Vitchyr H. Pong",
        "Tolly Powell",
        "Alethea Power",
        "Boris Power",
        "Elizabeth Proehl",
        "Raul Puri",
        "Alec Radford",
        "Jack Rae",
        "Aditya Ramesh",
        "Cameron Raymond",
        "Francis Real",
        "Kendra Rimbach",
        "Carl Ross",
        "Bob Rotsted",
        "Henri Roussez",
        "Nick Ryder",
        "Mario Saltarelli",
        "Ted Sanders",
        "Shibani Santurkar",
        "Girish Sastry",
        "Heather Schmidt",
        "David Schnurr",
        "John Schulman",
        "Daniel Selsam",
        "Kyla Sheppard",
        "Toki Sherbakov",
        "Jessica Shieh",
        "Sarah Shoker",
        "Pranav Shyam",
        "Szymon Sidor",
        "Eric Sigler",
        "Maddie Simens",
        "Jordan Sitkin",
        "Katarina Slama",
        "Ian Sohl",
        "Benjamin Sokolowsky",
        "Yang Song",
        "Natalie Staudacher",
        "Felipe Petroski Such",
        "Natalie Summers",
        "Ilya Sutskever",
        "Jie Tang",
        "Nikolas Tezak",
        "Madeleine B. Thompson",
        "Phil Tillet",
        "Amin Tootoonchian",
        "Elizabeth Tseng",
        "Preston Tuggle",
        "Nick Turley",
        "Jerry Tworek",
        "Juan Felipe Cerón Uribe",
        "Andrea Vallone",
        "Arun Vijayvergiya",
        "Chelsea Voss",
        "Carroll Wainwright",
        "Justin Jay Wang",
        "Alvin Wang",
        "Ben Wang",
        "Jonathan Ward",
        "Jason Wei",
        "CJ Weinmann",
        "Akila Welihinda",
        "Peter Welinder",
        "Jiayi Weng",
        "Lilian Weng",
        "Matt Wiethoff",
        "Dave Willner",
        "Clemens Winter",
        "Samuel Wolrich",
        "Hannah Wong",
        "Lauren Workman",
        "Sherwin Wu",
        "Jeff Wu",
        "Michael Wu",
        "Kai Xiao",
        "Tao Xu",
        "Sarah Yoo",
        "Kevin Yu",
        "Qiming Yuan",
        "Wojciech Zaremba",
        "Rowan Zellers",
        "Chong Zhang",
        "Marvin Zhang",
        "Shengjia Zhao",
        "Tianhao Zheng",
        "Juntang Zhuang",
        "William Zhuk",
        "Barret Zoph"
      ],
      "published_date": "2023-03-15",
      "pdf_url": "https://arxiv.org/pdf/2303.08774v6",
      "citation_count": null,
      "year": null
    },
    {
      "title": "LLaMA: Open and Efficient Foundation Language Models",
      "arxiv_id": "",
      "abstract": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro",
        "Faisal Azhar",
        "Aurelien Rodriguez",
        "Armand Joulin",
        "Edouard Grave",
        "Guillaume Lample"
      ],
      "published_date": "2023-02-27",
      "pdf_url": "https://arxiv.org/pdf/2302.13971v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
      "arxiv_id": "2201.11903",
      "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
      "authors": [
        "Jason Wei",
        "Xuezhi Wang",
        "Dale Schuurmans",
        "Maarten Bosma",
        "Brian Ichter",
        "Fei Xia",
        "Ed Chi",
        "Quoc Le",
        "Denny Zhou"
      ],
      "published_date": "2022-01-28",
      "pdf_url": "https://arxiv.org/pdf/2201.11903v6",
      "citation_count": 15210,
      "year": 2022
    },
    {
      "title": "Training Verifiers to Solve Math Word Problems",
      "arxiv_id": "",
      "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
      "authors": [
        "Karl Cobbe",
        "Vineet Kosaraju",
        "Mohammad Bavarian",
        "Mark Chen",
        "Heewoo Jun",
        "Lukasz Kaiser",
        "Matthias Plappert",
        "Jerry Tworek",
        "Jacob Hilton",
        "Reiichiro Nakano",
        "Christopher Hesse",
        "John Schulman"
      ],
      "published_date": "2021-10-27",
      "pdf_url": "https://arxiv.org/pdf/2110.14168v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Crystal structure of the nucleosome core particle at 2.8 Å resolution",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "K. Luger",
        "A. Mäder",
        "R. K. Richmond",
        "D. Sargent",
        "T. Richmond"
      ],
      "published_date": "1997",
      "pdf_url": "",
      "citation_count": 9262,
      "year": 1997
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "arxiv_id": "",
      "abstract": "Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.",
      "authors": [
        "Albert Gu",
        "Tri Dao"
      ],
      "published_date": "2023-12-01",
      "pdf_url": "https://arxiv.org/pdf/2312.00752v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "A catalogue of splice junction sequences.",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Stephen M. Mount"
      ],
      "published_date": "1982",
      "pdf_url": "",
      "citation_count": 3135,
      "year": 1982
    },
    {
      "title": "Origin of the Genetic Code",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "F. Crick"
      ],
      "published_date": "1967",
      "pdf_url": "",
      "citation_count": 2297,
      "year": 1967
    },
    {
      "title": "SGDR: Stochastic Gradient Descent with Warm Restarts",
      "arxiv_id": "",
      "abstract": "Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14% and 16.21%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "published_date": "2016-08-13",
      "pdf_url": "https://arxiv.org/pdf/1608.03983v5",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
      "arxiv_id": "",
      "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.",
      "authors": [
        "Noam Shazeer",
        "Azalia Mirhoseini",
        "Krzysztof Maziarz",
        "Andy Davis",
        "Quoc Le",
        "Geoffrey Hinton",
        "Jeff Dean"
      ],
      "published_date": "2017-01-23",
      "pdf_url": "https://arxiv.org/pdf/1701.06538v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Compression of individual sequences via variable-rate coding",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "J. Ziv",
        "A. Lempel"
      ],
      "published_date": "1978",
      "pdf_url": "",
      "citation_count": 3732,
      "year": 1978
    },
    {
      "title": "Pointer Sentinel Mixture Models",
      "arxiv_id": "",
      "abstract": "Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.",
      "authors": [
        "Stephen Merity",
        "Caiming Xiong",
        "James Bradbury",
        "Richard Socher"
      ],
      "published_date": "2016-09-26",
      "pdf_url": "https://arxiv.org/pdf/1609.07843v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Measuring Massive Multitask Language Understanding",
      "arxiv_id": "",
      "abstract": "We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",
      "authors": [
        "Dan Hendrycks",
        "Collin Burns",
        "Steven Basart",
        "Andy Zou",
        "Mantas Mazeika",
        "Dawn Song",
        "Jacob Steinhardt"
      ],
      "published_date": "2020-09-07",
      "pdf_url": "https://arxiv.org/pdf/2009.03300v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Let's Verify Step by Step",
      "arxiv_id": "",
      "abstract": "In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning. However, even state-of-the-art models still regularly produce logical mistakes. To train more reliable models, we can turn either to outcome supervision, which provides feedback for a final result, or process supervision, which provides feedback for each intermediate reasoning step. Given the importance of training reliable models, and given the high cost of human feedback, it is important to carefully compare the both methods. Recent work has already begun this comparison, but many questions still remain. We conduct our own investigation, finding that process supervision significantly outperforms outcome supervision for training models to solve problems from the challenging MATH dataset. Our process-supervised model solves 78% of problems from a representative subset of the MATH test set. Additionally, we show that active learning significantly improves the efficacy of process supervision. To support related research, we also release PRM800K, the complete dataset of 800,000 step-level human feedback labels used to train our best reward model.",
      "authors": [
        "Hunter Lightman",
        "Vineet Kosaraju",
        "Yura Burda",
        "Harri Edwards",
        "Bowen Baker",
        "Teddy Lee",
        "Jan Leike",
        "John Schulman",
        "Ilya Sutskever",
        "Karl Cobbe"
      ],
      "published_date": "2023-05-31",
      "pdf_url": "https://arxiv.org/pdf/2305.20050v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark",
      "arxiv_id": "",
      "abstract": "We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are \"Google-proof\"). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",
      "authors": [
        "David Rein",
        "Betty Li Hou",
        "Asa Cooper Stickland",
        "Jackson Petty",
        "Richard Yuanzhe Pang",
        "Julien Dirani",
        "Julian Michael",
        "Samuel R. Bowman"
      ],
      "published_date": "2023-11-20",
      "pdf_url": "https://arxiv.org/pdf/2311.12022v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
      "arxiv_id": "",
      "abstract": "Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.",
      "authors": [
        "Dmitry Lepikhin",
        "HyoukJoong Lee",
        "Yuanzhong Xu",
        "Dehao Chen",
        "Orhan Firat",
        "Yanping Huang",
        "Maxim Krikun",
        "Noam Shazeer",
        "Zhifeng Chen"
      ],
      "published_date": "2020-06-30",
      "pdf_url": "https://arxiv.org/pdf/2006.16668v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Pattern Recognition and Machine Learning",
      "arxiv_id": null,
      "abstract": "Probability Distributions.- Linear Models for Regression.- Linear Models for Classification.- Neural Networks.- Kernel Methods.- Sparse Kernel Machines.- Graphical Models.- Mixture Models and EM.- Approximate Inference.- Sampling Methods.- Continuous Latent Variables.- Sequential Data.- Combining Models.",
      "authors": [
        "Radford M. Neal"
      ],
      "published_date": "2006",
      "pdf_url": "",
      "citation_count": 38829,
      "year": 2006
    },
    {
      "title": "Bagging Predictors",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "L. Breiman"
      ],
      "published_date": "1996",
      "pdf_url": "",
      "citation_count": 25690,
      "year": 1996
    },
    {
      "title": "Automatic Image-Based Plant Disease Severity Estimation Using Deep Learning",
      "arxiv_id": null,
      "abstract": "Automatic and accurate estimation of disease severity is essential for food security, disease management, and yield loss prediction. Deep learning, the latest breakthrough in computer vision, is promising for fine-grained disease severity classification, as the method avoids the labor-intensive feature engineering and threshold-based segmentation. Using the apple black rot images in the PlantVillage dataset, which are further annotated by botanists with four severity stages as ground truth, a series of deep convolutional neural networks are trained to diagnose the severity of the disease. The performances of shallow networks trained from scratch and deep models fine-tuned by transfer learning are evaluated systemically in this paper. The best model is the deep VGG16 model trained with transfer learning, which yields an overall accuracy of 90.4% on the hold-out test set. The proposed deep learning model may have great potential in disease control for modern agriculture.",
      "authors": [
        "Guan Wang",
        "Yu Sun",
        "Jianxin Wang"
      ],
      "published_date": "2017",
      "pdf_url": "",
      "citation_count": 638,
      "year": 2017
    },
    {
      "title": "Plant identification using deep neural networks via optimization of transfer learning parameters",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Mostafa Mehdipour-Ghazi",
        "B. Yanikoglu",
        "E. Aptoula"
      ],
      "published_date": "2017",
      "pdf_url": "",
      "citation_count": 476,
      "year": 2017
    },
    {
      "title": "New perspectives on plant disease characterization based on deep learning",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Sue Han Lee",
        "H. Goëau",
        "P. Bonnet",
        "A. Joly"
      ],
      "published_date": "2020",
      "pdf_url": "",
      "citation_count": 264,
      "year": 2020
    },
    {
      "title": "Deep Learning for Plant Identification in Natural Environment",
      "arxiv_id": null,
      "abstract": "Plant image identification has become an interdisciplinary focus in both botanical taxonomy and computer vision. The first plant image dataset collected by mobile phone in natural scene is presented, which contains 10,000 images of 100 ornamental plant species in Beijing Forestry University campus. A 26-layer deep learning model consisting of 8 residual building blocks is designed for large-scale plant classification in natural environment. The proposed model achieves a recognition rate of 91.78% on the BJFU100 dataset, demonstrating that deep learning is a promising technology for smart forestry.",
      "authors": [
        "Yu Sun",
        "Yuan Liu",
        "Guan Wang",
        "Haiyan Zhang"
      ],
      "published_date": "2017",
      "pdf_url": "",
      "citation_count": 262,
      "year": 2017
    },
    {
      "title": "Going deeper in the automated identification of Herbarium specimens",
      "arxiv_id": null,
      "abstract": "Hundreds of herbarium collections have accumulated a valuable heritage and knowledge of plants over several centuries. Recent initiatives started ambitious preservation plans to digitize this information and make it available to botanists and the general public through web portals. However, thousands of sheets are still unidentified at the species level while numerous sheets should be reviewed and updated following more recent taxonomic knowledge. These annotations and revisions require an unrealistic amount of work for botanists to carry out in a reasonable time. Computer vision and machine learning approaches applied to herbarium sheets are promising but are still not well studied compared to automated species identification from leaf scans or pictures of plants in the field. In this work, we propose to study and evaluate the accuracy with which herbarium images can be potentially exploited for species identification with deep learning technology. In addition, we propose to study if the combination of herbarium sheets with photos of plants in the field is relevant in terms of accuracy, and finally, we explore if herbarium images from one region that has one specific flora can be used to do transfer learning to another region with other species; for example, on a region under-represented in terms of collected data. This is, to our knowledge, the first study that uses deep learning to analyze a big dataset with thousands of species from herbaria. Results show the potential of Deep Learning on herbarium species identification, particularly by training and testing across different datasets from different herbaria. This could potentially lead to the creation of a semi, or even fully automated system to help taxonomists and experts with their annotation, classification, and revision works.",
      "authors": [
        "Jose Carranza-Rojas",
        "Hervé Goeau",
        "P. Bonnet",
        "Erick Mata-Montero",
        "A. Joly"
      ],
      "published_date": "2017",
      "pdf_url": "",
      "citation_count": 243,
      "year": 2017
    },
    {
      "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks",
      "arxiv_id": "",
      "abstract": "In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3.\n  The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters",
      "authors": [
        "Mark Sandler",
        "Andrew Howard",
        "Menglong Zhu",
        "Andrey Zhmoginov",
        "Liang-Chieh Chen"
      ],
      "published_date": "2018-01-13",
      "pdf_url": "https://arxiv.org/pdf/1801.04381v4",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Communication-Efficient Learning of Deep Networks from Decentralized Data",
      "arxiv_id": "",
      "abstract": "Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning.\n  We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.",
      "authors": [
        "H. Brendan McMahan",
        "Eider Moore",
        "Daniel Ramage",
        "Seth Hampson",
        "Blaise Agüera y Arcas"
      ],
      "published_date": "2016-02-17",
      "pdf_url": "https://arxiv.org/pdf/1602.05629v4",
      "citation_count": null,
      "year": null
    },
    {
      "title": "A Comprehensive Survey on Transfer Learning",
      "arxiv_id": "",
      "abstract": "Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning researches, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey paper reviews more than forty representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over twenty representative transfer learning models are used for experiments. The models are performed on three different datasets, i.e., Amazon Reviews, Reuters-21578, and Office-31. And the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.",
      "authors": [
        "Fuzhen Zhuang",
        "Zhiyuan Qi",
        "Keyu Duan",
        "Dongbo Xi",
        "Yongchun Zhu",
        "Hengshu Zhu",
        "Hui Xiong",
        "Qing He"
      ],
      "published_date": "2019-11-07",
      "pdf_url": "https://arxiv.org/pdf/1911.02685v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Privacy Preserved and Decentralized Smartphone Recommendation System",
      "arxiv_id": null,
      "abstract": "The increasing popularity of mobile phones has led to an abundance of online reviews, making it challenging for consumers to make well-informed purchasing decisions. This study proposes a novel recommendation system-based mobile phone rating classification approach using federated learning and Term Frequency-Inverse Document Frequency (TF-IDF) features. We created a novel dataset by scraping over 13,000 mobile phone reviews from Flipkart’s website. The proposed approach involves the development of a federated deep neural network (FDNN) to classify the newly created Flipkart dataset. This approach includes data cleaning, balancing, TF-IDF feature extraction, and prediction using federated learning. We employ two clients and one server and conduct three rounds of experiments. The experimental results demonstrate that the proposed approach achieved an accuracy of 96.68% on the aggregated server side while maintaining the security of customer data on their local devices. The proposed approach has the potential to assist consumers in making well-informed purchasing decisions and can be extended to other e-commerce platforms with large datasets of online reviews.",
      "authors": [
        "A. Khan",
        "Maha Driss",
        "Wadii Boulila",
        "G. A. Sampedro",
        "Sidra Abbas",
        "Chitapong Wechtaisong"
      ],
      "published_date": "2024",
      "pdf_url": "",
      "citation_count": 53,
      "year": 2024
    },
    {
      "title": "A Survey on Heterogeneity Taxonomy, Security and Privacy Preservation in the Integration of IoT, Wireless Sensor Networks and Federated Learning",
      "arxiv_id": null,
      "abstract": "Federated learning (FL) is a machine learning (ML) technique that enables collaborative model training without sharing raw data, making it ideal for Internet of Things (IoT) applications where data are distributed across devices and privacy is a concern. Wireless Sensor Networks (WSNs) play a crucial role in IoT systems by collecting data from the physical environment. This paper presents a comprehensive survey of the integration of FL, IoT, and WSNs. It covers FL basics, strategies, and types and discusses the integration of FL, IoT, and WSNs in various domains. The paper addresses challenges related to heterogeneity in FL and summarizes state-of-the-art research in this area. It also explores security and privacy considerations and performance evaluation methodologies. The paper outlines the latest achievements and potential research directions in FL, IoT, and WSNs and emphasizes the significance of the surveyed topics within the context of current technological advancements.",
      "authors": [
        "Tesfahunegn Minwuyelet Mengistu",
        "Taewoon Kim",
        "Jenn-Wei Lin"
      ],
      "published_date": "2024",
      "pdf_url": "",
      "citation_count": 46,
      "year": 2024
    },
    {
      "title": "A privacy-preserving federated learning with a secure collaborative for malware detection models using Internet of Things resources",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "A. Alamer"
      ],
      "published_date": "2024",
      "pdf_url": "",
      "citation_count": 25,
      "year": 2024
    },
    {
      "title": "Graph-based deep learning techniques for remote sensing applications: Techniques, taxonomy, and applications - A comprehensive review",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Manel Khazri Khlifi",
        "Wadii Boulila",
        "I. Farah"
      ],
      "published_date": "2023",
      "pdf_url": "",
      "citation_count": 25,
      "year": 2023
    },
    {
      "title": "Federated Learning Architectures: A Performance Evaluation With Crop Yield Prediction Application",
      "arxiv_id": "2408.02998",
      "abstract": "Federated learning has become an emerging technology for data analysis for IoT applications. This paper implements centralized and decentralized federated learning frameworks for crop yield prediction based on Long Short-Term Memory Network. For centralized federated learning, multiple clients and one server is considered, where the clients exchange their model updates with the server that works as the aggregator to build the global model. For the decentralized framework, a collaborative network is formed among the devices either using ring topology or using mesh topology. In this network, each device receives model updates from the neighbour devices, and performs aggregation to build the upgraded model. The performance of the centralized and decentralized federated learning frameworks are evaluated in terms of prediction accuracy, precision, recall, F1-Score, and training time. The experimental results present that $\\geq$97% and $>$97.5% prediction accuracy are achieved using the centralized and decentralized federated learning-based frameworks respectively. The results also show that the using centralized federated learning the response time can be reduced by $\\sim$75% than the cloud-only framework. Finally, the future research directions of the use of federated learning in crop yield prediction are explored in this paper.",
      "authors": [
        "Anwesha Mukherjee",
        "Rajkumar Buyya"
      ],
      "published_date": "2024-08-06",
      "pdf_url": "https://arxiv.org/pdf/2408.02998v1",
      "citation_count": 18,
      "year": 2024
    },
    {
      "title": "Segment Anything",
      "arxiv_id": "",
      "abstract": "We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.",
      "authors": [
        "Alexander Kirillov",
        "Eric Mintun",
        "Nikhila Ravi",
        "Hanzi Mao",
        "Chloe Rolland",
        "Laura Gustafson",
        "Tete Xiao",
        "Spencer Whitehead",
        "Alexander C. Berg",
        "Wan-Yen Lo",
        "Piotr Dollár",
        "Ross Girshick"
      ],
      "published_date": "2023-04-05",
      "pdf_url": "https://arxiv.org/pdf/2304.02643v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Visual Instruction Tuning",
      "arxiv_id": "",
      "abstract": "Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Qingyang Wu",
        "Yong Jae Lee"
      ],
      "published_date": "2023-04-17",
      "pdf_url": "https://arxiv.org/pdf/2304.08485v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Improved Baselines with Visual Instruction Tuning",
      "arxiv_id": "",
      "abstract": "Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this note, we show that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~1 day on a single 8-A100 node. We hope this can make state-of-the-art LMM research more accessible. Code and model will be publicly available.",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Yuheng Li",
        "Yong Jae Lee"
      ],
      "published_date": "2023-10-05",
      "pdf_url": "https://arxiv.org/pdf/2310.03744v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset",
      "arxiv_id": "2405.18842",
      "abstract": "With the rapid advancement of Vision Language Models (VLMs), VLM-based Image Quality Assessment (IQA) seeks to describe image quality linguistically to align with human expression and capture the multifaceted nature of IQA tasks. However, current methods are still far from practical usage. First, prior works focus narrowly on specific sub-tasks or settings, which do not align with diverse real-world applications. Second, their performance is sub-optimal due to limitations in dataset coverage, scale, and quality. To overcome these challenges, we introduce the enhanced Depicted image Quality Assessment model (DepictQA-Wild). Our method includes a multi-functional IQA task paradigm that encompasses both assessment and comparison tasks, brief and detailed responses, full-reference and non-reference scenarios. We introduce a ground-truth-informed dataset construction approach to enhance data quality, and scale up the dataset to 495K under the brief-detail joint framework. Consequently, we construct a comprehensive, large-scale, and high-quality dataset, named DQ-495K. We also retain image resolution during training to better handle resolution-related quality issues, and estimate a confidence score that is helpful to filter out low-quality responses. Experimental results demonstrate that DepictQA-Wild significantly outperforms traditional score-based methods, prior VLM-based IQA models, and proprietary GPT-4V in distortion identification, instant rating, and reasoning tasks. Our advantages are further confirmed by real-world applications including assessing the web-downloaded images and ranking model-processed images. Codes, datasets, and model weights have been released in https://depictqa.github.io/.",
      "authors": [
        "Zhiyuan You",
        "Jinjin Gu",
        "Xin Cai",
        "Zheyuan Li",
        "Kaiwen Zhu",
        "Chao Dong",
        "Tianfan Xue"
      ],
      "published_date": "2024-05-29",
      "pdf_url": "https://arxiv.org/pdf/2405.18842v3",
      "citation_count": 38,
      "year": 2024
    },
    {
      "title": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs",
      "arxiv_id": "",
      "abstract": "Vision language models (VLMs) have achieved impressive performance across a variety of computer vision tasks. However, the multimodal reasoning capability has not been fully explored in existing models. In this paper, we propose a Chain-of-Focus (CoF) method that allows VLMs to perform adaptive focusing and zooming in on key image regions based on obtained visual cues and the given questions, achieving efficient multimodal reasoning. To enable this CoF capability, we present a two-stage training pipeline, including supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we construct the MM-CoF dataset, comprising 3K samples derived from a visual agent designed to adaptively identify key regions to solve visual tasks with different image resolutions and questions. We use MM-CoF to fine-tune the Qwen2.5-VL model for cold start. In the RL stage, we leverage the outcome accuracies and formats as rewards to update the Qwen2.5-VL model, enabling further refining the search and reasoning strategy of models without human priors. Our model achieves significant improvements on multiple benchmarks. On the V* benchmark that requires strong visual reasoning capability, our model outperforms existing VLMs by 5% among 8 image resolutions ranging from 224 to 4K, demonstrating the effectiveness of the proposed CoF method and facilitating the more efficient deployment of VLMs in practical applications.",
      "authors": [
        "Xintong Zhang",
        "Zhi Gao",
        "Bofei Zhang",
        "Pengxiang Li",
        "Xiaowen Zhang",
        "Yang Liu",
        "Tao Yuan",
        "Yuwei Wu",
        "Yunde Jia",
        "Song-Chun Zhu",
        "Qing Li"
      ],
      "published_date": "2025-05-21",
      "pdf_url": "https://arxiv.org/pdf/2505.15436v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views",
      "arxiv_id": "",
      "abstract": "Though recent advances in vision-language models (VLMs) have achieved remarkable progress across a wide range of multimodal tasks, understanding 3D spatial relationships from limited views remains a significant challenge. Previous reasoning methods typically rely on pure text (e.g., topological cognitive maps) or on 2D visual cues. However, their limited representational capacity hinders performance in specific tasks that require 3D spatial imagination. To address this limitation, we propose 3DThinker, a framework that can effectively exploits the rich geometric information embedded within images while reasoning, like humans do. Our framework is the first to enable 3D mentaling during reasoning without any 3D prior input, and it does not rely on explicitly labeled 3D data for training. Specifically, our training consists of two stages. First, we perform supervised training to align the 3D latent generated by VLM while reasoning with that of a 3D foundation model (e.g., VGGT). Then, we optimize the entire reasoning trajectory solely based on outcome signals, thereby refining the underlying 3D mentaling. Extensive experiments across multiple benchmarks show that 3DThinker consistently outperforms strong baselines and offers a new perspective toward unifying 3D representations into multimodal reasoning. Our code will be available at https://github.com/zhangquanchen/3DThinker.",
      "authors": [
        "Zhangquan Chen",
        "Manyuan Zhang",
        "Xinlei Yu",
        "Xufang Luo",
        "Mingze Sun",
        "Zihao Pan",
        "Yan Feng",
        "Peng Pei",
        "Xunliang Cai",
        "Ruqi Huang"
      ],
      "published_date": "2025-10-21",
      "pdf_url": "https://arxiv.org/pdf/2510.18632v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning",
      "arxiv_id": "",
      "abstract": "Universal multimodal embedding models are foundational to various tasks. Existing approaches typically employ in-batch negative mining by measuring the similarity of query-candidate pairs. However, these methods often struggle to capture subtle semantic differences among candidates and lack diversity in negative samples. Moreover, the embeddings exhibit limited discriminative ability in distinguishing false and hard negatives. In this paper, we leverage the advanced understanding capabilities of MLLMs to enhance representation learning and present a novel Universal Multimodal Embedding (UniME-V2) model. Our approach first constructs a potential hard negative set through global retrieval. We then introduce the MLLM-as-a-Judge mechanism, which utilizes MLLMs to assess the semantic alignment of query-candidate pairs and generate soft semantic matching scores. These scores serve as a foundation for hard negative mining, mitigating the impact of false negatives and enabling the identification of diverse, high-quality hard negatives. Furthermore, the semantic matching scores are used as soft labels to mitigate the rigid one-to-one mapping constraint. By aligning the similarity matrix with the soft semantic matching score matrix, the model learns semantic distinctions among candidates, significantly enhancing its discriminative capacity. To further improve performance, we propose UniME-V2-Reranker, a reranking model trained on our mined hard negatives through a joint pairwise and listwise optimization approach. We conduct comprehensive experiments on the MMEB benchmark and multiple retrieval tasks, demonstrating that our method achieves state-of-the-art performance on average across all tasks.",
      "authors": [
        "Tiancheng Gu",
        "Kaicheng Yang",
        "Kaichen Zhang",
        "Xiang An",
        "Ziyong Feng",
        "Yueyi Zhang",
        "Weidong Cai",
        "Jiankang Deng",
        "Lidong Bing"
      ],
      "published_date": "2025-10-15",
      "pdf_url": "https://arxiv.org/pdf/2510.13515v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe",
      "arxiv_id": "",
      "abstract": "Recent advancements in large reasoning models have fueled growing interest in extending such capabilities to multimodal domains. However, despite notable progress in visual reasoning, the lack of transparent and reproducible data curation and training strategies remains a major barrier to scalable research. In this work, we introduce OpenMMReasoner, a fully transparent two-stage recipe for multimodal reasoning spanning supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we construct an 874K-sample cold-start dataset with rigorous step-by-step validation, providing a strong foundation for reasoning capabilities. The subsequent RL stage leverages a 74K-sample dataset across diverse domains to further sharpen and stabilize these abilities, resulting in a more robust and efficient learning process. Extensive evaluations demonstrate that our training recipe not only surpasses strong baselines but also highlights the critical role of data quality and training design in shaping multimodal reasoning performance. Notably, our method achieves a 11.6% improvement over the Qwen2.5-VL-7B-Instruct baseline across nine multimodal reasoning benchmarks, establishing a solid empirical foundation for future large-scale multimodal reasoning research. We open-sourced all our codes, pipeline, and data at https://github.com/EvolvingLMMs-Lab/OpenMMReasoner.",
      "authors": [
        "Kaichen Zhang",
        "Keming Wu",
        "Zuhao Yang",
        "Bo Li",
        "Kairui Hu",
        "Bin Wang",
        "Ziwei Liu",
        "Xingxuan Li",
        "Lidong Bing"
      ],
      "published_date": "2025-11-20",
      "pdf_url": "https://arxiv.org/pdf/2511.16334v4",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Vision-Language-Action (VLA) Models: Concepts, Progress, Applications and Challenges",
      "arxiv_id": "",
      "abstract": "Vision-Language-Action (VLA) models mark a transformative advancement in artificial intelligence, aiming to unify perception, natural language understanding, and embodied action within a single computational framework. This foundational review presents a comprehensive synthesis of recent advancements in Vision-Language-Action models, systematically organized across five thematic pillars that structure the landscape of this rapidly evolving field. We begin by establishing the conceptual foundations of VLA systems, tracing their evolution from cross-modal learning architectures to generalist agents that tightly integrate vision-language models (VLMs), action planners, and hierarchical controllers. Our methodology adopts a rigorous literature review framework, covering over 80 VLA models published in the past three years. Key progress areas include architectural innovations, efficient training strategies, and real-time inference accelerations. We explore diverse application domains such as autonomous vehicles, medical and industrial robotics, precision agriculture, humanoid robotics, and augmented reality. We analyzed challenges and propose solutions including agentic adaptation and cross-embodiment planning. Furthermore, we outline a forward-looking roadmap where VLA models, VLMs, and agentic AI converge to strengthen socially aligned, adaptive, and general-purpose embodied agents. This work, is expected to serve as a foundational reference for advancing intelligent, real-world robotics and artificial general intelligence. The project repository is available on GitHub as https://github.com/Applied-AI-Research-Lab/Vision-Language-Action-Models-Concepts-Progress-Applications-and-Challenges. [Index Terms: Vision Language Action, VLA, Vision Language Models, VLMs, Action Tokenization, NLP]",
      "authors": [
        "Ranjan Sapkota",
        "Yang Cao",
        "Konstantinos I. Roumeliotis",
        "Manoj Karkee"
      ],
      "published_date": "2025-05-07",
      "pdf_url": "https://arxiv.org/pdf/2505.04769v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Leave No Observation Behind: Real-time Correction for VLA Action Chunks",
      "arxiv_id": "",
      "abstract": "To improve efficiency and temporal coherence, Vision-Language-Action (VLA) models often predict action chunks; however, this action chunking harms reactivity under inference delay and long horizons. We introduce Asynchronous Action Chunk Correction (A2C2), which is a lightweight real-time chunk correction head that runs every control step and adds a time-aware correction to any off-the-shelf VLA's action chunk. The module combines the latest observation, the predicted action from VLA (base action), a positional feature that encodes the index of the base action within the chunk, and some features from the base policy, then outputs a per-step correction. This preserves the base model's competence while restoring closed-loop responsiveness. The approach requires no retraining of the base policy and is orthogonal to asynchronous execution schemes such as Real Time Chunking (RTC). On the dynamic Kinetix task suite (12 tasks) and LIBERO Spatial, our method yields consistent success rate improvements across increasing delays and execution horizons (+23% point and +7% point respectively, compared to RTC), and also improves robustness for long horizons even with zero injected delay. Since the correction head is small and fast, there is minimal overhead compared to the inference of large VLA models. These results indicate that A2C2 is an effective, plug-in mechanism for deploying high-capacity chunking policies in real-time control.",
      "authors": [
        "Kohei Sendai",
        "Maxime Alvarez",
        "Tatsuya Matsushima",
        "Yutaka Matsuo",
        "Yusuke Iwasawa"
      ],
      "published_date": "2025-09-27",
      "pdf_url": "https://arxiv.org/pdf/2509.23224v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Latent Chain-of-Thought World Modeling for End-to-End Driving",
      "arxiv_id": "",
      "abstract": "Recent Vision-Language-Action (VLA) models for autonomous driving explore inference-time reasoning as a way to improve driving performance and safety in challenging scenarios. Most prior work uses natural language to express chain-of-thought (CoT) reasoning before producing driving actions. However, text may not be the most efficient representation for reasoning. In this work, we present Latent-CoT-Drive (LCDrive): a model that expresses CoT in a latent language that captures possible outcomes of the driving actions being considered. Our approach unifies CoT reasoning and decision making by representing both in an action-aligned latent space. Instead of natural language, the model reasons by interleaving (1) action-proposal tokens, which use the same vocabulary as the model's output actions; and (2) world model tokens, which are grounded in a learned latent world model and express future outcomes of these actions. We cold start latent CoT by supervising the model's action proposals and world model tokens based on ground-truth future rollouts of the scene. We then post-train with closed-loop reinforcement learning to strengthen reasoning capabilities. On a large-scale end-to-end driving benchmark, LCDrive achieves faster inference, better trajectory quality, and larger improvements from interactive reinforcement learning compared to both non-reasoning and text-reasoning baselines.",
      "authors": [
        "Shuhan Tan",
        "Kashyap Chitta",
        "Yuxiao Chen",
        "Ran Tian",
        "Yurong You",
        "Yan Wang",
        "Wenjie Luo",
        "Yulong Cao",
        "Philipp Krahenbuhl",
        "Marco Pavone",
        "Boris Ivanovic"
      ],
      "published_date": "2025-12-11",
      "pdf_url": "https://arxiv.org/pdf/2512.10226v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks",
      "arxiv_id": "",
      "abstract": "Built upon language and vision foundation models with strong generalization ability and trained on large-scale robotic data, Vision-Language-Action (VLA) models have recently emerged as a promising approach to learning generalist robotic policies. However, a key drawback of existing VLAs is their extremely high inference costs. In this paper, we propose HyperVLA to address this problem. Unlike existing monolithic VLAs that activate the whole model during both training and inference, HyperVLA uses a novel hypernetwork (HN)-based architecture that activates only a small task-specific policy during inference, while still retaining the high model capacity needed to accommodate diverse multi-task behaviors during training. Successfully training an HN-based VLA is nontrivial so HyperVLA contains several key algorithm design features that improve its performance, including properly utilizing the prior knowledge from existing vision foundation models, HN normalization, and an action generation strategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even higher success rate for both zero-shot generalization and few-shot adaptation, while significantly reducing inference costs. Compared to OpenVLA, a state-of-the-art VLA model, HyperVLA reduces the number of activated parameters at test time by $90\\times$, and accelerates inference speed by $120\\times$. Code is publicly available at https://github.com/MasterXiong/HyperVLA",
      "authors": [
        "Zheng Xiong",
        "Kang Li",
        "Zilin Wang",
        "Matthew Jackson",
        "Jakob Foerster",
        "Shimon Whiteson"
      ],
      "published_date": "2025-10-06",
      "pdf_url": "https://arxiv.org/pdf/2510.04898v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models",
      "arxiv_id": "",
      "abstract": "Vision-Language-Action (VLA) models pretrained on large-scale multimodal datasets have emerged as powerful foundations for robotic perception and control. However, their massive scale, often billions of parameters, poses significant challenges for real-time deployment, as inference becomes computationally expensive and latency-sensitive in dynamic environments. To address this, we propose Token Expand-and-Merge-VLA (TEAM-VLA), a training-free token compression framework that accelerates VLA inference while preserving task performance. TEAM-VLA introduces a dynamic token expansion mechanism that identifies and samples additional informative tokens in the spatial vicinity of attention-highlighted regions, enhancing contextual completeness. These expanded tokens are then selectively merged in deeper layers under action-aware guidance, effectively reducing redundancy while maintaining semantic coherence. By coupling expansion and merging within a single feed-forward pass, TEAM-VLA achieves a balanced trade-off between efficiency and effectiveness, without any retraining or parameter updates. Extensive experiments on LIBERO benchmark demonstrate that TEAM-VLA consistently improves inference speed while maintaining or even surpassing the task success rate of full VLA models. The code is public available on \\href{https://github.com/Jasper-aaa/TEAM-VLA}{https://github.com/Jasper-aaa/TEAM-VLA}",
      "authors": [
        "Yifan Ye",
        "Jiaqi Ma",
        "Jun Cen",
        "Zhihe Lu"
      ],
      "published_date": "2025-12-10",
      "pdf_url": "https://arxiv.org/pdf/2512.09927v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation",
      "arxiv_id": null,
      "abstract": "The Transformer architecture has demonstrated remarkable results in 3D medical image segmentation due to its capability of modeling global relationships. However, it poses a significant computational burden when processing high-dimensional medical images. Mamba, as a State Space Model (SSM), has recently emerged as a notable approach for modeling long-range dependencies in sequential data. Although a substantial amount of Mamba-based research has focused on natural language and 2D image processing, few studies explore the capability of Mamba on 3D medical images. In this paper, we propose SegMamba-V2, a novel 3D medical image segmentation model, to effectively capture long-range dependencies within whole-volume features at each scale. To achieve this goal, we first devise a hierarchical scale downsampling strategy to enhance the receptive field and mitigate information loss during downsampling. Furthermore, we design a novel tri-orientated spatial Mamba block that extends the global dependency modeling process from one plane to three orthogonal planes to improve feature representation capability. Moreover, we collect and annotate a large-scale dataset (named CRC-2000) with fine-grained categories to facilitate benchmarking evaluation in 3D colorectal cancer (CRC) segmentation. We evaluate the effectiveness of our SegMamba-V2 on CRC-2000 and three other large-scale 3D medical image segmentation datasets, covering various modalities, organs, and segmentation targets. Experimental results demonstrate that our Segmamba-V2 outperforms state-of-the-art methods by a significant margin, which indicates the universality and effectiveness of the proposed model on 3D medical image segmentation tasks. The code for SegMamba-V2 is publicly available at: https://github.com/ge-xing/SegMamba-V2",
      "authors": [
        "Zhaohu Xing",
        "Tian Ye",
        "Yijun Yang",
        "D. Cai",
        "Baowen Gai",
        "Xiao-Jian Wu",
        "Feng Gao",
        "Lei Zhu"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 12,
      "year": 2026
    },
    {
      "title": "TransSIL: A Silhouette Cue-Aware Image Classification Framework for Bird Ecological Monitoring Systems",
      "arxiv_id": null,
      "abstract": "How to automatically recognize the bird species has caused concerns in ecological systems and intelligent ecological monitoring system due to the increasing threat to bird ecological society and bird species diversity. However, traditional monitoring systems are susceptible to specific challenges when operating in complex environments, such as complex environments, multifarious postures, and backlight scenarios. To effectively address these challenges, we present a novel bird ecological intelligent detection system (TransSIL) for fine-grained bird image classification (FBIC) in diverse ecological to learn discriminative features by explicitly incorporating silhouette structural information alongside critical visual cues. Specifically, the approach begins with a silhouette token construction module to estimate the bird silhouette and extract silhouette tokens. Then, a silhouette relationship mining module is developed to fuse visual and silhouette tokens and capture long-range dependencies between them. In addition, to learn bird distinctive features at multiple levels, a critical cues awareness module is embedded within TransSIL. The performance of TransSIL was evaluated on two bird datasets, such as CUB200-2011 and NABirds. The framework demonstrates significant improvements over existing ecological intelligent surveillance methods. By utilizing silhouette and visual dependencies, we anticipate that our approach will ultimately contribute to the conservation of avian ecological societies.",
      "authors": [
        "Hai Liu",
        "Yu Song",
        "Tingting Liu",
        "Lin Chen",
        "Zhaoli Zhang",
        "Xiaolan Yang",
        "Neal N. Xiong"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 6,
      "year": 2026
    },
    {
      "title": "CBRFormer: rendering technology-based transformer for refinement segmentation of bridge crack images",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Honghu Chu",
        "Jiahao Gai",
        "Weiwei Chen",
        "Jun Ma"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 6,
      "year": 2026
    },
    {
      "title": "Deep contrastive learning enables genome-wide virtual screening.",
      "arxiv_id": null,
      "abstract": "Recent breakthroughs in protein structure prediction have opened new avenues for genome-wide drug discovery, yet existing virtual screening methods remain computationally prohibitive. We present DrugCLIP, a contrastive learning framework that achieves ultrafast and accurate virtual screening, up to 10 million times faster than docking, while consistently outperforming various baselines on in silico benchmarks. In wet-lab validations, DrugCLIP achieved a 15% hit rate for norepinephrine transporter, and structures of two identified inhibitors were determined in complex with the target protein. For thyroid hormone receptor interactor 12, a target that lacks holo structures and small-molecule binders, DrugCLIP achieved a 17.5% hit rate using only AlphaFold2-predicted structures. Finally, we released GenomeScreenDB, an open-access database providing precomputed results for ~10,000 human proteins screened against 500 million compounds, pioneering a drug discovery paradigm in the post-AlphaFold era.",
      "authors": [
        "Yinjun Jia",
        "Bowen Gao",
        "Jiaxin Tan",
        "Jiqing Zheng",
        "Xin Hong",
        "Wenyu Zhu",
        "Haichuan Tan",
        "Yuan Xiao",
        "Liping Tan",
        "Hongyi Cai",
        "Yanwen Huang",
        "Zhiheng Deng",
        "Xiangwei Wu",
        "Yue Jin",
        "Yafei Yuan",
        "Jiekang Tian",
        "Wei He",
        "Weiying Ma",
        "Ya-Qin Zhang",
        "Lei Liu",
        "Chuangye Yan",
        "Wei Zhang",
        "Yanyan Lan"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 9,
      "year": 2026
    },
    {
      "title": "LLaVA-based semantic feature modulation diffusion model for underwater image enhancement",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Guodong Fan",
        "Shengning Zhou",
        "Zhen Hua",
        "Jinjiang Li",
        "Jingchun Zhou"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 8,
      "year": 2026
    },
    {
      "title": "3DGS-Drag: Dragging Gaussians for Intuitive Point-Based 3D Editing",
      "arxiv_id": "",
      "abstract": "The transformative potential of 3D content creation has been progressively unlocked through advancements in generative models. Recently, intuitive drag editing with geometric changes has attracted significant attention in 2D editing yet remains challenging for 3D scenes. In this paper, we introduce 3DGS-Drag -- a point-based 3D editing framework that provides efficient, intuitive drag manipulation of real 3D scenes. Our approach bridges the gap between deformation-based and 2D-editing-based 3D editing methods, addressing their limitations to geometry-related content editing. We leverage two key innovations: deformation guidance utilizing 3D Gaussian Splatting for consistent geometric modifications and diffusion guidance for content correction and visual quality enhancement. A progressive editing strategy further supports aggressive 3D drag edits. Our method enables a wide range of edits, including motion change, shape adjustment, inpainting, and content extension. Experimental results demonstrate the effectiveness of 3DGS-Drag in various scenes, achieving state-of-the-art performance in geometry-related 3D content editing. Notably, the editing is efficient, taking 10 to 20 minutes on a single RTX 4090 GPU.",
      "authors": [
        "Jiahua Dong",
        "Yu-Xiong Wang"
      ],
      "published_date": "2026-01-12",
      "pdf_url": "https://arxiv.org/pdf/2601.07963v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Attention mechanisms in neural networks",
      "arxiv_id": "",
      "abstract": "Attention mechanisms represent a fundamental paradigm shift in neural network architectures, enabling models to selectively focus on relevant portions of input sequences through learned weighting functions. This monograph provides a comprehensive and rigorous mathematical treatment of attention mechanisms, encompassing their theoretical foundations, computational properties, and practical implementations in contemporary deep learning systems. Applications in natural language processing, computer vision, and multimodal learning demonstrate the versatility of attention mechanisms. We examine language modeling with autoregressive transformers, bidirectional encoders for representation learning, sequence-to-sequence translation, Vision Transformers for image classification, and cross-modal attention for vision-language tasks. Empirical analysis reveals training characteristics, scaling laws that relate performance to model size and computation, attention pattern visualizations, and performance benchmarks across standard datasets. We discuss the interpretability of learned attention patterns and their relationship to linguistic and visual structures. The monograph concludes with a critical examination of current limitations, including computational scalability, data efficiency, systematic generalization, and interpretability challenges.",
      "authors": [
        "Hasi Hays"
      ],
      "published_date": "2026-01-06",
      "pdf_url": "https://arxiv.org/pdf/2601.03329v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Bidirectional Normalizing Flow: From Data to Noise and Back",
      "arxiv_id": "",
      "abstract": "Normalizing Flows (NFs) have been established as a principled framework for generative modeling. Standard NFs consist of a forward process and a reverse process: the forward process maps data to noise, while the reverse process generates samples by inverting it. Typical NF forward transformations are constrained by explicit invertibility, ensuring that the reverse process can serve as their exact analytic inverse. Recent developments in TARFlow and its variants have revitalized NF methods by combining Transformers and autoregressive flows, but have also exposed causal decoding as a major bottleneck. In this work, we introduce Bidirectional Normalizing Flow ($\\textbf{BiFlow}$), a framework that removes the need for an exact analytic inverse. BiFlow learns a reverse model that approximates the underlying noise-to-data inverse mapping, enabling more flexible loss functions and architectures. Experiments on ImageNet demonstrate that BiFlow, compared to its causal decoding counterpart, improves generation quality while accelerating sampling by up to two orders of magnitude. BiFlow yields state-of-the-art results among NF-based methods and competitive performance among single-evaluation (\"1-NFE\") methods. Following recent encouraging progress on NFs, we hope our work will draw further attention to this classical paradigm.",
      "authors": [
        "Yiyang Lu",
        "Qiao Sun",
        "Xianbang Wang",
        "Zhicheng Jiang",
        "Hanhong Zhao",
        "Kaiming He"
      ],
      "published_date": "2025-12-11",
      "pdf_url": "https://arxiv.org/pdf/2512.10953v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "One-step Latent-free Image Generation with Pixel Mean Flows",
      "arxiv_id": "",
      "abstract": "Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose \"pixel MeanFlow\" (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models.",
      "authors": [
        "Yiyang Lu",
        "Susie Lu",
        "Qiao Sun",
        "Hanhong Zhao",
        "Zhicheng Jiang",
        "Xianbang Wang",
        "Tianhong Li",
        "Zhengyang Geng",
        "Kaiming He"
      ],
      "published_date": "2026-01-29",
      "pdf_url": "https://arxiv.org/pdf/2601.22158v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Meta Flow Maps enable scalable reward alignment",
      "arxiv_id": "",
      "abstract": "Controlling generative models is computationally expensive. This is because optimal alignment with a reward function--whether via inference-time steering or fine-tuning--requires estimating the value function. This task demands access to the conditional posterior $p_{1|t}(x_1|x_t)$, the distribution of clean data $x_1$ consistent with an intermediate state $x_t$, a requirement that typically compels methods to resort to costly trajectory simulations. To address this bottleneck, we introduce Meta Flow Maps (MFMs), a framework extending consistency models and flow maps into the stochastic regime. MFMs are trained to perform stochastic one-step posterior sampling, generating arbitrarily many i.i.d. draws of clean data $x_1$ from any intermediate state. Crucially, these samples provide a differentiable reparametrization that unlocks efficient value function estimation. We leverage this capability to solve bottlenecks in both paradigms: enabling inference-time steering without inner rollouts, and facilitating unbiased, off-policy fine-tuning to general rewards. Empirically, our single-particle steered-MFM sampler outperforms a Best-of-1000 baseline on ImageNet across multiple rewards at a fraction of the compute.",
      "authors": [
        "Peter Potaptchik",
        "Adhi Saravanan",
        "Abbas Mammadov",
        "Alvaro Prat",
        "Michael S. Albergo",
        "Yee Whye Teh"
      ],
      "published_date": "2026-01-20",
      "pdf_url": "https://arxiv.org/pdf/2601.14430v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective",
      "arxiv_id": "",
      "abstract": "Sequential prediction from streaming observations is a fundamental problem in stochastic dynamical systems, where inherent uncertainty often leads to multiple plausible futures. While diffusion and flow-matching models are capable of modeling complex, multi-modal trajectories, their deployment in real-time streaming environments typically relies on repeated sampling from a non-informative initial distribution, incurring substantial inference latency and potential system backlogs. In this work, we introduce Sequential Flow Matching, a principled framework grounded in Bayesian filtering. By treating streaming inference as learning a probability flow that transports the predictive distribution from one time step to the next, our approach naturally aligns with the recursive structure of Bayesian belief updates. We provide theoretical justification that initializing generation from the previous posterior offers a principled warm start that can accelerate sampling compared to naïve re-sampling. Across a wide range of forecasting, decision-making and state estimation tasks, our method achieves performance competitive with full-step diffusion while requiring only one or very few sampling steps, therefore with faster sampling. It suggests that framing sequential inference via Bayesian filtering provides a new and principled perspective towards efficient real-time deployment of flow-based models.",
      "authors": [
        "Yinan Huang",
        "Hans Hao-Hsun Hsu",
        "Junran Wang",
        "Bo Dai",
        "Pan Li"
      ],
      "published_date": "2026-02-05",
      "pdf_url": "https://arxiv.org/pdf/2602.05319v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Generative Modeling via Drifting",
      "arxiv_id": "",
      "abstract": "Generative modeling can be formulated as learning a mapping f such that its pushforward distribution matches the data distribution. The pushforward behavior can be carried out iteratively at inference time, for example in diffusion and flow-based models. In this paper, we propose a new paradigm called Drifting Models, which evolve the pushforward distribution during training and naturally admit one-step inference. We introduce a drifting field that governs the sample movement and achieves equilibrium when the distributions match. This leads to a training objective that allows the neural network optimizer to evolve the distribution. In experiments, our one-step generator achieves state-of-the-art results on ImageNet at 256 x 256 resolution, with an FID of 1.54 in latent space and 1.61 in pixel space. We hope that our work opens up new opportunities for high-quality one-step generation.",
      "authors": [
        "Mingyang Deng",
        "He Li",
        "Tianhong Li",
        "Yilun Du",
        "Kaiming He"
      ],
      "published_date": "2026-02-04",
      "pdf_url": "https://arxiv.org/pdf/2602.04770v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "arxiv_id": "",
      "abstract": "This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.",
      "authors": [
        "Ting Chen",
        "Simon Kornblith",
        "Mohammad Norouzi",
        "Geoffrey Hinton"
      ],
      "published_date": "2020-02-13",
      "pdf_url": "https://arxiv.org/pdf/2002.05709v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning",
      "arxiv_id": "",
      "abstract": "Recent studies have demonstrated significant progress in aligning text-to-image diffusion models with human preference via Reinforcement Learning from Human Feedback. However, while existing methods achieve high scores on automated reward metrics, they often lead to Preference Mode Collapse (PMC)-a specific form of reward hacking where models converge on narrow, high-scoring outputs (e.g., images with monolithic styles or pervasive overexposure), severely degrading generative diversity. In this work, we introduce and quantify this phenomenon, proposing DivGenBench, a novel benchmark designed to measure the extent of PMC. We posit that this collapse is driven by over-optimization along the reward model's inherent biases. Building on this analysis, we propose Directional Decoupling Alignment (D$^2$-Align), a novel framework that mitigates PMC by directionally correcting the reward signal. Specifically, our method first learns a directional correction within the reward model's embedding space while keeping the model frozen. This correction is then applied to the reward signal during the optimization process, preventing the model from collapsing into specific modes and thereby maintaining diversity. Our comprehensive evaluation, combining qualitative analysis with quantitative metrics for both quality and diversity, reveals that D$^2$-Align achieves superior alignment with human preference.",
      "authors": [
        "Chubin Chen",
        "Sujie Hu",
        "Jiashu Zhu",
        "Meiqi Wu",
        "Jintao Chen",
        "Yanxun Li",
        "Nisha Huang",
        "Chengyu Fang",
        "Jiahong Wu",
        "Xiangxiang Chu",
        "Xiu Li"
      ],
      "published_date": "2025-12-30",
      "pdf_url": "https://arxiv.org/pdf/2512.24146v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints",
      "arxiv_id": "",
      "abstract": "Video generation models have achieved remarkable progress, particularly excelling in realistic scenarios; however, their performance degrades notably in imaginative scenarios. These prompts often involve rarely co-occurring concepts with long-distance semantic relationships, falling outside training distributions. Existing methods typically apply test-time scaling for improving video quality, but their fixed search spaces and static reward designs limit adaptability to imaginative scenarios. To fill this gap, we propose ImagerySearch, a prompt-guided adaptive test-time search strategy that dynamically adjusts both the inference search space and reward function according to semantic relationships in the prompt. This enables more coherent and visually plausible videos in challenging imaginative settings. To evaluate progress in this direction, we introduce LDT-Bench, the first dedicated benchmark for long-distance semantic prompts, consisting of 2,839 diverse concept pairs and an automated protocol for assessing creative generation capabilities. Extensive experiments show that ImagerySearch consistently outperforms strong video generation baselines and existing test-time scaling approaches on LDT-Bench, and achieves competitive improvements on VBench, demonstrating its effectiveness across diverse prompt types. We will release LDT-Bench and code to facilitate future research on imaginative video generation.",
      "authors": [
        "Meiqi Wu",
        "Jiashu Zhu",
        "Xiaokun Feng",
        "Chubin Chen",
        "Chen Zhu",
        "Bingze Song",
        "Fangyuan Mao",
        "Jiahong Wu",
        "Xiangxiang Chu",
        "Kaiqi Huang"
      ],
      "published_date": "2025-10-16",
      "pdf_url": "https://arxiv.org/pdf/2510.14847v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders",
      "arxiv_id": "",
      "abstract": "Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.",
      "authors": [
        "Shengbang Tong",
        "Boyang Zheng",
        "Ziteng Wang",
        "Bingda Tang",
        "Nanye Ma",
        "Ellis Brown",
        "Jihan Yang",
        "Rob Fergus",
        "Yann LeCun",
        "Saining Xie"
      ],
      "published_date": "2026-01-22",
      "pdf_url": "https://arxiv.org/pdf/2601.16208v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Simulating the Visual World with Artificial Intelligence: A Roadmap",
      "arxiv_id": "",
      "abstract": "The landscape of video generation is shifting, from a focus on generating visually appealing clips to building virtual environments that support interaction and maintain physical plausibility. These developments point toward the emergence of video foundation models that function not only as visual generators but also as implicit world models, models that simulate the physical dynamics, agent-environment interactions, and task planning that govern real or imagined worlds. This survey provides a systematic overview of this evolution, conceptualizing modern video foundation models as the combination of two core components: an implicit world model and a video renderer. The world model encodes structured knowledge about the world, including physical laws, interaction dynamics, and agent behavior. It serves as a latent simulation engine that enables coherent visual reasoning, long-term temporal consistency, and goal-driven planning. The video renderer transforms this latent simulation into realistic visual observations, effectively producing videos as a \"window\" into the simulated world. We trace the progression of video generation through four generations, in which the core capabilities advance step by step, ultimately culminating in a world model, built upon a video generation model, that embodies intrinsic physical plausibility, real-time multimodal interaction, and planning capabilities spanning multiple spatiotemporal scales. For each generation, we define its core characteristics, highlight representative works, and examine their application domains such as robotics, autonomous driving, and interactive gaming. Finally, we discuss open challenges and design principles for next-generation world models, including the role of agent intelligence in shaping and evaluating these systems. An up-to-date list of related works is maintained at this link.",
      "authors": [
        "Jingtong Yue",
        "Ziqi Huang",
        "Zhaoxi Chen",
        "Xintao Wang",
        "Pengfei Wan",
        "Ziwei Liu"
      ],
      "published_date": "2025-11-11",
      "pdf_url": "https://arxiv.org/pdf/2511.08585v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks",
      "arxiv_id": "",
      "abstract": "World models have emerged as a critical frontier in AI research, aiming to enhance large models by infusing them with physical dynamics and world knowledge. The core objective is to enable agents to understand, predict, and interact with complex environments. However, current research landscape remains fragmented, with approaches predominantly focused on injecting world knowledge into isolated tasks, such as visual prediction, 3D estimation, or symbol grounding, rather than establishing a unified definition or framework. While these task-specific integrations yield performance gains, they often lack the systematic coherence required for holistic world understanding. In this paper, we analyze the limitations of such fragmented approaches and propose a unified design specification for world models. We suggest that a robust world model should not be a loose collection of capabilities but a normative framework that integrally incorporates interaction, perception, symbolic reasoning, and spatial representation. This work aims to provide a structured perspective to guide future research toward more general, robust, and principled models of the world.",
      "authors": [
        "Bohan Zeng",
        "Kaixin Zhu",
        "Daili Hua",
        "Bozhou Li",
        "Chengzhuo Tong",
        "Yuran Wang",
        "Xinyi Huang",
        "Yifan Dai",
        "Zixiang Zhang",
        "Yifan Yang",
        "Zhou Liu",
        "Hao Liang",
        "Xiaochen Ma",
        "Ruichuan An",
        "Tianyi Bai",
        "Hongcheng Gao",
        "Junbo Niu",
        "Yang Shi",
        "Xinlong Chen",
        "Yue Ding",
        "Minglei Shi",
        "Kai Zeng",
        "Yiwen Tang",
        "Yuanxing Zhang",
        "Pengfei Wan",
        "Xintao Wang",
        "Wentao Zhang"
      ],
      "published_date": "2026-02-02",
      "pdf_url": "https://arxiv.org/pdf/2602.01630v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "RecTok: Reconstruction Distillation along Rectified Flow",
      "arxiv_id": "",
      "abstract": "Visual tokenizers play a crucial role in diffusion models. The dimensionality of latent space governs both reconstruction fidelity and the semantic expressiveness of the latent feature. However, a fundamental trade-off is inherent between dimensionality and generation quality, constraining existing methods to low-dimensional latent spaces. Although recent works have leveraged vision foundation models to enrich the semantics of visual tokenizers and accelerate convergence, high-dimensional tokenizers still underperform their low-dimensional counterparts. In this work, we propose RecTok, which overcomes the limitations of high-dimensional visual tokenizers through two key innovations: flow semantic distillation and reconstruction--alignment distillation. Our key insight is to make the forward flow in flow matching semantically rich, which serves as the training space of diffusion transformers, rather than focusing on the latent space as in previous works. Specifically, our method distills the semantic information in VFMs into the forward flow trajectories in flow matching. And we further enhance the semantics by introducing a masked feature reconstruction loss. Our RecTok achieves superior image reconstruction, generation quality, and discriminative performance. It achieves state-of-the-art results on the gFID-50K under both with and without classifier-free guidance settings, while maintaining a semantically rich latent space structure. Furthermore, as the latent dimensionality increases, we observe consistent improvements. Code and model are available at https://shi-qingyu.github.io/rectok.github.io.",
      "authors": [
        "Qingyu Shi",
        "Size Wu",
        "Jinbin Bai",
        "Kaidong Yu",
        "Yujing Wang",
        "Yunhai Tong",
        "Xiangtai Li",
        "Xuelong Li"
      ],
      "published_date": "2025-12-15",
      "pdf_url": "https://arxiv.org/pdf/2512.13421v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "RePack then Refine: Efficient Diffusion Transformer with Vision Foundation Model",
      "arxiv_id": "",
      "abstract": "Semantic-rich features from Vision Foundation Models (VFMs) have been leveraged to enhance Latent Diffusion Models (LDMs). However, raw VFM features are typically high-dimensional and redundant, increasing the difficulty of learning and reducing training efficiency for Diffusion Transformers (DiTs). In this paper, we propose Repack then Refine, a three-stage framework that brings the semantic-rich VFM features to DiT while further accelerating learning efficiency. Specifically, the RePack module projects the high-dimensional features onto a compact, low-dimensional manifold. This filters out the redundancy while preserving essential structural information. A standard DiT is then trained for generative modeling on this highly compressed latent space. Finally, to restore the high-frequency details lost due to the compression in RePack, we propose a Latent-Guided Refiner, which is trained lastly for enhancing the image details. On ImageNet-1K, RePack-DiT-XL/1 achieves an FID of 1.82 in only 64 training epochs. With the Refiner module, performance further improves to an FID of 1.65, significantly surpassing latest LDMs in terms of convergence efficiency. Our results demonstrate that packing VFM features, followed by targeted refinement, is a highly effective strategy for balancing generative fidelity with training efficiency.",
      "authors": [
        "Guanfang Dong",
        "Luke Schultz",
        "Negar Hassanpour",
        "Chao Gao"
      ],
      "published_date": "2025-12-12",
      "pdf_url": "https://arxiv.org/pdf/2512.12083v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Diffusion Models Beat GANs on Image Synthesis",
      "arxiv_id": "",
      "abstract": "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion",
      "authors": [
        "Prafulla Dhariwal",
        "Alex Nichol"
      ],
      "published_date": "2021-05-11",
      "pdf_url": "https://arxiv.org/pdf/2105.05233v4",
      "citation_count": null,
      "year": null
    },
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "arxiv_id": "",
      "abstract": "The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.",
      "authors": [
        "Maxime Oquab",
        "Timothée Darcet",
        "Théo Moutakanni",
        "Huy Vo",
        "Marc Szafraniec",
        "Vasil Khalidov",
        "Pierre Fernandez",
        "Daniel Haziza",
        "Francisco Massa",
        "Alaaeldin El-Nouby",
        "Mahmoud Assran",
        "Nicolas Ballas",
        "Wojciech Galuba",
        "Russell Howes",
        "Po-Yao Huang",
        "Shang-Wen Li",
        "Ishan Misra",
        "Michael Rabbat",
        "Vasu Sharma",
        "Gabriel Synnaeve",
        "Hu Xu",
        "Hervé Jegou",
        "Julien Mairal",
        "Patrick Labatut",
        "Armand Joulin",
        "Piotr Bojanowski"
      ],
      "published_date": "2023-04-14",
      "pdf_url": "https://arxiv.org/pdf/2304.07193v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss",
      "arxiv_id": "",
      "abstract": "Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAEs in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leaving existing pixel diffusion methods lagging behind latent diffusion models. We propose PixelGen, a simple pixel diffusion framework with perceptual supervision. Instead of modeling the full image manifold, PixelGen introduces two complementary perceptual losses to guide diffusion model towards learning a more meaningful perceptual manifold. An LPIPS loss facilitates learning better local patterns, while a DINO-based perceptual loss strengthens global semantics. With perceptual supervision, PixelGen surpasses strong latent diffusion baselines. It achieves an FID of 5.11 on ImageNet-256 without classifier-free guidance using only 80 training epochs, and demonstrates favorable scaling performance on large-scale text-to-image generation with a GenEval score of 0.79. PixelGen requires no VAEs, no latent representations, and no auxiliary stages, providing a simpler yet more powerful generative paradigm. Codes are publicly available at https://github.com/Zehong-Ma/PixelGen.",
      "authors": [
        "Zehong Ma",
        "Ruihan Xu",
        "Shiliang Zhang"
      ],
      "published_date": "2026-02-02",
      "pdf_url": "https://arxiv.org/pdf/2602.02493v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
      "arxiv_id": "",
      "abstract": "Recent years have seen remarkable progress in both multimodal understanding models and image generation models. Despite their respective successes, these two domains have evolved independently, leading to distinct architectural paradigms: While autoregressive-based architectures have dominated multimodal understanding, diffusion-based models have become the cornerstone of image generation. Recently, there has been growing interest in developing unified frameworks that integrate these tasks. The emergence of GPT-4o's new capabilities exemplifies this trend, highlighting the potential for unification. However, the architectural differences between the two domains pose significant challenges. To provide a clear overview of current efforts toward unification, we present a comprehensive survey aimed at guiding future research. First, we introduce the foundational concepts and recent advancements in multimodal understanding and text-to-image generation models. Next, we review existing unified models, categorizing them into three main architectural paradigms: diffusion-based, autoregressive-based, and hybrid approaches that fuse autoregressive and diffusion mechanisms. For each category, we analyze the structural designs and innovations introduced by related works. Additionally, we compile datasets and benchmarks tailored for unified models, offering resources for future exploration. Finally, we discuss the key challenges facing this nascent field, including tokenization strategy, cross-modal attention, and data. As this area is still in its early stages, we anticipate rapid advancements and will regularly update this survey. Our goal is to inspire further research and provide a valuable reference for the community. The references associated with this survey are available on GitHub (https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models).",
      "authors": [
        "Shanshan Zhao",
        "Xinjie Zhang",
        "Jintao Guo",
        "Jiakui Hu",
        "Lunhao Duan",
        "Minghao Fu",
        "Yong Xien Chng",
        "Guo-Hua Wang",
        "Qing-Guo Chen",
        "Zhao Xu",
        "Weihua Luo",
        "Kaifu Zhang"
      ],
      "published_date": "2025-05-05",
      "pdf_url": "https://arxiv.org/pdf/2505.02567v6",
      "citation_count": null,
      "year": null
    },
    {
      "title": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery",
      "arxiv_id": "",
      "abstract": "Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner. To evaluate and probe this capability, we develop MentisOculi, a procedural, stratified suite of multi-step reasoning problems amenable to visual solution, tuned to challenge frontier models. Evaluating visual strategies ranging from latent tokens to explicit generated imagery, we find they generally fail to improve performance. Analysis of UMMs specifically exposes a critical limitation: While they possess the textual reasoning capacity to solve a task and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to leverage even ground-truth visualizations. Our findings suggest that despite their inherent appeal, visual thoughts do not yet benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and close this gap across diverse model families.",
      "authors": [
        "Jana Zeller",
        "Thaddäus Wiedemer",
        "Fanfei Li",
        "Thomas Klein",
        "Prasanna Mayilvahanan",
        "Matthias Bethge",
        "Felix Wichmann",
        "Ryan Cotterell",
        "Wieland Brendel"
      ],
      "published_date": "2026-02-02",
      "pdf_url": "https://arxiv.org/pdf/2602.02465v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation",
      "arxiv_id": "",
      "abstract": "This paper presents a family of advanced vision encoder, named OpenVision 3, that learns a single, unified visual representation that can serve both image understanding and image generation. Our core architecture is simple: we feed VAE-compressed image latents to a ViT encoder and train its output to support two complementary roles. First, the encoder output is passed to the ViT-VAE decoder to reconstruct the original image, encouraging the representation to capture generative structure. Second, the same representation is optimized with contrastive learning and image-captioning objectives, strengthening semantic features. By jointly optimizing reconstruction- and semantics-driven signals in a shared latent space, the encoder learns representations that synergize and generalize well across both regimes. We validate this unified design through extensive downstream evaluations with the encoder frozen. For multimodal understanding, we plug the encoder into the LLaVA-1.5 framework: it performs comparably with a standard CLIP vision encoder (e.g., 62.4 vs 62.2 on SeedBench, and 83.7 vs 82.9 on POPE). For generation, we test it under the RAE framework: ours substantially surpasses the standard CLIP-based encoder (e.g., gFID: 1.89 vs 2.54 on ImageNet). We hope this work can spur future research on unified modeling.",
      "authors": [
        "Letian Zhang",
        "Sucheng Ren",
        "Yanqing Liu",
        "Xianhang Li",
        "Zeyu Wang",
        "Yuyin Zhou",
        "Huaxiu Yao",
        "Zeyu Zheng",
        "Weili Nie",
        "Guilin Liu",
        "Zhiding Yu",
        "Cihang Xie"
      ],
      "published_date": "2026-01-21",
      "pdf_url": "https://arxiv.org/pdf/2601.15369v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Distributed Representations of Words and Phrases and their Compositionality",
      "arxiv_id": "",
      "abstract": "The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
      "authors": [
        "Tomas Mikolov",
        "Ilya Sutskever",
        "Kai Chen",
        "Greg Corrado",
        "Jeffrey Dean"
      ],
      "published_date": "2013-10-16",
      "pdf_url": "https://arxiv.org/pdf/1310.4546v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "GloVe: Global Vectors for Word Representation",
      "arxiv_id": null,
      "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
      "authors": [
        "Jeffrey Pennington",
        "R. Socher",
        "Christopher D. Manning"
      ],
      "published_date": "2014",
      "pdf_url": "",
      "citation_count": 33924,
      "year": 2014
    },
    {
      "title": "Deep Contextualized Word Representations",
      "arxiv_id": "1802.05365",
      "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
      "authors": [
        "Matthew E. Peters",
        "Mark Neumann",
        "Mohit Iyyer",
        "Matt Gardner",
        "Christopher Clark",
        "Kenton Lee",
        "Luke Zettlemoyer"
      ],
      "published_date": "2018-02-15",
      "pdf_url": "https://arxiv.org/pdf/1802.05365v2",
      "citation_count": 12012,
      "year": 2018
    },
    {
      "title": "Protein Language Models: Is Scaling Necessary?",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Quentin Fournier",
        "Robert M. Vernon",
        "Almer M. van der Sloot",
        "Benjamin Schulz",
        "Sarath Chandar",
        "C. Langmead"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 26,
      "year": 2026
    },
    {
      "title": "Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis",
      "arxiv_id": "",
      "abstract": "Modern models for common NLP tasks often employ machine learning techniques and train on journalistic, social media, or other culturally-derived text. These have recently been scrutinized for racial and gender biases, rooting from inherent bias in their training text. These biases are often sub-optimal and recent work poses methods to rectify them; however, these biases may shed light on actual racial or gender gaps in the culture(s) that produced the training text, thereby helping us understand cultural context through big data. This paper presents an approach for quantifying gender bias in word embeddings, and then using them to characterize statistical gender gaps in education, politics, economics, and health. We validate these metrics on 2018 Twitter data spanning 51 U.S. regions and 99 countries. We correlate state and country word embedding biases with 18 international and 5 U.S.-based statistical gender gaps, characterizing regularities and predictive strength.",
      "authors": [
        "Scott Friedman",
        "Sonja Schmer-Galunder",
        "Anthony Chen",
        "Jeffrey Rye"
      ],
      "published_date": "2026-01-23",
      "pdf_url": "https://arxiv.org/pdf/2601.17203v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Generative Classifiers Avoid Shortcut Solutions",
      "arxiv_id": "",
      "abstract": "Discriminative approaches to classification often learn shortcuts that hold in-distribution but fail even under minor distribution shift. This failure mode stems from an overreliance on features that are spuriously correlated with the label. We show that generative classifiers, which use class-conditional generative models, can avoid this issue by modeling all features, both core and spurious, instead of mainly spurious ones. These generative classifiers are simple to train, avoiding the need for specialized augmentations, strong regularization, extra hyperparameters, or knowledge of the specific spurious correlations to avoid. We find that diffusion-based and autoregressive generative classifiers achieve state-of-the-art performance on five standard image and text distribution shift benchmarks and reduce the impact of spurious correlations in realistic applications, such as medical or satellite datasets. Finally, we carefully analyze a Gaussian toy setting to understand the inductive biases of generative classifiers, as well as the data properties that determine when generative classifiers outperform discriminative ones.",
      "authors": [
        "Alexander C. Li",
        "Ananya Kumar",
        "Deepak Pathak"
      ],
      "published_date": "2025-12-31",
      "pdf_url": "https://arxiv.org/pdf/2512.25034v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Edge Large AI Model Agent-Empowered Cognitive Multimodal Semantic Communication",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Y. Sun",
        "Yinqiu Liu",
        "Shaoyong Guo",
        "Xuesong Qiu",
        "Jiewei Chen",
        "Jiakai Hao",
        "Dusist Niyato"
      ],
      "published_date": "2026",
      "pdf_url": "",
      "citation_count": 10,
      "year": 2026
    },
    {
      "title": "Language Models are Unsupervised Multitask Learners",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Alec Radford",
        "Jeff Wu",
        "R. Child",
        "D. Luan",
        "Dario Amodei",
        "I. Sutskever"
      ],
      "published_date": "2019",
      "pdf_url": "",
      "citation_count": 27061,
      "year": 2019
    },
    {
      "title": "The Llama 3 Herd of Models",
      "arxiv_id": "",
      "abstract": "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",
      "authors": [
        "Aaron Grattafiori",
        "Abhimanyu Dubey",
        "Abhinav Jauhri",
        "Abhinav Pandey",
        "Abhishek Kadian",
        "Ahmad Al-Dahle",
        "Aiesha Letman",
        "Akhil Mathur",
        "Alan Schelten",
        "Alex Vaughan",
        "Amy Yang",
        "Angela Fan",
        "Anirudh Goyal",
        "Anthony Hartshorn",
        "Aobo Yang",
        "Archi Mitra",
        "Archie Sravankumar",
        "Artem Korenev",
        "Arthur Hinsvark",
        "Arun Rao",
        "Aston Zhang",
        "Aurelien Rodriguez",
        "Austen Gregerson",
        "Ava Spataru",
        "Baptiste Roziere",
        "Bethany Biron",
        "Binh Tang",
        "Bobbie Chern",
        "Charlotte Caucheteux",
        "Chaya Nayak",
        "Chloe Bi",
        "Chris Marra",
        "Chris McConnell",
        "Christian Keller",
        "Christophe Touret",
        "Chunyang Wu",
        "Corinne Wong",
        "Cristian Canton Ferrer",
        "Cyrus Nikolaidis",
        "Damien Allonsius",
        "Daniel Song",
        "Danielle Pintz",
        "Danny Livshits",
        "Danny Wyatt",
        "David Esiobu",
        "Dhruv Choudhary",
        "Dhruv Mahajan",
        "Diego Garcia-Olano",
        "Diego Perino",
        "Dieuwke Hupkes",
        "Egor Lakomkin",
        "Ehab AlBadawy",
        "Elina Lobanova",
        "Emily Dinan",
        "Eric Michael Smith",
        "Filip Radenovic",
        "Francisco Guzmán",
        "Frank Zhang",
        "Gabriel Synnaeve",
        "Gabrielle Lee",
        "Georgia Lewis Anderson",
        "Govind Thattai",
        "Graeme Nail",
        "Gregoire Mialon",
        "Guan Pang",
        "Guillem Cucurell",
        "Hailey Nguyen",
        "Hannah Korevaar",
        "Hu Xu",
        "Hugo Touvron",
        "Iliyan Zarov",
        "Imanol Arrieta Ibarra",
        "Isabel Kloumann",
        "Ishan Misra",
        "Ivan Evtimov",
        "Jack Zhang",
        "Jade Copet",
        "Jaewon Lee",
        "Jan Geffert",
        "Jana Vranes",
        "Jason Park",
        "Jay Mahadeokar",
        "Jeet Shah",
        "Jelmer van der Linde",
        "Jennifer Billock",
        "Jenny Hong",
        "Jenya Lee",
        "Jeremy Fu",
        "Jianfeng Chi",
        "Jianyu Huang",
        "Jiawen Liu",
        "Jie Wang",
        "Jiecao Yu",
        "Joanna Bitton",
        "Joe Spisak",
        "Jongsoo Park",
        "Joseph Rocca",
        "Joshua Johnstun",
        "Joshua Saxe",
        "Junteng Jia",
        "Kalyan Vasuden Alwala",
        "Karthik Prasad",
        "Kartikeya Upasani",
        "Kate Plawiak",
        "Ke Li",
        "Kenneth Heafield",
        "Kevin Stone",
        "Khalid El-Arini",
        "Krithika Iyer",
        "Kshitiz Malik",
        "Kuenley Chiu",
        "Kunal Bhalla",
        "Kushal Lakhotia",
        "Lauren Rantala-Yeary",
        "Laurens van der Maaten",
        "Lawrence Chen",
        "Liang Tan",
        "Liz Jenkins",
        "Louis Martin",
        "Lovish Madaan",
        "Lubo Malo",
        "Lukas Blecher",
        "Lukas Landzaat",
        "Luke de Oliveira",
        "Madeline Muzzi",
        "Mahesh Pasupuleti",
        "Mannat Singh",
        "Manohar Paluri",
        "Marcin Kardas",
        "Maria Tsimpoukelli",
        "Mathew Oldham",
        "Mathieu Rita",
        "Maya Pavlova",
        "Melanie Kambadur",
        "Mike Lewis",
        "Min Si",
        "Mitesh Kumar Singh",
        "Mona Hassan",
        "Naman Goyal",
        "Narjes Torabi",
        "Nikolay Bashlykov",
        "Nikolay Bogoychev",
        "Niladri Chatterji",
        "Ning Zhang",
        "Olivier Duchenne",
        "Onur Çelebi",
        "Patrick Alrassy",
        "Pengchuan Zhang",
        "Pengwei Li",
        "Petar Vasic",
        "Peter Weng",
        "Prajjwal Bhargava",
        "Pratik Dubal",
        "Praveen Krishnan",
        "Punit Singh Koura",
        "Puxin Xu",
        "Qing He",
        "Qingxiao Dong",
        "Ragavan Srinivasan",
        "Raj Ganapathy",
        "Ramon Calderer",
        "Ricardo Silveira Cabral",
        "Robert Stojnic",
        "Roberta Raileanu",
        "Rohan Maheswari",
        "Rohit Girdhar",
        "Rohit Patel",
        "Romain Sauvestre",
        "Ronnie Polidoro",
        "Roshan Sumbaly",
        "Ross Taylor",
        "Ruan Silva",
        "Rui Hou",
        "Rui Wang",
        "Saghar Hosseini",
        "Sahana Chennabasappa",
        "Sanjay Singh",
        "Sean Bell",
        "Seohyun Sonia Kim",
        "Sergey Edunov",
        "Shaoliang Nie",
        "Sharan Narang",
        "Sharath Raparthy",
        "Sheng Shen",
        "Shengye Wan",
        "Shruti Bhosale",
        "Shun Zhang",
        "Simon Vandenhende",
        "Soumya Batra",
        "Spencer Whitman",
        "Sten Sootla",
        "Stephane Collot",
        "Suchin Gururangan",
        "Sydney Borodinsky",
        "Tamar Herman",
        "Tara Fowler",
        "Tarek Sheasha",
        "Thomas Georgiou",
        "Thomas Scialom",
        "Tobias Speckbacher",
        "Todor Mihaylov",
        "Tong Xiao",
        "Ujjwal Karn",
        "Vedanuj Goswami",
        "Vibhor Gupta",
        "Vignesh Ramanathan",
        "Viktor Kerkez",
        "Vincent Gonguet",
        "Virginie Do",
        "Vish Vogeti",
        "Vítor Albiero",
        "Vladan Petrovic",
        "Weiwei Chu",
        "Wenhan Xiong",
        "Wenyin Fu",
        "Whitney Meers",
        "Xavier Martinet",
        "Xiaodong Wang",
        "Xiaofang Wang",
        "Xiaoqing Ellen Tan",
        "Xide Xia",
        "Xinfeng Xie",
        "Xuchao Jia",
        "Xuewei Wang",
        "Yaelle Goldschlag",
        "Yashesh Gaur",
        "Yasmine Babaei",
        "Yi Wen",
        "Yiwen Song",
        "Yuchen Zhang",
        "Yue Li",
        "Yuning Mao",
        "Zacharie Delpierre Coudert",
        "Zheng Yan",
        "Zhengxing Chen",
        "Zoe Papakipos",
        "Aaditya Singh",
        "Aayushi Srivastava",
        "Abha Jain",
        "Adam Kelsey",
        "Adam Shajnfeld",
        "Adithya Gangidi",
        "Adolfo Victoria",
        "Ahuva Goldstand",
        "Ajay Menon",
        "Ajay Sharma",
        "Alex Boesenberg",
        "Alexei Baevski",
        "Allie Feinstein",
        "Amanda Kallet",
        "Amit Sangani",
        "Amos Teo",
        "Anam Yunus",
        "Andrei Lupu",
        "Andres Alvarado",
        "Andrew Caples",
        "Andrew Gu",
        "Andrew Ho",
        "Andrew Poulton",
        "Andrew Ryan",
        "Ankit Ramchandani",
        "Annie Dong",
        "Annie Franco",
        "Anuj Goyal",
        "Aparajita Saraf",
        "Arkabandhu Chowdhury",
        "Ashley Gabriel",
        "Ashwin Bharambe",
        "Assaf Eisenman",
        "Azadeh Yazdan",
        "Beau James",
        "Ben Maurer",
        "Benjamin Leonhardi",
        "Bernie Huang",
        "Beth Loyd",
        "Beto De Paola",
        "Bhargavi Paranjape",
        "Bing Liu",
        "Bo Wu",
        "Boyu Ni",
        "Braden Hancock",
        "Bram Wasti",
        "Brandon Spence",
        "Brani Stojkovic",
        "Brian Gamido",
        "Britt Montalvo",
        "Carl Parker",
        "Carly Burton",
        "Catalina Mejia",
        "Ce Liu",
        "Changhan Wang",
        "Changkyu Kim",
        "Chao Zhou",
        "Chester Hu",
        "Ching-Hsiang Chu",
        "Chris Cai",
        "Chris Tindal",
        "Christoph Feichtenhofer",
        "Cynthia Gao",
        "Damon Civin",
        "Dana Beaty",
        "Daniel Kreymer",
        "Daniel Li",
        "David Adkins",
        "David Xu",
        "Davide Testuggine",
        "Delia David",
        "Devi Parikh",
        "Diana Liskovich",
        "Didem Foss",
        "Dingkang Wang",
        "Duc Le",
        "Dustin Holland",
        "Edward Dowling",
        "Eissa Jamil",
        "Elaine Montgomery",
        "Eleonora Presani",
        "Emily Hahn",
        "Emily Wood",
        "Eric-Tuan Le",
        "Erik Brinkman",
        "Esteban Arcaute",
        "Evan Dunbar",
        "Evan Smothers",
        "Fei Sun",
        "Felix Kreuk",
        "Feng Tian",
        "Filippos Kokkinos",
        "Firat Ozgenel",
        "Francesco Caggioni",
        "Frank Kanayet",
        "Frank Seide",
        "Gabriela Medina Florez",
        "Gabriella Schwarz",
        "Gada Badeer",
        "Georgia Swee",
        "Gil Halpern",
        "Grant Herman",
        "Grigory Sizov",
        "Guangyi",
        "Zhang",
        "Guna Lakshminarayanan",
        "Hakan Inan",
        "Hamid Shojanazeri",
        "Han Zou",
        "Hannah Wang",
        "Hanwen Zha",
        "Haroun Habeeb",
        "Harrison Rudolph",
        "Helen Suk",
        "Henry Aspegren",
        "Hunter Goldman",
        "Hongyuan Zhan",
        "Ibrahim Damlaj",
        "Igor Molybog",
        "Igor Tufanov",
        "Ilias Leontiadis",
        "Irina-Elena Veliche",
        "Itai Gat",
        "Jake Weissman",
        "James Geboski",
        "James Kohli",
        "Janice Lam",
        "Japhet Asher",
        "Jean-Baptiste Gaya",
        "Jeff Marcus",
        "Jeff Tang",
        "Jennifer Chan",
        "Jenny Zhen",
        "Jeremy Reizenstein",
        "Jeremy Teboul",
        "Jessica Zhong",
        "Jian Jin",
        "Jingyi Yang",
        "Joe Cummings",
        "Jon Carvill",
        "Jon Shepard",
        "Jonathan McPhie",
        "Jonathan Torres",
        "Josh Ginsburg",
        "Junjie Wang",
        "Kai Wu",
        "Kam Hou U",
        "Karan Saxena",
        "Kartikay Khandelwal",
        "Katayoun Zand",
        "Kathy Matosich",
        "Kaushik Veeraraghavan",
        "Kelly Michelena",
        "Keqian Li",
        "Kiran Jagadeesh",
        "Kun Huang",
        "Kunal Chawla",
        "Kyle Huang",
        "Lailin Chen",
        "Lakshya Garg",
        "Lavender A",
        "Leandro Silva",
        "Lee Bell",
        "Lei Zhang",
        "Liangpeng Guo",
        "Licheng Yu",
        "Liron Moshkovich",
        "Luca Wehrstedt",
        "Madian Khabsa",
        "Manav Avalani",
        "Manish Bhatt",
        "Martynas Mankus",
        "Matan Hasson",
        "Matthew Lennie",
        "Matthias Reso",
        "Maxim Groshev",
        "Maxim Naumov",
        "Maya Lathi",
        "Meghan Keneally",
        "Miao Liu",
        "Michael L. Seltzer",
        "Michal Valko",
        "Michelle Restrepo",
        "Mihir Patel",
        "Mik Vyatskov",
        "Mikayel Samvelyan",
        "Mike Clark",
        "Mike Macey",
        "Mike Wang",
        "Miquel Jubert Hermoso",
        "Mo Metanat",
        "Mohammad Rastegari",
        "Munish Bansal",
        "Nandhini Santhanam",
        "Natascha Parks",
        "Natasha White",
        "Navyata Bawa",
        "Nayan Singhal",
        "Nick Egebo",
        "Nicolas Usunier",
        "Nikhil Mehta",
        "Nikolay Pavlovich Laptev",
        "Ning Dong",
        "Norman Cheng",
        "Oleg Chernoguz",
        "Olivia Hart",
        "Omkar Salpekar",
        "Ozlem Kalinli",
        "Parkin Kent",
        "Parth Parekh",
        "Paul Saab",
        "Pavan Balaji",
        "Pedro Rittner",
        "Philip Bontrager",
        "Pierre Roux",
        "Piotr Dollar",
        "Polina Zvyagina",
        "Prashant Ratanchandani",
        "Pritish Yuvraj",
        "Qian Liang",
        "Rachad Alao",
        "Rachel Rodriguez",
        "Rafi Ayub",
        "Raghotham Murthy",
        "Raghu Nayani",
        "Rahul Mitra",
        "Rangaprabhu Parthasarathy",
        "Raymond Li",
        "Rebekkah Hogan",
        "Robin Battey",
        "Rocky Wang",
        "Russ Howes",
        "Ruty Rinott",
        "Sachin Mehta",
        "Sachin Siby",
        "Sai Jayesh Bondu",
        "Samyak Datta",
        "Sara Chugh",
        "Sara Hunt",
        "Sargun Dhillon",
        "Sasha Sidorov",
        "Satadru Pan",
        "Saurabh Mahajan",
        "Saurabh Verma",
        "Seiji Yamamoto",
        "Sharadh Ramaswamy",
        "Shaun Lindsay",
        "Shaun Lindsay",
        "Sheng Feng",
        "Shenghao Lin",
        "Shengxin Cindy Zha",
        "Shishir Patil",
        "Shiva Shankar",
        "Shuqiang Zhang",
        "Shuqiang Zhang",
        "Sinong Wang",
        "Sneha Agarwal",
        "Soji Sajuyigbe",
        "Soumith Chintala",
        "Stephanie Max",
        "Stephen Chen",
        "Steve Kehoe",
        "Steve Satterfield",
        "Sudarshan Govindaprasad",
        "Sumit Gupta",
        "Summer Deng",
        "Sungmin Cho",
        "Sunny Virk",
        "Suraj Subramanian",
        "Sy Choudhury",
        "Sydney Goldman",
        "Tal Remez",
        "Tamar Glaser",
        "Tamara Best",
        "Thilo Koehler",
        "Thomas Robinson",
        "Tianhe Li",
        "Tianjun Zhang",
        "Tim Matthews",
        "Timothy Chou",
        "Tzook Shaked",
        "Varun Vontimitta",
        "Victoria Ajayi",
        "Victoria Montanez",
        "Vijai Mohan",
        "Vinay Satish Kumar",
        "Vishal Mangla",
        "Vlad Ionescu",
        "Vlad Poenaru",
        "Vlad Tiberiu Mihailescu",
        "Vladimir Ivanov",
        "Wei Li",
        "Wenchen Wang",
        "Wenwen Jiang",
        "Wes Bouaziz",
        "Will Constable",
        "Xiaocheng Tang",
        "Xiaojian Wu",
        "Xiaolan Wang",
        "Xilun Wu",
        "Xinbo Gao",
        "Yaniv Kleinman",
        "Yanjun Chen",
        "Ye Hu",
        "Ye Jia",
        "Ye Qi",
        "Yenda Li",
        "Yilin Zhang",
        "Ying Zhang",
        "Yossi Adi",
        "Youngjin Nam",
        "Yu",
        "Wang",
        "Yu Zhao",
        "Yuchen Hao",
        "Yundi Qian",
        "Yunlu Li",
        "Yuzi He",
        "Zach Rait",
        "Zachary DeVito",
        "Zef Rosnbrick",
        "Zhaoduo Wen",
        "Zhenyu Yang",
        "Zhiwei Zhao",
        "Zhiyu Ma"
      ],
      "published_date": "2024-07-31",
      "pdf_url": "https://arxiv.org/pdf/2407.21783v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "A Survey on Diffusion Language Models",
      "arxiv_id": "",
      "abstract": "Diffusion Language Models (DLMs) are rapidly emerging as a powerful and promising alternative to the dominant autoregressive (AR) paradigm. By generating tokens in parallel through an iterative denoising process, DLMs possess inherent advantages in reducing inference latency and capturing bidirectional context, thereby enabling fine-grained control over the generation process. While achieving a several-fold speed-up, recent advancements have allowed DLMs to show performance comparable to their autoregressive counterparts, making them a compelling choice for various natural language processing tasks. In this survey, we provide a holistic overview of the current DLM landscape. We trace its evolution and relationship with other paradigms, such as autoregressive and masked language models, and cover both foundational principles and state-of-the-art models. Our work offers an up-to-date, comprehensive taxonomy and an in-depth analysis of current techniques, from pre-training strategies to advanced post-training methods. Another contribution of this survey is a thorough review of DLM inference strategies and optimizations, including improvements in decoding parallelism, caching mechanisms, and generation quality. We also highlight the latest approaches to multimodal extensions of DLMs and delineate their applications across various practical scenarios. Furthermore, our discussion addresses the limitations and challenges of DLMs, including efficiency, long-sequence handling, and infrastructure requirements, while outlining future research directions to sustain progress in this rapidly evolving field. Project GitHub is available at https://github.com/VILA-Lab/Awesome-DLMs.",
      "authors": [
        "Tianyi Li",
        "Mingda Chen",
        "Bowei Guo",
        "Zhiqiang Shen"
      ],
      "published_date": "2025-08-14",
      "pdf_url": "https://arxiv.org/pdf/2508.10875v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Training Optimal Large Diffusion Language Models",
      "arxiv_id": "",
      "abstract": "We introduce Quokka, the first systematic scaling law for diffusion language models (DLMs), encompassing both compute-constrained and data-constrained regimes, and studying the key modeling and optimization designs. Quokka is a good friend of Chinchilla and provides wider scopes. We hope the results would bring short-term practical guidance in DLMs training and long-term inspirations for the whole AI community.",
      "authors": [
        "Jinjie Ni",
        "Qian Liu",
        "Chao Du",
        "Longxu Dou",
        "Hang Yan",
        "Zili Wang",
        "Tianyu Pang",
        "Michael Qizhe Shieh"
      ],
      "published_date": "2025-09-28",
      "pdf_url": "https://arxiv.org/pdf/2510.03280v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models",
      "arxiv_id": "",
      "abstract": "Masked diffusion large language models (dLLMs) are emerging as promising alternatives to autoregressive LLMs, offering competitive performance while supporting unique generation capabilities such as inpainting. We explore how inpainting can inform RL algorithm design for dLLMs. Aligning LLMs with reinforcement learning faces an exploration challenge: sparse reward signals and sample waste when models fail to discover correct solutions. While this inefficiency affects LLMs broadly, dLLMs offer a distinctive opportunity--their inpainting ability can guide exploration. We introduce IGPO (Inpainting Guided Policy Optimization), an RL framework that strategically inserts partial ground-truth reasoning traces during online sampling. Unlike providing full solutions, inpainting steers exploration toward promising trajectory spaces while preserving self-generated reasoning, bridging supervised fine-tuning and reinforcement learning. We apply IGPO to group-based optimization methods such as GRPO, where exploration failures cause zero advantages and gradients. IGPO restores meaningful gradients while improving sample efficiency. We also propose supervised fine-tuning on synthetically rewritten concise traces that better align with dLLM generation patterns. With additional techniques including entropy-based filtering, our training recipe yields substantial gains across three mathematical benchmarks--GSM8K, Math500, and AMC--achieving new state-of-the-art results for full-attention masked dLLMs.",
      "authors": [
        "Siyan Zhao",
        "Mengchen Liu",
        "Jing Huang",
        "Miao Liu",
        "Chenyu Wang",
        "Bo Liu",
        "Yuandong Tian",
        "Guan Pang",
        "Sean Bell",
        "Aditya Grover",
        "Feiyu Chen"
      ],
      "published_date": "2025-09-12",
      "pdf_url": "https://arxiv.org/pdf/2509.10396v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Encoder-Decoder Diffusion Language Models for Efficient Training and Inference",
      "arxiv_id": "",
      "abstract": "Discrete diffusion models enable parallel token sampling for faster inference than autoregressive approaches. However, prior diffusion models use a decoder-only architecture, which requires sampling algorithms that invoke the full network at every denoising step and incur high computational cost. Our key insight is that discrete diffusion models perform two types of computation: 1) representing clean tokens and 2) denoising corrupted tokens, which enables us to use separate modules for each task. We propose an encoder-decoder architecture to accelerate discrete diffusion inference, which relies on an encoder to represent clean tokens and a lightweight decoder to iteratively refine a noised sequence. We also show that this architecture enables faster training of block diffusion models, which partition sequences into blocks for better quality and are commonly used in diffusion language model inference. We introduce a framework for Efficient Encoder-Decoder Diffusion (E2D2), consisting of an architecture with specialized training and sampling algorithms, and we show that E2D2 achieves superior trade-offs between generation quality and inference throughput on summarization, translation, and mathematical reasoning tasks. We provide the code, model weights, and blog post on the project page: https://m-arriola.com/e2d2",
      "authors": [
        "Marianne Arriola",
        "Yair Schiff",
        "Hao Phung",
        "Aaron Gokaslan",
        "Volodymyr Kuleshov"
      ],
      "published_date": "2025-10-26",
      "pdf_url": "https://arxiv.org/pdf/2510.22852v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model",
      "arxiv_id": "",
      "abstract": "Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \\~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.",
      "authors": [
        "Chenghao Fan",
        "Wen Heng",
        "Bo Li",
        "Sichen Liu",
        "Yuxuan Song",
        "Jing Su",
        "Xiaoye Qu",
        "Kai Shen",
        "Wei Wei"
      ],
      "published_date": "2026-01-22",
      "pdf_url": "https://arxiv.org/pdf/2601.15892v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
      "arxiv_id": "",
      "abstract": "With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.",
      "authors": [
        "Zhilin Yang",
        "Zihang Dai",
        "Yiming Yang",
        "Jaime Carbonell",
        "Ruslan Salakhutdinov",
        "Quoc V. Le"
      ],
      "published_date": "2019-06-19",
      "pdf_url": "https://arxiv.org/pdf/1906.08237v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Paper",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "N. Cambridge"
      ],
      "published_date": "1977",
      "pdf_url": "",
      "citation_count": 3713,
      "year": 1977
    },
    {
      "title": "Qwen Technical Report",
      "arxiv_id": "",
      "abstract": "Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.",
      "authors": [
        "Jinze Bai",
        "Shuai Bai",
        "Yunfei Chu",
        "Zeyu Cui",
        "Kai Dang",
        "Xiaodong Deng",
        "Yang Fan",
        "Wenbin Ge",
        "Yu Han",
        "Fei Huang",
        "Binyuan Hui",
        "Luo Ji",
        "Mei Li",
        "Junyang Lin",
        "Runji Lin",
        "Dayiheng Liu",
        "Gao Liu",
        "Chengqiang Lu",
        "Keming Lu",
        "Jianxin Ma",
        "Rui Men",
        "Xingzhang Ren",
        "Xuancheng Ren",
        "Chuanqi Tan",
        "Sinan Tan",
        "Jianhong Tu",
        "Peng Wang",
        "Shijie Wang",
        "Wei Wang",
        "Shengguang Wu",
        "Benfeng Xu",
        "Jin Xu",
        "An Yang",
        "Hao Yang",
        "Jian Yang",
        "Shusheng Yang",
        "Yang Yao",
        "Bowen Yu",
        "Hongyi Yuan",
        "Zheng Yuan",
        "Jianwei Zhang",
        "Xingxuan Zhang",
        "Yichang Zhang",
        "Zhenru Zhang",
        "Chang Zhou",
        "Jingren Zhou",
        "Xiaohuan Zhou",
        "Tianhang Zhu"
      ],
      "published_date": "2023-09-28",
      "pdf_url": "https://arxiv.org/pdf/2309.16609v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Code Llama: Open Foundation Models for Code",
      "arxiv_id": "",
      "abstract": "We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B, 34B and 70B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B, 13B and 70B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 67% and 65% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.",
      "authors": [
        "Baptiste Rozière",
        "Jonas Gehring",
        "Fabian Gloeckle",
        "Sten Sootla",
        "Itai Gat",
        "Xiaoqing Ellen Tan",
        "Yossi Adi",
        "Jingyu Liu",
        "Romain Sauvestre",
        "Tal Remez",
        "Jérémy Rapin",
        "Artyom Kozhevnikov",
        "Ivan Evtimov",
        "Joanna Bitton",
        "Manish Bhatt",
        "Cristian Canton Ferrer",
        "Aaron Grattafiori",
        "Wenhan Xiong",
        "Alexandre Défossez",
        "Jade Copet",
        "Faisal Azhar",
        "Hugo Touvron",
        "Louis Martin",
        "Nicolas Usunier",
        "Thomas Scialom",
        "Gabriel Synnaeve"
      ],
      "published_date": "2023-08-24",
      "pdf_url": "https://arxiv.org/pdf/2308.12950v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way",
      "arxiv_id": "",
      "abstract": "Diffusion-based large language models (dLLMs) have exhibited substantial potential for parallel text generation, which may enable more efficient generation compared to autoregressive models. However, current dLLMs suffer from fixed generation lengths, which indicates the generation lengths of dLLMs have to be determined before decoding as a hyper-parameter, leading to issues in efficiency and flexibility. To solve these problems, in this work, we propose to train a diffusion LLM with native variable generation lengths, abbreviated as dLLM-Var. Concretely, we aim to train a model to accurately predict the [EOS] token in the generated text, which makes a dLLM be able to natively infer in a block diffusion manner, while still maintaining the ability of global bi-directional (full) attention and high parallelism. Experiments on standard benchmarks demonstrate that our method achieves a 30.1x speedup over traditional dLLM inference paradigms and a 2.4x speedup relative to autoregressive models such as Qwen and Llama. Our method achieves higher accuracy and faster inference, elevating dLLMs beyond mere academic novelty and supporting their practical use in real-world applications. Codes and models have been released.",
      "authors": [
        "Yicun Yang",
        "Cong Wang",
        "Shaobo Wang",
        "Zichen Wen",
        "Biqing Qi",
        "Hanlin Xu",
        "Linfeng Zhang"
      ],
      "published_date": "2025-10-28",
      "pdf_url": "https://arxiv.org/pdf/2510.24605v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Set Block Decoding is a Language Model Inference Accelerator",
      "arxiv_id": "",
      "abstract": "Autoregressive next token prediction language models offer powerful capabilities but face significant challenges in practical deployment due to the high computational and memory costs of inference, particularly during the decoding stage. We introduce Set Block Decoding (SBD), a simple and flexible paradigm that accelerates generation by integrating standard next token prediction (NTP) and masked token prediction (MATP) within a single architecture. SBD allows the model to sample multiple, not necessarily consecutive, future tokens in parallel, a key distinction from previous acceleration methods. This flexibility allows the use of advanced solvers from the discrete diffusion literature, offering significant speedups without sacrificing accuracy. SBD requires no architectural changes or extra training hyperparameters, maintains compatibility with exact KV-caching, and can be implemented by fine-tuning existing next token prediction models. By fine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x reduction in the number of forward passes required for generation while achieving same performance as equivalent NTP training.",
      "authors": [
        "Itai Gat",
        "Heli Ben-Hamu",
        "Marton Havasi",
        "Daniel Haziza",
        "Jeremy Reizenstein",
        "Gabriel Synnaeve",
        "David Lopez-Paz",
        "Brian Karrer",
        "Yaron Lipman"
      ],
      "published_date": "2025-09-04",
      "pdf_url": "https://arxiv.org/pdf/2509.04185v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows",
      "arxiv_id": "",
      "abstract": "We present OneFlow, the first non-autoregressive multimodal model that enables variable-length and concurrent mixed-modal generation. Unlike autoregressive models that enforce rigid causal ordering between text and image generation, OneFlow combines an insertion-based Edit Flow for discrete text tokens with Flow Matching for image latents. OneFlow enables concurrent text-image synthesis with hierarchical sampling that prioritizes content over grammar. Through controlled experiments across model sizes from 1B to 8B, we demonstrate that OneFlow outperforms autoregressive baselines on both generation and understanding tasks while using up to 50% fewer training FLOPs. OneFlow surpasses both autoregressive and diffusion-based approaches while unlocking new capabilities for concurrent generation, iterative refinement, and natural reasoning-like generation.",
      "authors": [
        "John Nguyen",
        "Marton Havasi",
        "Tariq Berrada",
        "Luke Zettlemoyer",
        "Ricky T. Q. Chen"
      ],
      "published_date": "2025-10-03",
      "pdf_url": "https://arxiv.org/pdf/2510.03506v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Constrained Decoding of Diffusion LLMs with Context-Free Grammars",
      "arxiv_id": "",
      "abstract": "Large language models (LLMs) have shown promising performance across diverse domains. Many practical applications of LLMs, such as code completion and structured data extraction, require adherence to syntactic constraints specified by a formal language. Yet, due to their probabilistic nature, LLM output is not guaranteed to adhere to such formal languages. Prior work has proposed constrained decoding as a means to restrict LLM generation to particular formal languages. However, existing works are not applicable to the emerging paradigm of diffusion LLMs, when used in practical scenarios such as the generation of formally correct C++ or JSON output. In this paper we address this challenge and present the first constrained decoding method for diffusion models, one that can handle formal languages captured by context-free grammars. We begin by reducing constrained decoding to the more general additive infilling problem, which asks whether a partial output can be completed to a valid word in the target language. This problem also naturally subsumes the previously unaddressed multi-region infilling constrained decoding. We then reduce this problem to the task of deciding whether the intersection of the target language and a regular language is empty and present an efficient algorithm to solve it for context-free languages. Empirical results on various applications, such as C++ code infilling and structured data extraction in JSON, demonstrate that our method achieves near-perfect syntactic correctness while consistently preserving or improving functional correctness. Importantly, our efficiency optimizations ensure that the computational overhead remains practical.",
      "authors": [
        "Niels Mündler",
        "Jasper Dekoninck",
        "Martin Vechev"
      ],
      "published_date": "2025-08-13",
      "pdf_url": "https://arxiv.org/pdf/2508.10111v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models",
      "arxiv_id": "",
      "abstract": "Masked Diffusion Language Models (MDLMs) have recently emerged as a promising alternative to Autoregressive Language Models (ARLMs), leveraging a denoising objective that, in principle, should enable more uniform context utilisation. In this work, we examine the context comprehension abilities of MDLMs and uncover two key limitations. First, despite their more global training objective and bidirectional attention mechanism, similarly to ARLMS, MDLMs exhibit a strong locality bias: performance is highly sensitive to the position of relevant information within the input, favouring local over distant context. Second, we show that appending a large number of mask tokens--required for generation--can significantly degrade context comprehension. Through systematic ablations, we find that these masks act as distractors, reducing the model's ability to process relevant information. To address this, we introduce a mask-agnostic loss function that encourages predictions to remain invariant to the number of appended masks. Fine-tuning with this objective substantially mitigates the distracting effect of masks, improving robustness of MDLMs. Overall, our findings reveal critical limitations of the current MDLM training paradigm and provide actionable insights for building diffusion-based language models with stronger context comprehension.",
      "authors": [
        "Julianna Piskorz",
        "Cristina Pinneri",
        "Alvaro Correia",
        "Motasem Alfarra",
        "Risheek Garrepalli",
        "Christos Louizos"
      ],
      "published_date": "2025-11-26",
      "pdf_url": "https://arxiv.org/pdf/2511.21338v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications",
      "arxiv_id": "",
      "abstract": "Memory emerges as the core module in the Large Language Model (LLM)-based agents for long-horizon complex tasks (e.g., multi-turn dialogue, game playing, scientific discovery), where memory can enable knowledge accumulation, iterative reasoning and self-evolution. Among diverse paradigms, graph stands out as a powerful structure for agent memory due to the intrinsic capabilities to model relational dependencies, organize hierarchical information, and support efficient retrieval. This survey presents a comprehensive review of agent memory from the graph-based perspective. First, we introduce a taxonomy of agent memory, including short-term vs. long-term memory, knowledge vs. experience memory, non-structural vs. structural memory, with an implementation view of graph-based memory. Second, according to the life cycle of agent memory, we systematically analyze the key techniques in graph-based agent memory, covering memory extraction for transforming the data into the contents, storage for organizing the data efficiently, retrieval for retrieving the relevant contents from memory to support reasoning, and evolution for updating the contents in the memory. Third, we summarize the open-sourced libraries and benchmarks that support the development and evaluation of self-evolving agent memory. We also explore diverse application scenarios. Finally, we identify critical challenges and future research directions. This survey aims to offer actionable insights to advance the development of more efficient and reliable graph-based agent memory systems. All the related resources, including research papers, open-source data, and projects, are collected for the community in https://github.com/DEEP-PolyU/Awesome-GraphMemory.",
      "authors": [
        "Chang Yang",
        "Chuang Zhou",
        "Yilin Xiao",
        "Su Dong",
        "Luyao Zhuang",
        "Yujing Zhang",
        "Zhu Wang",
        "Zijin Hong",
        "Zheng Yuan",
        "Zhishang Xiang",
        "Shengyuan Chen",
        "Huachi Zhou",
        "Qinggang Zhang",
        "Ninghao Liu",
        "Jinsong Su",
        "Xinrun Wang",
        "Yi Chang",
        "Xiao Huang"
      ],
      "published_date": "2026-02-05",
      "pdf_url": "https://arxiv.org/pdf/2602.05665v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search",
      "arxiv_id": "",
      "abstract": "Inference-time scaling strategies, particularly Monte Carlo Tree Search (MCTS), have significantly enhanced the reasoning capabilities of Large Language Models (LLMs). However, current approaches remain predominantly stateless, discarding successful reasoning patterns after each problem instance and failing to mimic the empirical accumulation of wisdom characteristic of human problem-solving. To bridge this gap, we introduce Empirical-MCTS, a dual-loop framework that transforms stateless search into a continuous, non-parametric learning process. The framework unifies local exploration with global memory optimization through two novel mechanisms: Pairwise-Experience-Evolutionary Meta-Prompting (PE-EMP) and a Memory Optimization Agent. PE-EMP functions as a reflexive optimizer within the local search, utilizing pairwise feedback to dynamically synthesize adaptive criteria and evolve meta-prompts (system prompts) in real-time. Simultaneously, the Memory Optimization Agent manages a global repository as a dynamic policy prior, employing atomic operations to distill high-quality insights across problems. Extensive evaluations on complex reasoning benchmarks, including AIME25, ARC-AGI-2, and MathArena Apex, demonstrate that Empirical-MCTS significantly outperforms both stateless MCTS strategies and standalone experience-driven agents. These results underscore the critical necessity of coupling structured search with empirical accumulation for mastering complex, open-ended reasoning tasks.",
      "authors": [
        "Hao Lu",
        "Haoyuan Huang",
        "Yulin Zhou",
        "Chen Li",
        "Ningxin Zhu"
      ],
      "published_date": "2026-02-04",
      "pdf_url": "https://arxiv.org/pdf/2602.04248v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking",
      "arxiv_id": "",
      "abstract": "Test-time evolution of agent memory serves as a pivotal paradigm for achieving AGI by bolstering complex reasoning through experience accumulation. However, even during benign task evolution, agent safety alignment remains vulnerable-a phenomenon known as Agent Memory Misevolution. To evaluate this phenomenon, we construct the Trust-Memevo benchmark to assess multi-dimensional trustworthiness during benign task evolution, revealing an overall decline in trustworthiness across various task domains and evaluation settings. To address this issue, we propose TAME, a dual-memory evolutionary framework that separately evolves executor memory to improve task performance by distilling generalizable methodologies, and evaluator memory to refine assessments of both safety and task utility based on historical feedback. Through a closed loop of memory filtering, draft generation, trustworthy refinement, execution, and dual-track memory updating, TAME preserves trustworthiness without sacrificing utility. Experiments demonstrate that TAME mitigates misevolution, achieving a joint improvement in both trustworthiness and task performance.",
      "authors": [
        "Yu Cheng",
        "Jiuan Zhou",
        "Yongkang Hu",
        "Yihang Chen",
        "Huichi Zhou",
        "Mingang Chen",
        "Zhizhong Zhang",
        "Kun Shao",
        "Yuan Xie",
        "Zhaoxia Yin"
      ],
      "published_date": "2026-02-03",
      "pdf_url": "https://arxiv.org/pdf/2602.03224v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents",
      "arxiv_id": "",
      "abstract": "LLM-driven agents demonstrate strong performance in sequential decision-making but often rely on on-the-fly reasoning, re-deriving solutions even in recurring scenarios. This insufficient experience reuse leads to computational redundancy and execution instability. To bridge this gap, we propose ProcMEM, a framework that enables agents to autonomously learn procedural memory from interaction experiences without parameter updates. By formalizing a Skill-MDP, ProcMEM transforms passive episodic narratives into executable Skills defined by activation, execution, and termination conditions to ensure executability. To achieve reliable reusability without capability degradation, we introduce Non-Parametric PPO, which leverages semantic gradients for high-quality candidate generation and a PPO Gate for robust Skill verification. Through score-based maintenance, ProcMEM sustains compact, high-quality procedural memory. Experimental results across in-domain, cross-task, and cross-agent scenarios demonstrate that ProcMEM achieves superior reuse rates and significant performance gains with extreme memory compression. Visualized evolutionary trajectories and Skill distributions further reveal how ProcMEM transparently accumulates, refines, and reuses procedural knowledge to facilitate long-term autonomy.",
      "authors": [
        "Qirui Mi",
        "Zhijian Ma",
        "Mengyue Yang",
        "Haoxuan Li",
        "Yisen Wang",
        "Haifeng Zhang",
        "Jun Wang"
      ],
      "published_date": "2026-02-02",
      "pdf_url": "https://arxiv.org/pdf/2602.01869v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting",
      "arxiv_id": "",
      "abstract": "Time series forecasting has traditionally been formulated as a model-centric, static, and single-pass prediction problem that maps historical observations to future values. While this paradigm has driven substantial progress, it proves insufficient in adaptive and multi-turn settings where forecasting requires informative feature extraction, reasoning-driven inference, iterative refinement, and continual adaptation over time. In this paper, we argue for agentic time series forecasting (ATSF), which reframes forecasting as an agentic process composed of perception, planning, action, reflection, and memory. Rather than focusing solely on predictive models, ATSF emphasizes organizing forecasting as an agentic workflow that can interact with tools, incorporate feedback from outcomes, and evolve through experience accumulation. We outline three representative implementation paradigms -- workflow-based design, agentic reinforcement learning, and a hybrid agentic workflow paradigm -- and discuss the opportunities and challenges that arise when shifting from model-centric prediction to agentic forecasting. Together, this position aims to establish agentic forecasting as a foundation for future research at the intersection of time series forecasting.",
      "authors": [
        "Mingyue Cheng",
        "Xiaoyu Tao",
        "Qi Liu",
        "Ze Guo",
        "Enhong Chen"
      ],
      "published_date": "2026-02-02",
      "pdf_url": "https://arxiv.org/pdf/2602.01776v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation",
      "arxiv_id": "",
      "abstract": "We present Follow-Your-Emoji-Faster, an efficient diffusion-based framework for freestyle portrait animation driven by facial landmarks. The main challenges in this task are preserving the identity of the reference portrait, accurately transferring target expressions, and maintaining long-term temporal consistency while ensuring generation efficiency. To address identity preservation and accurate expression retargeting, we enhance Stable Diffusion with two key components: a expression-aware landmarks as explicit motion signals, which improve motion alignment, support exaggerated expressions, and reduce identity leakage; and a fine-grained facial loss that leverages both expression and facial masks to better capture subtle expressions and faithfully preserve the reference appearance. With these components, our model supports controllable and expressive animation across diverse portrait types, including real faces, cartoons, sculptures, and animals. However, diffusion-based frameworks typically struggle to efficiently generate long-term stable animation results, which remains a core challenge in this task. To address this, we propose a progressive generation strategy for stable long-term animation, and introduce a Taylor-interpolated cache, achieving a 2.6X lossless acceleration. These two strategies ensure that our method produces high-quality results efficiently, making it user-friendly and accessible. Finally, we introduce EmojiBench++, a more comprehensive benchmark comprising diverse portraits, driving videos, and landmark sequences. Extensive evaluations on EmojiBench++ demonstrate that Follow-Your-Emoji-Faster achieves superior performance in both animation quality and controllability. The code, training dataset and benchmark will be found in https://follow-your-emoji.github.io/.",
      "authors": [
        "Yue Ma",
        "Zexuan Yan",
        "Hongyu Liu",
        "Hongfa Wang",
        "Heng Pan",
        "Yingqing He",
        "Junkun Yuan",
        "Ailing Zeng",
        "Chengfei Cai",
        "Heung-Yeung Shum",
        "Zhifeng Li",
        "Wei Liu",
        "Linfeng Zhang",
        "Qifeng Chen"
      ],
      "published_date": "2025-09-20",
      "pdf_url": "https://arxiv.org/pdf/2509.16630v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis",
      "arxiv_id": "",
      "abstract": "With the growing demands of AI-generated content (AIGC), the need for high-quality, diverse, and scalable data has become increasingly crucial. However, collecting large-scale real-world data remains costly and time-consuming, hindering the development of downstream applications. While some works attempt to collect task-specific data via a rendering process, most approaches still rely on manual scene construction, limiting their scalability and accuracy. To address these challenges, we propose Follow-Your-Instruction, a Multimodal Large Language Model (MLLM)-driven framework for automatically synthesizing high-quality 2D, 3D, and 4D data. Our \\textbf{Follow-Your-Instruction} first collects assets and their associated descriptions through multimodal inputs using the MLLM-Collector. Then it constructs 3D layouts, and leverages Vision-Language Models (VLMs) for semantic refinement through multi-view scenes with the MLLM-Generator and MLLM-Optimizer, respectively. Finally, it uses MLLM-Planner to generate temporally coherent future frames. We evaluate the quality of the generated data through comprehensive experiments on the 2D, 3D, and 4D generative tasks. The results show that our synthetic data significantly boosts the performance of existing baseline models, demonstrating Follow-Your-Instruction's potential as a scalable and effective data engine for generative intelligence.",
      "authors": [
        "Kunyu Feng",
        "Yue Ma",
        "Xinhua Zhang",
        "Boshi Liu",
        "Yikuang Yuluo",
        "Yinhan Zhang",
        "Runtao Liu",
        "Hongyu Liu",
        "Zhiyuan Qin",
        "Shanhui Mo",
        "Qifeng Chen",
        "Zeyu Wang"
      ],
      "published_date": "2025-08-07",
      "pdf_url": "https://arxiv.org/pdf/2508.05580v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing",
      "arxiv_id": "",
      "abstract": "Despite the rapid progress of instruction-based image editing, its extension to video remains underexplored, primarily due to the prohibitive cost and complexity of constructing large-scale paired video editing datasets. To address this challenge, we introduce a low-cost pretraining strategy for instruction-based video editing that leverages in-context learning from unpaired video clips. We show that pretraining a foundation video generation model with this strategy endows it with general editing capabilities, such as adding, replacing, or deleting operations, according to input editing instructions. The pretrained model can then be efficiently refined with a small amount of high-quality paired editing data. Built upon HunyuanVideoT2V, our framework first pretrains on approximately 1M real video clips to learn basic editing concepts, and subsequently fine-tunes on fewer than 150k curated editing pairs to extend more editing tasks and improve the editing quality. Comparative experiments show that our method surpasses existing instruction-based video editing approaches in both instruction alignment and visual fidelity, achieving a 12\\% improvement in editing instruction following and a 15\\% improvement in editing quality.",
      "authors": [
        "Xinyao Liao",
        "Xianfang Zeng",
        "Ziye Song",
        "Zhoujie Fu",
        "Gang Yu",
        "Guosheng Lin"
      ],
      "published_date": "2025-10-16",
      "pdf_url": "https://arxiv.org/pdf/2510.14648v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Paper2Video: Automatic Video Generation from Scientific Papers",
      "arxiv_id": "",
      "abstract": "Academic presentation videos have become an essential medium for research communication, yet producing them remains highly labor-intensive, often requiring hours of slide design, recording, and editing for a short 2 to 10 minutes video. Unlike natural video, presentation video generation involves distinctive challenges: inputs from research papers, dense multi-modal information (text, figures, tables), and the need to coordinate multiple aligned channels such as slides, subtitles, speech, and human talker. To address these challenges, we introduce Paper2Video, the first benchmark of 101 research papers paired with author-created presentation videos, slides, and speaker metadata. We further design four tailored evaluation metrics--Meta Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos convey the paper's information to the audience. Building on this foundation, we propose PaperTalker, the first multi-agent framework for academic presentation video generation. It integrates slide generation with effective layout refinement by a novel effective tree search visual choice, cursor grounding, subtitling, speech synthesis, and talking-head rendering, while parallelizing slide-wise generation for efficiency. Experiments on Paper2Video demonstrate that the presentation videos produced by our approach are more faithful and informative than existing baselines, establishing a practical step toward automated and ready-to-use academic video generation. Our dataset, agent, and code are available at https://github.com/showlab/Paper2Video.",
      "authors": [
        "Zeyu Zhu",
        "Kevin Qinghong Lin",
        "Mike Zheng Shou"
      ],
      "published_date": "2025-10-06",
      "pdf_url": "https://arxiv.org/pdf/2510.05096v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment",
      "arxiv_id": "",
      "abstract": "Training-free video object editing aims to achieve precise object-level manipulation, including object insertion, swapping, and deletion. However, it faces significant challenges in maintaining fidelity and temporal consistency. Existing methods, often designed for U-Net architectures, suffer from two primary limitations: inaccurate inversion due to first-order solvers, and contextual conflicts caused by crude \"hard\" feature replacement. These issues are more challenging in Diffusion Transformers (DiTs), where the unsuitability of prior layer-selection heuristics makes effective guidance challenging. To address these limitations, we introduce ContextFlow, a novel training-free framework for DiT-based video object editing. In detail, we first employ a high-order Rectified Flow solver to establish a robust editing foundation. The core of our framework is Adaptive Context Enrichment (for specifying what to edit), a mechanism that addresses contextual conflicts. Instead of replacing features, it enriches the self-attention context by concatenating Key-Value pairs from parallel reconstruction and editing paths, empowering the model to dynamically fuse information. Additionally, to determine where to apply this enrichment (for specifying where to edit), we propose a systematic, data-driven analysis to identify task-specific vital layers. Based on a novel Guidance Responsiveness Metric, our method pinpoints the most influential DiT blocks for different tasks (e.g., insertion, swapping), enabling targeted and highly effective guidance. Extensive experiments show that ContextFlow significantly outperforms existing training-free methods and even surpasses several state-of-the-art training-based approaches, delivering temporally coherent, high-fidelity results.",
      "authors": [
        "Yiyang Chen",
        "Xuanhua He",
        "Xiujun Ma",
        "Yue Ma"
      ],
      "published_date": "2025-09-22",
      "pdf_url": "https://arxiv.org/pdf/2509.17818v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning",
      "arxiv_id": "2501.12948",
      "abstract": "General reasoning represents a long-standing and formidable challenge in artificial intelligence. Recent breakthroughs, exemplified by large language models (LLMs) and chain-of-thought prompting, have achieved considerable success on foundational reasoning tasks. However, this success is heavily contingent upon extensive human-annotated demonstrations, and models' capabilities are still insufficient for more complex problems. Here we show that the reasoning abilities of LLMs can be incentivized through pure reinforcement learning (RL), obviating the need for human-labeled reasoning trajectories. The proposed RL framework facilitates the emergent development of advanced reasoning patterns, such as self-reflection, verification, and dynamic strategy adaptation. Consequently, the trained model achieves superior performance on verifiable tasks such as mathematics, coding competitions, and STEM fields, surpassing its counterparts trained via conventional supervised learning on human demonstrations. Moreover, the emergent reasoning patterns exhibited by these large-scale models can be systematically harnessed to guide and enhance the reasoning capabilities of smaller models.",
      "authors": [
        "DeepSeek-AI",
        "Daya Guo",
        "Dejian Yang",
        "Haowei Zhang",
        "Junxiao Song",
        "Peiyi Wang",
        "Qihao Zhu",
        "Runxin Xu",
        "Ruoyu Zhang",
        "Shirong Ma",
        "Xiao Bi",
        "Xiaokang Zhang",
        "Xingkai Yu",
        "Yu Wu",
        "Z. F. Wu",
        "Zhibin Gou",
        "Zhihong Shao",
        "Zhuoshu Li",
        "Ziyi Gao",
        "Aixin Liu",
        "Bing Xue",
        "Bingxuan Wang",
        "Bochao Wu",
        "Bei Feng",
        "Chengda Lu",
        "Chenggang Zhao",
        "Chengqi Deng",
        "Chenyu Zhang",
        "Chong Ruan",
        "Damai Dai",
        "Deli Chen",
        "Dongjie Ji",
        "Erhang Li",
        "Fangyun Lin",
        "Fucong Dai",
        "Fuli Luo",
        "Guangbo Hao",
        "Guanting Chen",
        "Guowei Li",
        "H. Zhang",
        "Han Bao",
        "Hanwei Xu",
        "Haocheng Wang",
        "Honghui Ding",
        "Huajian Xin",
        "Huazuo Gao",
        "Hui Qu",
        "Hui Li",
        "Jianzhong Guo",
        "Jiashi Li",
        "Jiawei Wang",
        "Jingchang Chen",
        "Jingyang Yuan",
        "Junjie Qiu",
        "Junlong Li",
        "J. L. Cai",
        "Jiaqi Ni",
        "Jian Liang",
        "Jin Chen",
        "Kai Dong",
        "Kai Hu",
        "Kaige Gao",
        "Kang Guan",
        "Kexin Huang",
        "Kuai Yu",
        "Lean Wang",
        "Lecong Zhang",
        "Liang Zhao",
        "Litong Wang",
        "Liyue Zhang",
        "Lei Xu",
        "Leyi Xia",
        "Mingchuan Zhang",
        "Minghua Zhang",
        "Minghui Tang",
        "Meng Li",
        "Miaojun Wang",
        "Mingming Li",
        "Ning Tian",
        "Panpan Huang",
        "Peng Zhang",
        "Qiancheng Wang",
        "Qinyu Chen",
        "Qiushi Du",
        "Ruiqi Ge",
        "Ruisong Zhang",
        "Ruizhe Pan",
        "Runji Wang",
        "R. J. Chen",
        "R. L. Jin",
        "Ruyi Chen",
        "Shanghao Lu",
        "Shangyan Zhou",
        "Shanhuang Chen",
        "Shengfeng Ye",
        "Shiyu Wang",
        "Shuiping Yu",
        "Shunfeng Zhou",
        "Shuting Pan",
        "S. S. Li",
        "Shuang Zhou",
        "Shaoqing Wu",
        "Shengfeng Ye",
        "Tao Yun",
        "Tian Pei",
        "Tianyu Sun",
        "T. Wang",
        "Wangding Zeng",
        "Wanjia Zhao",
        "Wen Liu",
        "Wenfeng Liang",
        "Wenjun Gao",
        "Wenqin Yu",
        "Wentao Zhang",
        "W. L. Xiao",
        "Wei An",
        "Xiaodong Liu",
        "Xiaohan Wang",
        "Xiaokang Chen",
        "Xiaotao Nie",
        "Xin Cheng",
        "Xin Liu",
        "Xin Xie",
        "Xingchao Liu",
        "Xinyu Yang",
        "Xinyuan Li",
        "Xuecheng Su",
        "Xuheng Lin",
        "X. Q. Li",
        "Xiangyue Jin",
        "Xiaojin Shen",
        "Xiaosha Chen",
        "Xiaowen Sun",
        "Xiaoxiang Wang",
        "Xinnan Song",
        "Xinyi Zhou",
        "Xianzu Wang",
        "Xinxia Shan",
        "Y. K. Li",
        "Y. Q. Wang",
        "Y. X. Wei",
        "Yang Zhang",
        "Yanhong Xu",
        "Yao Li",
        "Yao Zhao",
        "Yaofeng Sun",
        "Yaohui Wang",
        "Yi Yu",
        "Yichao Zhang",
        "Yifan Shi",
        "Yiliang Xiong",
        "Ying He",
        "Yishi Piao",
        "Yisong Wang",
        "Yixuan Tan",
        "Yiyang Ma",
        "Yiyuan Liu",
        "Yongqiang Guo",
        "Yuan Ou",
        "Yuduan Wang",
        "Yue Gong",
        "Yuheng Zou",
        "Yujia He",
        "Yunfan Xiong",
        "Yuxiang Luo",
        "Yuxiang You",
        "Yuxuan Liu",
        "Yuyang Zhou",
        "Y. X. Zhu",
        "Yanhong Xu",
        "Yanping Huang",
        "Yaohui Li",
        "Yi Zheng",
        "Yuchen Zhu",
        "Yunxian Ma",
        "Ying Tang",
        "Yukun Zha",
        "Yuting Yan",
        "Z. Z. Ren",
        "Zehui Ren",
        "Zhangli Sha",
        "Zhe Fu",
        "Zhean Xu",
        "Zhenda Xie",
        "Zhengyan Zhang",
        "Zhewen Hao",
        "Zhicheng Ma",
        "Zhigang Yan",
        "Zhiyu Wu",
        "Zihui Gu",
        "Zijia Zhu",
        "Zijun Liu",
        "Zilin Li",
        "Ziwei Xie",
        "Ziyang Song",
        "Zizheng Pan",
        "Zhen Huang",
        "Zhipeng Xu",
        "Zhongyu Zhang",
        "Zhen Zhang"
      ],
      "published_date": "2025-01-22",
      "pdf_url": "https://arxiv.org/pdf/2501.12948v2",
      "citation_count": 5469,
      "year": 2025
    },
    {
      "title": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone",
      "arxiv_id": "",
      "abstract": "We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. Our training dataset is a scaled-up version of the one used for phi-2, composed of heavily filtered publicly available web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide parameter-scaling results with a 7B, 14B models trained for 4.8T tokens, called phi-3-small, phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75%, 78% on MMLU, and 8.7, 8.9 on MT-bench). To enhance multilingual, multimodal, and long-context capabilities, we introduce three models in the phi-3.5 series: phi-3.5-mini, phi-3.5-MoE, and phi-3.5-Vision. The phi-3.5-MoE, a 16 x 3.8B MoE model with 6.6 billion active parameters, achieves superior performance in language reasoning, math, and code tasks compared to other open-source models of similar scale, such as Llama 3.1 and the Mixtral series, and on par with Gemini-1.5-Flash and GPT-4o-mini. Meanwhile, phi-3.5-Vision, a 4.2 billion parameter model derived from phi-3.5-mini, excels in reasoning tasks and is adept at handling both single-image and text prompts, as well as multi-image and text prompts.",
      "authors": [
        "Marah Abdin",
        "Jyoti Aneja",
        "Hany Awadalla",
        "Ahmed Awadallah",
        "Ammar Ahmad Awan",
        "Nguyen Bach",
        "Amit Bahree",
        "Arash Bakhtiari",
        "Jianmin Bao",
        "Harkirat Behl",
        "Alon Benhaim",
        "Misha Bilenko",
        "Johan Bjorck",
        "Sébastien Bubeck",
        "Martin Cai",
        "Qin Cai",
        "Vishrav Chaudhary",
        "Dong Chen",
        "Dongdong Chen",
        "Weizhu Chen",
        "Yen-Chun Chen",
        "Yi-Ling Chen",
        "Hao Cheng",
        "Parul Chopra",
        "Xiyang Dai",
        "Matthew Dixon",
        "Ronen Eldan",
        "Victor Fragoso",
        "Jianfeng Gao",
        "Mei Gao",
        "Min Gao",
        "Amit Garg",
        "Allie Del Giorno",
        "Abhishek Goswami",
        "Suriya Gunasekar",
        "Emman Haider",
        "Junheng Hao",
        "Russell J. Hewett",
        "Wenxiang Hu",
        "Jamie Huynh",
        "Dan Iter",
        "Sam Ade Jacobs",
        "Mojan Javaheripi",
        "Xin Jin",
        "Nikos Karampatziakis",
        "Piero Kauffmann",
        "Mahoud Khademi",
        "Dongwoo Kim",
        "Young Jin Kim",
        "Lev Kurilenko",
        "James R. Lee",
        "Yin Tat Lee",
        "Yuanzhi Li",
        "Yunsheng Li",
        "Chen Liang",
        "Lars Liden",
        "Xihui Lin",
        "Zeqi Lin",
        "Ce Liu",
        "Liyuan Liu",
        "Mengchen Liu",
        "Weishung Liu",
        "Xiaodong Liu",
        "Chong Luo",
        "Piyush Madan",
        "Ali Mahmoudzadeh",
        "David Majercak",
        "Matt Mazzola",
        "Caio César Teodoro Mendes",
        "Arindam Mitra",
        "Hardik Modi",
        "Anh Nguyen",
        "Brandon Norick",
        "Barun Patra",
        "Daniel Perez-Becker",
        "Thomas Portet",
        "Reid Pryzant",
        "Heyang Qin",
        "Marko Radmilac",
        "Liliang Ren",
        "Gustavo de Rosa",
        "Corby Rosset",
        "Sambudha Roy",
        "Olatunji Ruwase",
        "Olli Saarikivi",
        "Amin Saied",
        "Adil Salim",
        "Michael Santacroce",
        "Shital Shah",
        "Ning Shang",
        "Hiteshi Sharma",
        "Yelong Shen",
        "Swadheen Shukla",
        "Xia Song",
        "Masahiro Tanaka",
        "Andrea Tupini",
        "Praneetha Vaddamanu",
        "Chunyu Wang",
        "Guanhua Wang",
        "Lijuan Wang",
        "Shuohang Wang",
        "Xin Wang",
        "Yu Wang",
        "Rachel Ward",
        "Wen Wen",
        "Philipp Witte",
        "Haiping Wu",
        "Xiaoxia Wu",
        "Michael Wyatt",
        "Bin Xiao",
        "Can Xu",
        "Jiahang Xu",
        "Weijian Xu",
        "Jilong Xue",
        "Sonali Yadav",
        "Fan Yang",
        "Jianwei Yang",
        "Yifan Yang",
        "Ziyi Yang",
        "Donghan Yu",
        "Lu Yuan",
        "Chenruidong Zhang",
        "Cyril Zhang",
        "Jianwen Zhang",
        "Li Lyna Zhang",
        "Yi Zhang",
        "Yue Zhang",
        "Yunan Zhang",
        "Xiren Zhou"
      ],
      "published_date": "2024-04-22",
      "pdf_url": "https://arxiv.org/pdf/2404.14219v4",
      "citation_count": null,
      "year": null
    },
    {
      "title": "LLaVA-OneVision: Easy Visual Task Transfer",
      "arxiv_id": "",
      "abstract": "We present LLaVA-OneVision, a family of open large multimodal models (LMMs) developed by consolidating our insights into data, models, and visual representations in the LLaVA-NeXT blog series. Our experimental results demonstrate that LLaVA-OneVision is the first single model that can simultaneously push the performance boundaries of open LMMs in three important computer vision scenarios: single-image, multi-image, and video scenarios. Importantly, the design of LLaVA-OneVision allows strong transfer learning across different modalities/scenarios, yielding new emerging capabilities. In particular, strong video understanding and cross-scenario capabilities are demonstrated through task transfer from images to videos.",
      "authors": [
        "Bo Li",
        "Yuanhan Zhang",
        "Dong Guo",
        "Renrui Zhang",
        "Feng Li",
        "Hao Zhang",
        "Kaichen Zhang",
        "Peiyuan Zhang",
        "Yanwei Li",
        "Ziwei Liu",
        "Chunyuan Li"
      ],
      "published_date": "2024-08-06",
      "pdf_url": "https://arxiv.org/pdf/2408.03326v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Spatial Reasoning in Multimodal Large Language Models: A Survey of Tasks, Benchmarks and Methods",
      "arxiv_id": "",
      "abstract": "Spatial reasoning, which requires ability to perceive and manipulate spatial relationships in the 3D world, is a fundamental aspect of human intelligence, yet remains a persistent challenge for Multimodal large language models (MLLMs). While existing surveys often categorize recent progress based on input modality (e.g., text, image, video, or 3D), we argue that spatial ability is not solely determined by the input format. Instead, our survey introduces a taxonomy that organizes spatial intelligence from cognitive aspect and divides tasks in terms of reasoning complexity, linking them to several cognitive functions. We map existing benchmarks across text only, vision language, and embodied settings onto this taxonomy, and review evaluation metrics and methodologies for assessing spatial reasoning ability. This cognitive perspective enables more principled cross-task comparisons and reveals critical gaps between current model capabilities and human-like reasoning. In addition, we analyze methods for improving spatial ability, spanning both training-based and reasoning-based approaches. This dual perspective analysis clarifies their respective strengths, uncovers complementary mechanisms. By surveying tasks, benchmarks, and recent advances, we aim to provide new researchers with a comprehensive understanding of the field and actionable directions for future research.",
      "authors": [
        "Weichen Liu",
        "Qiyao Xue",
        "Haoming Wang",
        "Xiangyu Yin",
        "Boyuan Yang",
        "Wei Gao"
      ],
      "published_date": "2025-11-14",
      "pdf_url": "https://arxiv.org/pdf/2511.15722v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs",
      "arxiv_id": "",
      "abstract": "Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive \"thinking\" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.",
      "authors": [
        "Yuxi Xiao",
        "Longfei Li",
        "Shen Yan",
        "Xinhang Liu",
        "Sida Peng",
        "Yunchao Wei",
        "Xiaowei Zhou",
        "Bingyi Kang"
      ],
      "published_date": "2025-12-23",
      "pdf_url": "https://arxiv.org/pdf/2512.20617v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs",
      "arxiv_id": "",
      "abstract": "While Multimodal Large Language Models (MLLMs) have achieved impressive performance on semantic tasks, their spatial intelligence--crucial for robust and grounded AI systems--remains underdeveloped. Existing benchmarks fall short of diagnosing this limitation: they either focus on overly simplified qualitative reasoning or rely on domain-specific indoor data, constrained by the lack of outdoor datasets with verifiable metric ground truth. To bridge this gap, we introduce a large-scale benchmark built from pedestrian-perspective videos captured with synchronized stereo cameras, LiDAR, and IMU/GPS sensors. This dataset provides metrically precise 3D information, enabling the automatic generation of spatial reasoning questions that span a hierarchical spectrum--from qualitative relational reasoning to quantitative metric and kinematic understanding. Evaluations reveal that the performance gains observed in structured indoor benchmarks vanish in open-world settings. Further analysis using synthetic abnormal scenes and blinding tests confirms that current MLLMs depend heavily on linguistic priors instead of grounded visual reasoning. Our benchmark thus provides a principled platform for diagnosing these limitations and advancing physically grounded spatial intelligence.",
      "authors": [
        "Mingrui Wu",
        "Zhaozhi Wang",
        "Fangjinhua Wang",
        "Jiaolong Yang",
        "Marc Pollefeys",
        "Tong Zhang"
      ],
      "published_date": "2025-12-22",
      "pdf_url": "https://arxiv.org/pdf/2512.19683v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery",
      "arxiv_id": "",
      "abstract": "Despite advancements in Multi-modal Large Language Models (MLLMs) for scene understanding, their performance on complex spatial reasoning tasks requiring mental simulation remains significantly limited. Current methods often rely on passive observation of spatial data, failing to internalize an active mental imagery process. To bridge this gap, we propose SpatialDreamer, a reinforcement learning framework that enables spatial reasoning through a closedloop process of active exploration, visual imagination via a world model, and evidence-grounded reasoning. To address the lack of fine-grained reward supervision in longhorizontal reasoning tasks, we propose Geometric Policy Optimization (GeoPO), which introduces tree-structured sampling and step-level reward estimation with geometric consistency constraints. Extensive experiments demonstrate that SpatialDreamer delivers highly competitive results across multiple challenging benchmarks, signifying a critical advancement in human-like active spatial mental simulation for MLLMs.",
      "authors": [
        "Meng Cao",
        "Xingyu Li",
        "Xue Liu",
        "Ian Reid",
        "Xiaodan Liang"
      ],
      "published_date": "2025-12-08",
      "pdf_url": "https://arxiv.org/pdf/2512.07733v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "You Only Look Once: Unified, Real-Time Object Detection",
      "arxiv_id": "",
      "abstract": "We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.\n  Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.",
      "authors": [
        "Joseph Redmon",
        "Santosh Divvala",
        "Ross Girshick",
        "Ali Farhadi"
      ],
      "published_date": "2015-06-08",
      "pdf_url": "https://arxiv.org/pdf/1506.02640v5",
      "citation_count": null,
      "year": null
    },
    {
      "title": "RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes",
      "arxiv_id": "",
      "abstract": "Drones have become prevalent robotic platforms with diverse applications, showing significant potential in Embodied Artificial Intelligence (Embodied AI). Referring Expression Comprehension (REC) enables drones to locate objects based on natural language expressions, a crucial capability for Embodied AI. Despite advances in REC for ground-level scenes, aerial views introduce unique challenges including varying viewpoints, occlusions and scale variations. To address this gap, we introduce RefDrone, a REC benchmark for drone scenes. RefDrone reveals three key challenges in REC: 1) multi-scale and small-scale target detection; 2) multi-target and no-target samples; 3) complex environment with rich contextual expressions. To efficiently construct this dataset, we develop RDAgent (referring drone annotation framework with multi-agent system), a semi-automated annotation tool for REC tasks. RDAgent ensures high-quality contextual expressions and reduces annotation cost. Furthermore, we propose Number GroundingDINO (NGDINO), a novel method designed to handle multi-target and no-target cases. NGDINO explicitly learns and utilizes the number of objects referred to in the expression. Comprehensive experiments with state-of-the-art REC methods demonstrate that NGDINO achieves superior performance on both the proposed RefDrone and the existing gRefCOCO datasets. The dataset and code are be publicly at https://github.com/sunzc-sunny/refdrone.",
      "authors": [
        "Zhichao Sun",
        "Yepeng Liu",
        "Zhiling Su",
        "Huachao Zhu",
        "Yuliang Gu",
        "Yuda Zou",
        "Zelong Liu",
        "Gui-Song Xia",
        "Bo Du",
        "Yongchao Xu"
      ],
      "published_date": "2025-02-01",
      "pdf_url": "https://arxiv.org/pdf/2502.00392v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "WeDetect: Fast Open-Vocabulary Object Detection as Retrieval",
      "arxiv_id": "",
      "abstract": "Open-vocabulary object detection aims to detect arbitrary classes via text prompts. Methods without cross-modal fusion layers (non-fusion) offer faster inference by treating recognition as a retrieval problem, \\ie, matching regions to text queries in a shared embedding space. In this work, we fully explore this retrieval philosophy and demonstrate its unique advantages in efficiency and versatility through a model family named WeDetect: (1) State-of-the-art performance. WeDetect is a real-time detector with a dual-tower architecture. We show that, with well-curated data and full training, the non-fusion WeDetect surpasses other fusion models and establishes a strong open-vocabulary foundation. (2) Fast backtrack of historical data. WeDetect-Uni is a universal proposal generator based on WeDetect. We freeze the entire detector and only finetune an objectness prompt to retrieve generic object proposals across categories. Importantly, the proposal embeddings are class-specific and enable a new application, object retrieval, supporting retrieval objects in historical data. (3) Integration with LMMs for referring expression comprehension (REC). We further propose WeDetect-Ref, an LMM-based object classifier to handle complex referring expressions, which retrieves target objects from the proposal list extracted by WeDetect-Uni. It discards next-token prediction and classifies objects in a single forward pass. Together, the WeDetect family unifies detection, proposal generation, object retrieval, and REC under a coherent retrieval framework, achieving state-of-the-art performance across 15 benchmarks with high inference efficiency.",
      "authors": [
        "Shenghao Fu",
        "Yukun Su",
        "Fengyun Rao",
        "Jing Lyu",
        "Xiaohua Xie",
        "Wei-Shi Zheng"
      ],
      "published_date": "2025-12-13",
      "pdf_url": "https://arxiv.org/pdf/2512.12309v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "v1: Learning to Point Visual Tokens for Multimodal Grounded Reasoning",
      "arxiv_id": "",
      "abstract": "When thinking with images, humans rarely rely on a single glance: they revisit visual evidence while reasoning. In contrast, most Multimodal Language Models encode an image once to key-value cache and then reason purely in text, making it hard to re-ground intermediate steps. We empirically confirm this: as reasoning chains lengthen, models progressively lose focus on relevant regions. We introduce v1, a lightweight extension for active visual referencing via point-and-copy: the model selects relevant image patches and copies their embeddings back into the reasoning stream. Crucially, our point-and-copy mechanism retrieves patches using their semantic representations as keys, ensuring perceptual evidence remains aligned with the reasoning space. To train this behavior, we build v1, a dataset of 300K multimodal reasoning traces with interleaved grounding annotations. Across multimodal mathematical reasoning benchmarks, v1 consistently outperforms comparable baselines. We plan to release the model checkpoint and data.",
      "authors": [
        "Jiwan Chung",
        "Junhyeok Kim",
        "Siyeol Kim",
        "Jaeyoung Lee",
        "Min Soo Kim",
        "Youngjae Yu"
      ],
      "published_date": "2025-05-24",
      "pdf_url": "https://arxiv.org/pdf/2505.18842v5",
      "citation_count": null,
      "year": null
    },
    {
      "title": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design",
      "arxiv_id": "",
      "abstract": "Graphic design forms the cornerstone of modern visual communication, serving as a vital medium for promoting cultural and commercial events. Recent advances have explored automating this process using Large Multimodal Models (LMMs), yet existing methods often produce geometrically inaccurate layouts and lack the iterative, layer-specific editing required in professional workflows. To address these limitations, we present PosterCopilot, a framework that advances layout reasoning and controllable editing for professional graphic design. Specifically, we introduce a progressive three-stage training strategy that equips LMMs with geometric understanding and aesthetic reasoning for layout design, consisting of Perturbed Supervised Fine-Tuning, Reinforcement Learning for Visual-Reality Alignment, and Reinforcement Learning from Aesthetic Feedback. Furthermore, we develop a complete workflow that couples the trained LMM-based design model with generative models, enabling layer-controllable, iterative editing for precise element refinement while maintaining global visual consistency. Extensive experiments demonstrate that PosterCopilot achieves geometrically accurate and aesthetically superior layouts, offering unprecedented controllability for professional iterative design.",
      "authors": [
        "Jiazhe Wei",
        "Ken Li",
        "Tianyu Lao",
        "Haofan Wang",
        "Liang Wang",
        "Caifeng Shan",
        "Chenyang Si"
      ],
      "published_date": "2025-12-03",
      "pdf_url": "https://arxiv.org/pdf/2512.04082v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving",
      "arxiv_id": "",
      "abstract": "Autonomous driving heavily relies on accurate and robust spatial perception. Many failures arise from inaccuracies and instability, especially in long-tail scenarios and complex interactions. However, current vision-language models are weak at spatial grounding and understanding, and VLA systems built on them therefore show limited perception and localization ability. To address these challenges, we introduce Percept-WAM, a perception-enhanced World-Awareness-Action Model that is the first to implicitly integrate 2D/3D scene understanding abilities within a single vision-language model (VLM). Instead of relying on QA-style spatial reasoning, Percept-WAM unifies 2D/3D perception tasks into World-PV and World-BEV tokens, which encode both spatial coordinates and confidence. We propose a grid-conditioned prediction mechanism for dense object perception, incorporating IoU-aware scoring and parallel autoregressive decoding, improving stability in long-tail, far-range, and small-object scenarios. Additionally, Percept-WAM leverages pretrained VLM parameters to retain general intelligence (e.g., logical reasoning) and can output perception results and trajectory control outputs directly. Experiments show that Percept-WAM matches or surpasses classical detectors and segmenters on downstream perception benchmarks, achieving 51.7/58.9 mAP on COCO 2D detection and nuScenes BEV 3D detection. When integrated with trajectory decoders, it further improves planning performance on nuScenes and NAVSIM, e.g., surpassing DiffusionDrive by 2.1 in PMDS on NAVSIM. Qualitative results further highlight its strong open-vocabulary and long-tail generalization.",
      "authors": [
        "Jianhua Han",
        "Meng Tian",
        "Jiangtong Zhu",
        "Fan He",
        "Huixin Zhang",
        "Sitong Guo",
        "Dechang Zhu",
        "Hao Tang",
        "Pei Xu",
        "Yuze Guo",
        "Minzhe Niu",
        "Haojie Zhu",
        "Qichao Dong",
        "Xuechao Yan",
        "Siyuan Dong",
        "Lu Hou",
        "Qingqiu Huang",
        "Xiaosong Jia",
        "Hang Xu"
      ],
      "published_date": "2025-11-24",
      "pdf_url": "https://arxiv.org/pdf/2511.19221v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Reinforcement Learning: An Introduction",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "R. S. Sutton",
        "A. Barto"
      ],
      "published_date": "1998",
      "pdf_url": "",
      "citation_count": 40954,
      "year": 1998
    },
    {
      "title": "Continuous control with deep reinforcement learning",
      "arxiv_id": "",
      "abstract": "We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.",
      "authors": [
        "Timothy P. Lillicrap",
        "Jonathan J. Hunt",
        "Alexander Pritzel",
        "Nicolas Heess",
        "Tom Erez",
        "Yuval Tassa",
        "David Silver",
        "Daan Wierstra"
      ],
      "published_date": "2015-09-09",
      "pdf_url": "https://arxiv.org/pdf/1509.02971v6",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning",
      "arxiv_id": "",
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful approach to enhancing the reasoning capabilities of Large Language Models (LLMs), while its mechanisms are not yet well understood. In this work, we undertake a pioneering exploration of RLVR through the novel perspective of token entropy patterns, comprehensively analyzing how different tokens influence reasoning performance. By examining token entropy patterns in Chain-of-Thought (CoT) reasoning, we observe that only a small fraction of tokens exhibit high entropy, and these tokens act as critical forks that steer the model toward diverse reasoning pathways. Furthermore, studying how entropy patterns evolve during RLVR training reveals that RLVR largely adheres to the base model's entropy patterns, primarily adjusting the entropy of high-entropy tokens. These findings highlight the significance of high-entropy tokens (i.e., forking tokens) to RLVR. We ultimately improve RLVR by restricting policy gradient updates to forking tokens and uncover a finding even beyond the 80/20 rule: utilizing only 20% of the tokens while maintaining performance comparable to full-gradient updates on the Qwen3-8B base model and significantly surpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71 on AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models, highlighting a strong scaling trend. In contrast, training exclusively on the 80% lowest-entropy tokens leads to a marked decline in performance. These findings indicate that the efficacy of RLVR primarily arises from optimizing the high-entropy tokens that decide reasoning directions. Collectively, our results highlight the potential to understand RLVR through a token-entropy perspective and optimize RLVR by leveraging high-entropy minority tokens to further improve LLM reasoning.",
      "authors": [
        "Shenzhi Wang",
        "Le Yu",
        "Chang Gao",
        "Chujie Zheng",
        "Shixuan Liu",
        "Rui Lu",
        "Kai Dang",
        "Xionghui Chen",
        "Jianxin Yang",
        "Zhenru Zhang",
        "Yuqiong Liu",
        "An Yang",
        "Andrew Zhao",
        "Yang Yue",
        "Shiji Song",
        "Bowen Yu",
        "Gao Huang",
        "Junyang Lin"
      ],
      "published_date": "2025-06-02",
      "pdf_url": "https://arxiv.org/pdf/2506.01939v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models",
      "arxiv_id": "",
      "abstract": "This paper aims to overcome a major obstacle in scaling RL for reasoning with LLMs, namely the collapse of policy entropy. Such phenomenon is consistently observed across vast RL runs without entropy intervention, where the policy entropy dropped sharply at the early training stage, this diminished exploratory ability is always accompanied with the saturation of policy performance. In practice, we establish a transformation equation R=-a*e^H+b between entropy H and downstream performance R. This empirical law strongly indicates that, the policy performance is traded from policy entropy, thus bottlenecked by its exhaustion, and the ceiling is fully predictable H=0, R=-a+b. Our finding necessitates entropy management for continuous exploration toward scaling compute for RL. To this end, we investigate entropy dynamics both theoretically and empirically. Our derivation highlights that, the change in policy entropy is driven by the covariance between action probability and the change in logits, which is proportional to its advantage when using Policy Gradient-like algorithms. Empirical study shows that, the values of covariance term and entropy differences matched exactly, supporting the theoretical conclusion. Moreover, the covariance term stays mostly positive throughout training, further explaining why policy entropy would decrease monotonically. Through understanding the mechanism behind entropy dynamics, we motivate to control entropy by restricting the update of high-covariance tokens. Specifically, we propose two simple yet effective techniques, namely Clip-Cov and KL-Cov, which clip and apply KL penalty to tokens with high covariances respectively. Experiments show that these methods encourage exploration, thus helping policy escape entropy collapse and achieve better downstream performance.",
      "authors": [
        "Ganqu Cui",
        "Yuchen Zhang",
        "Jiacheng Chen",
        "Lifan Yuan",
        "Zhi Wang",
        "Yuxin Zuo",
        "Haozhan Li",
        "Yuchen Fan",
        "Huayu Chen",
        "Weize Chen",
        "Zhiyuan Liu",
        "Hao Peng",
        "Lei Bai",
        "Wanli Ouyang",
        "Yu Cheng",
        "Bowen Zhou",
        "Ning Ding"
      ],
      "published_date": "2025-05-28",
      "pdf_url": "https://arxiv.org/pdf/2505.22617v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Learning to Reason under Off-Policy Guidance",
      "arxiv_id": "",
      "abstract": "Recent advances in large reasoning models (LRMs) demonstrate that sophisticated behaviors such as multi-step reasoning and self-reflection can emerge via reinforcement learning with verifiable rewards~(\\textit{RLVR}). However, existing \\textit{RLVR} approaches are inherently ``on-policy'', limiting learning to a model's own outputs and failing to acquire reasoning abilities beyond its initial capabilities. To address this issue, we introduce \\textbf{LUFFY} (\\textbf{L}earning to reason \\textbf{U}nder o\\textbf{FF}-polic\\textbf{Y} guidance), a framework that augments \\textit{RLVR} with off-policy reasoning traces. LUFFY dynamically balances imitation and exploration by combining off-policy demonstrations with on-policy rollouts during training. Specifically, LUFFY combines the Mixed-Policy GRPO framework, which has a theoretically guaranteed convergence rate, alongside policy shaping via regularized importance sampling to avoid superficial and rigid imitation during mixed-policy training. Compared with previous RLVR methods, LUFFY achieves an over \\textbf{+6.4} average gain across six math benchmarks and an advantage of over \\textbf{+6.2} points in out-of-distribution tasks. Most significantly, we show that LUFFY successfully trains weak models in scenarios where on-policy RLVR completely fails. These results provide compelling evidence that LUFFY transcends the fundamental limitations of on-policy RLVR and demonstrates the great potential of utilizing off-policy guidance in RLVR.",
      "authors": [
        "Jianhao Yan",
        "Yafu Li",
        "Zican Hu",
        "Zhi Wang",
        "Ganqu Cui",
        "Xiaoye Qu",
        "Yu Cheng",
        "Yue Zhang"
      ],
      "published_date": "2025-04-21",
      "pdf_url": "https://arxiv.org/pdf/2504.14945v5",
      "citation_count": null,
      "year": null
    },
    {
      "title": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models",
      "arxiv_id": "",
      "abstract": "Large Language Models (LLMs) have transformed the natural language processing landscape and brought to life diverse applications. Pretraining on vast web-scale data has laid the foundation for these models, yet the research community is now increasingly shifting focus toward post-training techniques to achieve further breakthroughs. While pretraining provides a broad linguistic foundation, post-training methods enable LLMs to refine their knowledge, improve reasoning, enhance factual accuracy, and align more effectively with user intents and ethical considerations. Fine-tuning, reinforcement learning, and test-time scaling have emerged as critical strategies for optimizing LLMs performance, ensuring robustness, and improving adaptability across various real-world tasks. This survey provides a systematic exploration of post-training methodologies, analyzing their role in refining LLMs beyond pretraining, addressing key challenges such as catastrophic forgetting, reward hacking, and inference-time trade-offs. We highlight emerging directions in model alignment, scalable adaptation, and inference-time reasoning, and outline future research directions. We also provide a public repository to continually track developments in this fast-evolving field: https://github.com/mbzuai-oryx/Awesome-LLM-Post-training.",
      "authors": [
        "Komal Kumar",
        "Tajamul Ashraf",
        "Omkar Thawakar",
        "Rao Muhammad Anwer",
        "Hisham Cholakkal",
        "Mubarak Shah",
        "Ming-Hsuan Yang",
        "Phillip H. S. Torr",
        "Fahad Shahbaz Khan",
        "Salman Khan"
      ],
      "published_date": "2025-02-28",
      "pdf_url": "https://arxiv.org/pdf/2502.21321v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement learning for LLMs",
      "arxiv_id": "",
      "abstract": "We propose a new algorithm for fine-tuning large language models using reinforcement learning. Tapered Off-Policy REINFORCE (TOPR) uses an asymmetric, tapered variant of importance sampling to speed up learning while maintaining stable learning dynamics, even without the use of KL regularization. TOPR can be applied in a fully offline fashion, allows the handling of positive and negative examples in a unified framework, and benefits from the implementational simplicity that is typical of Monte Carlo algorithms. We demonstrate the effectiveness of our approach with a series of experiments on the GSM8K and MATH reasoning benchmarks, finding performance gains for training both a model for solution generation and as a generative verifier. We show that properly leveraging positive and negative examples alike in the off-policy regime simultaneously increases test-time accuracy and training data efficiency, all the while avoiding the ``wasted inference'' that comes with discarding negative examples. We find that this advantage persists over multiple iterations of training and can be amplified by dataset curation techniques, enabling us to match 70B-parameter model performance with 8B language models. As a corollary to this work, we find that REINFORCE's baseline parameter plays an important and unexpected role in defining dataset composition in the presence of negative examples, and is consequently critical in driving off-policy performance.",
      "authors": [
        "Nicolas Le Roux",
        "Marc G. Bellemare",
        "Jonathan Lebensold",
        "Arnaud Bergeron",
        "Joshua Greaves",
        "Alex Fréchette",
        "Carolyne Pelletier",
        "Eric Thibodeau-Laufer",
        "Sándor Toth",
        "Sam Work"
      ],
      "published_date": "2025-03-18",
      "pdf_url": "https://arxiv.org/pdf/2503.14286v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation",
      "arxiv_id": "",
      "abstract": "Recently, very deep convolutional neural networks (CNNs) have shown outstanding performance in object recognition and have also been the first choice for dense classification problems such as semantic segmentation. However, repeated subsampling operations like pooling or convolution striding in deep CNNs lead to a significant decrease in the initial image resolution. Here, we present RefineNet, a generic multi-path refinement network that explicitly exploits all the information available along the down-sampling process to enable high-resolution prediction using long-range residual connections. In this way, the deeper layers that capture high-level semantic features can be directly refined using fine-grained features from earlier convolutions. The individual components of RefineNet employ residual connections following the identity mapping mindset, which allows for effective end-to-end training. Further, we introduce chained residual pooling, which captures rich background context in an efficient manner. We carry out comprehensive experiments and set new state-of-the-art results on seven public datasets. In particular, we achieve an intersection-over-union score of 83.4 on the challenging PASCAL VOC 2012 dataset, which is the best reported result to date.",
      "authors": [
        "Guosheng Lin",
        "Anton Milan",
        "Chunhua Shen",
        "Ian Reid"
      ],
      "published_date": "2016-11-20",
      "pdf_url": "https://arxiv.org/pdf/1611.06612v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Fully convolutional networks for semantic segmentation",
      "arxiv_id": "1411.4038",
      "abstract": "Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.",
      "authors": [
        "Evan Shelhamer",
        "Jonathan Long",
        "Trevor Darrell"
      ],
      "published_date": "2014",
      "pdf_url": "",
      "citation_count": 40954,
      "year": 2014
    },
    {
      "title": "Diffusion Models in Vision: A Survey",
      "arxiv_id": "",
      "abstract": "Denoising diffusion models represent a recent emerging topic in computer vision, demonstrating remarkable results in the area of generative modeling. A diffusion model is a deep generative model that is based on two stages, a forward diffusion stage and a reverse diffusion stage. In the forward diffusion stage, the input data is gradually perturbed over several steps by adding Gaussian noise. In the reverse stage, a model is tasked at recovering the original input data by learning to gradually reverse the diffusion process, step by step. Diffusion models are widely appreciated for the quality and diversity of the generated samples, despite their known computational burdens, i.e. low speeds due to the high number of steps involved during sampling. In this survey, we provide a comprehensive review of articles on denoising diffusion models applied in vision, comprising both theoretical and practical contributions in the field. First, we identify and present three generic diffusion modeling frameworks, which are based on denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations. We further discuss the relations between diffusion models and other deep generative models, including variational auto-encoders, generative adversarial networks, energy-based models, autoregressive models and normalizing flows. Then, we introduce a multi-perspective categorization of diffusion models applied in computer vision. Finally, we illustrate the current limitations of diffusion models and envision some interesting directions for future research.",
      "authors": [
        "Florinel-Alin Croitoru",
        "Vlad Hondru",
        "Radu Tudor Ionescu",
        "Mubarak Shah"
      ],
      "published_date": "2022-09-10",
      "pdf_url": "https://arxiv.org/pdf/2209.04747v6",
      "citation_count": null,
      "year": null
    },
    {
      "title": "SegNeXt: Rethinking Convolutional Attention Design for Semantic Segmentation",
      "arxiv_id": "",
      "abstract": "We present SegNeXt, a simple convolutional network architecture for semantic segmentation. Recent transformer-based models have dominated the field of semantic segmentation due to the efficiency of self-attention in encoding spatial information. In this paper, we show that convolutional attention is a more efficient and effective way to encode contextual information than the self-attention mechanism in transformers. By re-examining the characteristics owned by successful segmentation models, we discover several key components leading to the performance improvement of segmentation models. This motivates us to design a novel convolutional attention network that uses cheap convolutional operations. Without bells and whistles, our SegNeXt significantly improves the performance of previous state-of-the-art methods on popular benchmarks, including ADE20K, Cityscapes, COCO-Stuff, Pascal VOC, Pascal Context, and iSAID. Notably, SegNeXt outperforms EfficientNet-L2 w/ NAS-FPN and achieves 90.6% mIoU on the Pascal VOC 2012 test leaderboard using only 1/10 parameters of it. On average, SegNeXt achieves about 2.0% mIoU improvements compared to the state-of-the-art methods on the ADE20K datasets with the same or fewer computations. Code is available at https://github.com/uyzhang/JSeg (Jittor) and https://github.com/Visual-Attention-Network/SegNeXt (Pytorch).",
      "authors": [
        "Meng-Hao Guo",
        "Cheng-Ze Lu",
        "Qibin Hou",
        "Zhengning Liu",
        "Ming-Ming Cheng",
        "Shi-Min Hu"
      ],
      "published_date": "2022-09-18",
      "pdf_url": "https://arxiv.org/pdf/2209.08575v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation With Transformers",
      "arxiv_id": "2203.04838",
      "abstract": "Scene understanding based on image segmentation is a crucial component of autonomous vehicles. Pixel-wise semantic segmentation of RGB images can be advanced by exploiting complementary features from the supplementary modality (X-modality). However, covering a wide variety of sensors with a modality-agnostic model remains an unresolved problem due to variations in sensor characteristics among different modalities. Unlike previous modality-specific methods, in this work, we propose a unified fusion framework, CMX, for RGB-X semantic segmentation. To generalize well across different modalities, that often include supplements as well as uncertainties, a unified cross-modal interaction is crucial for modality fusion. Specifically, we design a Cross-Modal Feature Rectification Module (CM-FRM) to calibrate bi-modal features by leveraging the features from one modality to rectify the features of the other modality. With rectified feature pairs, we deploy a Feature Fusion Module (FFM) to perform sufficient exchange of long-range contexts before mixing. To verify CMX, for the first time, we unify five modalities complementary to RGB, i.e., depth, thermal, polarization, event, and LiDAR. Extensive experiments show that CMX generalizes well to diverse multi-modal fusion, achieving state-of-the-art performances on five RGB-Depth benchmarks, as well as RGB-Thermal, RGB-Polarization, and RGB-LiDAR datasets. Besides, to investigate the generalizability to dense-sparse data fusion, we establish an RGB-Event semantic segmentation benchmark based on the EventScape dataset, on which CMX sets the new state-of-the-art. The source code of CMX is publicly available at https://github.com/huaaaliu/RGBX_Semantic_Segmentation.",
      "authors": [
        "Jiaming Zhang",
        "Huayao Liu",
        "Kailun Yang",
        "Xinxin Hu",
        "Ruiping Liu",
        "Rainer Stiefelhagen"
      ],
      "published_date": "2022-03-09",
      "pdf_url": "https://arxiv.org/pdf/2203.04838v5",
      "citation_count": 530,
      "year": 2022
    },
    {
      "title": "Segment Anything in High Quality",
      "arxiv_id": "",
      "abstract": "The recent Segment Anything Model (SAM) represents a big leap in scaling up segmentation models, allowing for powerful zero-shot capabilities and flexible prompting. Despite being trained with 1.1 billion masks, SAM's mask prediction quality falls short in many cases, particularly when dealing with objects that have intricate structures. We propose HQ-SAM, equipping SAM with the ability to accurately segment any object, while maintaining SAM's original promptable design, efficiency, and zero-shot generalizability. Our careful design reuses and preserves the pre-trained model weights of SAM, while only introducing minimal additional parameters and computation. We design a learnable High-Quality Output Token, which is injected into SAM's mask decoder and is responsible for predicting the high-quality mask. Instead of only applying it on mask-decoder features, we first fuse them with early and final ViT features for improved mask details. To train our introduced learnable parameters, we compose a dataset of 44K fine-grained masks from several sources. HQ-SAM is only trained on the introduced detaset of 44k masks, which takes only 4 hours on 8 GPUs. We show the efficacy of HQ-SAM in a suite of 10 diverse segmentation datasets across different downstream tasks, where 8 out of them are evaluated in a zero-shot transfer protocol. Our code and pretrained models are at https://github.com/SysCV/SAM-HQ.",
      "authors": [
        "Lei Ke",
        "Mingqiao Ye",
        "Martin Danelljan",
        "Yifan Liu",
        "Yu-Wing Tai",
        "Chi-Keung Tang",
        "Fisher Yu"
      ],
      "published_date": "2023-06-02",
      "pdf_url": "https://arxiv.org/pdf/2306.01567v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "PIDNet: A Real-time Semantic Segmentation Network Inspired by PID Controllers",
      "arxiv_id": "",
      "abstract": "Two-branch network architecture has shown its efficiency and effectiveness in real-time semantic segmentation tasks. However, direct fusion of high-resolution details and low-frequency context has the drawback of detailed features being easily overwhelmed by surrounding contextual information. This overshoot phenomenon limits the improvement of the segmentation accuracy of existing two-branch models. In this paper, we make a connection between Convolutional Neural Networks (CNN) and Proportional-Integral-Derivative (PID) controllers and reveal that a two-branch network is equivalent to a Proportional-Integral (PI) controller, which inherently suffers from similar overshoot issues. To alleviate this problem, we propose a novel three-branch network architecture: PIDNet, which contains three branches to parse detailed, context and boundary information, respectively, and employs boundary attention to guide the fusion of detailed and context branches. Our family of PIDNets achieve the best trade-off between inference speed and accuracy and their accuracy surpasses all the existing models with similar inference speed on the Cityscapes and CamVid datasets. Specifically, PIDNet-S achieves 78.6% mIOU with inference speed of 93.2 FPS on Cityscapes and 80.1% mIOU with speed of 153.7 FPS on CamVid.",
      "authors": [
        "Jiacong Xu",
        "Zixiang Xiong",
        "Shankar P. Bhattacharyya"
      ],
      "published_date": "2022-06-04",
      "pdf_url": "https://arxiv.org/pdf/2206.02066v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Qwen2 Technical Report",
      "arxiv_id": "",
      "abstract": "This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of foundational and instruction-tuned language models, encompassing a parameter range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts model. Qwen2 surpasses most prior open-weight models, including its predecessor Qwen1.5, and exhibits competitive performance relative to proprietary models across diverse benchmarks on language understanding, generation, multilingual proficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2 demonstrates robust multilingual capabilities, proficient in approximately 30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and global reach.\n  To foster community innovation and accessibility, we have made the Qwen2 model weights openly available on Hugging Face and ModelScope, and the supplementary materials including example code on GitHub. These platforms also include resources for quantization, fine-tuning, and deployment, facilitating a wide range of applications and research endeavors.",
      "authors": [
        "An Yang",
        "Baosong Yang",
        "Binyuan Hui",
        "Bo Zheng",
        "Bowen Yu",
        "Chang Zhou",
        "Chengpeng Li",
        "Chengyuan Li",
        "Dayiheng Liu",
        "Fei Huang",
        "Guanting Dong",
        "Haoran Wei",
        "Huan Lin",
        "Jialong Tang",
        "Jialin Wang",
        "Jian Yang",
        "Jianhong Tu",
        "Jianwei Zhang",
        "Jianxin Ma",
        "Jianxin Yang",
        "Jin Xu",
        "Jingren Zhou",
        "Jinze Bai",
        "Jinzheng He",
        "Junyang Lin",
        "Kai Dang",
        "Keming Lu",
        "Keqin Chen",
        "Kexin Yang",
        "Mei Li",
        "Mingfeng Xue",
        "Na Ni",
        "Pei Zhang",
        "Peng Wang",
        "Ru Peng",
        "Rui Men",
        "Ruize Gao",
        "Runji Lin",
        "Shijie Wang",
        "Shuai Bai",
        "Sinan Tan",
        "Tianhang Zhu",
        "Tianhao Li",
        "Tianyu Liu",
        "Wenbin Ge",
        "Xiaodong Deng",
        "Xiaohuan Zhou",
        "Xingzhang Ren",
        "Xinyu Zhang",
        "Xipin Wei",
        "Xuancheng Ren",
        "Xuejing Liu",
        "Yang Fan",
        "Yang Yao",
        "Yichang Zhang",
        "Yu Wan",
        "Yunfei Chu",
        "Yuqiong Liu",
        "Zeyu Cui",
        "Zhenru Zhang",
        "Zhifang Guo",
        "Zhihao Fan"
      ],
      "published_date": "2024-07-15",
      "pdf_url": "https://arxiv.org/pdf/2407.10671v4",
      "citation_count": null,
      "year": null
    },
    {
      "title": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving",
      "arxiv_id": "",
      "abstract": "Vision-Language-Action (VLA) models offer significant potential for end-to-end driving, yet their reasoning is often constrained by textual Chains-of-Thought (CoT). This symbolic compression of visual information creates a modality gap between perception and planning by blurring spatio-temporal relations and discarding fine-grained cues. We introduce FSDrive, a framework that empowers VLAs to \"think visually\" using a novel visual spatio-temporal CoT. FSDrive first operates as a world model, generating a unified future frame that combines a predicted background with explicit, physically-plausible priors like future lane dividers and 3D object boxes. This imagined scene serves as the visual spatio-temporal CoT, capturing both spatial structure and temporal evolution in a single representation. The same VLA then functions as an inverse-dynamics model to plan trajectories conditioned on current observations and this visual CoT. We enable this with a unified pre-training paradigm that expands the model's vocabulary with visual tokens and jointly optimizes for semantic understanding (VQA) and future-frame prediction. A progressive curriculum first generates structural priors to enforce physical laws before rendering the full scene. Evaluations on nuScenes and NAVSIM show FSDrive improves trajectory accuracy and reduces collisions, while also achieving competitive FID for video generation with a lightweight autoregressive model and advancing scene understanding on DriveLM. These results confirm that our visual spatio-temporal CoT bridges the perception-planning gap, enabling safer, more anticipatory autonomous driving. Code is available at https://github.com/MIV-XJTU/FSDrive.",
      "authors": [
        "Shuang Zeng",
        "Xinyuan Chang",
        "Mengwei Xie",
        "Xinran Liu",
        "Yifan Bai",
        "Zheng Pan",
        "Mu Xu",
        "Xing Wei",
        "Ning Guo"
      ],
      "published_date": "2025-05-23",
      "pdf_url": "https://arxiv.org/pdf/2505.17685v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning",
      "arxiv_id": "",
      "abstract": "Recent advancements in Vision-Language-Action (VLA) models have shown promise for end-to-end autonomous driving by leveraging world knowledge and reasoning capabilities. However, current VLA models often struggle with physically infeasible action outputs, complex model structures, or unnecessarily long reasoning. In this paper, we propose AutoVLA, a novel VLA model that unifies reasoning and action generation within a single autoregressive generation model for end-to-end autonomous driving. AutoVLA performs semantic reasoning and trajectory planning directly from raw visual inputs and language instructions. We tokenize continuous trajectories into discrete, feasible actions, enabling direct integration into the language model. For training, we employ supervised fine-tuning to equip the model with dual thinking modes: fast thinking (trajectory-only) and slow thinking (enhanced with chain-of-thought reasoning). To further enhance planning performance and efficiency, we introduce a reinforcement fine-tuning method based on Group Relative Policy Optimization (GRPO), reducing unnecessary reasoning in straightforward scenarios. Extensive experiments across real-world and simulated datasets and benchmarks, including nuPlan, nuScenes, Waymo, and CARLA, demonstrate the competitive performance of AutoVLA in both open-loop and closed-loop settings. Qualitative results showcase the adaptive reasoning and accurate planning capabilities of AutoVLA in diverse scenarios.",
      "authors": [
        "Zewei Zhou",
        "Tianhui Cai",
        "Seth Z. Zhao",
        "Yun Zhang",
        "Zhiyu Huang",
        "Bolei Zhou",
        "Jiaqi Ma"
      ],
      "published_date": "2025-06-16",
      "pdf_url": "https://arxiv.org/pdf/2506.13757v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models",
      "arxiv_id": "",
      "abstract": "Vision-Language-Action (VLA) models for autonomous driving show promise but falter in unstructured corner case scenarios, largely due to a scarcity of targeted benchmarks. To address this, we introduce Impromptu VLA. Our core contribution is the Impromptu VLA Dataset: over 80,000 meticulously curated video clips, distilled from over 2M source clips sourced from 8 open-source large-scale datasets. This dataset is built upon our novel taxonomy of four challenging unstructured categories and features rich, planning-oriented question-answering annotations and action trajectories. Crucially, experiments demonstrate that VLAs trained with our dataset achieve substantial performance gains on established benchmarks--improving closed-loop NeuroNCAP scores and collision rates, and reaching near state-of-the-art L2 accuracy in open-loop nuScenes trajectory prediction. Furthermore, our Q&A suite serves as an effective diagnostic, revealing clear VLM improvements in perception, prediction, and planning. Our code, data and models are available at https://github.com/ahydchh/Impromptu-VLA.",
      "authors": [
        "Haohan Chi",
        "Huan-ang Gao",
        "Ziming Liu",
        "Jianing Liu",
        "Chenyu Liu",
        "Jinwei Li",
        "Kaisen Yang",
        "Yangcheng Yu",
        "Zeda Wang",
        "Wenyi Li",
        "Leichen Wang",
        "Xingtao Hu",
        "Hao Sun",
        "Hang Zhao",
        "Hao Zhao"
      ],
      "published_date": "2025-05-29",
      "pdf_url": "https://arxiv.org/pdf/2505.23757v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving",
      "arxiv_id": "",
      "abstract": "Vision-Language Models (VLMs) show promise for autonomous driving, yet their struggle with hallucinations, inefficient reasoning, and limited real-world validation hinders accurate perception and robust step-by-step reasoning. To overcome this, we introduce \\textbf{AgentThink}, a pioneering unified framework that integrates Chain-of-Thought (CoT) reasoning with dynamic, agent-style tool invocation for autonomous driving tasks. AgentThink's core innovations include: \\textbf{(i) Structured Data Generation}, which establishes an autonomous driving tool library to automatically construct structured, self-verified reasoning data explicitly incorporating tool usage for diverse driving scenarios; \\textbf{(ii) A Two-stage Training Pipeline}, employing Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO) to equip VLMs with the capability for autonomous tool invocation; and \\textbf{(iii) Agent-style Tool-Usage Evaluation}, introducing a novel multi-tool assessment protocol to rigorously evaluate the model's tool invocation and utilization. Experiments on the DriveLMM-o1 benchmark demonstrate that AgentThink significantly boosts overall reasoning scores by \\textbf{53.91%} and enhances answer accuracy by \\textbf{33.54%}, while markedly improving reasoning quality and consistency. Furthermore, ablation studies and robust zero-shot/few-shot generalization experiments across various benchmarks underscore its powerful capabilities. These findings highlight a promising trajectory for developing trustworthy and tool-aware autonomous driving models. Code is available at https://github.com/curryqka/AgentThink.",
      "authors": [
        "Kangan Qian",
        "Sicong Jiang",
        "Yang Zhong",
        "Ziang Luo",
        "Zilin Huang",
        "Tianze Zhu",
        "Kun Jiang",
        "Mengmeng Yang",
        "Zheng Fu",
        "Jinyu Miao",
        "Yining Shi",
        "He Zhe Lim",
        "Li Liu",
        "Tianbao Zhou",
        "Huang Yu",
        "Yifei Hu",
        "Guang Li",
        "Guang Chen",
        "Hao Ye",
        "Lijun Sun",
        "Diange Yang"
      ],
      "published_date": "2025-05-21",
      "pdf_url": "https://arxiv.org/pdf/2505.15298v4",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Ronald J. Williams"
      ],
      "published_date": "2004",
      "pdf_url": "",
      "citation_count": 9723,
      "year": 2004
    },
    {
      "title": "Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning",
      "arxiv_id": "",
      "abstract": "Large vision-language models (VLMs) for autonomous driving (AD) are evolving beyond perception and cognition tasks toward motion planning. However, we identify two critical challenges in this direction: (1) VLMs tend to learn shortcuts by relying heavily on history input information, achieving seemingly strong planning results without genuinely understanding the visual inputs; and (2) the chain-ofthought (COT) reasoning processes are always misaligned with the motion planning outcomes, and how to effectively leverage the complex reasoning capability to enhance planning remains largely underexplored. In this paper, we start from a small-scale domain-specific VLM and propose Drive-R1 designed to bridges the scenario reasoning and motion planning for AD. Drive-R1 first undergoes the supervised finetuning on a elaborate dataset containing both long and short COT data. Drive-R1 is encouraged to reason step-by-step from visual input to final planning decisions. Subsequently, Drive-R1 is trained within a reinforcement learning framework that incentivizes the discovery of reasoning paths that are more informative for planning, guided by rewards based on predicted trajectories and meta actions. Experimental evaluations on the nuScenes and DriveLM-nuScenes benchmarks demonstrate that Drive-R1 achieves superior performance compared to existing state-of-the-art VLMs. We believe that Drive-R1 presents a promising direction for bridging reasoning and planning in AD, offering methodological insights for future research and applications.",
      "authors": [
        "Yue Li",
        "Meng Tian",
        "Dechang Zhu",
        "Jiangtong Zhu",
        "Zhenyu Lin",
        "Zhiwei Xiong",
        "Xinhai Zhao"
      ],
      "published_date": "2025-06-23",
      "pdf_url": "https://arxiv.org/pdf/2506.18234v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model",
      "arxiv_id": "",
      "abstract": "Vision-Language-Action (VLA) models have demonstrated potential in autonomous driving. However, two critical challenges hinder their development: (1) Existing VLA architectures are typically based on imitation learning in open-loop setup which tends to capture the recorded behaviors in the dataset, leading to suboptimal and constrained performance, (2) Close-loop training relies heavily on high-fidelity sensor simulation, where domain gaps and computational inefficiencies pose significant barriers. In this paper, we introduce IRL-VLA, a novel close-loop Reinforcement Learning via \\textbf{I}nverse \\textbf{R}einforcement \\textbf{L}earning reward world model with a self-built VLA approach. Our framework proceeds in a three-stage paradigm: In the first stage, we propose a VLA architecture and pretrain the VLA policy via imitation learning. In the second stage, we construct a lightweight reward world model via inverse reinforcement learning to enable efficient close-loop reward computation. To further enhance planning performance, finally, we design specialized reward world model guidence reinforcement learning via PPO(Proximal Policy Optimization) to effectively balance the safety incidents, comfortable driving, and traffic efficiency. Our approach achieves state-of-the-art performance in NAVSIM v2 end-to-end driving benchmark, 1st runner up in CVPR2025 Autonomous Grand Challenge. We hope that our framework will accelerate VLA research in close-loop autonomous driving.",
      "authors": [
        "Anqing Jiang",
        "Yu Gao",
        "Yiru Wang",
        "Zhigang Sun",
        "Shuo Wang",
        "Yuwen Heng",
        "Hao Sun",
        "Shichen Tang",
        "Lijuan Zhu",
        "Jinhao Chai",
        "Jijun Wang",
        "Zichong Gu",
        "Hao Jiang",
        "Li Sun"
      ],
      "published_date": "2025-08-07",
      "pdf_url": "https://arxiv.org/pdf/2508.06571v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving",
      "arxiv_id": "",
      "abstract": "Scaling Vision-Language-Action (VLA) models on large-scale data offers a promising path to achieving a more generalized driving intelligence. However, VLA models are limited by a ``supervision deficit'': the vast model capacity is supervised by sparse, low-dimensional actions, leaving much of their representational power underutilized. To remedy this, we propose \\textbf{DriveVLA-W0}, a training paradigm that employs world modeling to predict future images. This task generates a dense, self-supervised signal that compels the model to learn the underlying dynamics of the driving environment. We showcase the paradigm's versatility by instantiating it for two dominant VLA archetypes: an autoregressive world model for VLAs that use discrete visual tokens, and a diffusion world model for those operating on continuous visual features. Building on the rich representations learned from world modeling, we introduce a lightweight action expert to address the inference latency for real-time deployment. Extensive experiments on the NAVSIM v1/v2 benchmark and a 680x larger in-house dataset demonstrate that DriveVLA-W0 significantly outperforms BEV and VLA baselines. Crucially, it amplifies the data scaling law, showing that performance gains accelerate as the training dataset size increases.",
      "authors": [
        "Yingyan Li",
        "Shuyao Shang",
        "Weisong Liu",
        "Bing Zhan",
        "Haochen Wang",
        "Yuqi Wang",
        "Yuntao Chen",
        "Xiaoman Wang",
        "Yasong An",
        "Chufeng Tang",
        "Lu Hou",
        "Lue Fan",
        "Zhaoxiang Zhang"
      ],
      "published_date": "2025-10-14",
      "pdf_url": "https://arxiv.org/pdf/2510.12796v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "arxiv_id": "",
      "abstract": "Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (>= 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.",
      "authors": [
        "Bernhard Kerbl",
        "Georgios Kopanas",
        "Thomas Leimkühler",
        "George Drettakis"
      ],
      "published_date": "2023-08-08",
      "pdf_url": "https://arxiv.org/pdf/2308.04079v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Structure-from-Motion Revisited",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Johannes L. Schönberger",
        "Jan-Michael Frahm"
      ],
      "published_date": "2016",
      "pdf_url": "",
      "citation_count": 6710,
      "year": 2016
    },
    {
      "title": "Generalized Trajectory Scoring for End-to-end Multimodal Planning",
      "arxiv_id": "",
      "abstract": "End-to-end multi-modal planning is a promising paradigm in autonomous driving, enabling decision-making with diverse trajectory candidates. A key component is a robust trajectory scorer capable of selecting the optimal trajectory from these candidates. While recent trajectory scorers focus on scoring either large sets of static trajectories or small sets of dynamically generated ones, both approaches face significant limitations in generalization. Static vocabularies provide effective coarse discretization but struggle to make fine-grained adaptation, while dynamic proposals offer detailed precision but fail to capture broader trajectory distributions. To overcome these challenges, we propose GTRS (Generalized Trajectory Scoring), a unified framework for end-to-end multi-modal planning that combines coarse and fine-grained trajectory evaluation. GTRS consists of three complementary innovations: (1) a diffusion-based trajectory generator that produces diverse fine-grained proposals; (2) a vocabulary generalization technique that trains a scorer on super-dense trajectory sets with dropout regularization, enabling its robust inference on smaller subsets; and (3) a sensor augmentation strategy that enhances out-of-domain generalization while incorporating refinement training for critical trajectory discrimination. As the winning solution of the Navsim v2 Challenge, GTRS demonstrates superior performance even with sub-optimal sensor inputs, approaching privileged methods that rely on ground-truth perception. Code will be available at https://github.com/NVlabs/GTRS.",
      "authors": [
        "Zhenxin Li",
        "Wenhao Yao",
        "Zi Wang",
        "Xinglong Sun",
        "Joshua Chen",
        "Nadine Chang",
        "Maying Shen",
        "Zuxuan Wu",
        "Shiyi Lan",
        "Jose M. Alvarez"
      ],
      "published_date": "2025-06-07",
      "pdf_url": "https://arxiv.org/pdf/2506.06664v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "RAP: 3D Rasterization Augmented End-to-End Planning",
      "arxiv_id": "",
      "abstract": "Imitation learning for end-to-end driving trains policies only on expert demonstrations. Once deployed in a closed loop, such policies lack recovery data: small mistakes cannot be corrected and quickly compound into failures. A promising direction is to generate alternative viewpoints and trajectories beyond the logged path. Prior work explores photorealistic digital twins via neural rendering or game engines, but these methods are prohibitively slow and costly, and thus mainly used for evaluation. In this work, we argue that photorealism is unnecessary for training end-to-end planners. What matters is semantic fidelity and scalability: driving depends on geometry and dynamics, not textures or lighting. Motivated by this, we propose 3D Rasterization, which replaces costly rendering with lightweight rasterization of annotated primitives, enabling augmentations such as counterfactual recovery maneuvers and cross-agent view synthesis. To transfer these synthetic views effectively to real-world deployment, we introduce a Raster-to-Real feature-space alignment that bridges the sim-to-real gap. Together, these components form Rasterization Augmented Planning (RAP), a scalable data augmentation pipeline for planning. RAP achieves state-of-the-art closed-loop robustness and long-tail generalization, ranking first on four major benchmarks: NAVSIM v1/v2, Waymo Open Dataset Vision-based E2E Driving, and Bench2Drive. Our results show that lightweight rasterization with feature alignment suffices to scale E2E training, offering a practical alternative to photorealistic rendering. Project page: https://alan-lanfeng.github.io/RAP/.",
      "authors": [
        "Lan Feng",
        "Yang Gao",
        "Eloi Zablocki",
        "Quanyi Li",
        "Wuyang Li",
        "Sichao Liu",
        "Matthieu Cord",
        "Alexandre Alahi"
      ],
      "published_date": "2025-10-05",
      "pdf_url": "https://arxiv.org/pdf/2510.04333v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving",
      "arxiv_id": "",
      "abstract": "While end-to-end autonomous driving models show promising results, their practical deployment is often hindered by large model sizes, a reliance on expensive LiDAR sensors and computationally intensive BEV feature representations. This limits their scalability, especially for mass-market vehicles equipped only with cameras. To address these challenges, we propose PRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving architecture operates using only camera data, without explicit BEV representation and forgoing the need for LiDAR. PRIX leverages a visual feature extractor coupled with a generative planning head to predict safe trajectories from raw pixel inputs directly. A core component of our architecture is the Context-aware Recalibration Transformer (CaRT), a novel module designed to effectively enhance multi-level visual features for more robust planning. We demonstrate through comprehensive experiments that PRIX achieves state-of-the-art performance on the NavSim and nuScenes benchmarks, matching the capabilities of larger, multimodal diffusion planners while being significantly more efficient in terms of inference speed and model size, making it a practical solution for real-world deployment. Our work is open-source and the code will be at https://maxiuw.github.io/prix.",
      "authors": [
        "Maciej K. Wozniak",
        "Lianhang Liu",
        "Yixi Cai",
        "Patric Jensfelt"
      ],
      "published_date": "2025-07-23",
      "pdf_url": "https://arxiv.org/pdf/2507.17596v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
      "arxiv_id": "",
      "abstract": "The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.",
      "authors": [
        "Junnan Li",
        "Dongxu Li",
        "Silvio Savarese",
        "Steven Hoi"
      ],
      "published_date": "2023-01-30",
      "pdf_url": "https://arxiv.org/pdf/2301.12597v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Flow Matching for Generative Modeling",
      "arxiv_id": "",
      "abstract": "We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. Specifically, we present the notion of Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples -- which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers.",
      "authors": [
        "Yaron Lipman",
        "Ricky T. Q. Chen",
        "Heli Ben-Hamu",
        "Maximilian Nickel",
        "Matt Le"
      ],
      "published_date": "2022-10-06",
      "pdf_url": "https://arxiv.org/pdf/2210.02747v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Diffusion policy: Visuomotor policy learning via action diffusion",
      "arxiv_id": "2303.04137",
      "abstract": "This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot's visuomotor policy as a conditional denoising diffusion process. We benchmark Diffusion Policy across 12 different tasks from 4 different robot manipulation benchmarks and find that it consistently outperforms existing state-of-the-art robot learning methods with an average improvement of 46.9%. Diffusion Policy learns the gradient of the action-distribution score function and iteratively optimizes with respect to this gradient field during inference via a series of stochastic Langevin dynamics steps. We find that the diffusion formulation yields powerful advantages when used for robot policies, including gracefully handling multimodal action distributions, being suitable for high-dimensional action spaces, and exhibiting impressive training stability. To fully unlock the potential of diffusion models for visuomotor policy learning on physical robots, this paper presents a set of key technical contributions including the incorporation of receding horizon control, visual conditioning, and the time-series diffusion transformer. We hope this work will help motivate a new generation of policy learning techniques that are able to leverage the powerful generative modeling capabilities of diffusion models. Code, data, and training details is publicly available diffusion-policy.cs.columbia.edu",
      "authors": [
        "Cheng Chi",
        "Zhenjia Xu",
        "Siyuan Feng",
        "Eric Cousineau",
        "Yilun Du",
        "Benjamin Burchfiel",
        "Russ Tedrake",
        "Shuran Song"
      ],
      "published_date": "2023-03-07",
      "pdf_url": "https://arxiv.org/pdf/2303.04137v5",
      "citation_count": 2338,
      "year": 2023
    },
    {
      "title": "Large Language Models for Robotics: A Survey",
      "arxiv_id": "",
      "abstract": "The human ability to learn, generalize, and control complex manipulation tasks through multi-modality feedback suggests a unique capability, which we refer to as dexterity intelligence. Understanding and assessing this intelligence is a complex task. Amidst the swift progress and extensive proliferation of large language models (LLMs), their applications in the field of robotics have garnered increasing attention. LLMs possess the ability to process and generate natural language, facilitating efficient interaction and collaboration with robots. Researchers and engineers in the field of robotics have recognized the immense potential of LLMs in enhancing robot intelligence, human-robot interaction, and autonomy. Therefore, this comprehensive review aims to summarize the applications of LLMs in robotics, delving into their impact and contributions to key areas such as robot control, perception, decision-making, and planning. This survey first provides an overview of the background and development of LLMs for robotics, followed by a discussion of their benefits and recent advancements in LLM-based robotic models. It then explores various techniques, employed in perception, decision-making, control, and interaction, as well as cross-module coordination in practical tasks. Finally, we review current applications of LLMs in robotics and outline potential challenges they may face in the near future. Embodied intelligence represents the future of intelligent systems, and LLM-based robotics is one of the most promising yet challenging paths toward achieving it.",
      "authors": [
        "Fanlong Zeng",
        "Wensheng Gan",
        "Zezheng Huai",
        "Lichao Sun",
        "Hechang Chen",
        "Yongheng Wang",
        "Ning Liu",
        "Philip S. Yu"
      ],
      "published_date": "2023-11-13",
      "pdf_url": "https://arxiv.org/pdf/2311.07226v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)",
      "arxiv_id": "",
      "abstract": "Reinforcement Learning (RL) can mitigate the causal confusion and distribution shift inherent to imitation learning (IL). However, applying RL to end-to-end autonomous driving (E2E-AD) remains an open problem for its training difficulty, and IL is still the mainstream paradigm in both academia and industry. Recently Model-based Reinforcement Learning (MBRL) have demonstrated promising results in neural planning; however, these methods typically require privileged information as input rather than raw sensor data. We fill this gap by designing Raw2Drive, a dual-stream MBRL approach. Initially, we efficiently train an auxiliary privileged world model paired with a neural planner that uses privileged information as input. Subsequently, we introduce a raw sensor world model trained via our proposed Guidance Mechanism, which ensures consistency between the raw sensor world model and the privileged world model during rollouts. Finally, the raw sensor world model combines the prior knowledge embedded in the heads of the privileged world model to effectively guide the training of the raw sensor policy. Raw2Drive is so far the only RL based end-to-end method on CARLA Leaderboard 2.0, and Bench2Drive and it achieves state-of-the-art performance.",
      "authors": [
        "Zhenjie Yang",
        "Xiaosong Jia",
        "Qifeng Li",
        "Xue Yang",
        "Maoqing Yao",
        "Junchi Yan"
      ],
      "published_date": "2025-05-22",
      "pdf_url": "https://arxiv.org/pdf/2505.16394v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey",
      "arxiv_id": "",
      "abstract": "The emergence of Vision Language Action (VLA) models marks a paradigm shift from traditional policy-based control to generalized robotics, reframing Vision Language Models (VLMs) from passive sequence generators into active agents for manipulation and decision-making in complex, dynamic environments. This survey delves into advanced VLA methods, aiming to provide a clear taxonomy and a systematic, comprehensive review of existing research. It presents a comprehensive analysis of VLA applications across different scenarios and classifies VLA approaches into several paradigms: autoregression-based, diffusion-based, reinforcement-based, hybrid, and specialized methods; while examining their motivations, core strategies, and implementations in detail. In addition, foundational datasets, benchmarks, and simulation platforms are introduced. Building on the current VLA landscape, the review further proposes perspectives on key challenges and future directions to advance research in VLA models and generalizable robotics. By synthesizing insights from over three hundred recent studies, this survey maps the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose VLA methods.",
      "authors": [
        "Dapeng Zhang",
        "Jing Sun",
        "Chenghui Hu",
        "Xiaoyan Wu",
        "Zhenlong Yuan",
        "Rui Zhou",
        "Fei Shen",
        "Qingguo Zhou"
      ],
      "published_date": "2025-09-23",
      "pdf_url": "https://arxiv.org/pdf/2509.19012v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "ReSim: Reliable World Simulation for Autonomous Driving",
      "arxiv_id": "",
      "abstract": "How can we reliably simulate future driving scenarios under a wide range of ego driving behaviors? Recent driving world models, developed exclusively on real-world driving data composed mainly of safe expert trajectories, struggle to follow hazardous or non-expert behaviors, which are rare in such data. This limitation restricts their applicability to tasks such as policy evaluation. In this work, we address this challenge by enriching real-world human demonstrations with diverse non-expert data collected from a driving simulator (e.g., CARLA), and building a controllable world model trained on this heterogeneous corpus. Starting with a video generator featuring a diffusion transformer architecture, we devise several strategies to effectively integrate conditioning signals and improve prediction controllability and fidelity. The resulting model, ReSim, enables Reliable Simulation of diverse open-world driving scenarios under various actions, including hazardous non-expert ones. To close the gap between high-fidelity simulation and applications that require reward signals to judge different actions, we introduce a Video2Reward module that estimates a reward from ReSim's simulated future. Our ReSim paradigm achieves up to 44% higher visual fidelity, improves controllability for both expert and non-expert actions by over 50%, and boosts planning and policy selection performance on NAVSIM by 2% and 25%, respectively.",
      "authors": [
        "Jiazhi Yang",
        "Kashyap Chitta",
        "Shenyuan Gao",
        "Long Chen",
        "Yuqian Shao",
        "Xiaosong Jia",
        "Hongyang Li",
        "Andreas Geiger",
        "Xiangyu Yue",
        "Li Chen"
      ],
      "published_date": "2025-06-11",
      "pdf_url": "https://arxiv.org/pdf/2506.09981v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
      "arxiv_id": "",
      "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",
      "authors": [
        "Hugo Touvron",
        "Louis Martin",
        "Kevin Stone",
        "Peter Albert",
        "Amjad Almahairi",
        "Yasmine Babaei",
        "Nikolay Bashlykov",
        "Soumya Batra",
        "Prajjwal Bhargava",
        "Shruti Bhosale",
        "Dan Bikel",
        "Lukas Blecher",
        "Cristian Canton Ferrer",
        "Moya Chen",
        "Guillem Cucurull",
        "David Esiobu",
        "Jude Fernandes",
        "Jeremy Fu",
        "Wenyin Fu",
        "Brian Fuller",
        "Cynthia Gao",
        "Vedanuj Goswami",
        "Naman Goyal",
        "Anthony Hartshorn",
        "Saghar Hosseini",
        "Rui Hou",
        "Hakan Inan",
        "Marcin Kardas",
        "Viktor Kerkez",
        "Madian Khabsa",
        "Isabel Kloumann",
        "Artem Korenev",
        "Punit Singh Koura",
        "Marie-Anne Lachaux",
        "Thibaut Lavril",
        "Jenya Lee",
        "Diana Liskovich",
        "Yinghai Lu",
        "Yuning Mao",
        "Xavier Martinet",
        "Todor Mihaylov",
        "Pushkar Mishra",
        "Igor Molybog",
        "Yixin Nie",
        "Andrew Poulton",
        "Jeremy Reizenstein",
        "Rashi Rungta",
        "Kalyan Saladi",
        "Alan Schelten",
        "Ruan Silva",
        "Eric Michael Smith",
        "Ranjan Subramanian",
        "Xiaoqing Ellen Tan",
        "Binh Tang",
        "Ross Taylor",
        "Adina Williams",
        "Jian Xiang Kuan",
        "Puxin Xu",
        "Zheng Yan",
        "Iliyan Zarov",
        "Yuchen Zhang",
        "Angela Fan",
        "Melanie Kambadur",
        "Sharan Narang",
        "Aurelien Rodriguez",
        "Robert Stojnic",
        "Sergey Edunov",
        "Thomas Scialom"
      ],
      "published_date": "2023-07-18",
      "pdf_url": "https://arxiv.org/pdf/2307.09288v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey",
      "arxiv_id": "",
      "abstract": "Vision-Language-Action (VLA) models extend vision-language models to embodied control by mapping natural-language instructions and visual observations to robot actions. Despite their capabilities, VLA systems face significant challenges due to their massive computational and memory demands, which conflict with the constraints of edge platforms such as on-board mobile manipulators that require real-time performance. Addressing this tension has become a central focus of recent research. In light of the growing efforts toward more efficient and scalable VLA systems, this survey provides a systematic review of approaches for improving VLA efficiency, with an emphasis on reducing latency, memory footprint, and training and inference costs. We categorize existing solutions into four dimensions: model architecture, perception feature, action generation, and training/inference strategies, summarizing representative techniques within each category. Finally, we discuss future trends and open challenges, highlighting directions for advancing efficient embodied intelligence.",
      "authors": [
        "Weifan Guan",
        "Qinghao Hu",
        "Aosheng Li",
        "Jian Cheng"
      ],
      "published_date": "2025-10-20",
      "pdf_url": "https://arxiv.org/pdf/2510.17111v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving",
      "arxiv_id": "",
      "abstract": "While reasoning technology like Chain of Thought (CoT) has been widely adopted in Vision Language Action (VLA) models, it demonstrates promising capabilities in end to end autonomous driving. However, recent efforts to integrate CoT reasoning often fall short in simple scenarios, introducing unnecessary computational overhead without improving decision quality. To address this, we propose AdaThinkDrive, a novel VLA framework with a dual mode reasoning mechanism inspired by fast and slow thinking. First, our framework is pretrained on large scale autonomous driving (AD) scenarios using both question answering (QA) and trajectory datasets to acquire world knowledge and driving commonsense. During supervised fine tuning (SFT), we introduce a two mode dataset, fast answering (w/o CoT) and slow thinking (with CoT), enabling the model to distinguish between scenarios that require reasoning. Furthermore, an Adaptive Think Reward strategy is proposed in conjunction with the Group Relative Policy Optimization (GRPO), which rewards the model for selectively applying CoT by comparing trajectory quality across different reasoning modes. Extensive experiments on the Navsim benchmark show that AdaThinkDrive achieves a PDMS of 90.3, surpassing the best vision only baseline by 1.7 points. Moreover, ablations show that AdaThinkDrive surpasses both the never Think and always Think baselines, improving PDMS by 2.0 and 1.4, respectively. It also reduces inference time by 14% compared to the always Think baseline, demonstrating its ability to balance accuracy and efficiency through adaptive reasoning.",
      "authors": [
        "Yuechen Luo",
        "Fang Li",
        "Shaoqing Xu",
        "Zhiyi Lai",
        "Lei Yang",
        "Qimao Chen",
        "Ziang Luo",
        "Zixun Xie",
        "Shengyin Jiang",
        "Jiaxin Liu",
        "Long Chen",
        "Bing Wang",
        "Zhi-xin Yang"
      ],
      "published_date": "2025-09-17",
      "pdf_url": "https://arxiv.org/pdf/2509.13769v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition",
      "arxiv_id": "",
      "abstract": "With the rapid advancement of autonomous driving technology, vehicle-to-everything (V2X) communication has emerged as a key enabler for extending perception range and enhancing driving safety by providing visibility beyond the line of sight. However, integrating multi-source sensor data from both ego-vehicles and infrastructure under real-world constraints, such as limited communication bandwidth and dynamic environments, presents significant technical challenges. To facilitate research in this area, we organized the End-to-End Autonomous Driving through V2X Cooperation Challenge, which features two tracks: cooperative temporal perception and cooperative end-to-end planning. Built on the UniV2X framework and the V2X-Seq-SPD dataset, the challenge attracted participation from over 30 teams worldwide and established a unified benchmark for evaluating cooperative driving systems. This paper describes the design and outcomes of the challenge, highlights key research problems including bandwidth-aware fusion, robust multi-agent planning, and heterogeneous sensor integration, and analyzes emerging technical trends among top-performing solutions. By addressing practical constraints in communication and data fusion, the challenge contributes to the development of scalable and reliable V2X-cooperative autonomous driving systems.",
      "authors": [
        "Ruiyang Hao",
        "Haibao Yu",
        "Jiaru Zhong",
        "Chuanye Wang",
        "Jiahao Wang",
        "Yiming Kan",
        "Wenxian Yang",
        "Siqi Fan",
        "Huilin Yin",
        "Jianing Qiu",
        "Yao Mu",
        "Jiankai Sun",
        "Li Chen",
        "Walter Zimmer",
        "Dandan Zhang",
        "Shanghang Zhang",
        "Mac Schwager",
        "Ping Luo",
        "Zaiqing Nie"
      ],
      "published_date": "2025-07-29",
      "pdf_url": "https://arxiv.org/pdf/2507.21610v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI",
      "arxiv_id": "2407.06886",
      "abstract": "Embodied Artificial Intelligence (Embodied AI) is crucial for achieving Artificial General Intelligence (AGI) and serves as a foundation for various applications (e.g., intelligent mechatronics systems, smart manufacturing) that bridge cyberspace and the physical world. Recently, the emergence of Multi-modal Large Models (MLMs) and World Models (WMs) have attracted significant attention due to their remarkable perception, interaction, and reasoning capabilities, making them a promising architecture for embodied agents. In this survey, we give a comprehensive exploration of the latest advancements in Embodied AI. Our analysis firstly navigates through the forefront of representative works of embodied robots and simulators, to fully understand the research focuses and their limitations. Then, we analyze four main research targets: 1) embodied perception, 2) embodied interaction, 3) embodied agent, and 4) sim-to-real adaptation, covering state-of-the-art methods, essential paradigms, and comprehensive datasets. Additionally, we explore the complexities of MLMs in virtual and real embodied agents, highlighting their significance in facilitating interactions in digital and physical environments. Finally, we summarize the challenges and limitations of embodied AI and discuss potential future directions. We hope this survey will serve as a foundational reference for the research community. The associated project can be found at https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List.",
      "authors": [
        "Yang Liu",
        "Weixing Chen",
        "Yongjie Bai",
        "Xiaodan Liang",
        "Guanbin Li",
        "Wen Gao",
        "Liang Lin"
      ],
      "published_date": "2024-07-09",
      "pdf_url": "https://arxiv.org/pdf/2407.06886v8",
      "citation_count": 206,
      "year": 2024
    },
    {
      "title": "A Survey on Vision-Language-Action Models for Embodied AI",
      "arxiv_id": "",
      "abstract": "Embodied AI is widely recognized as a cornerstone of artificial general intelligence because it involves controlling embodied agents to perform tasks in the physical world. Building on the success of large language models and vision-language models, a new category of multimodal models -- referred to as vision-language-action models (VLAs) -- has emerged to address language-conditioned robotic tasks in embodied AI by leveraging their distinct ability to generate actions. The recent proliferation of VLAs necessitates a comprehensive survey to capture the rapidly evolving landscape. To this end, we present the first survey on VLAs for embodied AI. This work provides a detailed taxonomy of VLAs, organized into three major lines of research. The first line focuses on individual components of VLAs. The second line is dedicated to developing VLA-based control policies adept at predicting low-level actions. The third line comprises high-level task planners capable of decomposing long-horizon tasks into a sequence of subtasks, thereby guiding VLAs to follow more general user instructions. Furthermore, we provide an extensive summary of relevant resources, including datasets, simulators, and benchmarks. Finally, we discuss the challenges facing VLAs and outline promising future directions in embodied AI. A curated repository associated with this survey is available at: https://github.com/yueen-ma/Awesome-VLA.",
      "authors": [
        "Yueen Ma",
        "Zixing Song",
        "Yuzheng Zhuang",
        "Jianye Hao",
        "Irwin King"
      ],
      "published_date": "2024-05-23",
      "pdf_url": "https://arxiv.org/pdf/2405.14093v7",
      "citation_count": null,
      "year": null
    },
    {
      "title": "DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation",
      "arxiv_id": "",
      "abstract": "Closed-loop simulation is essential for advancing end-to-end autonomous driving systems. Contemporary sensor simulation methods, such as NeRF and 3DGS, rely predominantly on conditions closely aligned with training data distributions, which are largely confined to forward-driving scenarios. Consequently, these methods face limitations when rendering complex maneuvers (e.g., lane change, acceleration, deceleration). Recent advancements in autonomous-driving world models have demonstrated the potential to generate diverse driving videos. However, these approaches remain constrained to 2D video generation, inherently lacking the spatiotemporal coherence required to capture intricacies of dynamic driving environments. In this paper, we introduce DriveDreamer4D, which enhances 4D driving scene representation leveraging world model priors. Specifically, we utilize the world model as a data machine to synthesize novel trajectory videos, where structured conditions are explicitly leveraged to control the spatial-temporal consistency of traffic elements. Besides, the cousin data training strategy is proposed to facilitate merging real and synthetic data for optimizing 4DGS. To our knowledge, DriveDreamer4D is the first to utilize video generation models for improving 4D reconstruction in driving scenarios. Experimental results reveal that DriveDreamer4D significantly enhances generation quality under novel trajectory views, achieving a relative improvement in FID by 32.1%, 46.4%, and 16.3% compared to PVG, S3Gaussian, and Deformable-GS. Moreover, DriveDreamer4D markedly enhances the spatiotemporal coherence of driving agents, which is verified by a comprehensive user study and the relative increases of 22.6%, 43.5%, and 15.6% in the NTA-IoU metric.",
      "authors": [
        "Guosheng Zhao",
        "Chaojun Ni",
        "Xiaofeng Wang",
        "Zheng Zhu",
        "Xueyang Zhang",
        "Yida Wang",
        "Guan Huang",
        "Xinze Chen",
        "Boyuan Wang",
        "Youyi Zhang",
        "Wenjun Mei",
        "Xingang Wang"
      ],
      "published_date": "2024-10-17",
      "pdf_url": "https://arxiv.org/pdf/2410.13571v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation",
      "arxiv_id": "",
      "abstract": "Text-to-video (T2V) models like Sora have made significant strides in visualizing complex prompts, which is increasingly viewed as a promising path towards constructing the universal world simulator. Cognitive psychologists believe that the foundation for achieving this goal is the ability to understand intuitive physics. However, the capacity of these models to accurately represent intuitive physics remains largely unexplored. To bridge this gap, we introduce PhyGenBench, a comprehensive \\textbf{Phy}sics \\textbf{Gen}eration \\textbf{Ben}chmark designed to evaluate physical commonsense correctness in T2V generation. PhyGenBench comprises 160 carefully crafted prompts across 27 distinct physical laws, spanning four fundamental domains, which could comprehensively assesses models' understanding of physical commonsense. Alongside PhyGenBench, we propose a novel evaluation framework called PhyGenEval. This framework employs a hierarchical evaluation structure utilizing appropriate advanced vision-language models and large language models to assess physical commonsense. Through PhyGenBench and PhyGenEval, we can conduct large-scale automated assessments of T2V models' understanding of physical commonsense, which align closely with human feedback. Our evaluation results and in-depth analysis demonstrate that current models struggle to generate videos that comply with physical commonsense. Moreover, simply scaling up models or employing prompt engineering techniques is insufficient to fully address the challenges presented by PhyGenBench (e.g., dynamic scenarios). We hope this study will inspire the community to prioritize the learning of physical commonsense in these models beyond entertainment applications. We will release the data and codes at https://github.com/OpenGVLab/PhyGenBench",
      "authors": [
        "Fanqing Meng",
        "Jiaqi Liao",
        "Xinyu Tan",
        "Wenqi Shao",
        "Quanfeng Lu",
        "Kaipeng Zhang",
        "Yu Cheng",
        "Dianqi Li",
        "Yu Qiao",
        "Ping Luo"
      ],
      "published_date": "2024-10-07",
      "pdf_url": "https://arxiv.org/pdf/2410.05363v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "AgentSquare: Automatic LLM Agent Search in Modular Design Space",
      "arxiv_id": "",
      "abstract": "Recent advancements in Large Language Models (LLMs) have led to a rapid growth of agentic systems capable of handling a wide range of complex tasks. However, current research largely relies on manual, task-specific design, limiting their adaptability to novel tasks. In this paper, we introduce a new research problem: Modularized LLM Agent Search (MoLAS). We propose a modular design space that abstracts existing LLM agent designs into four fundamental modules with uniform IO interface: Planning, Reasoning, Tool Use, and Memory. Building on this design space, we present a novel LLM agent search framework called AgentSquare, which introduces two core mechanisms, i.e., module evolution and recombination, to efficiently search for optimized LLM agents. To further accelerate the process, we design a performance predictor that uses in-context surrogate models to skip unpromising agent designs. Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Moreover, AgentSquare can generate interpretable design insights, enabling a deeper understanding of agentic architecture and its impact on task performance. We believe that the modular design space and AgentSquare search framework offer a platform for fully exploiting the potential of prior successful designs and consolidating the collective efforts of research community. Code repo is available at https://github.com/tsinghua-fib-lab/AgentSquare.",
      "authors": [
        "Yu Shang",
        "Yu Li",
        "Keyu Zhao",
        "Likai Ma",
        "Jiahe Liu",
        "Fengli Xu",
        "Yong Li"
      ],
      "published_date": "2024-10-08",
      "pdf_url": "https://arxiv.org/pdf/2410.06153v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "CityGPT: Empowering Urban Spatial Cognition of Large Language Models",
      "arxiv_id": "",
      "abstract": "Large language models(LLMs), with their powerful language generation and reasoning capabilities, have already achieved notable success in many domains, e.g., math and code generation. However, they often fall short when tackling real-life geospatial tasks within urban environments. This limitation stems from a lack of physical world knowledge and relevant data during training. To address this gap, we propose \\textit{CityGPT}, a systematic framework designed to enhance LLMs' understanding of urban space and improve their ability to solve the related urban tasks by integrating a city-scale `world model' into the model. Firstly, we construct a diverse instruction tuning dataset, \\textit{CityInstruction}, for injecting urban knowledge into LLMs and effectively boosting their spatial reasoning capabilities. Using a combination of \\textit{CityInstruction} and open source general instruction data, we introduce a novel and easy-to-use self-weighted fine-tuning method (\\textit{SWFT}) to train various LLMs (including ChatGLM3-6B, Llama3-8B, and Qwen2.5-7B) to enhance their urban spatial capabilities without compromising, or even improving, their general abilities. Finally, to validate the effectiveness of our proposed framework, we develop a comprehensive text-based spatial benchmark \\textit{CityEval} for evaluating the performance of LLMs across a wide range of urban scenarios and geospatial tasks. Extensive evaluation results demonstrate that smaller LLMs trained with \\textit{CityInstruction} by \\textit{SWFT} method can achieve performance that is competitive with, and in some cases superior to, proprietary LLMs when assessed using \\textit{CityEval}.",
      "authors": [
        "Jie Feng",
        "Tianhui Liu",
        "Yuwei Du",
        "Siqi Guo",
        "Yuming Lin",
        "Yong Li"
      ],
      "published_date": "2024-06-20",
      "pdf_url": "https://arxiv.org/pdf/2406.13948v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Epona: Autoregressive Diffusion World Model for Autonomous Driving",
      "arxiv_id": "",
      "abstract": "Diffusion models have demonstrated exceptional visual quality in video generation, making them promising for autonomous driving world modeling. However, existing video diffusion-based world models struggle with flexible-length, long-horizon predictions and integrating trajectory planning. This is because conventional video diffusion models rely on global joint distribution modeling of fixed-length frame sequences rather than sequentially constructing localized distributions at each timestep. In this work, we propose Epona, an autoregressive diffusion world model that enables localized spatiotemporal distribution modeling through two key innovations: 1) Decoupled spatiotemporal factorization that separates temporal dynamics modeling from fine-grained future world generation, and 2) Modular trajectory and video prediction that seamlessly integrate motion planning with visual modeling in an end-to-end framework. Our architecture enables high-resolution, long-duration generation while introducing a novel chain-of-forward training strategy to address error accumulation in autoregressive loops. Experimental results demonstrate state-of-the-art performance with 7.4\\% FVD improvement and minutes longer prediction duration compared to prior works. The learned world model further serves as a real-time motion planner, outperforming strong end-to-end planners on NAVSIM benchmarks. Code will be publicly available at \\href{https://github.com/Kevin-thu/Epona/}{https://github.com/Kevin-thu/Epona/}.",
      "authors": [
        "Kaiwen Zhang",
        "Zhenyu Tang",
        "Xiaotao Hu",
        "Xingang Pan",
        "Xiaoyang Guo",
        "Yuan Liu",
        "Jingwei Huang",
        "Li Yuan",
        "Qian Zhang",
        "Xiao-Xiao Long",
        "Xun Cao",
        "Wei Yin"
      ],
      "published_date": "2025-06-30",
      "pdf_url": "https://arxiv.org/pdf/2506.24113v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "arxiv_id": "",
      "abstract": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "published_date": "2019-07-26",
      "pdf_url": "https://arxiv.org/pdf/1907.11692v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Unlearning Concepts in Diffusion Model via Concept Domain Correction and Concept Preserving Gradient",
      "arxiv_id": "",
      "abstract": "Text-to-image diffusion models have achieved remarkable success in generating photorealistic images. However, the inclusion of sensitive information during pre-training poses significant risks. Machine Unlearning (MU) offers a promising solution to eliminate sensitive concepts from these models. Despite its potential, existing MU methods face two main challenges: 1) limited generalization, where concept erasure is effective only within the unlearned set, failing to prevent sensitive concept generation from out-of-set prompts; and 2) utility degradation, where removing target concepts significantly impacts the model's overall performance. To address these issues, we propose a novel concept domain correction framework named \\textbf{DoCo} (\\textbf{Do}main \\textbf{Co}rrection). By aligning the output domains of sensitive and anchor concepts through adversarial training, our approach ensures comprehensive unlearning of target concepts. Additionally, we introduce a concept-preserving gradient surgery technique that mitigates conflicting gradient components, thereby preserving the model's utility while unlearning specific concepts. Extensive experiments across various instances, styles, and offensive concepts demonstrate the effectiveness of our method in unlearning targeted concepts with minimal impact on related concepts, outperforming previous approaches even for out-of-distribution prompts.",
      "authors": [
        "Yongliang Wu",
        "Shiji Zhou",
        "Mingzhuo Yang",
        "Lianzhe Wang",
        "Heng Chang",
        "Wenbo Zhu",
        "Xinting Hu",
        "Xiao Zhou",
        "Xu Yang"
      ],
      "published_date": "2024-05-24",
      "pdf_url": "https://arxiv.org/pdf/2405.15304v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Safety at Scale: A Comprehensive Survey of Large Model Safety",
      "arxiv_id": null,
      "abstract": "Abstract—The rapid advancement of large models, driven by their exceptional abilities in learning and generalization through large-scale pre-training, has reshaped the landscape of Artificial Intelligence (AI). These models are now foundational to a wide range of applications, including conversational AI, recommendation systems, autonomous driving, content generation, medical diagnostics, and scientific discovery. However, their widespread deployment also exposes them to significant safety risks, raising concerns about robustness, reliability, and ethical implications. This survey provides a systematic review of current safety research on large models, covering Vision Foundation Models (VFMs), Large Language Models (LLMs), Vision-Language Pre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models (DMs), and large-model-based Agents. Our contributions are summarized as follows: (1) We present a comprehensive taxonomy of safety threats to these models, including adversarial attacks, data poisoning, backdoor attacks, jailbreak and prompt injection attacks, energy-latency attacks, data and model extraction attacks, and emerging agent-specific threats. (2) We review defense strategies proposed for each type of attacks if available and summarize the commonly used datasets and benchmarks for safety research. (3) Building on this, we identify and discuss the open challenges in large model safety, emphasizing the need for comprehensive safety evaluations, scalable and effective defense mechanisms, and sustainable data practices. More importantly, we highlight the necessity of collective efforts from the research community and international collaboration. Our work can serve as a useful reference for researchers and practitioners, fostering the ongoing development of comprehensive defense systems and platforms to safeguard AI models. GitHub: https://github.com/xingjunm/Awesome-Large-Model-Safety. Index Terms—Large Model Safety, AI Safety, Attacks and Defenses",
      "authors": [
        "Xingjun Ma",
        "Yifeng Gao",
        "Yixu Wang",
        "Ruofan Wang",
        "Xin Wang",
        "Ye Sun",
        "Yifan Ding",
        "Hengyuan Xu",
        "Yunhao Chen",
        "Yunhan Zhao",
        "Hanxun Huang",
        "Yige Li",
        "Jiaming Zhang",
        "Xiang Zheng",
        "Yang Bai",
        "Henghui Ding",
        "Zuxuan Wu",
        "Xipeng Qiu",
        "Jingfeng Zhang",
        "Yiming Li",
        "Jun Sun",
        "Cong Wang",
        "Jindong Gu",
        "Baoyuan Wu",
        "Siheng Chen",
        "Tianwei Zhang",
        "Yang Liu",
        "Min Gong",
        "Tongliang Liu",
        "Shirui Pan",
        "Cihang Xie",
        "Tianyu Pang",
        "Yinpeng Dong",
        "Ruoxi Jia",
        "Yang Zhang",
        "Shi-jie Ma",
        "Xiangyu Zhang",
        "Neil Gong",
        "Chaowei Xiao",
        "Sarah Erfani",
        "Bo Li",
        "Masashi Sugiyama",
        "Dacheng Tao",
        "James Bailey",
        "Yu-Gang Jiang"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 48,
      "year": 2025
    },
    {
      "title": "Large multimodal models evaluation: a survey",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Zicheng Zhang",
        "Junying Wang",
        "Farong Wen",
        "Yijin Guo",
        "Xiangyu Zhao",
        "Xinyu Fang",
        "Shengyuan Ding",
        "Ziheng Jia",
        "Jiahao Xiao",
        "Ye Shen",
        "Yushuo Zheng",
        "Xiaorong Zhu",
        "Yalun Wu",
        "Ziheng Jiao",
        "Wei Sun",
        "Zijian Chen",
        "Kaiwei Zhang",
        "Kang Fu",
        "Yuqin Cao",
        "Ming Hu",
        "Yue Zhou",
        "Xuemei Zhou",
        "Juntai Cao",
        "Wei Zhou",
        "Jinyu Cao",
        "Ronghui Li",
        "Donghao Zhou",
        "Yuan Tian",
        "Xiangyang Zhu",
        "Chun-yuan Li",
        "Haoning Wu",
        "Xiaohong Liu",
        "Junjun He",
        "Yu Zhou",
        "Hui Liu",
        "Lin Zhang",
        "Zesheng Wang",
        "Huiyu Duan",
        "Yingjie Zhou",
        "Xiongkuo Min",
        "Qi Jia",
        "Dongzhan Zhou",
        "Wenlong Zhang",
        "Jiezhang Cao",
        "Xue Yang",
        "Junzhi Yu",
        "Songyang Zhang",
        "Haodong Duan",
        "Guangtao Zhai"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 18,
      "year": 2025
    },
    {
      "title": "The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense",
      "arxiv_id": "",
      "abstract": "The vulnerability of Vision Large Language Models (VLLMs) to jailbreak attacks appears as no surprise. However, recent defense mechanisms against these attacks have reached near-saturation performance on benchmark evaluations, often with minimal effort. This \\emph{dual high performance} in both attack and defense raises a fundamental and perplexing paradox. To gain a deep understanding of this issue and thus further help strengthen the trustworthiness of VLLMs, this paper makes three key contributions: i) One tentative explanation for VLLMs being prone to jailbreak attacks--\\textbf{inclusion of vision inputs}, as well as its in-depth analysis. ii) The recognition of a largely ignored problem in existing defense mechanisms--\\textbf{over-prudence}. The problem causes these defense methods to exhibit unintended abstention, even in the presence of benign inputs, thereby undermining their reliability in faithfully defending against attacks. iii) A simple safety-aware method--\\textbf{LLM-Pipeline}. Our method repurposes the more advanced guardrails of LLMs on the shelf, serving as an effective alternative detector prior to VLLM response. Last but not least, we find that the two representative evaluation methods for jailbreak often exhibit chance agreement. This limitation makes it potentially misleading when evaluating attack strategies or defense mechanisms. We believe the findings from this paper offer useful insights to rethink the foundational development of VLLM safety with respect to benchmark datasets, defense strategies, and evaluation methods.",
      "authors": [
        "Yangyang Guo",
        "Fangkai Jiao",
        "Liqiang Nie",
        "Mohan Kankanhalli"
      ],
      "published_date": "2024-11-13",
      "pdf_url": "https://arxiv.org/pdf/2411.08410v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs",
      "arxiv_id": "",
      "abstract": "The increasing deployment of Large Vision-Language Models (LVLMs) raises safety concerns under potential malicious inputs. However, existing multimodal safety evaluations primarily focus on model vulnerabilities exposed by static image inputs, ignoring the temporal dynamics of video that may induce distinct safety risks. To bridge this gap, we introduce Video-SafetyBench, the first comprehensive benchmark designed to evaluate the safety of LVLMs under video-text attacks. It comprises 2,264 video-text pairs spanning 48 fine-grained unsafe categories, each pairing a synthesized video with either a harmful query, which contains explicit malice, or a benign query, which appears harmless but triggers harmful behavior when interpreted alongside the video. To generate semantically accurate videos for safety evaluation, we design a controllable pipeline that decomposes video semantics into subject images (what is shown) and motion text (how it moves), which jointly guide the synthesis of query-relevant videos. To effectively evaluate uncertain or borderline harmful outputs, we propose RJScore, a novel LLM-based metric that incorporates the confidence of judge models and human-aligned decision threshold calibration. Extensive experiments show that benign-query video composition achieves average attack success rates of 67.2%, revealing consistent vulnerabilities to video-induced attacks. We believe Video-SafetyBench will catalyze future research into video-based safety evaluation and defense strategies.",
      "authors": [
        "Xuannan Liu",
        "Zekun Li",
        "Zheqi He",
        "Peipei Li",
        "Shuhan Xia",
        "Xing Cui",
        "Huaibo Huang",
        "Xi Yang",
        "Ran He"
      ],
      "published_date": "2025-05-17",
      "pdf_url": "https://arxiv.org/pdf/2505.11842v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Gradient-Guided Learning Network for Infrared Small Target Detection",
      "arxiv_id": "",
      "abstract": "Recently, infrared small target detection has attracted extensive attention. However, due to the small size and the lack of intrinsic features of infrared small targets, the existing methods generally have the problem of inaccurate edge positioning and the target is easily submerged by the background. Therefore, we propose an innovative gradient-guided learning network (GGL-Net). Specifically, we are the first to explore the introduction of gradient magnitude images into the deep learning-based infrared small target detection method, which is conducive to emphasizing the edge details and alleviating the problem of inaccurate edge positioning of small targets. On this basis, we propose a novel dual-branch feature extraction network that utilizes the proposed gradient supplementary module (GSM) to encode raw gradient information into deeper network layers and embeds attention mechanisms reasonably to enhance feature extraction ability. In addition, we construct a two-way guidance fusion module (TGFM), which fully considers the characteristics of feature maps at different levels. It can facilitate the effective fusion of multi-scale feature maps and extract richer semantic information and detailed information through reasonable two-way guidance. Extensive experiments prove that GGL-Net has achieves state-of-the-art results on the public real NUAA-SIRST dataset and the public synthetic NUDT-SIRST dataset. Our code has been integrated into https://github.com/YuChuang1205/MSDA-Net",
      "authors": [
        "Jinmiao Zhao",
        "Chuang Yu",
        "Zelin Shi",
        "Yunpeng Liu",
        "Yingdi Zhang"
      ],
      "published_date": "2025-12-10",
      "pdf_url": "https://arxiv.org/pdf/2512.09497v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "A Retrieval-Augmented Generation Approach to Extracting Algorithmic Logic from Neural Networks",
      "arxiv_id": "",
      "abstract": "Reusing existing neural-network components is central to research efficiency, yet discovering, extracting, and validating such modules across thousands of open-source repositories remains difficult. We introduce NN-RAG, a retrieval-augmented generation system that converts large, heterogeneous PyTorch codebases into a searchable and executable library of validated neural modules. Unlike conventional code search or clone-detection tools, NN-RAG performs scope-aware dependency resolution, import-preserving reconstruction, and validator-gated promotion -- ensuring that every retrieved block is scope-closed, compilable, and runnable. Applied to 19 major repositories, the pipeline extracted 1,289 candidate blocks, validated 941 (73.0%), and demonstrated that over 80% are structurally unique. Through multi-level de-duplication (exact, lexical, structural), we find that NN-RAG contributes the overwhelming majority of unique architectures to the LEMUR dataset, supplying approximately 72% of all novel network structures. Beyond quantity, NN-RAG uniquely enables cross-repository migration of architectural patterns, automatically identifying reusable modules in one project and regenerating them, dependency-complete, in another context. To our knowledge, no other open-source system provides this capability at scale. The framework's neutral specifications further allow optional integration with language models for synthesis or dataset registration without redistributing third-party code. Overall, NN-RAG transforms fragmented vision code into a reproducible, provenance-tracked substrate for algorithmic discovery, offering a first open-source solution that both quantifies and expands the diversity of executable neural architectures across repositories.",
      "authors": [
        "Waleed Khalid",
        "Dmitry Ignatov",
        "Radu Timofte"
      ],
      "published_date": "2025-12-03",
      "pdf_url": "https://arxiv.org/pdf/2512.04329v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation",
      "arxiv_id": "",
      "abstract": "Brain lesion segmentation remains challenging due to small, low-contrast lesions, anisotropic sampling, and cross-slice discontinuities. We propose CenterMamba-SAM, an end-to-end framework that freezes a pretrained backbone and trains only lightweight adapters for efficient fine-tuning. At its core is the CenterMamba encoder, which employs a novel 3x3 corner-axis-center short-sequence scanning strategy to enable center-prioritized, axis-reinforced, and diagonally compensated information aggregation. This design enhances sensitivity to weak boundaries and tiny foci while maintaining sparse yet effective feature representation. A memory-driven structural prompt generator maintains a prototype bank across neighboring slices, enabling automatic synthesis of reliable prompts without user interaction, thereby improving inter-slice coherence. The memory-augmented multi-scale decoder integrates memory attention modules at multiple levels, combining deep supervision with progressive refinement to restore fine details while preserving global consistency. Extensive experiments on public benchmarks demonstrate that CenterMamba-SAM achieves state-of-the-art performance in brain lesion segmentation.",
      "authors": [
        "Yu Tian",
        "Zhongheng Yang",
        "Chenshi Liu",
        "Yiyun Su",
        "Ziwei Hong",
        "Zexi Gong",
        "Jingyuan Xu"
      ],
      "published_date": "2025-11-03",
      "pdf_url": "https://arxiv.org/pdf/2511.01243v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "IMobileTransformer: A fusion-based lightweight model for rice disease identification",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Yang Lu",
        "Haoyang Zhou",
        "Peng Wang",
        "Erzhi Wang",
        "Gongfa Li",
        "Tongjian Yu"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 11,
      "year": 2025
    },
    {
      "title": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Jinhui Yi",
        "Gina Lopez",
        "S. Hadir",
        "Jan Weyler",
        "Lasse Klingbeil",
        "Marion Deichmann",
        "Juergen Gall",
        "S. J. Seidel"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 5,
      "year": 2025
    },
    {
      "title": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers",
      "arxiv_id": "",
      "abstract": "Open-vocabulary detectors achieve impressive performance on COCO, but often fail to generalize to real-world datasets with out-of-distribution classes not typically found in their pre-training. Rather than simply fine-tuning a heavy-weight vision-language model (VLM) for new domains, we introduce RF-DETR, a light-weight specialist detection transformer that discovers accuracy-latency Pareto curves for any target dataset with weight-sharing neural architecture search (NAS). Our approach fine-tunes a pre-trained base network on a target dataset and evaluates thousands of network configurations with different accuracy-latency tradeoffs without re-training. Further, we revisit the \"tunable knobs\" for NAS to improve the transferability of DETRs to diverse target domains. Notably, RF-DETR significantly improves over prior state-of-the-art real-time methods on COCO and Roboflow100-VL. RF-DETR (nano) achieves 48.0 AP on COCO, beating D-FINE (nano) by 5.3 AP at similar latency, and RF-DETR (2x-large) outperforms GroundingDINO (tiny) by 1.2 AP on Roboflow100-VL while running 20x as fast. To the best of our knowledge, RF-DETR (2x-large) is the first real-time detector to surpass 60 AP on COCO. Our code is available at https://github.com/roboflow/rf-detr",
      "authors": [
        "Isaac Robinson",
        "Peter Robicheaux",
        "Matvei Popov",
        "Deva Ramanan",
        "Neehar Peri"
      ],
      "published_date": "2025-11-12",
      "pdf_url": "https://arxiv.org/pdf/2511.09554v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Image quality assessment: from error visibility to structural similarity",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Zhou Wang",
        "A. Bovik",
        "H. Sheikh",
        "Eero P. Simoncelli"
      ],
      "published_date": "2004",
      "pdf_url": "",
      "citation_count": 54339,
      "year": 2004
    },
    {
      "title": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction",
      "arxiv_id": "",
      "abstract": "Camera redirection aims to replay a dynamic scene from a single monocular video under a user-specified camera trajectory. However, large-angle redirection is inherently ill-posed: a monocular video captures only a narrow spatio-temporal view of a dynamic 3D scene, providing highly partial observations of the underlying 4D world. The key challenge is therefore to recover a complete and coherent representation from this limited input, with consistent geometry and motion. While recent diffusion-based methods achieve impressive results, they often break down under large-angle viewpoint changes far from the original trajectory, where missing visual grounding leads to severe geometric ambiguity and temporal inconsistency. To address this, we present FreeOrbit4D, an effective training-free framework that tackles this geometric ambiguity by recovering a geometry-complete 4D proxy as structural grounding for video generation. We obtain this proxy by decoupling foreground and background reconstructions: we unproject the monocular video into a static background and geometry-incomplete foreground point clouds in a unified global space, then leverage an object-centric multi-view diffusion model to synthesize multi-view images and reconstruct geometry-complete foreground point clouds in canonical object space. By aligning the canonical foreground point cloud to the global scene space via dense pixel-synchronized 3D--3D correspondences and projecting the geometry-complete 4D proxy onto target camera viewpoints, we provide geometric scaffolds that guide a conditional video diffusion model. Extensive experiments show that FreeOrbit4D produces more faithful redirected videos under challenging large-angle trajectories, and our geometry-complete 4D proxy further opens a potential avenue for practical applications such as edit propagation and 4D data generation. Project page and code will be released soon.",
      "authors": [
        "Wei Cao",
        "Hao Zhang",
        "Fengrui Tian",
        "Yulun Wu",
        "Yingying Li",
        "Shenlong Wang",
        "Ning Yu",
        "Yaoyao Liu"
      ],
      "published_date": "2026-01-26",
      "pdf_url": "https://arxiv.org/pdf/2601.18993v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos",
      "arxiv_id": "",
      "abstract": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io",
      "authors": [
        "Yuxue Yang",
        "Lue Fan",
        "Ziqi Shi",
        "Junran Peng",
        "Feng Wang",
        "Zhaoxiang Zhang"
      ],
      "published_date": "2026-01-01",
      "pdf_url": "https://arxiv.org/pdf/2601.00393v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Scalable Diffusion Models with Transformers",
      "arxiv_id": "",
      "abstract": "We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops -- through increased transformer depth/width or increased number of input tokens -- consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512x512 and 256x256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.",
      "authors": [
        "William Peebles",
        "Saining Xie"
      ],
      "published_date": "2022-12-19",
      "pdf_url": "https://arxiv.org/pdf/2212.09748v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding",
      "arxiv_id": "",
      "abstract": "Position encoding recently has shown effective in the transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding(RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: \\url{https://huggingface.co/docs/transformers/model_doc/roformer}.",
      "authors": [
        "Jianlin Su",
        "Yu Lu",
        "Shengfeng Pan",
        "Ahmed Murtadha",
        "Bo Wen",
        "Yunfeng Liu"
      ],
      "published_date": "2021-04-20",
      "pdf_url": "https://arxiv.org/pdf/2104.09864v5",
      "citation_count": null,
      "year": null
    },
    {
      "title": "The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey",
      "arxiv_id": "",
      "abstract": "The Driving World Model (DWM), which focuses on predicting scene evolution during the driving process, has emerged as a promising paradigm in the pursuit of autonomous driving (AD). DWMs enable AD systems to better perceive, understand, and interact with dynamic driving environments. In this survey, we provide a comprehensive overview of the latest progress in DWM. First, we review the DWM ecosystem, which is constructed using mainstream simulators, high-impact datasets, and various metrics that evaluate DWMs across multiple dimensions. We then categorize existing approaches based on the modalities of the predicted scenes, including video, point cloud, occupancy, latent feature, and traffic map, and summarize their specific applications in AD research. In addition, the performance of representative approaches across generating and driving tasks is presented. Finally, we discuss the potential limitations of current research and propose future directions. This survey provides valuable insights into the development and application of DWM, fostering its broader adoption in AD. The relevant papers are collected at https://github.com/LMD0311/Awesome-World-Model.",
      "authors": [
        "Sifan Tu",
        "Xin Zhou",
        "Dingkang Liang",
        "Xingyu Jiang",
        "Yumeng Zhang",
        "Xiaofan Li",
        "Xiang Bai"
      ],
      "published_date": "2025-02-14",
      "pdf_url": "https://arxiv.org/pdf/2502.10498v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Vision meets robotics: The KITTI dataset",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Andreas Geiger",
        "Philip Lenz",
        "C. Stiller",
        "R. Urtasun"
      ],
      "published_date": "2013",
      "pdf_url": "",
      "citation_count": 9855,
      "year": 2013
    },
    {
      "title": "Least-Squares Estimation of Transformation Parameters Between Two Point Patterns",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "S. Umeyama"
      ],
      "published_date": "1991",
      "pdf_url": "",
      "citation_count": 2613,
      "year": 1991
    },
    {
      "title": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations",
      "arxiv_id": "",
      "abstract": "High-definition (HD) maps provide essential semantic information of road structures for autonomous driving systems, yet current HD map construction methods require calibrated multi-camera setups and either implicit or explicit 2D-to-BEV transformations, making them fragile when sensors fail or camera configurations vary across vehicle fleets. We introduce FlexMap, unlike prior methods that are fixed to a specific N-camera rig, our approach adapts to variable camera configurations without any architectural changes or per-configuration retraining. Our key innovation eliminates explicit geometric projections by using a geometry-aware foundation model with cross-frame attention to implicitly encode 3D scene understanding in feature space. FlexMap features two core components: a spatial-temporal enhancement module that separates cross-view spatial reasoning from temporal dynamics, and a camera-aware decoder with latent camera tokens, enabling view-adaptive attention without the need for projection matrices. Experiments demonstrate that FlexMap outperforms existing methods across multiple configurations while maintaining robustness to missing views and sensor variations, enabling more practical real-world deployment.",
      "authors": [
        "Run Wang",
        "Chaoyi Zhou",
        "Amir Salarpour",
        "Xi Liu",
        "Zhi-Qi Cheng",
        "Feng Luo",
        "Mert D. Pesé",
        "Siyu Huang"
      ],
      "published_date": "2026-01-29",
      "pdf_url": "https://arxiv.org/pdf/2601.22376v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Deep Learning",
      "arxiv_id": null,
      "abstract": "",
      "authors": [
        "Xingbang Hao",
        "Guigang Zhang",
        "Shang Ma"
      ],
      "published_date": "2016",
      "pdf_url": "",
      "citation_count": 70954,
      "year": 2016
    },
    {
      "title": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction",
      "arxiv_id": "",
      "abstract": "We introduce MapAnything, a unified transformer-based feed-forward model that ingests one or more images along with optional geometric inputs such as camera intrinsics, poses, depth, or partial reconstructions, and then directly regresses the metric 3D scene geometry and cameras. MapAnything leverages a factored representation of multi-view scene geometry, i.e., a collection of depth maps, local ray maps, camera poses, and a metric scale factor that effectively upgrades local reconstructions into a globally consistent metric frame. Standardizing the supervision and training across diverse datasets, along with flexible input augmentation, enables MapAnything to address a broad range of 3D vision tasks in a single feed-forward pass, including uncalibrated structure-from-motion, calibrated multi-view stereo, monocular depth estimation, camera localization, depth completion, and more. We provide extensive experimental analyses and model ablations demonstrating that MapAnything outperforms or matches specialist feed-forward models while offering more efficient joint training behavior, thus paving the way toward a universal 3D reconstruction backbone.",
      "authors": [
        "Nikhil Keetha",
        "Norman Müller",
        "Johannes Schönberger",
        "Lorenzo Porzi",
        "Yuchen Zhang",
        "Tobias Fischer",
        "Arno Knapitsch",
        "Duncan Zauss",
        "Ethan Weber",
        "Nelson Antunes",
        "Jonathon Luiten",
        "Manuel Lopez-Antequera",
        "Samuel Rota Bulò",
        "Christian Richardt",
        "Deva Ramanan",
        "Sebastian Scherer",
        "Peter Kontschieder"
      ],
      "published_date": "2025-09-16",
      "pdf_url": "https://arxiv.org/pdf/2509.13414v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Seedream 4.0: Toward Next-generation Multimodal Image Generation",
      "arxiv_id": "",
      "abstract": "We introduce Seedream 4.0, an efficient and high-performance multimodal image generation system that unifies text-to-image (T2I) synthesis, image editing, and multi-image composition within a single framework. We develop a highly efficient diffusion transformer with a powerful VAE which also can reduce the number of image tokens considerably. This allows for efficient training of our model, and enables it to fast generate native high-resolution images (e.g., 1K-4K). Seedream 4.0 is pretrained on billions of text-image pairs spanning diverse taxonomies and knowledge-centric concepts. Comprehensive data collection across hundreds of vertical scenarios, coupled with optimized strategies, ensures stable and large-scale training, with strong generalization. By incorporating a carefully fine-tuned VLM model, we perform multi-modal post-training for training both T2I and image editing tasks jointly. For inference acceleration, we integrate adversarial distillation, distribution matching, and quantization, as well as speculative decoding. It achieves an inference time of up to 1.8 seconds for generating a 2K image (without a LLM/VLM as PE model). Comprehensive evaluations reveal that Seedream 4.0 can achieve state-of-the-art results on both T2I and multimodal image editing. In particular, it demonstrates exceptional multimodal capabilities in complex tasks, including precise image editing and in-context reasoning, and also allows for multi-image reference, and can generate multiple output images. This extends traditional T2I systems into an more interactive and multidimensional creative tool, pushing the boundary of generative AI for both creativity and professional applications. We further scale our model and data as Seedream 4.5. Seedream 4.0 and Seedream 4.5 are accessible on Volcano Engine https://www.volcengine.com/experience/ark?launch=seedream.",
      "authors": [
        "Team Seedream",
        ":",
        "Yunpeng Chen",
        "Yu Gao",
        "Lixue Gong",
        "Meng Guo",
        "Qiushan Guo",
        "Zhiyao Guo",
        "Xiaoxia Hou",
        "Weilin Huang",
        "Yixuan Huang",
        "Xiaowen Jian",
        "Huafeng Kuang",
        "Zhichao Lai",
        "Fanshi Li",
        "Liang Li",
        "Xiaochen Lian",
        "Chao Liao",
        "Liyang Liu",
        "Wei Liu",
        "Yanzuo Lu",
        "Zhengxiong Luo",
        "Tongtong Ou",
        "Guang Shi",
        "Yichun Shi",
        "Shiqi Sun",
        "Yu Tian",
        "Zhi Tian",
        "Peng Wang",
        "Rui Wang",
        "Xun Wang",
        "Ye Wang",
        "Guofeng Wu",
        "Jie Wu",
        "Wenxu Wu",
        "Yonghui Wu",
        "Xin Xia",
        "Xuefeng Xiao",
        "Shuang Xu",
        "Xin Yan",
        "Ceyuan Yang",
        "Jianchao Yang",
        "Zhonghua Zhai",
        "Chenlin Zhang",
        "Heng Zhang",
        "Qi Zhang",
        "Xinyu Zhang",
        "Yuwei Zhang",
        "Shijia Zhao",
        "Wenliang Zhao",
        "Wenjia Zhu"
      ],
      "published_date": "2025-09-24",
      "pdf_url": "https://arxiv.org/pdf/2509.20427v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation",
      "arxiv_id": "",
      "abstract": "Recently, GPT-4o has garnered significant attention for its strong performance in image generation, yet open-source models still lag behind. Several studies have explored distilling image data from GPT-4o to enhance open-source models, achieving notable progress. However, a key question remains: given that real-world image datasets already constitute a natural source of high-quality data, why should we use GPT-4o-generated synthetic data? In this work, we identify two key advantages of synthetic images. First, they can complement rare scenarios in real-world datasets, such as surreal fantasy or multi-reference image generation, which frequently occur in user queries. Second, they provide clean and controllable supervision. Real-world data often contains complex background noise and inherent misalignment between text descriptions and image content, whereas synthetic images offer pure backgrounds and long-tailed supervision signals, facilitating more accurate text-to-image alignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale synthetic dataset generated by GPT-4o, harnessing the power of synthetic image data to address blind spots in real-world coverage. Using this dataset, we fine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o. In addition, we propose two new evaluation benchmarks for a more accurate and challenging assessment of image generation capabilities: GenEval++, which increases instruction complexity to mitigate score saturation, and Imagine-Bench, which focuses on evaluating both the understanding and generation of imaginative content. Echo-4o demonstrates strong performance across standard benchmarks. Moreover, applying Echo-4o-Image to other foundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains across multiple metrics, highlighting the datasets strong transferability.",
      "authors": [
        "Junyan Ye",
        "Dongzhi Jiang",
        "Zihao Wang",
        "Leqi Zhu",
        "Zhenghao Hu",
        "Zilong Huang",
        "Jun He",
        "Zhiyuan Yan",
        "Jinghua Yu",
        "Hongsheng Li",
        "Conghui He",
        "Weijia Li"
      ],
      "published_date": "2025-08-13",
      "pdf_url": "https://arxiv.org/pdf/2508.09987v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding",
      "arxiv_id": "",
      "abstract": "We introduce Lumina-DiMOO, an open-source foundational model for seamless multi-modal generation and understanding. Lumina-DiMOO sets itself apart from prior unified models by utilizing a fully discrete diffusion modeling to handle inputs and outputs across various modalities. This innovative approach allows Lumina-DiMOO to achieve higher sampling efficiency compared to previous autoregressive (AR) or hybrid AR-Diffusion paradigms and adeptly support a broad spectrum of multi-modal tasks, including text-to-image generation, image-to-image generation (e.g., image editing, subject-driven generation, and image inpainting, etc.), as well as image understanding. Lumina-DiMOO achieves state-of-the-art performance on multiple benchmarks, surpassing existing open-source unified multi-modal models. To foster further advancements in multi-modal and discrete diffusion model research, we release our code and checkpoints to the community. Project Page: https://synbol.github.io/Lumina-DiMOO.",
      "authors": [
        "Yi Xin",
        "Qi Qin",
        "Siqi Luo",
        "Kaiwen Zhu",
        "Juncheng Yan",
        "Yan Tai",
        "Jiayi Lei",
        "Yuewen Cao",
        "Keqi Wang",
        "Yibin Wang",
        "Jinbin Bai",
        "Qian Yu",
        "Dengyang Jiang",
        "Yuandong Pu",
        "Haoxing Chen",
        "Le Zhuo",
        "Junjun He",
        "Gen Luo",
        "Tianbin Li",
        "Ming Hu",
        "Jin Ye",
        "Shenglong Ye",
        "Bo Zhang",
        "Chang Xu",
        "Wenhai Wang",
        "Hongsheng Li",
        "Guangtao Zhai",
        "Tianfan Xue",
        "Bin Fu",
        "Xiaohong Liu",
        "Yu Qiao",
        "Yihao Liu"
      ],
      "published_date": "2025-10-07",
      "pdf_url": "https://arxiv.org/pdf/2510.06308v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale",
      "arxiv_id": "",
      "abstract": "Prevailing autoregressive (AR) models for text-to-image generation either rely on heavy, computationally-intensive diffusion models to process continuous image tokens, or employ vector quantization (VQ) to obtain discrete tokens with quantization loss. In this paper, we push the autoregressive paradigm forward with NextStep-1, a 14B autoregressive model paired with a 157M flow matching head, training on discrete text tokens and continuous image tokens with next-token prediction objectives. NextStep-1 achieves state-of-the-art performance for autoregressive models in text-to-image generation tasks, exhibiting strong capabilities in high-fidelity image synthesis. Furthermore, our method shows strong performance in image editing, highlighting the power and versatility of our unified approach. To facilitate open research, we will release our code and models to the community.",
      "authors": [
        "NextStep Team",
        "Chunrui Han",
        "Guopeng Li",
        "Jingwei Wu",
        "Quan Sun",
        "Yan Cai",
        "Yuang Peng",
        "Zheng Ge",
        "Deyu Zhou",
        "Haomiao Tang",
        "Hongyu Zhou",
        "Kenkun Liu",
        "Ailin Huang",
        "Bin Wang",
        "Changxin Miao",
        "Deshan Sun",
        "En Yu",
        "Fukun Yin",
        "Gang Yu",
        "Hao Nie",
        "Haoran Lv",
        "Hanpeng Hu",
        "Jia Wang",
        "Jian Zhou",
        "Jianjian Sun",
        "Kaijun Tan",
        "Kang An",
        "Kangheng Lin",
        "Liang Zhao",
        "Mei Chen",
        "Peng Xing",
        "Rui Wang",
        "Shiyu Liu",
        "Shutao Xia",
        "Tianhao You",
        "Wei Ji",
        "Xianfang Zeng",
        "Xin Han",
        "Xuelin Zhang",
        "Yana Wei",
        "Yanming Xu",
        "Yimin Jiang",
        "Yingming Wang",
        "Yu Zhou",
        "Yucheng Han",
        "Ziyang Meng",
        "Binxing Jiao",
        "Daxin Jiang",
        "Xiangyu Zhang",
        "Yibo Zhu"
      ],
      "published_date": "2025-08-14",
      "pdf_url": "https://arxiv.org/pdf/2508.10711v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Training language models to follow instructions with human feedback",
      "arxiv_id": "",
      "abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",
      "authors": [
        "Long Ouyang",
        "Jeff Wu",
        "Xu Jiang",
        "Diogo Almeida",
        "Carroll L. Wainwright",
        "Pamela Mishkin",
        "Chong Zhang",
        "Sandhini Agarwal",
        "Katarina Slama",
        "Alex Ray",
        "John Schulman",
        "Jacob Hilton",
        "Fraser Kelton",
        "Luke Miller",
        "Maddie Simens",
        "Amanda Askell",
        "Peter Welinder",
        "Paul Christiano",
        "Jan Leike",
        "Ryan Lowe"
      ],
      "published_date": "2022-03-04",
      "pdf_url": "https://arxiv.org/pdf/2203.02155v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning",
      "arxiv_id": "",
      "abstract": "Efficiently acquiring external knowledge and up-to-date information is essential for effective reasoning and text generation in large language models (LLMs). Prompting advanced LLMs with reasoning capabilities to use search engines during inference is often suboptimal, as the LLM might not fully possess the capability on how to interact optimally with the search engine. This paper introduces Search-R1, an extension of reinforcement learning (RL) for reasoning frameworks where the LLM learns to autonomously generate (multiple) search queries during step-by-step reasoning with real-time retrieval. Search-R1 optimizes LLM reasoning trajectories with multi-turn search interactions, leveraging retrieved token masking for stable RL training and a simple outcome-based reward function. Experiments on seven question-answering datasets show that Search-R1 improves performance by 41% (Qwen2.5-7B) and 20% (Qwen2.5-3B) over various RAG baselines under the same setting. This paper further provides empirical insights into RL optimization methods, LLM choices, and response length dynamics in retrieval-augmented reasoning. The code and model checkpoints are available at https://github.com/PeterGriffinJin/Search-R1.",
      "authors": [
        "Bowen Jin",
        "Hansi Zeng",
        "Zhenrui Yue",
        "Jinsung Yoon",
        "Sercan Arik",
        "Dong Wang",
        "Hamed Zamani",
        "Jiawei Han"
      ],
      "published_date": "2025-03-12",
      "pdf_url": "https://arxiv.org/pdf/2503.09516v5",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Toward expert-level medical question answering with large language models",
      "arxiv_id": null,
      "abstract": "Large language models (LLMs) have shown promise in medical question answering, with Med-PaLM being the first to exceed a ‘passing’ score in United States Medical Licensing Examination style questions. However, challenges remain in long-form medical question answering and handling real-world workflows. Here, we present Med-PaLM 2, which bridges these gaps with a combination of base LLM improvements, medical domain fine-tuning and new strategies for improving reasoning and grounding through ensemble refinement and chain of retrieval. Med-PaLM 2 scores up to 86.5% on the MedQA dataset, improving upon Med-PaLM by over 19%, and demonstrates dramatic performance increases across MedMCQA, PubMedQA and MMLU clinical topics datasets. Our detailed human evaluations framework shows that physicians prefer Med-PaLM 2 answers to those from other physicians on eight of nine clinical axes. Med-PaLM 2 also demonstrates significant improvements over its predecessor across all evaluation metrics, particularly on new adversarial datasets designed to probe LLM limitations (P < 0.001). In a pilot study using real-world medical questions, specialists preferred Med-PaLM 2 answers to generalist physician answers 65% of the time. While specialist answers were still preferred overall, both specialists and generalists rated Med-PaLM 2 to be as safe as physician answers, demonstrating its growing potential in real-world medical applications. With an improved framework for model development and evaluation, a large language model is shown to provide answers to medical questions that are comparable or preferred with respect to those provided by human physicians.",
      "authors": [
        "Karan Singhal",
        "Tao Tu",
        "Juraj Gottweis",
        "R. Sayres",
        "Ellery Wulczyn",
        "Mohamed Amin",
        "Le Hou",
        "Kevin Clark",
        "Stephen R. Pfohl",
        "Heather Cole-Lewis",
        "Darlene Neal",
        "Q. Rashid",
        "Mike Schaekermann",
        "Amy Wang",
        "Dev Dash",
        "Jonathan H. Chen",
        "Nigam H. Shah",
        "Sami Lachgar",
        "P. Mansfield",
        "Sushant Prakash",
        "Bradley Green",
        "Ewa Dominowska",
        "Blaise Agüera y Arcas",
        "Nenad Tomašev",
        "Yun Liu",
        "Renee Wong",
        "Christopher Semturs",
        "S. Mahdavi",
        "Joelle K. Barral",
        "Dale R. Webster",
        "G. Corrado",
        "Yossi Matias",
        "Shekoofeh Azizi",
        "A. Karthikesalingam",
        "Vivek Natarajan"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 585,
      "year": 2025
    },
    {
      "title": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training",
      "arxiv_id": "",
      "abstract": "Supervised fine-tuning (SFT) and reinforcement learning (RL) are widely used post-training techniques for foundation models. However, their roles in enhancing model generalization capabilities remain unclear. This paper studies the difference between SFT and RL on generalization and memorization, focusing on text-based rule variants and visual variants. We introduce GeneralPoints, an arithmetic reasoning card game, and adopt V-IRL, a real-world navigation environment, to assess how models trained with SFT and RL generalize to unseen variants in both textual and visual domains. We show that RL, especially when trained with an outcome-based reward, generalizes across both rule-based textual and visual variants. SFT, in contrast, tends to memorize training data and struggles to generalize out-of-distribution scenarios. Further analysis reveals that RL improves the model's underlying visual recognition capabilities, contributing to its enhanced generalization in the visual domain. Despite RL's superior generalization, we show that SFT remains essential for effective RL training; SFT stabilizes the model's output format, enabling subsequent RL to achieve its performance gains. These findings demonstrates the capability of RL for acquiring generalizable knowledge in complex, multi-modal tasks.",
      "authors": [
        "Tianzhe Chu",
        "Yuexiang Zhai",
        "Jihan Yang",
        "Shengbang Tong",
        "Saining Xie",
        "Dale Schuurmans",
        "Quoc V. Le",
        "Sergey Levine",
        "Yi Ma"
      ],
      "published_date": "2025-01-28",
      "pdf_url": "https://arxiv.org/pdf/2501.17161v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Equivariant Diffusion for Crystal Structure Prediction",
      "arxiv_id": "",
      "abstract": "In addressing the challenge of Crystal Structure Prediction (CSP), symmetry-aware deep learning models, particularly diffusion models, have been extensively studied, which treat CSP as a conditional generation task. However, ensuring permutation, rotation, and periodic translation equivariance during diffusion process remains incompletely addressed. In this work, we propose EquiCSP, a novel equivariant diffusion-based generative model. We not only address the overlooked issue of lattice permutation equivariance in existing models, but also develop a unique noising algorithm that rigorously maintains periodic translation equivariance throughout both training and inference processes. Our experiments indicate that EquiCSP significantly surpasses existing models in terms of generating accurate structures and demonstrates faster convergence during the training process.",
      "authors": [
        "Peijia Lin",
        "Pin Chen",
        "Rui Jiao",
        "Qing Mo",
        "Jianhuan Cen",
        "Wenbing Huang",
        "Yang Liu",
        "Dan Huang",
        "Yutong Lu"
      ],
      "published_date": "2025-12-08",
      "pdf_url": "https://arxiv.org/pdf/2512.07289v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling",
      "arxiv_id": "",
      "abstract": "This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.",
      "authors": [
        "Wenqiang Sun",
        "Haiyu Zhang",
        "Haoyuan Wang",
        "Junta Wu",
        "Zehan Wang",
        "Zhenwei Wang",
        "Yunhong Wang",
        "Jun Zhang",
        "Tengfei Wang",
        "Chunchao Guo"
      ],
      "published_date": "2025-12-16",
      "pdf_url": "https://arxiv.org/pdf/2512.14614v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Native and Compact Structured Latents for 3D Generation",
      "arxiv_id": "",
      "abstract": "Recent advancements in 3D generative modeling have significantly improved the generation realism, yet the field is still hampered by existing representations, which struggle to capture assets with complex topologies and detailed appearance. This paper present an approach for learning a structured latent representation from native 3D data to address this challenge. At its core is a new sparse voxel structure called O-Voxel, an omni-voxel representation that encodes both geometry and appearance. O-Voxel can robustly model arbitrary topology, including open, non-manifold, and fully-enclosed surfaces, while capturing comprehensive surface attributes beyond texture color, such as physically-based rendering parameters. Based on O-Voxel, we design a Sparse Compression VAE which provides a high spatial compression rate and a compact latent space. We train large-scale flow-matching models comprising 4B parameters for 3D generation using diverse public 3D asset datasets. Despite their scale, inference remains highly efficient. Meanwhile, the geometry and material quality of our generated assets far exceed those of existing models. We believe our approach offers a significant advancement in 3D generative modeling.",
      "authors": [
        "Jianfeng Xiang",
        "Xiaoxue Chen",
        "Sicheng Xu",
        "Ruicheng Wang",
        "Zelong Lv",
        "Yu Deng",
        "Hongyuan Zhu",
        "Yue Dong",
        "Hao Zhao",
        "Nicholas Jing Yuan",
        "Jiaolong Yang"
      ],
      "published_date": "2025-12-16",
      "pdf_url": "https://arxiv.org/pdf/2512.14692v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?",
      "arxiv_id": "",
      "abstract": "A long-standing challenge in AI is to develop agents capable of solving a wide range of physical tasks and generalizing to new, unseen tasks and environments. A popular recent approach involves training a world model from state-action trajectories and subsequently use it with a planning algorithm to solve new tasks. Planning is commonly performed in the input space, but a recent family of methods has introduced planning algorithms that optimize in the learned representation space of the world model, with the promise that abstracting irrelevant details yields more efficient planning. In this work, we characterize models from this family as JEPA-WMs and investigate the technical choices that make algorithms from this class work. We propose a comprehensive study of several key components with the objective of finding the optimal approach within the family. We conducted experiments using both simulated environments and real-world robotic data, and studied how the model architecture, the training objective, and the planning algorithm affect planning success. We combine our findings to propose a model that outperforms two established baselines, DINO-WM and V-JEPA-2-AC, in both navigation and manipulation tasks. Code, data and checkpoints are available at https://github.com/facebookresearch/jepa-wms.",
      "authors": [
        "Basile Terver",
        "Tsung-Yen Yang",
        "Jean Ponce",
        "Adrien Bardes",
        "Yann LeCun"
      ],
      "published_date": "2025-12-30",
      "pdf_url": "https://arxiv.org/pdf/2512.24497v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Progressive Learned Image Transmission for Semantic Communication Using Hierarchical VAE",
      "arxiv_id": null,
      "abstract": "We present a novel framework called progressive learned image transmission (PLIT) that utilizes a hierarchical variational autoencoder (VAE) to catalyze semantic communication. PLIT employs autoregressive generation through bottom-up and top-down paths to create several feature representations of the transmitted image, effectively capturing contextual information. In this paper, we investigate the progressive transmission of these representations, particularly in the context of successive refinement. In this scenario, the representations are sent to the receiver in phases, with representations in later phases serving to enhance the image quality. Diverging from previous works, our proposed PLIT offers improved flexibility as it is able to dynamically determine the transmission rate. Specifically, PLIT can transform each representation into different numbers of channel symbols, guided by the hierarchical VAE’s learned priors that indicate the entropy of each representation. In addition, we devise a rate attention mechanism to help adjust the encoding strategy to realize different transmission rates. Furthermore, we introduce a spatial grouping strategy to reduce communication overhead for rate matching without compromising image fidelity. Extensive experiments show that our proposed approach outperforms existing baseline methods in terms of rate-distortion performance and maintains robust performance against channel noise.",
      "authors": [
        "Guangyi Zhang",
        "Hanlei Li",
        "Yunlong Cai",
        "Qiyu Hu",
        "Guanding Yu",
        "Zhijing Qin"
      ],
      "published_date": "2025",
      "pdf_url": "",
      "citation_count": 5,
      "year": 2025
    },
    {
      "title": "STORM: Search-Guided Generative World Models for Robotic Manipulation",
      "arxiv_id": "",
      "abstract": "We present STORM (Search-Guided Generative World Models), a novel framework for spatio-temporal reasoning in robotic manipulation that unifies diffusion-based action generation, conditional video prediction, and search-based planning. Unlike prior Vision-Language-Action (VLA) models that rely on abstract latent dynamics or delegate reasoning to language components, STORM grounds planning in explicit visual rollouts, enabling interpretable and foresight-driven decision-making. A diffusion-based VLA policy proposes diverse candidate actions, a generative video world model simulates their visual and reward outcomes, and Monte Carlo Tree Search (MCTS) selectively refines plans through lookahead evaluation. Experiments on the SimplerEnv manipulation benchmark demonstrate that STORM achieves a new state-of-the-art average success rate of 51.0 percent, outperforming strong baselines such as CogACT. Reward-augmented video prediction substantially improves spatio-temporal fidelity and task relevance, reducing Frechet Video Distance by over 75 percent. Moreover, STORM exhibits robust re-planning and failure recovery behavior, highlighting the advantages of search-guided generative world models for long-horizon robotic manipulation.",
      "authors": [
        "Wenjun Lin",
        "Jensen Zhang",
        "Kaitong Cai",
        "Keze Wang"
      ],
      "published_date": "2025-12-20",
      "pdf_url": "https://arxiv.org/pdf/2512.18477v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "A New Approach to Linear Filtering and Prediction Problems",
      "arxiv_id": null,
      "abstract": "",
      "authors": [],
      "published_date": "2002",
      "pdf_url": "",
      "citation_count": 27427,
      "year": 2002
    },
    {
      "title": "Continuous 3D Perception Model with Persistent State",
      "arxiv_id": "",
      "abstract": "We present a unified framework capable of solving a broad range of 3D tasks. Our approach features a stateful recurrent model that continuously updates its state representation with each new observation. Given a stream of images, this evolving state can be used to generate metric-scale pointmaps (per-pixel 3D points) for each new input in an online fashion. These pointmaps reside within a common coordinate system, and can be accumulated into a coherent, dense scene reconstruction that updates as new images arrive. Our model, called CUT3R (Continuous Updating Transformer for 3D Reconstruction), captures rich priors of real-world scenes: not only can it predict accurate pointmaps from image observations, but it can also infer unseen regions of the scene by probing at virtual, unobserved views. Our method is simple yet highly flexible, naturally accepting varying lengths of images that may be either video streams or unordered photo collections, containing both static and dynamic content. We evaluate our method on various 3D/4D tasks and demonstrate competitive or state-of-the-art performance in each. Project Page: https://cut3r.github.io/",
      "authors": [
        "Qianqian Wang",
        "Yifei Zhang",
        "Aleksander Holynski",
        "Alexei A. Efros",
        "Angjoo Kanazawa"
      ],
      "published_date": "2025-01-21",
      "pdf_url": "https://arxiv.org/pdf/2501.12387v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Gen3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control",
      "arxiv_id": "2503.03751",
      "abstract": "We present GEN3C, a generative video model with precise Camera Control and temporal 3D Consistency. Prior video models already generate realistic videos, but they tend to leverage little 3D information, leading to inconsistencies, such as objects popping in and out of existence. Camera control, if implemented at all, is imprecise, because camera parameters are mere inputs to the neural network which must then infer how the video depends on the camera. In contrast, GEN3C is guided by a 3D cache: point clouds obtained by predicting the pixel-wise depth of seed images or previously generated frames. When generating the next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with the new camera trajectory provided by the user. Crucially, this means that GEN3C neither has to remember what it previously generated nor does it have to infer the image structure from the camera pose. The model, instead, can focus all its generative power on previously unobserved regions, as well as advancing the scene state to the next frame. Our results demonstrate more precise camera control than prior work, as well as state-of-the-art results in sparse-view novel view synthesis, even in challenging settings such as driving scenes and monocular dynamic video. Results are best viewed in videos. Check out our webpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/",
      "authors": [
        "Xuanchi Ren",
        "Tianchang Shen",
        "Jiahui Huang",
        "Huan Ling",
        "Yifan Lu",
        "Merlin Nimier-David",
        "Thomas Müller",
        "Alexander Keller",
        "Sanja Fidler",
        "Jun Gao"
      ],
      "published_date": "2025-03-05",
      "pdf_url": "https://arxiv.org/pdf/2503.03751v1",
      "citation_count": 140,
      "year": 2025
    },
    {
      "title": "FLARE: Feed-Forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views",
      "arxiv_id": "2502.12138",
      "abstract": "We present FLARE, a feed-forward model designed to infer high-quality camera poses and 3D geometry from uncalibrated sparse-view images (i.e., as few as 2-8 inputs), which is a challenging yet practical setting in real-world applications. Our solution features a cascaded learning paradigm with camera pose serving as the critical bridge, recognizing its essential role in mapping 3D structures onto 2D image planes. Concretely, FLARE starts with camera pose estimation, whose results condition the subsequent learning of geometric structure and appearance, optimized through the objectives of geometry reconstruction and novel-view synthesis. Utilizing large-scale public datasets for training, our method delivers state-of-the-art performance in the tasks of pose estimation, geometry reconstruction, and novel view synthesis, while maintaining the inference efficiency (i.e., less than 0.5 seconds). The project page and code can be found at: https://zhanghe3z.github.io/FLARE/",
      "authors": [
        "Shangzhan Zhang",
        "Jianyuan Wang",
        "Yinghao Xu",
        "Nan Xue",
        "Christian Rupprecht",
        "Xiaowei Zhou",
        "Yujun Shen",
        "Gordon Wetzstein"
      ],
      "published_date": "2025-02-17",
      "pdf_url": "https://arxiv.org/pdf/2502.12138v7",
      "citation_count": 98,
      "year": 2025
    },
    {
      "title": "UniDepthV2: Universal Monocular Metric Depth Estimation Made Simpler",
      "arxiv_id": "",
      "abstract": "Accurate monocular metric depth estimation (MMDE) is crucial to solving downstream tasks in 3D perception and modeling. However, the remarkable accuracy of recent MMDE methods is confined to their training domains. These methods fail to generalize to unseen domains even in the presence of moderate domain gaps, which hinders their practical applicability. We propose a new model, UniDepthV2, capable of reconstructing metric 3D scenes from solely single images across domains. Departing from the existing MMDE paradigm, UniDepthV2 directly predicts metric 3D points from the input image at inference time without any additional information, striving for a universal and flexible MMDE solution. In particular, UniDepthV2 implements a self-promptable camera module predicting a dense camera representation to condition depth features. Our model exploits a pseudo-spherical output representation, which disentangles the camera and depth representations. In addition, we propose a geometric invariance loss that promotes the invariance of camera-prompted depth features. UniDepthV2 improves its predecessor UniDepth model via a new edge-guided loss which enhances the localization and sharpness of edges in the metric depth outputs, a revisited, simplified and more efficient architectural design, and an additional uncertainty-level output which enables downstream tasks requiring confidence. Thorough evaluations on ten depth datasets in a zero-shot regime consistently demonstrate the superior performance and generalization of UniDepthV2. Code and models are available at https://github.com/lpiccinelli-eth/UniDepth",
      "authors": [
        "Luigi Piccinelli",
        "Christos Sakaridis",
        "Yung-Hsu Yang",
        "Mattia Segu",
        "Siyuan Li",
        "Wim Abbeloos",
        "Luc Van Gool"
      ],
      "published_date": "2025-02-27",
      "pdf_url": "https://arxiv.org/pdf/2502.20110v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving",
      "arxiv_id": "",
      "abstract": "Since the advent of Multimodal Large Language Models (MLLMs), they have made a significant impact across a wide range of real-world applications, particularly in Autonomous Driving (AD). Their ability to process complex visual data and reason about intricate driving scenarios has paved the way for a new paradigm in end-to-end AD systems. However, the progress of developing end-to-end models for AD has been slow, as existing fine-tuning methods demand substantial resources, including extensive computational power, large-scale datasets, and significant funding. Drawing inspiration from recent advancements in inference computing, we propose OpenEMMA, an open-source end-to-end framework based on MLLMs. By incorporating the Chain-of-Thought reasoning process, OpenEMMA achieves significant improvements compared to the baseline when leveraging a diverse range of MLLMs. Furthermore, OpenEMMA demonstrates effectiveness, generalizability, and robustness across a variety of challenging driving scenarios, offering a more efficient and effective approach to autonomous driving. We release all the codes in https://github.com/taco-group/OpenEMMA.",
      "authors": [
        "Shuo Xing",
        "Chengyuan Qian",
        "Yuping Wang",
        "Hongyuan Hua",
        "Kexin Tian",
        "Yang Zhou",
        "Zhengzhong Tu"
      ],
      "published_date": "2024-12-19",
      "pdf_url": "https://arxiv.org/pdf/2412.15208v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Adaptive transfer learning for surgical tool presence detection in laparoscopic videos through gradual freezing fine-tuning",
      "arxiv_id": "",
      "abstract": "Minimally invasive surgery can benefit significantly from automated surgical tool detection, enabling advanced analysis and assistance. However, the limited availability of annotated data in surgical settings poses a challenge for training robust deep learning models. This paper introduces a novel staged adaptive fine-tuning approach consisting of two steps: a linear probing stage to condition additional classification layers on a pre-trained CNN-based architecture and a gradual freezing stage to dynamically reduce the fine-tunable layers, aiming to regulate adaptation to the surgical domain. This strategy reduces network complexity and improves efficiency, requiring only a single training loop and eliminating the need for multiple iterations. We validated our method on the Cholec80 dataset, employing CNN architectures (ResNet-50 and DenseNet-121) pre-trained on ImageNet for detecting surgical tools in cholecystectomy endoscopic videos. Our results demonstrate that our method improves detection performance compared to existing approaches and established fine-tuning techniques, achieving a mean average precision (mAP) of 96.4%. To assess its broader applicability, the generalizability of the fine-tuning strategy was further confirmed on the CATARACTS dataset, a distinct domain of minimally invasive ophthalmic surgery. These findings suggest that gradual freezing fine-tuning is a promising technique for improving tool presence detection in diverse surgical procedures and may have broader applications in general image classification tasks.",
      "authors": [
        "Ana Davila",
        "Jacinto Colan",
        "Yasuhisa Hasegawa"
      ],
      "published_date": "2025-10-17",
      "pdf_url": "https://arxiv.org/pdf/2510.15372v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
      "arxiv_id": "",
      "abstract": "Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012---achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.",
      "authors": [
        "Ross Girshick",
        "Jeff Donahue",
        "Trevor Darrell",
        "Jitendra Malik"
      ],
      "published_date": "2013-11-11",
      "pdf_url": "https://arxiv.org/pdf/1311.2524v5",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism",
      "arxiv_id": "",
      "abstract": "Ground Penetrating Radar (GPR) has emerged as a pivotal tool for non-destructive evaluation of subsurface road defects. However, conventional GPR image interpretation remains heavily reliant on subjective expertise, introducing inefficiencies and inaccuracies. This study introduces a comprehensive framework to address these limitations: (1) A DCGAN-based data augmentation strategy synthesizes high-fidelity GPR images to mitigate data scarcity while preserving defect morphology under complex backgrounds; (2) A novel Multi-modal Chain and Global Attention Network (MCGA-Net) is proposed, integrating Multi-modal Chain Feature Fusion (MCFF) for hierarchical multi-scale defect representation and Global Attention Mechanism (GAM) for context-aware feature enhancement; (3) MS COCO transfer learning fine-tunes the backbone network, accelerating convergence and improving generalization. Ablation and comparison experiments validate the framework's efficacy. MCGA-Net achieves Precision (92.8%), Recall (92.5%), and mAP@50 (95.9%). In the detection of Gaussian noise, weak signals and small targets, MCGA-Net maintains robustness and outperforms other models. This work establishes a new paradigm for automated GPR-based defect detection, balancing computational efficiency with high accuracy in complex subsurface environments.",
      "authors": [
        "Haotian Lv",
        "Yuhui Zhang",
        "Jiangbo Dai",
        "Hanli Wu",
        "Jiaji Wang",
        "Dawei Wang"
      ],
      "published_date": "2025-12-25",
      "pdf_url": "https://arxiv.org/pdf/2512.21452v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Evolutionary Optimization of Model Merging Recipes",
      "arxiv_id": "",
      "abstract": "Large language models (LLMs) have become increasingly capable, but their development often requires substantial computational resources. While model merging has emerged as a cost-effective promising approach for creating new models by combining existing ones, it currently relies on human intuition and domain knowledge, limiting its potential. Here, we propose an evolutionary approach that overcomes this limitation by automatically discovering effective combinations of diverse open-source models, harnessing their collective intelligence without requiring extensive additional training data or compute. Our approach operates in both parameter space and data flow space, allowing for optimization beyond just the weights of the individual models. This approach even facilitates cross-domain merging, generating models like a Japanese LLM with Math reasoning capabilities. Surprisingly, our Japanese Math LLM achieved state-of-the-art performance on a variety of established Japanese LLM benchmarks, even surpassing models with significantly more parameters, despite not being explicitly trained for such tasks. Furthermore, a culturally-aware Japanese VLM generated through our approach demonstrates its effectiveness in describing Japanese culture-specific content, outperforming previous Japanese VLMs. This work not only contributes new state-of-the-art models back to the open-source community, but also introduces a new paradigm for automated model composition, paving the way for exploring alternative, efficient approaches to foundation model development.",
      "authors": [
        "Takuya Akiba",
        "Makoto Shing",
        "Yujin Tang",
        "Qi Sun",
        "David Ha"
      ],
      "published_date": "2024-03-19",
      "pdf_url": "https://arxiv.org/pdf/2403.13187v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
      "arxiv_id": "",
      "abstract": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
      "authors": [
        "Kyunghyun Cho",
        "Bart van Merrienboer",
        "Caglar Gulcehre",
        "Dzmitry Bahdanau",
        "Fethi Bougares",
        "Holger Schwenk",
        "Yoshua Bengio"
      ],
      "published_date": "2014-06-03",
      "pdf_url": "https://arxiv.org/pdf/1406.1078v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving",
      "arxiv_id": "",
      "abstract": "Large language models (LLMs) have opened up new possibilities for intelligent agents, endowing them with human-like thinking and cognitive abilities. In this work, we delve into the potential of large language models (LLMs) in autonomous driving (AD). We introduce DriveMLM, an LLM-based AD framework that can perform close-loop autonomous driving in realistic simulators. To this end, (1) we bridge the gap between the language decisions and the vehicle control commands by standardizing the decision states according to the off-the-shelf motion planning module. (2) We employ a multimodal LLM (MLLM) to model the behavior planning module of a module AD system, which uses driving rules, user commands, and inputs from various sensors (e.g., camera, lidar) as input and makes driving decisions and provide explanations; This model can plug-and-play in existing AD systems such as Autopilot and Apollo for close-loop driving. (3) We design an effective data engine to collect a dataset that includes decision state and corresponding explanation annotation for model training and evaluation. We conduct extensive experiments and show that replacing the decision-making modules of the Autopilot and Apollo with DriveMLM resulted in significant improvements of 3.2 and 4.7 points on the CARLA Town05 Long respectively, demonstrating the effectiveness of our model. We hope this work can serve as a baseline for autonomous driving with LLMs.",
      "authors": [
        "Erfei Cui",
        "Wenhai Wang",
        "Zhiqi Li",
        "Jiangwei Xie",
        "Haoming Zou",
        "Hanming Deng",
        "Gen Luo",
        "Lewei Lu",
        "Xizhou Zhu",
        "Jifeng Dai"
      ],
      "published_date": "2023-12-14",
      "pdf_url": "https://arxiv.org/pdf/2312.09245v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models",
      "arxiv_id": "",
      "abstract": "We present VLMEvalKit: an open-source toolkit for evaluating large multi-modality models based on PyTorch. The toolkit aims to provide a user-friendly and comprehensive framework for researchers and developers to evaluate existing multi-modality models and publish reproducible evaluation results. In VLMEvalKit, we implement over 200+ different large multi-modality models, including both proprietary APIs and open-source models, as well as more than 80 different multi-modal benchmarks. By implementing a single interface, new models can be easily added to the toolkit, while the toolkit automatically handles the remaining workloads, including data preparation, distributed inference, prediction post-processing, and metric calculation. Although the toolkit is currently mainly used for evaluating large vision-language models, its design is compatible with future updates that incorporate additional modalities, such as audio and video. Based on the evaluation results obtained with the toolkit, we host OpenVLM Leaderboard, a comprehensive leaderboard to track the progress of multi-modality learning research. The toolkit is released on https://github.com/open-compass/VLMEvalKit and is actively maintained.",
      "authors": [
        "Haodong Duan",
        "Xinyu Fang",
        "Junming Yang",
        "Xiangyu Zhao",
        "Yuxuan Qiao",
        "Mo Li",
        "Amit Agarwal",
        "Zhe Chen",
        "Lin Chen",
        "Yuan Liu",
        "Yubo Ma",
        "Hailong Sun",
        "Yifan Zhang",
        "Shiyin Lu",
        "Tack Hwa Wong",
        "Weiyun Wang",
        "Peiheng Zhou",
        "Xiaozhe Li",
        "Chaoyou Fu",
        "Junbo Cui",
        "Jixuan Chen",
        "Enxin Song",
        "Song Mao",
        "Shengyuan Ding",
        "Tianhao Liang",
        "Zicheng Zhang",
        "Xiaoyi Dong",
        "Yuhang Zang",
        "Pan Zhang",
        "Jiaqi Wang",
        "Dahua Lin",
        "Kai Chen"
      ],
      "published_date": "2024-07-16",
      "pdf_url": "https://arxiv.org/pdf/2407.11691v4",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "arxiv_id": "",
      "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
      "authors": [
        "Jason Wei",
        "Xuezhi Wang",
        "Dale Schuurmans",
        "Maarten Bosma",
        "Brian Ichter",
        "Fei Xia",
        "Ed Chi",
        "Quoc Le",
        "Denny Zhou"
      ],
      "published_date": "2022-01-28",
      "pdf_url": "https://arxiv.org/pdf/2201.11903v6",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Federated Learning Architectures: A Performance Evaluation with Crop Yield Prediction Application",
      "arxiv_id": "",
      "abstract": "Federated learning has become an emerging technology for data analysis for IoT applications. This paper implements centralized and decentralized federated learning frameworks for crop yield prediction based on Long Short-Term Memory Network. For centralized federated learning, multiple clients and one server is considered, where the clients exchange their model updates with the server that works as the aggregator to build the global model. For the decentralized framework, a collaborative network is formed among the devices either using ring topology or using mesh topology. In this network, each device receives model updates from the neighbour devices, and performs aggregation to build the upgraded model. The performance of the centralized and decentralized federated learning frameworks are evaluated in terms of prediction accuracy, precision, recall, F1-Score, and training time. The experimental results present that $\\geq$97% and $>$97.5% prediction accuracy are achieved using the centralized and decentralized federated learning-based frameworks respectively. The results also show that the using centralized federated learning the response time can be reduced by $\\sim$75% than the cloud-only framework. Finally, the future research directions of the use of federated learning in crop yield prediction are explored in this paper.",
      "authors": [
        "Anwesha Mukherjee",
        "Rajkumar Buyya"
      ],
      "published_date": "2024-08-06",
      "pdf_url": "https://arxiv.org/pdf/2408.02998v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Enhancing Descriptive Image Quality Assessment with A Large-scale Multi-modal Dataset",
      "arxiv_id": "",
      "abstract": "With the rapid advancement of Vision Language Models (VLMs), VLM-based Image Quality Assessment (IQA) seeks to describe image quality linguistically to align with human expression and capture the multifaceted nature of IQA tasks. However, current methods are still far from practical usage. First, prior works focus narrowly on specific sub-tasks or settings, which do not align with diverse real-world applications. Second, their performance is sub-optimal due to limitations in dataset coverage, scale, and quality. To overcome these challenges, we introduce the enhanced Depicted image Quality Assessment model (DepictQA-Wild). Our method includes a multi-functional IQA task paradigm that encompasses both assessment and comparison tasks, brief and detailed responses, full-reference and non-reference scenarios. We introduce a ground-truth-informed dataset construction approach to enhance data quality, and scale up the dataset to 495K under the brief-detail joint framework. Consequently, we construct a comprehensive, large-scale, and high-quality dataset, named DQ-495K. We also retain image resolution during training to better handle resolution-related quality issues, and estimate a confidence score that is helpful to filter out low-quality responses. Experimental results demonstrate that DepictQA-Wild significantly outperforms traditional score-based methods, prior VLM-based IQA models, and proprietary GPT-4V in distortion identification, instant rating, and reasoning tasks. Our advantages are further confirmed by real-world applications including assessing the web-downloaded images and ranking model-processed images. Codes, datasets, and model weights have been released in https://depictqa.github.io/.",
      "authors": [
        "Zhiyuan You",
        "Jinjin Gu",
        "Xin Cai",
        "Zheyuan Li",
        "Kaiwen Zhu",
        "Chao Dong",
        "Tianfan Xue"
      ],
      "published_date": "2024-05-29",
      "pdf_url": "https://arxiv.org/pdf/2405.18842v3",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Deep contextualized word representations",
      "arxiv_id": "",
      "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
      "authors": [
        "Matthew E. Peters",
        "Mark Neumann",
        "Mohit Iyyer",
        "Matt Gardner",
        "Christopher Clark",
        "Kenton Lee",
        "Luke Zettlemoyer"
      ],
      "published_date": "2018-02-15",
      "pdf_url": "https://arxiv.org/pdf/1802.05365v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
      "arxiv_id": "",
      "abstract": "General reasoning represents a long-standing and formidable challenge in artificial intelligence. Recent breakthroughs, exemplified by large language models (LLMs) and chain-of-thought prompting, have achieved considerable success on foundational reasoning tasks. However, this success is heavily contingent upon extensive human-annotated demonstrations, and models' capabilities are still insufficient for more complex problems. Here we show that the reasoning abilities of LLMs can be incentivized through pure reinforcement learning (RL), obviating the need for human-labeled reasoning trajectories. The proposed RL framework facilitates the emergent development of advanced reasoning patterns, such as self-reflection, verification, and dynamic strategy adaptation. Consequently, the trained model achieves superior performance on verifiable tasks such as mathematics, coding competitions, and STEM fields, surpassing its counterparts trained via conventional supervised learning on human demonstrations. Moreover, the emergent reasoning patterns exhibited by these large-scale models can be systematically harnessed to guide and enhance the reasoning capabilities of smaller models.",
      "authors": [
        "DeepSeek-AI",
        "Daya Guo",
        "Dejian Yang",
        "Haowei Zhang",
        "Junxiao Song",
        "Peiyi Wang",
        "Qihao Zhu",
        "Runxin Xu",
        "Ruoyu Zhang",
        "Shirong Ma",
        "Xiao Bi",
        "Xiaokang Zhang",
        "Xingkai Yu",
        "Yu Wu",
        "Z. F. Wu",
        "Zhibin Gou",
        "Zhihong Shao",
        "Zhuoshu Li",
        "Ziyi Gao",
        "Aixin Liu",
        "Bing Xue",
        "Bingxuan Wang",
        "Bochao Wu",
        "Bei Feng",
        "Chengda Lu",
        "Chenggang Zhao",
        "Chengqi Deng",
        "Chenyu Zhang",
        "Chong Ruan",
        "Damai Dai",
        "Deli Chen",
        "Dongjie Ji",
        "Erhang Li",
        "Fangyun Lin",
        "Fucong Dai",
        "Fuli Luo",
        "Guangbo Hao",
        "Guanting Chen",
        "Guowei Li",
        "H. Zhang",
        "Han Bao",
        "Hanwei Xu",
        "Haocheng Wang",
        "Honghui Ding",
        "Huajian Xin",
        "Huazuo Gao",
        "Hui Qu",
        "Hui Li",
        "Jianzhong Guo",
        "Jiashi Li",
        "Jiawei Wang",
        "Jingchang Chen",
        "Jingyang Yuan",
        "Junjie Qiu",
        "Junlong Li",
        "J. L. Cai",
        "Jiaqi Ni",
        "Jian Liang",
        "Jin Chen",
        "Kai Dong",
        "Kai Hu",
        "Kaige Gao",
        "Kang Guan",
        "Kexin Huang",
        "Kuai Yu",
        "Lean Wang",
        "Lecong Zhang",
        "Liang Zhao",
        "Litong Wang",
        "Liyue Zhang",
        "Lei Xu",
        "Leyi Xia",
        "Mingchuan Zhang",
        "Minghua Zhang",
        "Minghui Tang",
        "Meng Li",
        "Miaojun Wang",
        "Mingming Li",
        "Ning Tian",
        "Panpan Huang",
        "Peng Zhang",
        "Qiancheng Wang",
        "Qinyu Chen",
        "Qiushi Du",
        "Ruiqi Ge",
        "Ruisong Zhang",
        "Ruizhe Pan",
        "Runji Wang",
        "R. J. Chen",
        "R. L. Jin",
        "Ruyi Chen",
        "Shanghao Lu",
        "Shangyan Zhou",
        "Shanhuang Chen",
        "Shengfeng Ye",
        "Shiyu Wang",
        "Shuiping Yu",
        "Shunfeng Zhou",
        "Shuting Pan",
        "S. S. Li",
        "Shuang Zhou",
        "Shaoqing Wu",
        "Shengfeng Ye",
        "Tao Yun",
        "Tian Pei",
        "Tianyu Sun",
        "T. Wang",
        "Wangding Zeng",
        "Wanjia Zhao",
        "Wen Liu",
        "Wenfeng Liang",
        "Wenjun Gao",
        "Wenqin Yu",
        "Wentao Zhang",
        "W. L. Xiao",
        "Wei An",
        "Xiaodong Liu",
        "Xiaohan Wang",
        "Xiaokang Chen",
        "Xiaotao Nie",
        "Xin Cheng",
        "Xin Liu",
        "Xin Xie",
        "Xingchao Liu",
        "Xinyu Yang",
        "Xinyuan Li",
        "Xuecheng Su",
        "Xuheng Lin",
        "X. Q. Li",
        "Xiangyue Jin",
        "Xiaojin Shen",
        "Xiaosha Chen",
        "Xiaowen Sun",
        "Xiaoxiang Wang",
        "Xinnan Song",
        "Xinyi Zhou",
        "Xianzu Wang",
        "Xinxia Shan",
        "Y. K. Li",
        "Y. Q. Wang",
        "Y. X. Wei",
        "Yang Zhang",
        "Yanhong Xu",
        "Yao Li",
        "Yao Zhao",
        "Yaofeng Sun",
        "Yaohui Wang",
        "Yi Yu",
        "Yichao Zhang",
        "Yifan Shi",
        "Yiliang Xiong",
        "Ying He",
        "Yishi Piao",
        "Yisong Wang",
        "Yixuan Tan",
        "Yiyang Ma",
        "Yiyuan Liu",
        "Yongqiang Guo",
        "Yuan Ou",
        "Yuduan Wang",
        "Yue Gong",
        "Yuheng Zou",
        "Yujia He",
        "Yunfan Xiong",
        "Yuxiang Luo",
        "Yuxiang You",
        "Yuxuan Liu",
        "Yuyang Zhou",
        "Y. X. Zhu",
        "Yanhong Xu",
        "Yanping Huang",
        "Yaohui Li",
        "Yi Zheng",
        "Yuchen Zhu",
        "Yunxian Ma",
        "Ying Tang",
        "Yukun Zha",
        "Yuting Yan",
        "Z. Z. Ren",
        "Zehui Ren",
        "Zhangli Sha",
        "Zhe Fu",
        "Zhean Xu",
        "Zhenda Xie",
        "Zhengyan Zhang",
        "Zhewen Hao",
        "Zhicheng Ma",
        "Zhigang Yan",
        "Zhiyu Wu",
        "Zihui Gu",
        "Zijia Zhu",
        "Zijun Liu",
        "Zilin Li",
        "Ziwei Xie",
        "Ziyang Song",
        "Zizheng Pan",
        "Zhen Huang",
        "Zhipeng Xu",
        "Zhongyu Zhang",
        "Zhen Zhang"
      ],
      "published_date": "2025-01-22",
      "pdf_url": "https://arxiv.org/pdf/2501.12948v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Fully Convolutional Networks for Semantic Segmentation",
      "arxiv_id": "",
      "abstract": "Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build \"fully convolutional\" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.",
      "authors": [
        "Jonathan Long",
        "Evan Shelhamer",
        "Trevor Darrell"
      ],
      "published_date": "2014-11-14",
      "pdf_url": "https://arxiv.org/pdf/1411.4038v2",
      "citation_count": null,
      "year": null
    },
    {
      "title": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with Transformers",
      "arxiv_id": "",
      "abstract": "Scene understanding based on image segmentation is a crucial component of autonomous vehicles. Pixel-wise semantic segmentation of RGB images can be advanced by exploiting complementary features from the supplementary modality (X-modality). However, covering a wide variety of sensors with a modality-agnostic model remains an unresolved problem due to variations in sensor characteristics among different modalities. Unlike previous modality-specific methods, in this work, we propose a unified fusion framework, CMX, for RGB-X semantic segmentation. To generalize well across different modalities, that often include supplements as well as uncertainties, a unified cross-modal interaction is crucial for modality fusion. Specifically, we design a Cross-Modal Feature Rectification Module (CM-FRM) to calibrate bi-modal features by leveraging the features from one modality to rectify the features of the other modality. With rectified feature pairs, we deploy a Feature Fusion Module (FFM) to perform sufficient exchange of long-range contexts before mixing. To verify CMX, for the first time, we unify five modalities complementary to RGB, i.e., depth, thermal, polarization, event, and LiDAR. Extensive experiments show that CMX generalizes well to diverse multi-modal fusion, achieving state-of-the-art performances on five RGB-Depth benchmarks, as well as RGB-Thermal, RGB-Polarization, and RGB-LiDAR datasets. Besides, to investigate the generalizability to dense-sparse data fusion, we establish an RGB-Event semantic segmentation benchmark based on the EventScape dataset, on which CMX sets the new state-of-the-art. The source code of CMX is publicly available at https://github.com/huaaaliu/RGBX_Semantic_Segmentation.",
      "authors": [
        "Jiaming Zhang",
        "Huayao Liu",
        "Kailun Yang",
        "Xinxin Hu",
        "Ruiping Liu",
        "Rainer Stiefelhagen"
      ],
      "published_date": "2022-03-09",
      "pdf_url": "https://arxiv.org/pdf/2203.04838v5",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion",
      "arxiv_id": "",
      "abstract": "This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot's visuomotor policy as a conditional denoising diffusion process. We benchmark Diffusion Policy across 12 different tasks from 4 different robot manipulation benchmarks and find that it consistently outperforms existing state-of-the-art robot learning methods with an average improvement of 46.9%. Diffusion Policy learns the gradient of the action-distribution score function and iteratively optimizes with respect to this gradient field during inference via a series of stochastic Langevin dynamics steps. We find that the diffusion formulation yields powerful advantages when used for robot policies, including gracefully handling multimodal action distributions, being suitable for high-dimensional action spaces, and exhibiting impressive training stability. To fully unlock the potential of diffusion models for visuomotor policy learning on physical robots, this paper presents a set of key technical contributions including the incorporation of receding horizon control, visual conditioning, and the time-series diffusion transformer. We hope this work will help motivate a new generation of policy learning techniques that are able to leverage the powerful generative modeling capabilities of diffusion models. Code, data, and training details is publicly available diffusion-policy.cs.columbia.edu",
      "authors": [
        "Cheng Chi",
        "Zhenjia Xu",
        "Siyuan Feng",
        "Eric Cousineau",
        "Yilun Du",
        "Benjamin Burchfiel",
        "Russ Tedrake",
        "Shuran Song"
      ],
      "published_date": "2023-03-07",
      "pdf_url": "https://arxiv.org/pdf/2303.04137v5",
      "citation_count": null,
      "year": null
    },
    {
      "title": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI",
      "arxiv_id": "",
      "abstract": "Embodied Artificial Intelligence (Embodied AI) is crucial for achieving Artificial General Intelligence (AGI) and serves as a foundation for various applications (e.g., intelligent mechatronics systems, smart manufacturing) that bridge cyberspace and the physical world. Recently, the emergence of Multi-modal Large Models (MLMs) and World Models (WMs) have attracted significant attention due to their remarkable perception, interaction, and reasoning capabilities, making them a promising architecture for embodied agents. In this survey, we give a comprehensive exploration of the latest advancements in Embodied AI. Our analysis firstly navigates through the forefront of representative works of embodied robots and simulators, to fully understand the research focuses and their limitations. Then, we analyze four main research targets: 1) embodied perception, 2) embodied interaction, 3) embodied agent, and 4) sim-to-real adaptation, covering state-of-the-art methods, essential paradigms, and comprehensive datasets. Additionally, we explore the complexities of MLMs in virtual and real embodied agents, highlighting their significance in facilitating interactions in digital and physical environments. Finally, we summarize the challenges and limitations of embodied AI and discuss potential future directions. We hope this survey will serve as a foundational reference for the research community. The associated project can be found at https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List.",
      "authors": [
        "Yang Liu",
        "Weixing Chen",
        "Yongjie Bai",
        "Xiaodan Liang",
        "Guanbin Li",
        "Wen Gao",
        "Liang Lin"
      ],
      "published_date": "2024-07-09",
      "pdf_url": "https://arxiv.org/pdf/2407.06886v8",
      "citation_count": null,
      "year": null
    },
    {
      "title": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control",
      "arxiv_id": "",
      "abstract": "We present GEN3C, a generative video model with precise Camera Control and temporal 3D Consistency. Prior video models already generate realistic videos, but they tend to leverage little 3D information, leading to inconsistencies, such as objects popping in and out of existence. Camera control, if implemented at all, is imprecise, because camera parameters are mere inputs to the neural network which must then infer how the video depends on the camera. In contrast, GEN3C is guided by a 3D cache: point clouds obtained by predicting the pixel-wise depth of seed images or previously generated frames. When generating the next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with the new camera trajectory provided by the user. Crucially, this means that GEN3C neither has to remember what it previously generated nor does it have to infer the image structure from the camera pose. The model, instead, can focus all its generative power on previously unobserved regions, as well as advancing the scene state to the next frame. Our results demonstrate more precise camera control than prior work, as well as state-of-the-art results in sparse-view novel view synthesis, even in challenging settings such as driving scenes and monocular dynamic video. Results are best viewed in videos. Check out our webpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/",
      "authors": [
        "Xuanchi Ren",
        "Tianchang Shen",
        "Jiahui Huang",
        "Huan Ling",
        "Yifan Lu",
        "Merlin Nimier-David",
        "Thomas Müller",
        "Alexander Keller",
        "Sanja Fidler",
        "Jun Gao"
      ],
      "published_date": "2025-03-05",
      "pdf_url": "https://arxiv.org/pdf/2503.03751v1",
      "citation_count": null,
      "year": null
    },
    {
      "title": "FLARE: Feed-forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views",
      "arxiv_id": "",
      "abstract": "We present FLARE, a feed-forward model designed to infer high-quality camera poses and 3D geometry from uncalibrated sparse-view images (i.e., as few as 2-8 inputs), which is a challenging yet practical setting in real-world applications. Our solution features a cascaded learning paradigm with camera pose serving as the critical bridge, recognizing its essential role in mapping 3D structures onto 2D image planes. Concretely, FLARE starts with camera pose estimation, whose results condition the subsequent learning of geometric structure and appearance, optimized through the objectives of geometry reconstruction and novel-view synthesis. Utilizing large-scale public datasets for training, our method delivers state-of-the-art performance in the tasks of pose estimation, geometry reconstruction, and novel view synthesis, while maintaining the inference efficiency (i.e., less than 0.5 seconds). The project page and code can be found at: https://zhanghe3z.github.io/FLARE/",
      "authors": [
        "Shangzhan Zhang",
        "Jianyuan Wang",
        "Yinghao Xu",
        "Nan Xue",
        "Christian Rupprecht",
        "Xiaowei Zhou",
        "Yujun Shen",
        "Gordon Wetzstein"
      ],
      "published_date": "2025-02-17",
      "pdf_url": "https://arxiv.org/pdf/2502.12138v7",
      "citation_count": null,
      "year": null
    }
  ],
  "top_k": 5,
  "depth": 4,
  "knowledge_graph": {
    "entities": [
      {
        "name": "Attention Is All You Need",
        "type": "AIPaper",
        "arxiv_id": "1706.03762"
      },
      {
        "name": "Deep Residual Learning for Image Recognition",
        "type": "AIPaper",
        "arxiv_id": "1512.03385"
      },
      {
        "name": "Adam: A Method for Stochastic Optimization",
        "type": "AIPaper",
        "arxiv_id": "1412.6980"
      },
      {
        "name": "Long Short-Term Memory",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Dropout: a simple way to prevent neural networks from overfitting",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Rethinking the Inception Architecture for Computer Vision",
        "type": "AIPaper",
        "arxiv_id": "1512.00567"
      },
      {
        "name": "A comprehensive review of recommender systems: Transitioning from theory to practice",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
        "type": "AIPaper",
        "arxiv_id": "2602.03242"
      },
      {
        "name": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "DiffusionEngine: Diffusion model is scalable data engine for object detection",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "ImageNet classification with deep convolutional neural networks",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
        "type": "AIPaper",
        "arxiv_id": "1409.1556"
      },
      {
        "name": "Et al",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
        "type": "AIPaper",
        "arxiv_id": "1506.01497"
      },
      {
        "name": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Noise Perception Self-Supervised Learning for Unsupervised Person Re-Identification",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Auto-Encoding Variational Bayes",
        "type": "AIPaper",
        "arxiv_id": "1312.6114"
      },
      {
        "name": "Supporting Online Material for Reducing the Dimensionality of Data with Neural Networks",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Speech recognition with deep recurrent neural networks",
        "type": "AIPaper",
        "arxiv_id": "1303.5778"
      },
      {
        "name": "Weakly Supervised Image Dehazing via Physics-Based Decomposition",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
        "type": "AIPaper",
        "arxiv_id": "2601.07372"
      },
      {
        "name": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Going deeper with convolutions",
        "type": "AIPaper",
        "arxiv_id": "1409.4842"
      },
      {
        "name": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
        "type": "AIPaper",
        "arxiv_id": "1502.03167"
      },
      {
        "name": "ImageNet Large Scale Visual Recognition Challenge",
        "type": "AIPaper",
        "arxiv_id": "1409.0575"
      },
      {
        "name": "Diffusion Transformers with Representation Autoencoders",
        "type": "AIPaper",
        "arxiv_id": "2510.11690"
      },
      {
        "name": "Scaling Large Language Models for Next-Generation Single-Cell Analysis",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "HybridVisionNet: An advanced hybrid deep learning framework for automated multi-class ocular disease diagnosis using fundus imaging",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Attention is All you Need",
        "type": "AIPaper",
        "arxiv_id": "1706.03762"
      },
      {
        "name": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "type": "AIPaper",
        "arxiv_id": "1910.10683"
      },
      {
        "name": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "nuScenes: A Multimodal Dataset for Autonomous Driving",
        "type": "AIPaper",
        "arxiv_id": "1903.11027"
      },
      {
        "name": "CARLA: An Open Urban Driving Simulator",
        "type": "AIPaper",
        "arxiv_id": "1711.03938"
      },
      {
        "name": "Sora as a World Model? A Complete Survey on Text-to-Video Generation",
        "type": "AIPaper",
        "arxiv_id": "2403.05131"
      },
      {
        "name": "OmniNWM: Omniscient Driving Navigation World Models",
        "type": "AIPaper",
        "arxiv_id": "2510.18313"
      },
      {
        "name": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask",
        "type": "AIPaper",
        "arxiv_id": "2602.03213"
      },
      {
        "name": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving",
        "type": "AIPaper",
        "arxiv_id": "2602.02002"
      },
      {
        "name": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
        "type": "AIPaper",
        "arxiv_id": "2601.09452"
      },
      {
        "name": "I and J",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "ImageNet: A large-scale hierarchical image database",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "A and V",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "What matters for Representation Alignment: Global Information or Spatial Structure?",
        "type": "AIPaper",
        "arxiv_id": "2512.10794"
      },
      {
        "name": "SCB-DETR: Multiscale Deformable Transformers for Occlusion-Resilient Student Learning Behavior Detection in Smart Classroom",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Bio-Inspired Fine-Tuning for Selective Transfer Learning in Image Classification",
        "type": "AIPaper",
        "arxiv_id": "2601.11235"
      },
      {
        "name": "Hand Sign Language Detection Using Deep Learning",
        "type": "AIPaper",
        "arxiv_id": "2601.08262"
      },
      {
        "name": "and as an in",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Microsoft COCO: Common Objects in Context",
        "type": "AIPaper",
        "arxiv_id": "1405.0312"
      },
      {
        "name": "FANet: Frequency-Aware Attention-Based Tiny-Object Detection in Remote Sensing Images",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "UAV-based multimodal object detection via feature enhancement and dynamic gated fusion",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "An empirical analysis of deep learning methods for small object detection from satellite imagery",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Representation Learning: A Review and New Perspectives",
        "type": "AIPaper",
        "arxiv_id": "1206.5538"
      },
      {
        "name": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "In Advances in Neural Information Processing Systems",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation",
        "type": "AIPaper",
        "arxiv_id": "2402.13929"
      },
      {
        "name": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions",
        "type": "AIPaper",
        "arxiv_id": "2407.08136"
      },
      {
        "name": "GenAD: Generative End-to-End Autonomous Driving",
        "type": "AIPaper",
        "arxiv_id": "2402.11502"
      },
      {
        "name": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model",
        "type": "AIPaper",
        "arxiv_id": "2405.20797"
      },
      {
        "name": "Improving Video Generation with Human Feedback",
        "type": "AIPaper",
        "arxiv_id": "2501.13918"
      },
      {
        "name": "Speech Recognition with Deep Recurrent Neural Networks",
        "type": "AIPaper",
        "arxiv_id": "1303.5778"
      },
      {
        "name": "Learning representations by back-propagating errors",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Bidirectional recurrent neural networks",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Connectionist Temporal Classiﬁcation: Labelling Unsegmented Sequence Data with Recurrent Neural Networks",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "2005 Special Issue: Framewise phoneme classification with bidirectional LSTM and other neural network architectures",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "A high-performance neuroprosthesis for speech decoding and avatar control",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "A high-performance speech neuroprosthesis",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Loss of plasticity in deep continual learning",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "An analog-AI chip for energy-efficient speech recognition and transcription",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "A Mathematical Theory of Communication",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers",
        "type": "AIPaper",
        "arxiv_id": "2602.04706"
      },
      {
        "name": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling",
        "type": "AIPaper",
        "arxiv_id": "2602.03359"
      },
      {
        "name": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram",
        "type": "AIPaper",
        "arxiv_id": "2601.22203"
      },
      {
        "name": "L$^3$: Large Lookup Layers",
        "type": "AIPaper",
        "arxiv_id": "2601.21461"
      },
      {
        "name": "Scaling Embeddings Outperforms Scaling Experts in Language Models",
        "type": "AIPaper",
        "arxiv_id": "2601.21204"
      },
      {
        "name": "Going Deeper with Convolutions",
        "type": "AIPaper",
        "arxiv_id": "1409.4842"
      },
      {
        "name": "Gradient-based learning applied to document recognition",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Regression Shrinkage and Selection via the Lasso",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "LifeCLEF Plant Identification Task 2015",
        "type": "AIPaper",
        "arxiv_id": "2509.23891"
      },
      {
        "name": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics",
        "type": "AIPaper",
        "arxiv_id": "2510.20852"
      },
      {
        "name": "Enhancing pine wilt disease detection with synthetic data and external attention-based transformers",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "A comprehensive review on YOLO versions for object detection",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Harnessing large vision and language models in agriculture: a review",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Multi-axis vision transformer for medical image segmentation",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "A comprehensive review of facial beauty prediction using deep learning techniques",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "A systematic comparison of predictive models on the retina",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Distinctive Image Features from Scale-Invariant Keypoints",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training",
        "type": "AIPaper",
        "arxiv_id": "2509.23661"
      },
      {
        "name": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications",
        "type": "AIPaper",
        "arxiv_id": "2510.07077"
      },
      {
        "name": "Aligning machine and human visual representations across abstraction levels",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
        "type": "AIPaper",
        "arxiv_id": "2010.11929"
      },
      {
        "name": "Learning Transferable Visual Models From Natural Language Supervision",
        "type": "AIPaper",
        "arxiv_id": "2103.00020"
      },
      {
        "name": "GENERATIVE ADVERSARIAL NETS",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Improved Mean Flows: On the Challenges of Fastforward Generative Models",
        "type": "AIPaper",
        "arxiv_id": "2512.02012"
      },
      {
        "name": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training",
        "type": "AIPaper",
        "arxiv_id": "2510.12586"
      },
      {
        "name": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
        "type": "AIPaper",
        "arxiv_id": "2512.11749"
      },
      {
        "name": "PixelDiT: Pixel Diffusion Transformers for Image Generation",
        "type": "AIPaper",
        "arxiv_id": "2511.20645"
      },
      {
        "name": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models",
        "type": "AIPaper",
        "arxiv_id": "2512.02014"
      },
      {
        "name": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "type": "AIPaper",
        "arxiv_id": "1810.04805"
      },
      {
        "name": "A comprehensive review of object detection with traditional and deep learning methods",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Diffusion Language Models are Super Data Learners",
        "type": "AIPaper",
        "arxiv_id": "2511.03276"
      },
      {
        "name": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas",
        "type": "AIPaper",
        "arxiv_id": "2602.01326"
      },
      {
        "name": "FLEX: Continuous Agent Evolution via Forward Learning from Experience",
        "type": "AIPaper",
        "arxiv_id": "2511.06449"
      },
      {
        "name": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning",
        "type": "AIPaper",
        "arxiv_id": "2511.14460"
      },
      {
        "name": "nuScenes: A multimodal dataset for autonomous driving",
        "type": "AIPaper",
        "arxiv_id": "1903.11027"
      },
      {
        "name": "Histograms of oriented gradients for human detection",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Controllable Video Generation: A Survey",
        "type": "AIPaper",
        "arxiv_id": "2507.16869"
      },
      {
        "name": "Adversarial Attacks on Autonomous Driving Systems in the Physical World: A Survey",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail",
        "type": "AIPaper",
        "arxiv_id": "2511.00088"
      },
      {
        "name": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes",
        "type": "AIPaper",
        "arxiv_id": "2509.06266"
      },
      {
        "name": "Detect Anything via Next Point Prediction",
        "type": "AIPaper",
        "arxiv_id": "2510.12798"
      },
      {
        "name": "Human-level control through deep reinforcement learning",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Asynchronous Methods for Deep Reinforcement Learning",
        "type": "AIPaper",
        "arxiv_id": "1602.01783"
      },
      {
        "name": "RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation",
        "type": "AIPaper",
        "arxiv_id": "1611.06612"
      },
      {
        "name": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving",
        "type": "AIPaper",
        "arxiv_id": "2505.00284"
      },
      {
        "name": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving",
        "type": "AIPaper",
        "arxiv_id": "2506.08052"
      },
      {
        "name": "Pseudo-Simulation for Autonomous Driving",
        "type": "AIPaper",
        "arxiv_id": "2506.04218"
      },
      {
        "name": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving",
        "type": "AIPaper",
        "arxiv_id": "2505.16278"
      },
      {
        "name": "A Survey on Vision-Language-Action Models for Autonomous Driving",
        "type": "AIPaper",
        "arxiv_id": "2506.24044"
      },
      {
        "name": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond",
        "type": "AIPaper",
        "arxiv_id": "2405.03520"
      },
      {
        "name": "Understanding World or Predicting Future? A Comprehensive Survey of World Models",
        "type": "AIPaper",
        "arxiv_id": "2411.14499"
      },
      {
        "name": "A Survey of Multimodal Learning: Methods, Applications, and Future",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey",
        "type": "AIPaper",
        "arxiv_id": "2411.09259"
      },
      {
        "name": "Effects of Generative AI in Tourism Industry",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Squeeze-and-Excitation Networks",
        "type": "AIPaper",
        "arxiv_id": "1709.01507"
      },
      {
        "name": "Decoupled Weight Decay Regularization",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
        "type": "AIPaper",
        "arxiv_id": "1905.11946"
      },
      {
        "name": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control",
        "type": "AIPaper",
        "arxiv_id": "2512.05115"
      },
      {
        "name": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World",
        "type": "AIPaper",
        "arxiv_id": "2512.23421"
      },
      {
        "name": "DVGT: Driving Visual Geometry Transformer",
        "type": "AIPaper",
        "arxiv_id": "2512.16919"
      },
      {
        "name": "Adding Conditional Control to Text-to-Image Diffusion Models",
        "type": "AIPaper",
        "arxiv_id": "2302.05543"
      },
      {
        "name": "Scaling Instruction-Finetuned Language Models",
        "type": "AIPaper",
        "arxiv_id": "2210.11416"
      },
      {
        "name": "High-Resolution Image Synthesis with Latent Diffusion Models",
        "type": "AIPaper",
        "arxiv_id": "2112.10752"
      },
      {
        "name": "AUTO-ENCODING VARIATIONAL BAYES",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
        "type": "AIPaper",
        "arxiv_id": "1801.03924"
      },
      {
        "name": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset",
        "type": "AIPaper",
        "arxiv_id": "1912.04838"
      },
      {
        "name": "Generative Adversarial Networks",
        "type": "AIPaper",
        "arxiv_id": "2203.00667"
      },
      {
        "name": "Generalization of Diffusion Models Arises with a Balanced Representation Space",
        "type": "AIPaper",
        "arxiv_id": "2512.20963"
      },
      {
        "name": "Stable Velocity: A Variance Perspective on Flow Matching",
        "type": "AIPaper",
        "arxiv_id": "2602.05435"
      },
      {
        "name": "Laminating Representation Autoencoders for Efficient Diffusion",
        "type": "AIPaper",
        "arxiv_id": "2602.04873"
      },
      {
        "name": "Adaptive 1D Video Diffusion Autoencoder",
        "type": "AIPaper",
        "arxiv_id": "2602.04220"
      },
      {
        "name": "Test-Time Conditioning with Representation-Aligned Visual Features",
        "type": "AIPaper",
        "arxiv_id": "2602.03753"
      },
      {
        "name": "Bio-inspired fine-tuning for selective transfer learning in image classification",
        "type": "AIPaper",
        "arxiv_id": "2601.11235"
      },
      {
        "name": "Densely Connected Convolutional Networks",
        "type": "AIPaper",
        "arxiv_id": "1608.06993"
      },
      {
        "name": "An extratropical cyclone center location method on satellite images based on transfer learning",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "An Advanced Convolutional Neural Network Architecture Utilizing Transfer Learning for Melanoma Detection",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Adaptive Transfer Learning for Surgical Tool Presence Detection in Laparoscopic Videos Through Gradual Freezing Fine‐Tuning",
        "type": "AIPaper",
        "arxiv_id": "2510.15372"
      },
      {
        "name": "VGG Induced Deep Hand Sign Language Detection",
        "type": "AIPaper",
        "arxiv_id": "2601.08262"
      },
      {
        "name": "MediaPipe: A Framework for Building Perception Pipelines",
        "type": "AIPaper",
        "arxiv_id": "1906.08172"
      },
      {
        "name": "Hand Pose Estimation and Hand Shape Classification Using Multi-layered Randomized Decision Forests",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "A robust static hand gesture recognition system using geometry based normalizations and Krawtchouk moments",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Hand signal classification system for sign language communication in Virtual Reality",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Real-Time Static Hand Sign Recognition System using MediaPipe and Fully Connected Neural Network",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Real-Time Gesture Recognition to Aid Communication in Children with Motor Impairments",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Learning Multiple Layers of Features from Tiny Images",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation",
        "type": "AIPaper",
        "arxiv_id": "1311.2524"
      },
      {
        "name": "MuMu-LLaMA: Multi-modal music understanding and generation via large language models",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy",
        "type": "AIPaper",
        "arxiv_id": "2601.03305"
      },
      {
        "name": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism",
        "type": "AIPaper",
        "arxiv_id": "2512.21452"
      },
      {
        "name": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
        "type": "AIPaper",
        "arxiv_id": "2601.04720"
      },
      {
        "name": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Visualizing Data using t-SNE",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "LLM Social Simulations Are a Promising Research Method",
        "type": "AIPaper",
        "arxiv_id": "2504.02234"
      },
      {
        "name": "Federated Contrastive Learning With Feature-Based Distillation for Human Activity Recognition",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Diffuse and Disperse: Image Generation with Representation Regularization",
        "type": "AIPaper",
        "arxiv_id": "2506.09027"
      },
      {
        "name": "Deep Autoencoder Neural Networks: A Comprehensive Review and New Perspectives",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems",
        "type": "AIPaper",
        "arxiv_id": "2502.07350"
      },
      {
        "name": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
        "type": "AIPaper",
        "arxiv_id": "1505.04597"
      },
      {
        "name": "Improved Distribution Matching Distillation for Fast Image Synthesis",
        "type": "AIPaper",
        "arxiv_id": "2405.14867"
      },
      {
        "name": "Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation",
        "type": "AIPaper",
        "arxiv_id": "2403.12015"
      },
      {
        "name": "Evolutionary optimization of model merging recipes",
        "type": "AIPaper",
        "arxiv_id": "2403.13187"
      },
      {
        "name": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
        "type": "AIPaper",
        "arxiv_id": "2412.07772"
      },
      {
        "name": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment",
        "type": "AIPaper",
        "arxiv_id": "2404.16022"
      },
      {
        "name": "Denoising Diffusion Probabilistic Models",
        "type": "AIPaper",
        "arxiv_id": "2006.11239"
      },
      {
        "name": "HunyuanVideo: A Systematic Framework For Large Video Generative Models",
        "type": "AIPaper",
        "arxiv_id": "2412.03603"
      },
      {
        "name": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency",
        "type": "AIPaper",
        "arxiv_id": "2409.02634"
      },
      {
        "name": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models",
        "type": "AIPaper",
        "arxiv_id": "2502.01061"
      },
      {
        "name": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation",
        "type": "AIPaper",
        "arxiv_id": "2410.07718"
      },
      {
        "name": "EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation",
        "type": "AIPaper",
        "arxiv_id": "2411.10061"
      },
      {
        "name": "Feature Pyramid Networks for Object Detection",
        "type": "AIPaper",
        "arxiv_id": "1612.03144"
      },
      {
        "name": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation",
        "type": "AIPaper",
        "arxiv_id": "1406.1078"
      },
      {
        "name": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving",
        "type": "AIPaper",
        "arxiv_id": "2312.09245"
      },
      {
        "name": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability",
        "type": "AIPaper",
        "arxiv_id": "2405.17398"
      },
      {
        "name": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving",
        "type": "AIPaper",
        "arxiv_id": "2411.15139"
      },
      {
        "name": "How Far is Video Generation from World Model: A Physical Law Perspective",
        "type": "AIPaper",
        "arxiv_id": "2411.02385"
      },
      {
        "name": "EMMA: End-to-End Multimodal Model for Autonomous Driving",
        "type": "AIPaper",
        "arxiv_id": "2410.23262"
      },
      {
        "name": "Language Models are Few-Shot Learners",
        "type": "AIPaper",
        "arxiv_id": "2005.14165"
      },
      {
        "name": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling",
        "type": "AIPaper",
        "arxiv_id": "2412.05271"
      },
      {
        "name": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models",
        "type": "AIPaper",
        "arxiv_id": "2504.10479"
      },
      {
        "name": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models",
        "type": "AIPaper",
        "arxiv_id": "2407.11691"
      },
      {
        "name": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step",
        "type": "AIPaper",
        "arxiv_id": "2411.10440"
      },
      {
        "name": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency",
        "type": "AIPaper",
        "arxiv_id": "2508.18265"
      },
      {
        "name": "Proximal Policy Optimization Algorithms",
        "type": "AIPaper",
        "arxiv_id": "1707.06347"
      },
      {
        "name": "Flow-GRPO: Training Flow Matching Models via Online RL",
        "type": "AIPaper",
        "arxiv_id": "2505.05470"
      },
      {
        "name": "DanceGRPO: Unleashing GRPO on Visual Generation",
        "type": "AIPaper",
        "arxiv_id": "2505.07818"
      },
      {
        "name": "Seedance 1.0: Exploring the Boundaries of Video Generation Models",
        "type": "AIPaper",
        "arxiv_id": "2506.09113"
      },
      {
        "name": "SkyReels-V2: Infinite-length Film Generative Model",
        "type": "AIPaper",
        "arxiv_id": "2504.13074"
      },
      {
        "name": "Unified Reward Model for Multimodal Understanding and Generation",
        "type": "AIPaper",
        "arxiv_id": "2503.05236"
      },
      {
        "name": "GPT-4 Technical Report",
        "type": "AIPaper",
        "arxiv_id": "2303.08774"
      },
      {
        "name": "LLaMA: Open and Efficient Foundation Language Models",
        "type": "AIPaper",
        "arxiv_id": "2302.13971"
      },
      {
        "name": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
        "type": "AIPaper",
        "arxiv_id": "2201.11903"
      },
      {
        "name": "Training Verifiers to Solve Math Word Problems",
        "type": "AIPaper",
        "arxiv_id": "2110.14168"
      },
      {
        "name": "Crystal structure of the nucleosome core particle at 2.8 Å resolution",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
        "type": "AIPaper",
        "arxiv_id": "2312.00752"
      },
      {
        "name": "A catalogue of splice junction sequences.",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Origin of the Genetic Code",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "SGDR: Stochastic Gradient Descent with Warm Restarts",
        "type": "AIPaper",
        "arxiv_id": "1608.03983"
      },
      {
        "name": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
        "type": "AIPaper",
        "arxiv_id": "1701.06538"
      },
      {
        "name": "Compression of individual sequences via variable-rate coding",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Pointer Sentinel Mixture Models",
        "type": "AIPaper",
        "arxiv_id": "1609.07843"
      },
      {
        "name": "Measuring Massive Multitask Language Understanding",
        "type": "AIPaper",
        "arxiv_id": "2009.03300"
      },
      {
        "name": "Let's Verify Step by Step",
        "type": "AIPaper",
        "arxiv_id": "2305.20050"
      },
      {
        "name": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark",
        "type": "AIPaper",
        "arxiv_id": "2311.12022"
      },
      {
        "name": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
        "type": "AIPaper",
        "arxiv_id": "2006.16668"
      },
      {
        "name": "Pattern Recognition and Machine Learning",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Bagging Predictors",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Automatic Image-Based Plant Disease Severity Estimation Using Deep Learning",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Plant identification using deep neural networks via optimization of transfer learning parameters",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "New perspectives on plant disease characterization based on deep learning",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Deep Learning for Plant Identification in Natural Environment",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Going deeper in the automated identification of Herbarium specimens",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "MobileNetV2: Inverted Residuals and Linear Bottlenecks",
        "type": "AIPaper",
        "arxiv_id": "1801.04381"
      },
      {
        "name": "Communication-Efficient Learning of Deep Networks from Decentralized Data",
        "type": "AIPaper",
        "arxiv_id": "1602.05629"
      },
      {
        "name": "A Comprehensive Survey on Transfer Learning",
        "type": "AIPaper",
        "arxiv_id": "1911.02685"
      },
      {
        "name": "Privacy Preserved and Decentralized Smartphone Recommendation System",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "A Survey on Heterogeneity Taxonomy, Security and Privacy Preservation in the Integration of IoT, Wireless Sensor Networks and Federated Learning",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "A privacy-preserving federated learning with a secure collaborative for malware detection models using Internet of Things resources",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Graph-based deep learning techniques for remote sensing applications: Techniques, taxonomy, and applications - A comprehensive review",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Federated Learning Architectures: A Performance Evaluation With Crop Yield Prediction Application",
        "type": "AIPaper",
        "arxiv_id": "2408.02998"
      },
      {
        "name": "Segment Anything",
        "type": "AIPaper",
        "arxiv_id": "2304.02643"
      },
      {
        "name": "Visual Instruction Tuning",
        "type": "AIPaper",
        "arxiv_id": "2304.08485"
      },
      {
        "name": "Improved Baselines with Visual Instruction Tuning",
        "type": "AIPaper",
        "arxiv_id": "2310.03744"
      },
      {
        "name": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset",
        "type": "AIPaper",
        "arxiv_id": "2405.18842"
      },
      {
        "name": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs",
        "type": "AIPaper",
        "arxiv_id": "2505.15436"
      },
      {
        "name": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views",
        "type": "AIPaper",
        "arxiv_id": "2510.18632"
      },
      {
        "name": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning",
        "type": "AIPaper",
        "arxiv_id": "2510.13515"
      },
      {
        "name": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe",
        "type": "AIPaper",
        "arxiv_id": "2511.16334"
      },
      {
        "name": "Vision-Language-Action (VLA) Models: Concepts, Progress, Applications and Challenges",
        "type": "AIPaper",
        "arxiv_id": "2505.04769"
      },
      {
        "name": "Leave No Observation Behind: Real-time Correction for VLA Action Chunks",
        "type": "AIPaper",
        "arxiv_id": "2509.23224"
      },
      {
        "name": "Latent Chain-of-Thought World Modeling for End-to-End Driving",
        "type": "AIPaper",
        "arxiv_id": "2512.10226"
      },
      {
        "name": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks",
        "type": "AIPaper",
        "arxiv_id": "2510.04898"
      },
      {
        "name": "Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models",
        "type": "AIPaper",
        "arxiv_id": "2512.09927"
      },
      {
        "name": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "TransSIL: A Silhouette Cue-Aware Image Classification Framework for Bird Ecological Monitoring Systems",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "CBRFormer: rendering technology-based transformer for refinement segmentation of bridge crack images",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Deep contrastive learning enables genome-wide virtual screening.",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "LLaVA-based semantic feature modulation diffusion model for underwater image enhancement",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "3DGS-Drag: Dragging Gaussians for Intuitive Point-Based 3D Editing",
        "type": "AIPaper",
        "arxiv_id": "2601.07963"
      },
      {
        "name": "Attention mechanisms in neural networks",
        "type": "AIPaper",
        "arxiv_id": "2601.03329"
      },
      {
        "name": "Bidirectional Normalizing Flow: From Data to Noise and Back",
        "type": "AIPaper",
        "arxiv_id": "2512.10953"
      },
      {
        "name": "One-step Latent-free Image Generation with Pixel Mean Flows",
        "type": "AIPaper",
        "arxiv_id": "2601.22158"
      },
      {
        "name": "Meta Flow Maps enable scalable reward alignment",
        "type": "AIPaper",
        "arxiv_id": "2601.14430"
      },
      {
        "name": "Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective",
        "type": "AIPaper",
        "arxiv_id": "2602.05319"
      },
      {
        "name": "Generative Modeling via Drifting",
        "type": "AIPaper",
        "arxiv_id": "2602.04770"
      },
      {
        "name": "A Simple Framework for Contrastive Learning of Visual Representations",
        "type": "AIPaper",
        "arxiv_id": "2002.05709"
      },
      {
        "name": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning",
        "type": "AIPaper",
        "arxiv_id": "2512.24146"
      },
      {
        "name": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints",
        "type": "AIPaper",
        "arxiv_id": "2510.14847"
      },
      {
        "name": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders",
        "type": "AIPaper",
        "arxiv_id": "2601.16208"
      },
      {
        "name": "Simulating the Visual World with Artificial Intelligence: A Roadmap",
        "type": "AIPaper",
        "arxiv_id": "2511.08585"
      },
      {
        "name": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks",
        "type": "AIPaper",
        "arxiv_id": "2602.01630"
      },
      {
        "name": "RecTok: Reconstruction Distillation along Rectified Flow",
        "type": "AIPaper",
        "arxiv_id": "2512.13421"
      },
      {
        "name": "RePack then Refine: Efficient Diffusion Transformer with Vision Foundation Model",
        "type": "AIPaper",
        "arxiv_id": "2512.12083"
      },
      {
        "name": "Diffusion Models Beat GANs on Image Synthesis",
        "type": "AIPaper",
        "arxiv_id": "2105.05233"
      },
      {
        "name": "DINOv2: Learning Robust Visual Features without Supervision",
        "type": "AIPaper",
        "arxiv_id": "2304.07193"
      },
      {
        "name": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss",
        "type": "AIPaper",
        "arxiv_id": "2602.02493"
      },
      {
        "name": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
        "type": "AIPaper",
        "arxiv_id": "2505.02567"
      },
      {
        "name": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery",
        "type": "AIPaper",
        "arxiv_id": "2602.02465"
      },
      {
        "name": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation",
        "type": "AIPaper",
        "arxiv_id": "2601.15369"
      },
      {
        "name": "Distributed Representations of Words and Phrases and their Compositionality",
        "type": "AIPaper",
        "arxiv_id": "1310.4546"
      },
      {
        "name": "GloVe: Global Vectors for Word Representation",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Deep Contextualized Word Representations",
        "type": "AIPaper",
        "arxiv_id": "1802.05365"
      },
      {
        "name": "Protein Language Models: Is Scaling Necessary?",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis",
        "type": "AIPaper",
        "arxiv_id": "2601.17203"
      },
      {
        "name": "Generative Classifiers Avoid Shortcut Solutions",
        "type": "AIPaper",
        "arxiv_id": "2512.25034"
      },
      {
        "name": "Edge Large AI Model Agent-Empowered Cognitive Multimodal Semantic Communication",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Language Models are Unsupervised Multitask Learners",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "The Llama 3 Herd of Models",
        "type": "AIPaper",
        "arxiv_id": "2407.21783"
      },
      {
        "name": "A Survey on Diffusion Language Models",
        "type": "AIPaper",
        "arxiv_id": "2508.10875"
      },
      {
        "name": "Training Optimal Large Diffusion Language Models",
        "type": "AIPaper",
        "arxiv_id": "2510.03280"
      },
      {
        "name": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models",
        "type": "AIPaper",
        "arxiv_id": "2509.10396"
      },
      {
        "name": "Encoder-Decoder Diffusion Language Models for Efficient Training and Inference",
        "type": "AIPaper",
        "arxiv_id": "2510.22852"
      },
      {
        "name": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model",
        "type": "AIPaper",
        "arxiv_id": "2601.15892"
      },
      {
        "name": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
        "type": "AIPaper",
        "arxiv_id": "1906.08237"
      },
      {
        "name": "Paper",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Qwen Technical Report",
        "type": "AIPaper",
        "arxiv_id": "2309.16609"
      },
      {
        "name": "Code Llama: Open Foundation Models for Code",
        "type": "AIPaper",
        "arxiv_id": "2308.12950"
      },
      {
        "name": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way",
        "type": "AIPaper",
        "arxiv_id": "2510.24605"
      },
      {
        "name": "Set Block Decoding is a Language Model Inference Accelerator",
        "type": "AIPaper",
        "arxiv_id": "2509.04185"
      },
      {
        "name": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows",
        "type": "AIPaper",
        "arxiv_id": "2510.03506"
      },
      {
        "name": "Constrained Decoding of Diffusion LLMs with Context-Free Grammars",
        "type": "AIPaper",
        "arxiv_id": "2508.10111"
      },
      {
        "name": "Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models",
        "type": "AIPaper",
        "arxiv_id": "2511.21338"
      },
      {
        "name": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications",
        "type": "AIPaper",
        "arxiv_id": "2602.05665"
      },
      {
        "name": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search",
        "type": "AIPaper",
        "arxiv_id": "2602.04248"
      },
      {
        "name": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking",
        "type": "AIPaper",
        "arxiv_id": "2602.03224"
      },
      {
        "name": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents",
        "type": "AIPaper",
        "arxiv_id": "2602.01869"
      },
      {
        "name": "Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting",
        "type": "AIPaper",
        "arxiv_id": "2602.01776"
      },
      {
        "name": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation",
        "type": "AIPaper",
        "arxiv_id": "2509.16630"
      },
      {
        "name": "Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis",
        "type": "AIPaper",
        "arxiv_id": "2508.05580"
      },
      {
        "name": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing",
        "type": "AIPaper",
        "arxiv_id": "2510.14648"
      },
      {
        "name": "Paper2Video: Automatic Video Generation from Scientific Papers",
        "type": "AIPaper",
        "arxiv_id": "2510.05096"
      },
      {
        "name": "ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment",
        "type": "AIPaper",
        "arxiv_id": "2509.17818"
      },
      {
        "name": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning",
        "type": "AIPaper",
        "arxiv_id": "2501.12948"
      },
      {
        "name": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone",
        "type": "AIPaper",
        "arxiv_id": "2404.14219"
      },
      {
        "name": "LLaVA-OneVision: Easy Visual Task Transfer",
        "type": "AIPaper",
        "arxiv_id": "2408.03326"
      },
      {
        "name": "Spatial Reasoning in Multimodal Large Language Models: A Survey of Tasks, Benchmarks and Methods",
        "type": "AIPaper",
        "arxiv_id": "2511.15722"
      },
      {
        "name": "SpatialTree: How Spatial Abilities Branch Out in MLLMs",
        "type": "AIPaper",
        "arxiv_id": "2512.20617"
      },
      {
        "name": "From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs",
        "type": "AIPaper",
        "arxiv_id": "2512.19683"
      },
      {
        "name": "SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery",
        "type": "AIPaper",
        "arxiv_id": "2512.07733"
      },
      {
        "name": "You Only Look Once: Unified, Real-Time Object Detection",
        "type": "AIPaper",
        "arxiv_id": "1506.02640"
      },
      {
        "name": "RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes",
        "type": "AIPaper",
        "arxiv_id": "2502.00392"
      },
      {
        "name": "WeDetect: Fast Open-Vocabulary Object Detection as Retrieval",
        "type": "AIPaper",
        "arxiv_id": "2512.12309"
      },
      {
        "name": "v1: Learning to Point Visual Tokens for Multimodal Grounded Reasoning",
        "type": "AIPaper",
        "arxiv_id": "2505.18842"
      },
      {
        "name": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design",
        "type": "AIPaper",
        "arxiv_id": "2512.04082"
      },
      {
        "name": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving",
        "type": "AIPaper",
        "arxiv_id": "2511.19221"
      },
      {
        "name": "Reinforcement Learning: An Introduction",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Continuous control with deep reinforcement learning",
        "type": "AIPaper",
        "arxiv_id": "1509.02971"
      },
      {
        "name": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning",
        "type": "AIPaper",
        "arxiv_id": "2506.01939"
      },
      {
        "name": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models",
        "type": "AIPaper",
        "arxiv_id": "2505.22617"
      },
      {
        "name": "Learning to Reason under Off-Policy Guidance",
        "type": "AIPaper",
        "arxiv_id": "2504.14945"
      },
      {
        "name": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models",
        "type": "AIPaper",
        "arxiv_id": "2502.21321"
      },
      {
        "name": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement learning for LLMs",
        "type": "AIPaper",
        "arxiv_id": "2503.14286"
      },
      {
        "name": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation",
        "type": "AIPaper",
        "arxiv_id": "1611.06612"
      },
      {
        "name": "Fully convolutional networks for semantic segmentation",
        "type": "AIPaper",
        "arxiv_id": "1411.4038"
      },
      {
        "name": "Diffusion Models in Vision: A Survey",
        "type": "AIPaper",
        "arxiv_id": "2209.04747"
      },
      {
        "name": "SegNeXt: Rethinking Convolutional Attention Design for Semantic Segmentation",
        "type": "AIPaper",
        "arxiv_id": "2209.08575"
      },
      {
        "name": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation With Transformers",
        "type": "AIPaper",
        "arxiv_id": "2203.04838"
      },
      {
        "name": "Segment Anything in High Quality",
        "type": "AIPaper",
        "arxiv_id": "2306.01567"
      },
      {
        "name": "PIDNet: A Real-time Semantic Segmentation Network Inspired by PID Controllers",
        "type": "AIPaper",
        "arxiv_id": "2206.02066"
      },
      {
        "name": "Qwen2 Technical Report",
        "type": "AIPaper",
        "arxiv_id": "2407.10671"
      },
      {
        "name": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving",
        "type": "AIPaper",
        "arxiv_id": "2505.17685"
      },
      {
        "name": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning",
        "type": "AIPaper",
        "arxiv_id": "2506.13757"
      },
      {
        "name": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models",
        "type": "AIPaper",
        "arxiv_id": "2505.23757"
      },
      {
        "name": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving",
        "type": "AIPaper",
        "arxiv_id": "2505.15298"
      },
      {
        "name": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning",
        "type": "AIPaper",
        "arxiv_id": "2506.18234"
      },
      {
        "name": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model",
        "type": "AIPaper",
        "arxiv_id": "2508.06571"
      },
      {
        "name": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving",
        "type": "AIPaper",
        "arxiv_id": "2510.12796"
      },
      {
        "name": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
        "type": "AIPaper",
        "arxiv_id": "2308.04079"
      },
      {
        "name": "Structure-from-Motion Revisited",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Generalized Trajectory Scoring for End-to-end Multimodal Planning",
        "type": "AIPaper",
        "arxiv_id": "2506.06664"
      },
      {
        "name": "RAP: 3D Rasterization Augmented End-to-End Planning",
        "type": "AIPaper",
        "arxiv_id": "2510.04333"
      },
      {
        "name": "PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving",
        "type": "AIPaper",
        "arxiv_id": "2507.17596"
      },
      {
        "name": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
        "type": "AIPaper",
        "arxiv_id": "2301.12597"
      },
      {
        "name": "Flow Matching for Generative Modeling",
        "type": "AIPaper",
        "arxiv_id": "2210.02747"
      },
      {
        "name": "Diffusion policy: Visuomotor policy learning via action diffusion",
        "type": "AIPaper",
        "arxiv_id": "2303.04137"
      },
      {
        "name": "Large Language Models for Robotics: A Survey",
        "type": "AIPaper",
        "arxiv_id": "2311.07226"
      },
      {
        "name": "Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)",
        "type": "AIPaper",
        "arxiv_id": "2505.16394"
      },
      {
        "name": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey",
        "type": "AIPaper",
        "arxiv_id": "2509.19012"
      },
      {
        "name": "ReSim: Reliable World Simulation for Autonomous Driving",
        "type": "AIPaper",
        "arxiv_id": "2506.09981"
      },
      {
        "name": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
        "type": "AIPaper",
        "arxiv_id": "2307.09288"
      },
      {
        "name": "Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey",
        "type": "AIPaper",
        "arxiv_id": "2510.17111"
      },
      {
        "name": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving",
        "type": "AIPaper",
        "arxiv_id": "2509.13769"
      },
      {
        "name": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition",
        "type": "AIPaper",
        "arxiv_id": "2507.21610"
      },
      {
        "name": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI",
        "type": "AIPaper",
        "arxiv_id": "2407.06886"
      },
      {
        "name": "A Survey on Vision-Language-Action Models for Embodied AI",
        "type": "AIPaper",
        "arxiv_id": "2405.14093"
      },
      {
        "name": "DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation",
        "type": "AIPaper",
        "arxiv_id": "2410.13571"
      },
      {
        "name": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation",
        "type": "AIPaper",
        "arxiv_id": "2410.05363"
      },
      {
        "name": "AgentSquare: Automatic LLM Agent Search in Modular Design Space",
        "type": "AIPaper",
        "arxiv_id": "2410.06153"
      },
      {
        "name": "CityGPT: Empowering Urban Spatial Cognition of Large Language Models",
        "type": "AIPaper",
        "arxiv_id": "2406.13948"
      },
      {
        "name": "Epona: Autoregressive Diffusion World Model for Autonomous Driving",
        "type": "AIPaper",
        "arxiv_id": "2506.24113"
      },
      {
        "name": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
        "type": "AIPaper",
        "arxiv_id": "1907.11692"
      },
      {
        "name": "Unlearning Concepts in Diffusion Model via Concept Domain Correction and Concept Preserving Gradient",
        "type": "AIPaper",
        "arxiv_id": "2405.15304"
      },
      {
        "name": "Safety at Scale: A Comprehensive Survey of Large Model Safety",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Large multimodal models evaluation: a survey",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense",
        "type": "AIPaper",
        "arxiv_id": "2411.08410"
      },
      {
        "name": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs",
        "type": "AIPaper",
        "arxiv_id": "2505.11842"
      },
      {
        "name": "Gradient-Guided Learning Network for Infrared Small Target Detection",
        "type": "AIPaper",
        "arxiv_id": "2512.09497"
      },
      {
        "name": "A Retrieval-Augmented Generation Approach to Extracting Algorithmic Logic from Neural Networks",
        "type": "AIPaper",
        "arxiv_id": "2512.04329"
      },
      {
        "name": "CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation",
        "type": "AIPaper",
        "arxiv_id": "2511.01243"
      },
      {
        "name": "IMobileTransformer: A fusion-based lightweight model for rice disease identification",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers",
        "type": "AIPaper",
        "arxiv_id": "2511.09554"
      },
      {
        "name": "Image quality assessment: from error visibility to structural similarity",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction",
        "type": "AIPaper",
        "arxiv_id": "2601.18993"
      },
      {
        "name": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos",
        "type": "AIPaper",
        "arxiv_id": "2601.00393"
      },
      {
        "name": "Scalable Diffusion Models with Transformers",
        "type": "AIPaper",
        "arxiv_id": "2212.09748"
      },
      {
        "name": "RoFormer: Enhanced Transformer with Rotary Position Embedding",
        "type": "AIPaper",
        "arxiv_id": "2104.09864"
      },
      {
        "name": "The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey",
        "type": "AIPaper",
        "arxiv_id": "2502.10498"
      },
      {
        "name": "Vision meets robotics: The KITTI dataset",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Least-Squares Estimation of Transformation Parameters Between Two Point Patterns",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations",
        "type": "AIPaper",
        "arxiv_id": "2601.22376"
      },
      {
        "name": "Deep Learning",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction",
        "type": "AIPaper",
        "arxiv_id": "2509.13414"
      },
      {
        "name": "Seedream 4.0: Toward Next-generation Multimodal Image Generation",
        "type": "AIPaper",
        "arxiv_id": "2509.20427"
      },
      {
        "name": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation",
        "type": "AIPaper",
        "arxiv_id": "2508.09987"
      },
      {
        "name": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding",
        "type": "AIPaper",
        "arxiv_id": "2510.06308"
      },
      {
        "name": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale",
        "type": "AIPaper",
        "arxiv_id": "2508.10711"
      },
      {
        "name": "Training language models to follow instructions with human feedback",
        "type": "AIPaper",
        "arxiv_id": "2203.02155"
      },
      {
        "name": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning",
        "type": "AIPaper",
        "arxiv_id": "2503.09516"
      },
      {
        "name": "Toward expert-level medical question answering with large language models",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training",
        "type": "AIPaper",
        "arxiv_id": "2501.17161"
      },
      {
        "name": "Equivariant Diffusion for Crystal Structure Prediction",
        "type": "AIPaper",
        "arxiv_id": "2512.07289"
      },
      {
        "name": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling",
        "type": "AIPaper",
        "arxiv_id": "2512.14614"
      },
      {
        "name": "Native and Compact Structured Latents for 3D Generation",
        "type": "AIPaper",
        "arxiv_id": "2512.14692"
      },
      {
        "name": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?",
        "type": "AIPaper",
        "arxiv_id": "2512.24497"
      },
      {
        "name": "Progressive Learned Image Transmission for Semantic Communication Using Hierarchical VAE",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "STORM: Search-Guided Generative World Models for Robotic Manipulation",
        "type": "AIPaper",
        "arxiv_id": "2512.18477"
      },
      {
        "name": "A New Approach to Linear Filtering and Prediction Problems",
        "type": "AIPaper",
        "arxiv_id": ""
      },
      {
        "name": "Continuous 3D Perception Model with Persistent State",
        "type": "AIPaper",
        "arxiv_id": "2501.12387"
      },
      {
        "name": "Gen3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control",
        "type": "AIPaper",
        "arxiv_id": "2503.03751"
      },
      {
        "name": "FLARE: Feed-Forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views",
        "type": "AIPaper",
        "arxiv_id": "2502.12138"
      },
      {
        "name": "UniDepthV2: Universal Monocular Metric Depth Estimation Made Simpler",
        "type": "AIPaper",
        "arxiv_id": "2502.20110"
      },
      {
        "name": "OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving",
        "type": "AIPaper",
        "arxiv_id": "2412.15208"
      },
      {
        "name": "Adaptive transfer learning for surgical tool presence detection in laparoscopic videos through gradual freezing fine-tuning",
        "type": "AIPaper",
        "arxiv_id": "2510.15372"
      },
      {
        "name": "Rich feature hierarchies for accurate object detection and semantic segmentation",
        "type": "AIPaper",
        "arxiv_id": "1311.2524"
      },
      {
        "name": "Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism",
        "type": "AIPaper",
        "arxiv_id": "2512.21452"
      },
      {
        "name": "Evolutionary Optimization of Model Merging Recipes",
        "type": "AIPaper",
        "arxiv_id": "2403.13187"
      },
      {
        "name": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
        "type": "AIPaper",
        "arxiv_id": "1406.1078"
      },
      {
        "name": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving",
        "type": "AIPaper",
        "arxiv_id": "2312.09245"
      },
      {
        "name": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models",
        "type": "AIPaper",
        "arxiv_id": "2407.11691"
      },
      {
        "name": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
        "type": "AIPaper",
        "arxiv_id": "2201.11903"
      },
      {
        "name": "Federated Learning Architectures: A Performance Evaluation with Crop Yield Prediction Application",
        "type": "AIPaper",
        "arxiv_id": "2408.02998"
      },
      {
        "name": "Enhancing Descriptive Image Quality Assessment with A Large-scale Multi-modal Dataset",
        "type": "AIPaper",
        "arxiv_id": "2405.18842"
      },
      {
        "name": "Deep contextualized word representations",
        "type": "AIPaper",
        "arxiv_id": "1802.05365"
      },
      {
        "name": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
        "type": "AIPaper",
        "arxiv_id": "2501.12948"
      },
      {
        "name": "Fully Convolutional Networks for Semantic Segmentation",
        "type": "AIPaper",
        "arxiv_id": "1411.4038"
      },
      {
        "name": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with Transformers",
        "type": "AIPaper",
        "arxiv_id": "2203.04838"
      },
      {
        "name": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion",
        "type": "AIPaper",
        "arxiv_id": "2303.04137"
      },
      {
        "name": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI",
        "type": "AIPaper",
        "arxiv_id": "2407.06886"
      },
      {
        "name": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control",
        "type": "AIPaper",
        "arxiv_id": "2503.03751"
      },
      {
        "name": "FLARE: Feed-forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views",
        "type": "AIPaper",
        "arxiv_id": "2502.12138"
      },
      {
        "name": "Ashish Vaswani",
        "type": "Researcher"
      },
      {
        "name": "Noam Shazeer",
        "type": "Researcher"
      },
      {
        "name": "Niki Parmar",
        "type": "Researcher"
      },
      {
        "name": "Jakob Uszkoreit",
        "type": "Researcher"
      },
      {
        "name": "Llion Jones",
        "type": "Researcher"
      },
      {
        "name": "Aidan N. Gomez",
        "type": "Researcher"
      },
      {
        "name": "Lukasz Kaiser",
        "type": "Researcher"
      },
      {
        "name": "Illia Polosukhin",
        "type": "Researcher"
      },
      {
        "name": "Kaiming He",
        "type": "Researcher"
      },
      {
        "name": "Xiangyu Zhang",
        "type": "Researcher"
      },
      {
        "name": "Shaoqing Ren",
        "type": "Researcher"
      },
      {
        "name": "Jian Sun",
        "type": "Researcher"
      },
      {
        "name": "Diederik P. Kingma",
        "type": "Researcher"
      },
      {
        "name": "Jimmy Ba",
        "type": "Researcher"
      },
      {
        "name": "Sepp Hochreiter",
        "type": "Researcher"
      },
      {
        "name": "J. Schmidhuber",
        "type": "Researcher"
      },
      {
        "name": "Nitish Srivastava",
        "type": "Researcher"
      },
      {
        "name": "Geoffrey E. Hinton",
        "type": "Researcher"
      },
      {
        "name": "A. Krizhevsky",
        "type": "Researcher"
      },
      {
        "name": "I. Sutskever",
        "type": "Researcher"
      },
      {
        "name": "R. Salakhutdinov",
        "type": "Researcher"
      },
      {
        "name": "Christian Szegedy",
        "type": "Researcher"
      },
      {
        "name": "Vincent Vanhoucke",
        "type": "Researcher"
      },
      {
        "name": "Sergey Ioffe",
        "type": "Researcher"
      },
      {
        "name": "Jonathon Shlens",
        "type": "Researcher"
      },
      {
        "name": "Zbigniew Wojna",
        "type": "Researcher"
      },
      {
        "name": "Shaina Raza",
        "type": "Researcher"
      },
      {
        "name": "Mizanur Rahman",
        "type": "Researcher"
      },
      {
        "name": "Safiullah Kamawal",
        "type": "Researcher"
      },
      {
        "name": "Armin Toroghi",
        "type": "Researcher"
      },
      {
        "name": "Ananya Raval",
        "type": "Researcher"
      },
      {
        "name": "F. Navah",
        "type": "Researcher"
      },
      {
        "name": "Amirmohammad Kazemeini",
        "type": "Researcher"
      },
      {
        "name": "Zhuoran Yang",
        "type": "Researcher"
      },
      {
        "name": "Xi Guo",
        "type": "Researcher"
      },
      {
        "name": "Chenjing Ding",
        "type": "Researcher"
      },
      {
        "name": "Chiyu Wang",
        "type": "Researcher"
      },
      {
        "name": "Wei Wu",
        "type": "Researcher"
      },
      {
        "name": "Yanyong Zhang",
        "type": "Researcher"
      },
      {
        "name": "Zisheng Wang",
        "type": "Researcher"
      },
      {
        "name": "Junjie Chen",
        "type": "Researcher"
      },
      {
        "name": "Chisen Wang",
        "type": "Researcher"
      },
      {
        "name": "Cong Peng",
        "type": "Researcher"
      },
      {
        "name": "Jianping Xuan",
        "type": "Researcher"
      },
      {
        "name": "Tielin Shi",
        "type": "Researcher"
      },
      {
        "name": "Ming J. Zuo",
        "type": "Researcher"
      },
      {
        "name": "Jinghuan Zhang",
        "type": "Researcher"
      },
      {
        "name": "Wang Chen",
        "type": "Researcher"
      },
      {
        "name": "Jian Zhang",
        "type": "Researcher"
      },
      {
        "name": "Manlin Zhang",
        "type": "Researcher"
      },
      {
        "name": "Jie Wu",
        "type": "Researcher"
      },
      {
        "name": "Yuxi Ren",
        "type": "Researcher"
      },
      {
        "name": "Jiahong Yang",
        "type": "Researcher"
      },
      {
        "name": "Ming Li",
        "type": "Researcher"
      },
      {
        "name": "Andy J. Ma",
        "type": "Researcher"
      },
      {
        "name": "Karen Simonyan",
        "type": "Researcher"
      },
      {
        "name": "Andrew Zisserman",
        "type": "Researcher"
      },
      {
        "name": "P. Cochat",
        "type": "Researcher"
      },
      {
        "name": "L. Vaucoret",
        "type": "Researcher"
      },
      {
        "name": "J. Sarles",
        "type": "Researcher"
      },
      {
        "name": "Ross Girshick",
        "type": "Researcher"
      },
      {
        "name": "Yuqi Cheng",
        "type": "Researcher"
      },
      {
        "name": "Yunkang Cao",
        "type": "Researcher"
      },
      {
        "name": "Haiming Yao",
        "type": "Researcher"
      },
      {
        "name": "Wei Luo",
        "type": "Researcher"
      },
      {
        "name": "Cheng Jiang",
        "type": "Researcher"
      },
      {
        "name": "Hui Zhang",
        "type": "Researcher"
      },
      {
        "name": "Weiming Shen",
        "type": "Researcher"
      },
      {
        "name": "Naveen Kumar Srinivasa",
        "type": "Researcher"
      },
      {
        "name": "Ajeet Rao Chalamala",
        "type": "Researcher"
      },
      {
        "name": "Kumar Singh",
        "type": "Researcher"
      },
      {
        "name": "Ieee Krishna Mohan Senior Member",
        "type": "Researcher"
      },
      {
        "name": "K. Naveen",
        "type": "Researcher"
      },
      {
        "name": "Srinivasa Rao",
        "type": "Researcher"
      },
      {
        "name": "Ajeet Kumar Singh",
        "type": "Researcher"
      },
      {
        "name": "Hongbo Jiang",
        "type": "Researcher"
      },
      {
        "name": "Lei Ye",
        "type": "Researcher"
      },
      {
        "name": "Jingyang Hu",
        "type": "Researcher"
      },
      {
        "name": "Xiaotian Chen",
        "type": "Researcher"
      },
      {
        "name": "Siyu Chen",
        "type": "Researcher"
      },
      {
        "name": "Wei Zhang",
        "type": "Researcher"
      },
      {
        "name": "Kehua Yang",
        "type": "Researcher"
      },
      {
        "name": "Jingya Wang",
        "type": "Researcher"
      },
      {
        "name": "Jianfeng Wen",
        "type": "Researcher"
      },
      {
        "name": "Weiping Ding",
        "type": "Researcher"
      },
      {
        "name": "Chunlin Yu",
        "type": "Researcher"
      },
      {
        "name": "Xiatian Zhu",
        "type": "Researcher"
      },
      {
        "name": "Zhiyong Wang",
        "type": "Researcher"
      },
      {
        "name": "Diederik P Kingma",
        "type": "Researcher"
      },
      {
        "name": "Max Welling",
        "type": "Researcher"
      },
      {
        "name": "John C. Duchi",
        "type": "Researcher"
      },
      {
        "name": "Elad Hazan",
        "type": "Researcher"
      },
      {
        "name": "Y. Singer",
        "type": "Researcher"
      },
      {
        "name": "Alex Graves",
        "type": "Researcher"
      },
      {
        "name": "Abdel-rahman Mohamed",
        "type": "Researcher"
      },
      {
        "name": "Geoffrey Hinton",
        "type": "Researcher"
      },
      {
        "name": "Nian Wang",
        "type": "Researcher"
      },
      {
        "name": "Zhigao Cui",
        "type": "Researcher"
      },
      {
        "name": "Yanzhao Su",
        "type": "Researcher"
      },
      {
        "name": "Yunwei Lan",
        "type": "Researcher"
      },
      {
        "name": "Yuanliang Xue",
        "type": "Researcher"
      },
      {
        "name": "Cong Zhang",
        "type": "Researcher"
      },
      {
        "name": "Aihua Li",
        "type": "Researcher"
      },
      {
        "name": "Leong Kah Meng",
        "type": "Researcher"
      },
      {
        "name": "Ho Hooi Yi",
        "type": "Researcher"
      },
      {
        "name": "Ng Bo Wei",
        "type": "Researcher"
      },
      {
        "name": "Lim Jia Xin",
        "type": "Researcher"
      },
      {
        "name": "Zailan Arabee Abdul Salam",
        "type": "Researcher"
      },
      {
        "name": "Xin Cheng",
        "type": "Researcher"
      },
      {
        "name": "Wangding Zeng",
        "type": "Researcher"
      },
      {
        "name": "Damai Dai",
        "type": "Researcher"
      },
      {
        "name": "Qinyu Chen",
        "type": "Researcher"
      },
      {
        "name": "Bingxuan Wang",
        "type": "Researcher"
      },
      {
        "name": "Zhenda Xie",
        "type": "Researcher"
      },
      {
        "name": "Kezhao Huang",
        "type": "Researcher"
      },
      {
        "name": "Xingkai Yu",
        "type": "Researcher"
      },
      {
        "name": "Zhewen Hao",
        "type": "Researcher"
      },
      {
        "name": "Yukun Li",
        "type": "Researcher"
      },
      {
        "name": "Han Zhang",
        "type": "Researcher"
      },
      {
        "name": "Huishuai Zhang",
        "type": "Researcher"
      },
      {
        "name": "Dongyan Zhao",
        "type": "Researcher"
      },
      {
        "name": "Wenfeng Liang",
        "type": "Researcher"
      },
      {
        "name": "Mostafa Saberian",
        "type": "Researcher"
      },
      {
        "name": "Vidya Samadi",
        "type": "Researcher"
      },
      {
        "name": "Ioana Popescu",
        "type": "Researcher"
      },
      {
        "name": "Husheng Fang",
        "type": "Researcher"
      },
      {
        "name": "Shunlin Liang",
        "type": "Researcher"
      },
      {
        "name": "Wenyuan Li",
        "type": "Researcher"
      },
      {
        "name": "Yongzhe Chen",
        "type": "Researcher"
      },
      {
        "name": "Han Ma",
        "type": "Researcher"
      },
      {
        "name": "Jianglei Xu",
        "type": "Researcher"
      },
      {
        "name": "Yichuan Ma",
        "type": "Researcher"
      },
      {
        "name": "Tao He",
        "type": "Researcher"
      },
      {
        "name": "Feng Tian",
        "type": "Researcher"
      },
      {
        "name": "Fengjiao Zhang",
        "type": "Researcher"
      },
      {
        "name": "Hui Liang",
        "type": "Researcher"
      },
      {
        "name": "Wei Liu",
        "type": "Researcher"
      },
      {
        "name": "Yangqing Jia",
        "type": "Researcher"
      },
      {
        "name": "Pierre Sermanet",
        "type": "Researcher"
      },
      {
        "name": "Scott Reed",
        "type": "Researcher"
      },
      {
        "name": "Dragomir Anguelov",
        "type": "Researcher"
      },
      {
        "name": "Dumitru Erhan",
        "type": "Researcher"
      },
      {
        "name": "Andrew Rabinovich",
        "type": "Researcher"
      },
      {
        "name": "Olga Russakovsky",
        "type": "Researcher"
      },
      {
        "name": "Jia Deng",
        "type": "Researcher"
      },
      {
        "name": "Hao Su",
        "type": "Researcher"
      },
      {
        "name": "Jonathan Krause",
        "type": "Researcher"
      },
      {
        "name": "Sanjeev Satheesh",
        "type": "Researcher"
      },
      {
        "name": "Sean Ma",
        "type": "Researcher"
      },
      {
        "name": "Zhiheng Huang",
        "type": "Researcher"
      },
      {
        "name": "Andrej Karpathy",
        "type": "Researcher"
      },
      {
        "name": "Aditya Khosla",
        "type": "Researcher"
      },
      {
        "name": "Michael Bernstein",
        "type": "Researcher"
      },
      {
        "name": "Alexander C. Berg",
        "type": "Researcher"
      },
      {
        "name": "Li Fei-Fei",
        "type": "Researcher"
      },
      {
        "name": "Boyang Zheng",
        "type": "Researcher"
      },
      {
        "name": "Nanye Ma",
        "type": "Researcher"
      },
      {
        "name": "Shengbang Tong",
        "type": "Researcher"
      },
      {
        "name": "Saining Xie",
        "type": "Researcher"
      },
      {
        "name": "S. Rizvi",
        "type": "Researcher"
      },
      {
        "name": "Daniel Levine",
        "type": "Researcher"
      },
      {
        "name": "Aakash Patel",
        "type": "Researcher"
      },
      {
        "name": "Shiyang Zhang",
        "type": "Researcher"
      },
      {
        "name": "Eric Wang",
        "type": "Researcher"
      },
      {
        "name": "Curtis Jamison Perry",
        "type": "Researcher"
      },
      {
        "name": "Ivan Vrkic",
        "type": "Researcher"
      },
      {
        "name": "Nicole Mayerli Constante",
        "type": "Researcher"
      },
      {
        "name": "Zirui Fu",
        "type": "Researcher"
      },
      {
        "name": "Sizhuang He",
        "type": "Researcher"
      },
      {
        "name": "David Zhang",
        "type": "Researcher"
      },
      {
        "name": "Cerise Tang",
        "type": "Researcher"
      },
      {
        "name": "Zhuoyang Lyu",
        "type": "Researcher"
      },
      {
        "name": "Rayyan Y Darji",
        "type": "Researcher"
      },
      {
        "name": "Chang Li",
        "type": "Researcher"
      },
      {
        "name": "Emily Sun",
        "type": "Researcher"
      },
      {
        "name": "David Jeong",
        "type": "Researcher"
      },
      {
        "name": "Lawrence Zhao",
        "type": "Researcher"
      },
      {
        "name": "Jennifer Kwan",
        "type": "Researcher"
      },
      {
        "name": "David Braun",
        "type": "Researcher"
      },
      {
        "name": "Brian Hafler",
        "type": "Researcher"
      },
      {
        "name": "Hattie Chung",
        "type": "Researcher"
      },
      {
        "name": "R. M. Dhodapkar",
        "type": "Researcher"
      },
      {
        "name": "Paul F. Jaeger",
        "type": "Researcher"
      },
      {
        "name": "Bryan Perozzi",
        "type": "Researcher"
      },
      {
        "name": "Jeffrey Ishizuka",
        "type": "Researcher"
      },
      {
        "name": "Shekoofeh Azizi",
        "type": "Researcher"
      },
      {
        "name": "D. van Dijk",
        "type": "Researcher"
      },
      {
        "name": "Zhengyu Zhao",
        "type": "Researcher"
      },
      {
        "name": "Hanwei Zhang",
        "type": "Researcher"
      },
      {
        "name": "Renjue Li",
        "type": "Researcher"
      },
      {
        "name": "R. Sicre",
        "type": "Researcher"
      },
      {
        "name": "L. Amsaleg",
        "type": "Researcher"
      },
      {
        "name": "Michael Backes",
        "type": "Researcher"
      },
      {
        "name": "Qi Li",
        "type": "Researcher"
      },
      {
        "name": "Chao Shen",
        "type": "Researcher"
      },
      {
        "name": "Şafak Kılıç",
        "type": "Researcher"
      },
      {
        "name": "Yifei Ge",
        "type": "Researcher"
      },
      {
        "name": "Zhuo Li",
        "type": "Researcher"
      },
      {
        "name": "Xuebin Yue",
        "type": "Researcher"
      },
      {
        "name": "Hengyi Li",
        "type": "Researcher"
      },
      {
        "name": "Lin Meng",
        "type": "Researcher"
      },
      {
        "name": "Colin Raffel",
        "type": "Researcher"
      },
      {
        "name": "Adam Roberts",
        "type": "Researcher"
      },
      {
        "name": "Katherine Lee",
        "type": "Researcher"
      },
      {
        "name": "Sharan Narang",
        "type": "Researcher"
      },
      {
        "name": "Michael Matena",
        "type": "Researcher"
      },
      {
        "name": "Yanqi Zhou",
        "type": "Researcher"
      },
      {
        "name": "Wei Li",
        "type": "Researcher"
      },
      {
        "name": "Peter J. Liu",
        "type": "Researcher"
      },
      {
        "name": "M. Heusel",
        "type": "Researcher"
      },
      {
        "name": "Hubert Ramsauer",
        "type": "Researcher"
      },
      {
        "name": "Thomas Unterthiner",
        "type": "Researcher"
      },
      {
        "name": "Bernhard Nessler",
        "type": "Researcher"
      },
      {
        "name": "Holger Caesar",
        "type": "Researcher"
      },
      {
        "name": "Varun Bankiti",
        "type": "Researcher"
      },
      {
        "name": "Alex H. Lang",
        "type": "Researcher"
      },
      {
        "name": "Sourabh Vora",
        "type": "Researcher"
      },
      {
        "name": "Venice Erin Liong",
        "type": "Researcher"
      },
      {
        "name": "Qiang Xu",
        "type": "Researcher"
      },
      {
        "name": "Anush Krishnan",
        "type": "Researcher"
      },
      {
        "name": "Yu Pan",
        "type": "Researcher"
      },
      {
        "name": "Giancarlo Baldan",
        "type": "Researcher"
      },
      {
        "name": "Oscar Beijbom",
        "type": "Researcher"
      },
      {
        "name": "Alexey Dosovitskiy",
        "type": "Researcher"
      },
      {
        "name": "German Ros",
        "type": "Researcher"
      },
      {
        "name": "Felipe Codevilla",
        "type": "Researcher"
      },
      {
        "name": "Antonio Lopez",
        "type": "Researcher"
      },
      {
        "name": "Vladlen Koltun",
        "type": "Researcher"
      },
      {
        "name": "Fachrina Dewi Puspitasari",
        "type": "Researcher"
      },
      {
        "name": "Chaoning Zhang",
        "type": "Researcher"
      },
      {
        "name": "Joseph Cho",
        "type": "Researcher"
      },
      {
        "name": "Adnan Haider",
        "type": "Researcher"
      },
      {
        "name": "Noor Ul Eman",
        "type": "Researcher"
      },
      {
        "name": "Omer Amin",
        "type": "Researcher"
      },
      {
        "name": "Alexis Mankowski",
        "type": "Researcher"
      },
      {
        "name": "Muhammad Umair",
        "type": "Researcher"
      },
      {
        "name": "Jingyao Zheng",
        "type": "Researcher"
      },
      {
        "name": "Sheng Zheng",
        "type": "Researcher"
      },
      {
        "name": "Lik-Hang Lee",
        "type": "Researcher"
      },
      {
        "name": "Caiyan Qin",
        "type": "Researcher"
      },
      {
        "name": "Tae-Ho Kim",
        "type": "Researcher"
      },
      {
        "name": "Choong Seon Hong",
        "type": "Researcher"
      },
      {
        "name": "Yang Yang",
        "type": "Researcher"
      },
      {
        "name": "Heng Tao Shen",
        "type": "Researcher"
      },
      {
        "name": "Bohan Li",
        "type": "Researcher"
      },
      {
        "name": "Zhuang Ma",
        "type": "Researcher"
      },
      {
        "name": "Dalong Du",
        "type": "Researcher"
      },
      {
        "name": "Baorui Peng",
        "type": "Researcher"
      },
      {
        "name": "Zhujin Liang",
        "type": "Researcher"
      },
      {
        "name": "Zhenqiang Liu",
        "type": "Researcher"
      },
      {
        "name": "Chao Ma",
        "type": "Researcher"
      },
      {
        "name": "Yueming Jin",
        "type": "Researcher"
      },
      {
        "name": "Hao Zhao",
        "type": "Researcher"
      },
      {
        "name": "Wenjun Zeng",
        "type": "Researcher"
      },
      {
        "name": "Xin Jin",
        "type": "Researcher"
      },
      {
        "name": "Guosheng Zhao",
        "type": "Researcher"
      },
      {
        "name": "Yaozeng Wang",
        "type": "Researcher"
      },
      {
        "name": "Xiaofeng Wang",
        "type": "Researcher"
      },
      {
        "name": "Zheng Zhu",
        "type": "Researcher"
      },
      {
        "name": "Tingdong Yu",
        "type": "Researcher"
      },
      {
        "name": "Guan Huang",
        "type": "Researcher"
      },
      {
        "name": "Yongchen Zai",
        "type": "Researcher"
      },
      {
        "name": "Ji Jiao",
        "type": "Researcher"
      },
      {
        "name": "Changliang Xue",
        "type": "Researcher"
      },
      {
        "name": "Xiaole Wang",
        "type": "Researcher"
      },
      {
        "name": "Zhen Yang",
        "type": "Researcher"
      },
      {
        "name": "Futang Zhu",
        "type": "Researcher"
      },
      {
        "name": "Xingang Wang",
        "type": "Researcher"
      },
      {
        "name": "Ahmad Rahimi",
        "type": "Researcher"
      },
      {
        "name": "Valentin Gerard",
        "type": "Researcher"
      },
      {
        "name": "Eloi Zablocki",
        "type": "Researcher"
      },
      {
        "name": "Matthieu Cord",
        "type": "Researcher"
      },
      {
        "name": "Alexandre Alahi",
        "type": "Researcher"
      },
      {
        "name": "W. Marsden",
        "type": "Researcher"
      },
      {
        "name": "Wei Dong",
        "type": "Researcher"
      },
      {
        "name": "R. Socher",
        "type": "Researcher"
      },
      {
        "name": "Li-Jia Li",
        "type": "Researcher"
      },
      {
        "name": "K. Li",
        "type": "Researcher"
      },
      {
        "name": "R. Stephenson",
        "type": "Researcher"
      },
      {
        "name": "Jaskirat Singh",
        "type": "Researcher"
      },
      {
        "name": "Xingjian Leng",
        "type": "Researcher"
      },
      {
        "name": "Zongze Wu",
        "type": "Researcher"
      },
      {
        "name": "Liang Zheng",
        "type": "Researcher"
      },
      {
        "name": "Richard Zhang",
        "type": "Researcher"
      },
      {
        "name": "Eli Shechtman",
        "type": "Researcher"
      },
      {
        "name": "Zhifeng Wang",
        "type": "Researcher"
      },
      {
        "name": "Minghui Wang",
        "type": "Researcher"
      },
      {
        "name": "Chunyan Zeng",
        "type": "Researcher"
      },
      {
        "name": "Longlong Li",
        "type": "Researcher"
      },
      {
        "name": "Ehsan Zakeri",
        "type": "Researcher"
      },
      {
        "name": "Amanda Spilkin",
        "type": "Researcher"
      },
      {
        "name": "Hanae Elmekki",
        "type": "Researcher"
      },
      {
        "name": "Antonela Zanuttini",
        "type": "Researcher"
      },
      {
        "name": "L. Kadem",
        "type": "Researcher"
      },
      {
        "name": "Jamal Bentahar",
        "type": "Researcher"
      },
      {
        "name": "Wen-Fang Xie",
        "type": "Researcher"
      },
      {
        "name": "Philippe Pibarot",
        "type": "Researcher"
      },
      {
        "name": "Ana Davila",
        "type": "Researcher"
      },
      {
        "name": "Jacinto Colan",
        "type": "Researcher"
      },
      {
        "name": "Yasuhisa Hasegawa",
        "type": "Researcher"
      },
      {
        "name": "Subham Sharma",
        "type": "Researcher"
      },
      {
        "name": "Sharmila Subudhi",
        "type": "Researcher"
      },
      {
        "name": "Tsung-Yi Lin",
        "type": "Researcher"
      },
      {
        "name": "Michael Maire",
        "type": "Researcher"
      },
      {
        "name": "Serge Belongie",
        "type": "Researcher"
      },
      {
        "name": "Lubomir Bourdev",
        "type": "Researcher"
      },
      {
        "name": "James Hays",
        "type": "Researcher"
      },
      {
        "name": "Pietro Perona",
        "type": "Researcher"
      },
      {
        "name": "Deva Ramanan",
        "type": "Researcher"
      },
      {
        "name": "C. Lawrence Zitnick",
        "type": "Researcher"
      },
      {
        "name": "Piotr Dollár",
        "type": "Researcher"
      },
      {
        "name": "Zixiao Wen",
        "type": "Researcher"
      },
      {
        "name": "Peifeng Li",
        "type": "Researcher"
      },
      {
        "name": "Yuhan Liu",
        "type": "Researcher"
      },
      {
        "name": "Jingming Chen",
        "type": "Researcher"
      },
      {
        "name": "Xiantai Xiang",
        "type": "Researcher"
      },
      {
        "name": "Yuan Li",
        "type": "Researcher"
      },
      {
        "name": "Huixian Wang",
        "type": "Researcher"
      },
      {
        "name": "Yongchao Zhao",
        "type": "Researcher"
      },
      {
        "name": "Guangyao Zhou",
        "type": "Researcher"
      },
      {
        "name": "Yu Gu",
        "type": "Researcher"
      },
      {
        "name": "Weili Chen",
        "type": "Researcher"
      },
      {
        "name": "Dongliang Peng",
        "type": "Researcher"
      },
      {
        "name": "Xiaohui Yuan",
        "type": "Researcher"
      },
      {
        "name": "Aniv Chakravarty",
        "type": "Researcher"
      },
      {
        "name": "Elinor M. Lichtenberg",
        "type": "Researcher"
      },
      {
        "name": "Lichuan Gu",
        "type": "Researcher"
      },
      {
        "name": "Zhenchun Wei",
        "type": "Researcher"
      },
      {
        "name": "Tian Chen",
        "type": "Researcher"
      },
      {
        "name": "Yoshua Bengio",
        "type": "Researcher"
      },
      {
        "name": "Aaron Courville",
        "type": "Researcher"
      },
      {
        "name": "Pascal Vincent",
        "type": "Researcher"
      },
      {
        "name": "H. Larochelle",
        "type": "Researcher"
      },
      {
        "name": "Isabelle Lajoie",
        "type": "Researcher"
      },
      {
        "name": "Pierre-Antoine Manzagol",
        "type": "Researcher"
      },
      {
        "name": "D. Touretzky",
        "type": "Researcher"
      },
      {
        "name": "M. C. Mozer",
        "type": "Researcher"
      },
      {
        "name": "M. E. Hasselmo",
        "type": "Researcher"
      },
      {
        "name": "RegressionChristopher",
        "type": "Researcher"
      },
      {
        "name": "I. K.",
        "type": "Researcher"
      },
      {
        "name": "WilliamsNeural",
        "type": "Researcher"
      },
      {
        "name": "GroupAston",
        "type": "Researcher"
      },
      {
        "name": "UniversityBirmingham",
        "type": "Researcher"
      },
      {
        "name": "Danilo Jimenez Rezende",
        "type": "Researcher"
      },
      {
        "name": "S. Mohamed",
        "type": "Researcher"
      },
      {
        "name": "Daan Wierstra",
        "type": "Researcher"
      },
      {
        "name": "Shanchuan Lin",
        "type": "Researcher"
      },
      {
        "name": "Anran Wang",
        "type": "Researcher"
      },
      {
        "name": "Xiao Yang",
        "type": "Researcher"
      },
      {
        "name": "Zhiyuan Chen",
        "type": "Researcher"
      },
      {
        "name": "Jiajiong Cao",
        "type": "Researcher"
      },
      {
        "name": "Zhiquan Chen",
        "type": "Researcher"
      },
      {
        "name": "Yuming Li",
        "type": "Researcher"
      },
      {
        "name": "Chenguang Ma",
        "type": "Researcher"
      },
      {
        "name": "Wenzhao Zheng",
        "type": "Researcher"
      },
      {
        "name": "Ruiqi Song",
        "type": "Researcher"
      },
      {
        "name": "Xianda Guo",
        "type": "Researcher"
      },
      {
        "name": "Chenming Zhang",
        "type": "Researcher"
      },
      {
        "name": "Long Chen",
        "type": "Researcher"
      },
      {
        "name": "Shiyin Lu",
        "type": "Researcher"
      },
      {
        "name": "Yang Li",
        "type": "Researcher"
      },
      {
        "name": "Qing-Guo Chen",
        "type": "Researcher"
      },
      {
        "name": "Zhao Xu",
        "type": "Researcher"
      },
      {
        "name": "Weihua Luo",
        "type": "Researcher"
      },
      {
        "name": "Kaifu Zhang",
        "type": "Researcher"
      },
      {
        "name": "Han-Jia Ye",
        "type": "Researcher"
      },
      {
        "name": "Jie Liu",
        "type": "Researcher"
      },
      {
        "name": "Gongye Liu",
        "type": "Researcher"
      },
      {
        "name": "Jiajun Liang",
        "type": "Researcher"
      },
      {
        "name": "Ziyang Yuan",
        "type": "Researcher"
      },
      {
        "name": "Xiaokun Liu",
        "type": "Researcher"
      },
      {
        "name": "Mingwu Zheng",
        "type": "Researcher"
      },
      {
        "name": "Xiele Wu",
        "type": "Researcher"
      },
      {
        "name": "Qiulin Wang",
        "type": "Researcher"
      },
      {
        "name": "Menghan Xia",
        "type": "Researcher"
      },
      {
        "name": "Xintao Wang",
        "type": "Researcher"
      },
      {
        "name": "Xiaohong Liu",
        "type": "Researcher"
      },
      {
        "name": "Fei Yang",
        "type": "Researcher"
      },
      {
        "name": "Pengfei Wan",
        "type": "Researcher"
      },
      {
        "name": "Di Zhang",
        "type": "Researcher"
      },
      {
        "name": "Kun Gai",
        "type": "Researcher"
      },
      {
        "name": "Yujiu Yang",
        "type": "Researcher"
      },
      {
        "name": "Wanli Ouyang",
        "type": "Researcher"
      },
      {
        "name": "D. Rumelhart",
        "type": "Researcher"
      },
      {
        "name": "Ronald J. Williams",
        "type": "Researcher"
      },
      {
        "name": "M. Schuster",
        "type": "Researcher"
      },
      {
        "name": "K. Paliwal",
        "type": "Researcher"
      },
      {
        "name": "Santiago Fern´andez",
        "type": "Researcher"
      },
      {
        "name": "Faustino J. Gomez",
        "type": "Researcher"
      },
      {
        "name": "J¨urgen Schmidhuber",
        "type": "Researcher"
      },
      {
        "name": "Sean L. Metzger",
        "type": "Researcher"
      },
      {
        "name": "K. T. Littlejohn",
        "type": "Researcher"
      },
      {
        "name": "Alexander B. Silva",
        "type": "Researcher"
      },
      {
        "name": "D. Moses",
        "type": "Researcher"
      },
      {
        "name": "Margaret P. Seaton",
        "type": "Researcher"
      },
      {
        "name": "Ran Wang",
        "type": "Researcher"
      },
      {
        "name": "Maximilian E. Dougherty",
        "type": "Researcher"
      },
      {
        "name": "Jessie R. Liu",
        "type": "Researcher"
      },
      {
        "name": "Peter Wu",
        "type": "Researcher"
      },
      {
        "name": "M. Berger",
        "type": "Researcher"
      },
      {
        "name": "Inga Zhuravleva",
        "type": "Researcher"
      },
      {
        "name": "A. Tu-Chan",
        "type": "Researcher"
      },
      {
        "name": "K. Ganguly",
        "type": "Researcher"
      },
      {
        "name": "G. Anumanchipalli",
        "type": "Researcher"
      },
      {
        "name": "Edward F. Chang",
        "type": "Researcher"
      },
      {
        "name": "Tianming Sun",
        "type": "Researcher"
      },
      {
        "name": "Bin Feng",
        "type": "Researcher"
      },
      {
        "name": "Jinpeng Huo",
        "type": "Researcher"
      },
      {
        "name": "Yu Xiao",
        "type": "Researcher"
      },
      {
        "name": "Wengan Wang",
        "type": "Researcher"
      },
      {
        "name": "Jin Peng",
        "type": "Researcher"
      },
      {
        "name": "Zehua Li",
        "type": "Researcher"
      },
      {
        "name": "Chengjie Du",
        "type": "Researcher"
      },
      {
        "name": "Wenxian Wang",
        "type": "Researcher"
      },
      {
        "name": "G. Zou",
        "type": "Researcher"
      },
      {
        "name": "Lei Liu",
        "type": "Researcher"
      },
      {
        "name": "Francis R. Willett",
        "type": "Researcher"
      },
      {
        "name": "Erin M. Kunz",
        "type": "Researcher"
      },
      {
        "name": "Chaofei Fan",
        "type": "Researcher"
      },
      {
        "name": "Donald T. Avansino",
        "type": "Researcher"
      },
      {
        "name": "G. Wilson",
        "type": "Researcher"
      },
      {
        "name": "Eun Young Choi",
        "type": "Researcher"
      },
      {
        "name": "Foram B. Kamdar",
        "type": "Researcher"
      },
      {
        "name": "M. Glasser",
        "type": "Researcher"
      },
      {
        "name": "L. Hochberg",
        "type": "Researcher"
      },
      {
        "name": "S. Druckmann",
        "type": "Researcher"
      },
      {
        "name": "K. Shenoy",
        "type": "Researcher"
      },
      {
        "name": "J. Henderson",
        "type": "Researcher"
      },
      {
        "name": "Shibhansh Dohare",
        "type": "Researcher"
      },
      {
        "name": "J. F. Hernandez-Garcia",
        "type": "Researcher"
      },
      {
        "name": "Qingfeng Lan",
        "type": "Researcher"
      },
      {
        "name": "Parash Rahman",
        "type": "Researcher"
      },
      {
        "name": "A. Mahmood",
        "type": "Researcher"
      },
      {
        "name": "R. Sutton",
        "type": "Researcher"
      },
      {
        "name": "S. Ambrogio",
        "type": "Researcher"
      },
      {
        "name": "P. Narayanan",
        "type": "Researcher"
      },
      {
        "name": "A. Okazaki",
        "type": "Researcher"
      },
      {
        "name": "A. Fasoli",
        "type": "Researcher"
      },
      {
        "name": "C. Mackin",
        "type": "Researcher"
      },
      {
        "name": "K. Hosokawa",
        "type": "Researcher"
      },
      {
        "name": "A. Nomura",
        "type": "Researcher"
      },
      {
        "name": "Takeo Yasuda",
        "type": "Researcher"
      },
      {
        "name": "An Chen",
        "type": "Researcher"
      },
      {
        "name": "A. Friz",
        "type": "Researcher"
      },
      {
        "name": "M. Ishii",
        "type": "Researcher"
      },
      {
        "name": "J. Luquin",
        "type": "Researcher"
      },
      {
        "name": "Y. Kohda",
        "type": "Researcher"
      },
      {
        "name": "N. Saulnier",
        "type": "Researcher"
      },
      {
        "name": "K. Brew",
        "type": "Researcher"
      },
      {
        "name": "Samuel Choi",
        "type": "Researcher"
      },
      {
        "name": "I. Ok",
        "type": "Researcher"
      },
      {
        "name": "Timothy Philip",
        "type": "Researcher"
      },
      {
        "name": "Victor Chan",
        "type": "Researcher"
      },
      {
        "name": "M. Silvestre",
        "type": "Researcher"
      },
      {
        "name": "Ishtiaq Ahsan",
        "type": "Researcher"
      },
      {
        "name": "Vijay Narayanan",
        "type": "Researcher"
      },
      {
        "name": "H. Tsai",
        "type": "Researcher"
      },
      {
        "name": "Geoffrey W. Burr",
        "type": "Researcher"
      },
      {
        "name": "J. Shin",
        "type": "Researcher"
      },
      {
        "name": "Sang Joon Kim",
        "type": "Researcher"
      },
      {
        "name": "Yike Sun",
        "type": "Researcher"
      },
      {
        "name": "Haotong Yang",
        "type": "Researcher"
      },
      {
        "name": "Zhouchen Lin",
        "type": "Researcher"
      },
      {
        "name": "Muhan Zhang",
        "type": "Researcher"
      },
      {
        "name": "Ning Ding",
        "type": "Researcher"
      },
      {
        "name": "Fangcheng Liu",
        "type": "Researcher"
      },
      {
        "name": "Kyungrae Kim",
        "type": "Researcher"
      },
      {
        "name": "Linji Hao",
        "type": "Researcher"
      },
      {
        "name": "Kyeng-Hun Lee",
        "type": "Researcher"
      },
      {
        "name": "Hyeonmok Ko",
        "type": "Researcher"
      },
      {
        "name": "Yehui Tang",
        "type": "Researcher"
      },
      {
        "name": "Huinan Xu",
        "type": "Researcher"
      },
      {
        "name": "Xuyang Feng",
        "type": "Researcher"
      },
      {
        "name": "Junhong Chen",
        "type": "Researcher"
      },
      {
        "name": "Junchen Liu",
        "type": "Researcher"
      },
      {
        "name": "Kaiwen Deng",
        "type": "Researcher"
      },
      {
        "name": "Kai Ding",
        "type": "Researcher"
      },
      {
        "name": "Shengning Long",
        "type": "Researcher"
      },
      {
        "name": "Jiaxue Shuai",
        "type": "Researcher"
      },
      {
        "name": "Zhaorong Li",
        "type": "Researcher"
      },
      {
        "name": "Shiping Liu",
        "type": "Researcher"
      },
      {
        "name": "Guirong Xue",
        "type": "Researcher"
      },
      {
        "name": "Zhan Xiao",
        "type": "Researcher"
      },
      {
        "name": "Albert Tseng",
        "type": "Researcher"
      },
      {
        "name": "Christopher De Sa",
        "type": "Researcher"
      },
      {
        "name": "Hong Liu",
        "type": "Researcher"
      },
      {
        "name": "Jiaqi Zhang",
        "type": "Researcher"
      },
      {
        "name": "Chao Wang",
        "type": "Researcher"
      },
      {
        "name": "Xing Hu",
        "type": "Researcher"
      },
      {
        "name": "Linkun Lyu",
        "type": "Researcher"
      },
      {
        "name": "Jiaqi Sun",
        "type": "Researcher"
      },
      {
        "name": "Xurui Yang",
        "type": "Researcher"
      },
      {
        "name": "Bo Wang",
        "type": "Researcher"
      },
      {
        "name": "Fengcun Li",
        "type": "Researcher"
      },
      {
        "name": "Yulei Qian",
        "type": "Researcher"
      },
      {
        "name": "Lingtong Si",
        "type": "Researcher"
      },
      {
        "name": "Yerui Sun",
        "type": "Researcher"
      },
      {
        "name": "Rumei Li",
        "type": "Researcher"
      },
      {
        "name": "Peng Pei",
        "type": "Researcher"
      },
      {
        "name": "Yuchen Xie",
        "type": "Researcher"
      },
      {
        "name": "Xunliang Cai",
        "type": "Researcher"
      },
      {
        "name": "R. Tibshirani",
        "type": "Researcher"
      },
      {
        "name": "Herve Goeau",
        "type": "Researcher"
      },
      {
        "name": "Pierre Bonnet",
        "type": "Researcher"
      },
      {
        "name": "Alexis Joly",
        "type": "Researcher"
      },
      {
        "name": "Safa Ben Atitallah",
        "type": "Researcher"
      },
      {
        "name": "Maha Driss",
        "type": "Researcher"
      },
      {
        "name": "Henda Ben Ghezela",
        "type": "Researcher"
      },
      {
        "name": "Sareer Ul Amin",
        "type": "Researcher"
      },
      {
        "name": "Yonghoon Jung",
        "type": "Researcher"
      },
      {
        "name": "Muhammad Fayaz",
        "type": "Researcher"
      },
      {
        "name": "Bumsoo Kim",
        "type": "Researcher"
      },
      {
        "name": "Sanghyun Seo",
        "type": "Researcher"
      },
      {
        "name": "Ayşe Aybilge Murat",
        "type": "Researcher"
      },
      {
        "name": "M. S. Kıran",
        "type": "Researcher"
      },
      {
        "name": "Hongyan Zhu",
        "type": "Researcher"
      },
      {
        "name": "Shuai Qin",
        "type": "Researcher"
      },
      {
        "name": "Min Su",
        "type": "Researcher"
      },
      {
        "name": "Chengzhi Lin",
        "type": "Researcher"
      },
      {
        "name": "Anjie Li",
        "type": "Researcher"
      },
      {
        "name": "Junfeng Gao",
        "type": "Researcher"
      },
      {
        "name": "Abdul Rehman Khan",
        "type": "Researcher"
      },
      {
        "name": "Asifullah Khan",
        "type": "Researcher"
      },
      {
        "name": "D. E. Boukhari",
        "type": "Researcher"
      },
      {
        "name": "F. Dornaika",
        "type": "Researcher"
      },
      {
        "name": "A. Chemsa",
        "type": "Researcher"
      },
      {
        "name": "Abdelmalik Taleb-Ahmed",
        "type": "Researcher"
      },
      {
        "name": "Michaela Vystrčilová",
        "type": "Researcher"
      },
      {
        "name": "Shashwat Sridhar",
        "type": "Researcher"
      },
      {
        "name": "Max F. Burg",
        "type": "Researcher"
      },
      {
        "name": "M. Khani",
        "type": "Researcher"
      },
      {
        "name": "Dimokratis Karamanlis",
        "type": "Researcher"
      },
      {
        "name": "H. Schreyer",
        "type": "Researcher"
      },
      {
        "name": "Varsha Ramakrishna",
        "type": "Researcher"
      },
      {
        "name": "Steffen Krüppel",
        "type": "Researcher"
      },
      {
        "name": "Sören J. Zapp",
        "type": "Researcher"
      },
      {
        "name": "Matthias Mietsch",
        "type": "Researcher"
      },
      {
        "name": "T. Gollisch",
        "type": "Researcher"
      },
      {
        "name": "Alexander S. Ecker",
        "type": "Researcher"
      },
      {
        "name": "D. Lowe",
        "type": "Researcher"
      },
      {
        "name": "Xiang An",
        "type": "Researcher"
      },
      {
        "name": "Yin Xie",
        "type": "Researcher"
      },
      {
        "name": "Kaicheng Yang",
        "type": "Researcher"
      },
      {
        "name": "Wenkang Zhang",
        "type": "Researcher"
      },
      {
        "name": "Xiuwei Zhao",
        "type": "Researcher"
      },
      {
        "name": "Zheng Cheng",
        "type": "Researcher"
      },
      {
        "name": "Yirui Wang",
        "type": "Researcher"
      },
      {
        "name": "Songcen Xu",
        "type": "Researcher"
      },
      {
        "name": "Changrui Chen",
        "type": "Researcher"
      },
      {
        "name": "Didi Zhu",
        "type": "Researcher"
      },
      {
        "name": "Chunsheng Wu",
        "type": "Researcher"
      },
      {
        "name": "Huajie Tan",
        "type": "Researcher"
      },
      {
        "name": "Chunyuan Li",
        "type": "Researcher"
      },
      {
        "name": "Jing Yang",
        "type": "Researcher"
      },
      {
        "name": "Jie Yu",
        "type": "Researcher"
      },
      {
        "name": "Xiyao Wang",
        "type": "Researcher"
      },
      {
        "name": "Bin Qin",
        "type": "Researcher"
      },
      {
        "name": "Yumeng Wang",
        "type": "Researcher"
      },
      {
        "name": "Zizhen Yan",
        "type": "Researcher"
      },
      {
        "name": "Ziyong Feng",
        "type": "Researcher"
      },
      {
        "name": "Ziwei Liu",
        "type": "Researcher"
      },
      {
        "name": "Bo Li",
        "type": "Researcher"
      },
      {
        "name": "Jiankang Deng",
        "type": "Researcher"
      },
      {
        "name": "Kento Kawaharazuka",
        "type": "Researcher"
      },
      {
        "name": "Jihoon Oh",
        "type": "Researcher"
      },
      {
        "name": "Jun Yamada",
        "type": "Researcher"
      },
      {
        "name": "Ingmar Posner",
        "type": "Researcher"
      },
      {
        "name": "Yuke Zhu",
        "type": "Researcher"
      },
      {
        "name": "Lukas Muttenthaler",
        "type": "Researcher"
      },
      {
        "name": "Klaus Greff",
        "type": "Researcher"
      },
      {
        "name": "Frieda Born",
        "type": "Researcher"
      },
      {
        "name": "Bernhard Spitzer",
        "type": "Researcher"
      },
      {
        "name": "Simon Kornblith",
        "type": "Researcher"
      },
      {
        "name": "Klaus-Robert Muller",
        "type": "Researcher"
      },
      {
        "name": "Andrew Kyle Lampinen",
        "type": "Researcher"
      },
      {
        "name": "Lucas Beyer",
        "type": "Researcher"
      },
      {
        "name": "Alexander Kolesnikov",
        "type": "Researcher"
      },
      {
        "name": "Dirk Weissenborn",
        "type": "Researcher"
      },
      {
        "name": "Xiaohua Zhai",
        "type": "Researcher"
      },
      {
        "name": "Mostafa Dehghani",
        "type": "Researcher"
      },
      {
        "name": "Matthias Minderer",
        "type": "Researcher"
      },
      {
        "name": "Georg Heigold",
        "type": "Researcher"
      },
      {
        "name": "Sylvain Gelly",
        "type": "Researcher"
      },
      {
        "name": "Neil Houlsby",
        "type": "Researcher"
      },
      {
        "name": "Alec Radford",
        "type": "Researcher"
      },
      {
        "name": "Jong Wook Kim",
        "type": "Researcher"
      },
      {
        "name": "Chris Hallacy",
        "type": "Researcher"
      },
      {
        "name": "Aditya Ramesh",
        "type": "Researcher"
      },
      {
        "name": "Gabriel Goh",
        "type": "Researcher"
      },
      {
        "name": "Sandhini Agarwal",
        "type": "Researcher"
      },
      {
        "name": "Girish Sastry",
        "type": "Researcher"
      },
      {
        "name": "Amanda Askell",
        "type": "Researcher"
      },
      {
        "name": "Pamela Mishkin",
        "type": "Researcher"
      },
      {
        "name": "Jack Clark",
        "type": "Researcher"
      },
      {
        "name": "Gretchen Krueger",
        "type": "Researcher"
      },
      {
        "name": "Ilya Sutskever",
        "type": "Researcher"
      },
      {
        "name": "Individualized Treat",
        "type": "Researcher"
      },
      {
        "name": "Jinsung Yoon",
        "type": "Researcher"
      },
      {
        "name": "Zhengyang Geng",
        "type": "Researcher"
      },
      {
        "name": "Yiyang Lu",
        "type": "Researcher"
      },
      {
        "name": "J. Zico Kolter",
        "type": "Researcher"
      },
      {
        "name": "Jiachen Lei",
        "type": "Researcher"
      },
      {
        "name": "Keli Liu",
        "type": "Researcher"
      },
      {
        "name": "Julius Berner",
        "type": "Researcher"
      },
      {
        "name": "Haiming Yu",
        "type": "Researcher"
      },
      {
        "name": "Hongkai Zheng",
        "type": "Researcher"
      },
      {
        "name": "Jiahong Wu",
        "type": "Researcher"
      },
      {
        "name": "Xiangxiang Chu",
        "type": "Researcher"
      },
      {
        "name": "Minglei Shi",
        "type": "Researcher"
      },
      {
        "name": "Haolin Wang",
        "type": "Researcher"
      },
      {
        "name": "Borui Zhang",
        "type": "Researcher"
      },
      {
        "name": "Bohan Zeng",
        "type": "Researcher"
      },
      {
        "name": "Xiaoshi Wu",
        "type": "Researcher"
      },
      {
        "name": "Yuanxing Zhang",
        "type": "Researcher"
      },
      {
        "name": "Huan Yang",
        "type": "Researcher"
      },
      {
        "name": "Jie Zhou",
        "type": "Researcher"
      },
      {
        "name": "Jiwen Lu",
        "type": "Researcher"
      },
      {
        "name": "Yongsheng Yu",
        "type": "Researcher"
      },
      {
        "name": "Wei Xiong",
        "type": "Researcher"
      },
      {
        "name": "Weili Nie",
        "type": "Researcher"
      },
      {
        "name": "Yichen Sheng",
        "type": "Researcher"
      },
      {
        "name": "Shiqiu Liu",
        "type": "Researcher"
      },
      {
        "name": "Jiebo Luo",
        "type": "Researcher"
      },
      {
        "name": "Zhiheng Liu",
        "type": "Researcher"
      },
      {
        "name": "Weiming Ren",
        "type": "Researcher"
      },
      {
        "name": "Haozhe Liu",
        "type": "Researcher"
      },
      {
        "name": "Zijian Zhou",
        "type": "Researcher"
      },
      {
        "name": "Shoufa Chen",
        "type": "Researcher"
      },
      {
        "name": "Haonan Qiu",
        "type": "Researcher"
      },
      {
        "name": "Xiaoke Huang",
        "type": "Researcher"
      },
      {
        "name": "Zhaochong An",
        "type": "Researcher"
      },
      {
        "name": "Fanny Yang",
        "type": "Researcher"
      },
      {
        "name": "Aditya Patel",
        "type": "Researcher"
      },
      {
        "name": "Viktar Atliha",
        "type": "Researcher"
      },
      {
        "name": "Tony Ng",
        "type": "Researcher"
      },
      {
        "name": "Xiao Han",
        "type": "Researcher"
      },
      {
        "name": "Chuyan Zhu",
        "type": "Researcher"
      },
      {
        "name": "Chenyang Zhang",
        "type": "Researcher"
      },
      {
        "name": "Ding Liu",
        "type": "Researcher"
      },
      {
        "name": "Juan-Manuel Perez-Rua",
        "type": "Researcher"
      },
      {
        "name": "Sen He",
        "type": "Researcher"
      },
      {
        "name": "Jürgen Schmidhuber",
        "type": "Researcher"
      },
      {
        "name": "Wenhu Chen",
        "type": "Researcher"
      },
      {
        "name": "Ping Luo",
        "type": "Researcher"
      },
      {
        "name": "Tao Xiang",
        "type": "Researcher"
      },
      {
        "name": "Jonas Schult",
        "type": "Researcher"
      },
      {
        "name": "Yuren Cong",
        "type": "Researcher"
      },
      {
        "name": "Jacob Devlin",
        "type": "Researcher"
      },
      {
        "name": "Ming-Wei Chang",
        "type": "Researcher"
      },
      {
        "name": "Kenton Lee",
        "type": "Researcher"
      },
      {
        "name": "Kristina Toutanova",
        "type": "Researcher"
      },
      {
        "name": "Vrushali Pagire",
        "type": "Researcher"
      },
      {
        "name": "M. Chavali",
        "type": "Researcher"
      },
      {
        "name": "Ashish Kale",
        "type": "Researcher"
      },
      {
        "name": "Jinjie Ni",
        "type": "Researcher"
      },
      {
        "name": "Qian Liu",
        "type": "Researcher"
      },
      {
        "name": "Longxu Dou",
        "type": "Researcher"
      },
      {
        "name": "Chao Du",
        "type": "Researcher"
      },
      {
        "name": "Zili Wang",
        "type": "Researcher"
      },
      {
        "name": "Hang Yan",
        "type": "Researcher"
      },
      {
        "name": "Tianyu Pang",
        "type": "Researcher"
      },
      {
        "name": "Michael Qizhe Shieh",
        "type": "Researcher"
      },
      {
        "name": "Zirui Wu",
        "type": "Researcher"
      },
      {
        "name": "Lin Zheng",
        "type": "Researcher"
      },
      {
        "name": "Zhihui Xie",
        "type": "Researcher"
      },
      {
        "name": "Jiacheng Ye",
        "type": "Researcher"
      },
      {
        "name": "Jiahui Gao",
        "type": "Researcher"
      },
      {
        "name": "Shansan Gong",
        "type": "Researcher"
      },
      {
        "name": "Yansong Feng",
        "type": "Researcher"
      },
      {
        "name": "Zhenguo Li",
        "type": "Researcher"
      },
      {
        "name": "Wei Bi",
        "type": "Researcher"
      },
      {
        "name": "Guorui Zhou",
        "type": "Researcher"
      },
      {
        "name": "Lingpeng Kong",
        "type": "Researcher"
      },
      {
        "name": "Zhicheng Cai",
        "type": "Researcher"
      },
      {
        "name": "Xinyuan Guo",
        "type": "Researcher"
      },
      {
        "name": "Yu Pei",
        "type": "Researcher"
      },
      {
        "name": "Jiangtao Feng",
        "type": "Researcher"
      },
      {
        "name": "Jinsong Su",
        "type": "Researcher"
      },
      {
        "name": "Jiangjie Chen",
        "type": "Researcher"
      },
      {
        "name": "Ya-Qin Zhang",
        "type": "Researcher"
      },
      {
        "name": "Wei-Ying Ma",
        "type": "Researcher"
      },
      {
        "name": "Mingxuan Wang",
        "type": "Researcher"
      },
      {
        "name": "Hao Zhou",
        "type": "Researcher"
      },
      {
        "name": "Mingyue Cheng",
        "type": "Researcher"
      },
      {
        "name": "Jie Ouyang",
        "type": "Researcher"
      },
      {
        "name": "Shuo Yu",
        "type": "Researcher"
      },
      {
        "name": "Ruiran Yan",
        "type": "Researcher"
      },
      {
        "name": "Yucong Luo",
        "type": "Researcher"
      },
      {
        "name": "Zirui Liu",
        "type": "Researcher"
      },
      {
        "name": "Daoyu Wang",
        "type": "Researcher"
      },
      {
        "name": "Qi Liu",
        "type": "Researcher"
      },
      {
        "name": "Enhong Chen",
        "type": "Researcher"
      },
      {
        "name": "Navneet Dalal",
        "type": "Researcher"
      },
      {
        "name": "B. Triggs",
        "type": "Researcher"
      },
      {
        "name": "Yue Ma",
        "type": "Researcher"
      },
      {
        "name": "Kunyu Feng",
        "type": "Researcher"
      },
      {
        "name": "Zhongyuan Hu",
        "type": "Researcher"
      },
      {
        "name": "Xinyu Wang",
        "type": "Researcher"
      },
      {
        "name": "Yucheng Wang",
        "type": "Researcher"
      },
      {
        "name": "Mingzhe Zheng",
        "type": "Researcher"
      },
      {
        "name": "Bingyuan Wang",
        "type": "Researcher"
      },
      {
        "name": "Qinghe Wang",
        "type": "Researcher"
      },
      {
        "name": "Xuanhua He",
        "type": "Researcher"
      },
      {
        "name": "Hongfa Wang",
        "type": "Researcher"
      },
      {
        "name": "Chenyang Zhu",
        "type": "Researcher"
      },
      {
        "name": "Hongyu Liu",
        "type": "Researcher"
      },
      {
        "name": "Yingqing He",
        "type": "Researcher"
      },
      {
        "name": "Zeyu Wang",
        "type": "Researcher"
      },
      {
        "name": "Zhifeng Li",
        "type": "Researcher"
      },
      {
        "name": "Xiu Li",
        "type": "Researcher"
      },
      {
        "name": "Sirui Han",
        "type": "Researcher"
      },
      {
        "name": "Yike Guo",
        "type": "Researcher"
      },
      {
        "name": "Dan Xu",
        "type": "Researcher"
      },
      {
        "name": "Linfeng Zhang",
        "type": "Researcher"
      },
      {
        "name": "Qifeng Chen",
        "type": "Researcher"
      },
      {
        "name": "Lijun Chi",
        "type": "Researcher"
      },
      {
        "name": "M. Msahli",
        "type": "Researcher"
      },
      {
        "name": "Qingjie Zhang",
        "type": "Researcher"
      },
      {
        "name": "Han Qiu",
        "type": "Researcher"
      },
      {
        "name": "Tianwei Zhang",
        "type": "Researcher"
      },
      {
        "name": "Gérard Memmi",
        "type": "Researcher"
      },
      {
        "name": "Meikang Qiu",
        "type": "Researcher"
      },
      {
        "name": "NVIDIA",
        "type": "Researcher"
      },
      {
        "name": ":",
        "type": "Researcher"
      },
      {
        "name": "Yan Wang",
        "type": "Researcher"
      },
      {
        "name": "Wenjie Luo",
        "type": "Researcher"
      },
      {
        "name": "Junjie Bai",
        "type": "Researcher"
      },
      {
        "name": "Yulong Cao",
        "type": "Researcher"
      },
      {
        "name": "Tong Che",
        "type": "Researcher"
      },
      {
        "name": "Ke Chen",
        "type": "Researcher"
      },
      {
        "name": "Yuxiao Chen",
        "type": "Researcher"
      },
      {
        "name": "Jenna Diamond",
        "type": "Researcher"
      },
      {
        "name": "Yifan Ding",
        "type": "Researcher"
      },
      {
        "name": "Wenhao Ding",
        "type": "Researcher"
      },
      {
        "name": "Liang Feng",
        "type": "Researcher"
      },
      {
        "name": "Greg Heinrich",
        "type": "Researcher"
      },
      {
        "name": "Jack Huang",
        "type": "Researcher"
      },
      {
        "name": "Peter Karkus",
        "type": "Researcher"
      },
      {
        "name": "Boyi Li",
        "type": "Researcher"
      },
      {
        "name": "Pinyi Li",
        "type": "Researcher"
      },
      {
        "name": "Dongran Liu",
        "type": "Researcher"
      },
      {
        "name": "Ming-Yu Liu",
        "type": "Researcher"
      },
      {
        "name": "Langechuan Liu",
        "type": "Researcher"
      },
      {
        "name": "Zhijian Liu",
        "type": "Researcher"
      },
      {
        "name": "Jason Lu",
        "type": "Researcher"
      },
      {
        "name": "Yunxiang Mao",
        "type": "Researcher"
      },
      {
        "name": "Pavlo Molchanov",
        "type": "Researcher"
      },
      {
        "name": "Lindsey Pavao",
        "type": "Researcher"
      },
      {
        "name": "Zhenghao Peng",
        "type": "Researcher"
      },
      {
        "name": "Mike Ranzinger",
        "type": "Researcher"
      },
      {
        "name": "Ed Schmerling",
        "type": "Researcher"
      },
      {
        "name": "Shida Shen",
        "type": "Researcher"
      },
      {
        "name": "Yunfei Shi",
        "type": "Researcher"
      },
      {
        "name": "Sarah Tariq",
        "type": "Researcher"
      },
      {
        "name": "Ran Tian",
        "type": "Researcher"
      },
      {
        "name": "Tilman Wekel",
        "type": "Researcher"
      },
      {
        "name": "Xinshuo Weng",
        "type": "Researcher"
      },
      {
        "name": "Tianjun Xiao",
        "type": "Researcher"
      },
      {
        "name": "Eric Yang",
        "type": "Researcher"
      },
      {
        "name": "Xiaodong Yang",
        "type": "Researcher"
      },
      {
        "name": "Yurong You",
        "type": "Researcher"
      },
      {
        "name": "Xiaohui Zeng",
        "type": "Researcher"
      },
      {
        "name": "Wenyuan Zhang",
        "type": "Researcher"
      },
      {
        "name": "Boris Ivanovic",
        "type": "Researcher"
      },
      {
        "name": "Marco Pavone",
        "type": "Researcher"
      },
      {
        "name": "Mohsen Gholami",
        "type": "Researcher"
      },
      {
        "name": "Ahmad Rezaei",
        "type": "Researcher"
      },
      {
        "name": "Zhou Weimin",
        "type": "Researcher"
      },
      {
        "name": "Sitong Mao",
        "type": "Researcher"
      },
      {
        "name": "Shunbo Zhou",
        "type": "Researcher"
      },
      {
        "name": "Yong Zhang",
        "type": "Researcher"
      },
      {
        "name": "Mohammad Akbari",
        "type": "Researcher"
      },
      {
        "name": "Qing Jiang",
        "type": "Researcher"
      },
      {
        "name": "Junan Huo",
        "type": "Researcher"
      },
      {
        "name": "Xingyu Chen",
        "type": "Researcher"
      },
      {
        "name": "Yuda Xiong",
        "type": "Researcher"
      },
      {
        "name": "Zhaoyang Zeng",
        "type": "Researcher"
      },
      {
        "name": "Yihao Chen",
        "type": "Researcher"
      },
      {
        "name": "Tianhe Ren",
        "type": "Researcher"
      },
      {
        "name": "Junzhi Yu",
        "type": "Researcher"
      },
      {
        "name": "Lei Zhang",
        "type": "Researcher"
      },
      {
        "name": "Volodymyr Mnih",
        "type": "Researcher"
      },
      {
        "name": "K. Kavukcuoglu",
        "type": "Researcher"
      },
      {
        "name": "David Silver",
        "type": "Researcher"
      },
      {
        "name": "Andrei A. Rusu",
        "type": "Researcher"
      },
      {
        "name": "J. Veness",
        "type": "Researcher"
      },
      {
        "name": "Marc G. Bellemare",
        "type": "Researcher"
      },
      {
        "name": "Martin A. Riedmiller",
        "type": "Researcher"
      },
      {
        "name": "A. Fidjeland",
        "type": "Researcher"
      },
      {
        "name": "Georg Ostrovski",
        "type": "Researcher"
      },
      {
        "name": "Stig Petersen",
        "type": "Researcher"
      },
      {
        "name": "Charlie Beattie",
        "type": "Researcher"
      },
      {
        "name": "Amir Sadik",
        "type": "Researcher"
      },
      {
        "name": "Ioannis Antonoglou",
        "type": "Researcher"
      },
      {
        "name": "Helen King",
        "type": "Researcher"
      },
      {
        "name": "D. Kumaran",
        "type": "Researcher"
      },
      {
        "name": "S. Legg",
        "type": "Researcher"
      },
      {
        "name": "D. Hassabis",
        "type": "Researcher"
      },
      {
        "name": "Adrià Puigdomènech Badia",
        "type": "Researcher"
      },
      {
        "name": "Mehdi Mirza",
        "type": "Researcher"
      },
      {
        "name": "Timothy P. Lillicrap",
        "type": "Researcher"
      },
      {
        "name": "Tim Harley",
        "type": "Researcher"
      },
      {
        "name": "Koray Kavukcuoglu",
        "type": "Researcher"
      },
      {
        "name": "Guosheng Lin",
        "type": "Researcher"
      },
      {
        "name": "Anton Milan",
        "type": "Researcher"
      },
      {
        "name": "Chunhua Shen",
        "type": "Researcher"
      },
      {
        "name": "Ian Reid",
        "type": "Researcher"
      },
      {
        "name": "Zhijie Qiao",
        "type": "Researcher"
      },
      {
        "name": "Haowei Li",
        "type": "Researcher"
      },
      {
        "name": "Zhong Cao",
        "type": "Researcher"
      },
      {
        "name": "Henry X. Liu",
        "type": "Researcher"
      },
      {
        "name": "Yongkang Li",
        "type": "Researcher"
      },
      {
        "name": "Kaixin Xiong",
        "type": "Researcher"
      },
      {
        "name": "Xiangyu Guo",
        "type": "Researcher"
      },
      {
        "name": "Fang Li",
        "type": "Researcher"
      },
      {
        "name": "Sixu Yan",
        "type": "Researcher"
      },
      {
        "name": "Gangwei Xu",
        "type": "Researcher"
      },
      {
        "name": "Lijun Zhou",
        "type": "Researcher"
      },
      {
        "name": "Haiyang Sun",
        "type": "Researcher"
      },
      {
        "name": "Bing Wang",
        "type": "Researcher"
      },
      {
        "name": "Kun Ma",
        "type": "Researcher"
      },
      {
        "name": "Guang Chen",
        "type": "Researcher"
      },
      {
        "name": "Hangjun Ye",
        "type": "Researcher"
      },
      {
        "name": "Wenyu Liu",
        "type": "Researcher"
      },
      {
        "name": "Xinggang Wang",
        "type": "Researcher"
      },
      {
        "name": "Wei Cao",
        "type": "Researcher"
      },
      {
        "name": "Marcel Hallgarten",
        "type": "Researcher"
      },
      {
        "name": "Tianyu Li",
        "type": "Researcher"
      },
      {
        "name": "Daniel Dauner",
        "type": "Researcher"
      },
      {
        "name": "Xunjiang Gu",
        "type": "Researcher"
      },
      {
        "name": "Caojun Wang",
        "type": "Researcher"
      },
      {
        "name": "Yakov Miron",
        "type": "Researcher"
      },
      {
        "name": "Marco Aiello",
        "type": "Researcher"
      },
      {
        "name": "Hongyang Li",
        "type": "Researcher"
      },
      {
        "name": "Igor Gilitschenski",
        "type": "Researcher"
      },
      {
        "name": "Andreas Geiger",
        "type": "Researcher"
      },
      {
        "name": "Kashyap Chitta",
        "type": "Researcher"
      },
      {
        "name": "Zhenjie Yang",
        "type": "Researcher"
      },
      {
        "name": "Yilin Chai",
        "type": "Researcher"
      },
      {
        "name": "Xiaosong Jia",
        "type": "Researcher"
      },
      {
        "name": "Qifeng Li",
        "type": "Researcher"
      },
      {
        "name": "Yuqian Shao",
        "type": "Researcher"
      },
      {
        "name": "Xuekai Zhu",
        "type": "Researcher"
      },
      {
        "name": "Haisheng Su",
        "type": "Researcher"
      },
      {
        "name": "Junchi Yan",
        "type": "Researcher"
      },
      {
        "name": "Sicong Jiang",
        "type": "Researcher"
      },
      {
        "name": "Zilin Huang",
        "type": "Researcher"
      },
      {
        "name": "Kangan Qian",
        "type": "Researcher"
      },
      {
        "name": "Ziang Luo",
        "type": "Researcher"
      },
      {
        "name": "Tianze Zhu",
        "type": "Researcher"
      },
      {
        "name": "Yang Zhong",
        "type": "Researcher"
      },
      {
        "name": "Yihong Tang",
        "type": "Researcher"
      },
      {
        "name": "Menglin Kong",
        "type": "Researcher"
      },
      {
        "name": "Yunlong Wang",
        "type": "Researcher"
      },
      {
        "name": "Siwen Jiao",
        "type": "Researcher"
      },
      {
        "name": "Hao Ye",
        "type": "Researcher"
      },
      {
        "name": "Zihao Sheng",
        "type": "Researcher"
      },
      {
        "name": "Xin Zhao",
        "type": "Researcher"
      },
      {
        "name": "Tuopu Wen",
        "type": "Researcher"
      },
      {
        "name": "Zheng Fu",
        "type": "Researcher"
      },
      {
        "name": "Sikai Chen",
        "type": "Researcher"
      },
      {
        "name": "Kun Jiang",
        "type": "Researcher"
      },
      {
        "name": "Diange Yang",
        "type": "Researcher"
      },
      {
        "name": "Seongjin Choi",
        "type": "Researcher"
      },
      {
        "name": "Lijun Sun",
        "type": "Researcher"
      },
      {
        "name": "M. Page",
        "type": "Researcher"
      },
      {
        "name": "J. McKenzie",
        "type": "Researcher"
      },
      {
        "name": "P. Bossuyt",
        "type": "Researcher"
      },
      {
        "name": "I. Boutron",
        "type": "Researcher"
      },
      {
        "name": "T. Hoffmann",
        "type": "Researcher"
      },
      {
        "name": "C. Mulrow",
        "type": "Researcher"
      },
      {
        "name": "Larissa Shamseer",
        "type": "Researcher"
      },
      {
        "name": "J. Tetzlaff",
        "type": "Researcher"
      },
      {
        "name": "E. Akl",
        "type": "Researcher"
      },
      {
        "name": "S. Brennan",
        "type": "Researcher"
      },
      {
        "name": "R. Chou",
        "type": "Researcher"
      },
      {
        "name": "Julie May Glanville",
        "type": "Researcher"
      },
      {
        "name": "J. Grimshaw",
        "type": "Researcher"
      },
      {
        "name": "A. Hrõbjartsson",
        "type": "Researcher"
      },
      {
        "name": "M. Lalu",
        "type": "Researcher"
      },
      {
        "name": "Tianjing Li",
        "type": "Researcher"
      },
      {
        "name": "E. Loder",
        "type": "Researcher"
      },
      {
        "name": "E. Mayo-Wilson",
        "type": "Researcher"
      },
      {
        "name": "Steve McDonald",
        "type": "Researcher"
      },
      {
        "name": "L. McGuinness",
        "type": "Researcher"
      },
      {
        "name": "L. Stewart",
        "type": "Researcher"
      },
      {
        "name": "James Thomas",
        "type": "Researcher"
      },
      {
        "name": "A. Tricco",
        "type": "Researcher"
      },
      {
        "name": "V. Welch",
        "type": "Researcher"
      },
      {
        "name": "P. Whiting",
        "type": "Researcher"
      },
      {
        "name": "D. Moher",
        "type": "Researcher"
      },
      {
        "name": "Wangbo Zhao",
        "type": "Researcher"
      },
      {
        "name": "Chen Min",
        "type": "Researcher"
      },
      {
        "name": "Nianchen Deng",
        "type": "Researcher"
      },
      {
        "name": "Min Dou",
        "type": "Researcher"
      },
      {
        "name": "Yuqi Wang",
        "type": "Researcher"
      },
      {
        "name": "Botian Shi",
        "type": "Researcher"
      },
      {
        "name": "Kai Wang",
        "type": "Researcher"
      },
      {
        "name": "Chi Zhang",
        "type": "Researcher"
      },
      {
        "name": "Yang You",
        "type": "Researcher"
      },
      {
        "name": "Zhaoxiang Zhang",
        "type": "Researcher"
      },
      {
        "name": "Dawei Zhao",
        "type": "Researcher"
      },
      {
        "name": "Liang Xiao",
        "type": "Researcher"
      },
      {
        "name": "Jian Zhao",
        "type": "Researcher"
      },
      {
        "name": "Jingtao Ding",
        "type": "Researcher"
      },
      {
        "name": "Yunke Zhang",
        "type": "Researcher"
      },
      {
        "name": "Yu Shang",
        "type": "Researcher"
      },
      {
        "name": "Jie Feng",
        "type": "Researcher"
      },
      {
        "name": "Yuheng Zhang",
        "type": "Researcher"
      },
      {
        "name": "Zefang Zong",
        "type": "Researcher"
      },
      {
        "name": "Yuan Yuan",
        "type": "Researcher"
      },
      {
        "name": "Hongyuan Su",
        "type": "Researcher"
      },
      {
        "name": "Nian Li",
        "type": "Researcher"
      },
      {
        "name": "Jinghua Piao",
        "type": "Researcher"
      },
      {
        "name": "Yucheng Deng",
        "type": "Researcher"
      },
      {
        "name": "Nicholas Sukiennik",
        "type": "Researcher"
      },
      {
        "name": "Chen Gao",
        "type": "Researcher"
      },
      {
        "name": "Fengli Xu",
        "type": "Researcher"
      },
      {
        "name": "Yong Li",
        "type": "Researcher"
      },
      {
        "name": "Zhaojian Li",
        "type": "Researcher"
      },
      {
        "name": "Bin Zhao",
        "type": "Researcher"
      },
      {
        "name": "Xuannan Liu",
        "type": "Researcher"
      },
      {
        "name": "Xing Cui",
        "type": "Researcher"
      },
      {
        "name": "Peipei Li",
        "type": "Researcher"
      },
      {
        "name": "Zekun Li",
        "type": "Researcher"
      },
      {
        "name": "Huaibo Huang",
        "type": "Researcher"
      },
      {
        "name": "Shuhan Xia",
        "type": "Researcher"
      },
      {
        "name": "Miaoxuan Zhang",
        "type": "Researcher"
      },
      {
        "name": "Yueying Zou",
        "type": "Researcher"
      },
      {
        "name": "Ran He",
        "type": "Researcher"
      },
      {
        "name": "Galina Ilieva",
        "type": "Researcher"
      },
      {
        "name": "Tania Yankova",
        "type": "Researcher"
      },
      {
        "name": "Stanislava Klisarova-Belcheva",
        "type": "Researcher"
      },
      {
        "name": "Jie Hu",
        "type": "Researcher"
      },
      {
        "name": "Li Shen",
        "type": "Researcher"
      },
      {
        "name": "Samuel Albanie",
        "type": "Researcher"
      },
      {
        "name": "Gang Sun",
        "type": "Researcher"
      },
      {
        "name": "Enhua Wu",
        "type": "Researcher"
      },
      {
        "name": "I. Loshchilov",
        "type": "Researcher"
      },
      {
        "name": "F. Hutter",
        "type": "Researcher"
      },
      {
        "name": "Mingxing Tan",
        "type": "Researcher"
      },
      {
        "name": "Quoc V. Le",
        "type": "Researcher"
      },
      {
        "name": "Tianqi Liu",
        "type": "Researcher"
      },
      {
        "name": "Zhaoxi Chen",
        "type": "Researcher"
      },
      {
        "name": "Zihao Huang",
        "type": "Researcher"
      },
      {
        "name": "Shaocong Xu",
        "type": "Researcher"
      },
      {
        "name": "Saining Zhang",
        "type": "Researcher"
      },
      {
        "name": "Chongjie Ye",
        "type": "Researcher"
      },
      {
        "name": "Zhiguo Cao",
        "type": "Researcher"
      },
      {
        "name": "Tianze Xia",
        "type": "Researcher"
      },
      {
        "name": "Jingfeng Yao",
        "type": "Researcher"
      },
      {
        "name": "Sicheng Zuo",
        "type": "Researcher"
      },
      {
        "name": "Zixun Xie",
        "type": "Researcher"
      },
      {
        "name": "Shaoqing Xu",
        "type": "Researcher"
      },
      {
        "name": "Shengyin Jiang",
        "type": "Researcher"
      },
      {
        "name": "Zhi-Xin Yang",
        "type": "Researcher"
      },
      {
        "name": "Lvmin Zhang",
        "type": "Researcher"
      },
      {
        "name": "Anyi Rao",
        "type": "Researcher"
      },
      {
        "name": "Maneesh Agrawala",
        "type": "Researcher"
      },
      {
        "name": "Hyung Won Chung",
        "type": "Researcher"
      },
      {
        "name": "Le Hou",
        "type": "Researcher"
      },
      {
        "name": "Shayne Longpre",
        "type": "Researcher"
      },
      {
        "name": "Barret Zoph",
        "type": "Researcher"
      },
      {
        "name": "Yi Tay",
        "type": "Researcher"
      },
      {
        "name": "William Fedus",
        "type": "Researcher"
      },
      {
        "name": "Yunxuan Li",
        "type": "Researcher"
      },
      {
        "name": "Xuezhi Wang",
        "type": "Researcher"
      },
      {
        "name": "Siddhartha Brahma",
        "type": "Researcher"
      },
      {
        "name": "Albert Webson",
        "type": "Researcher"
      },
      {
        "name": "Shixiang Shane Gu",
        "type": "Researcher"
      },
      {
        "name": "Zhuyun Dai",
        "type": "Researcher"
      },
      {
        "name": "Mirac Suzgun",
        "type": "Researcher"
      },
      {
        "name": "Xinyun Chen",
        "type": "Researcher"
      },
      {
        "name": "Aakanksha Chowdhery",
        "type": "Researcher"
      },
      {
        "name": "Alex Castro-Ros",
        "type": "Researcher"
      },
      {
        "name": "Marie Pellat",
        "type": "Researcher"
      },
      {
        "name": "Kevin Robinson",
        "type": "Researcher"
      },
      {
        "name": "Dasha Valter",
        "type": "Researcher"
      },
      {
        "name": "Gaurav Mishra",
        "type": "Researcher"
      },
      {
        "name": "Adams Yu",
        "type": "Researcher"
      },
      {
        "name": "Vincent Zhao",
        "type": "Researcher"
      },
      {
        "name": "Yanping Huang",
        "type": "Researcher"
      },
      {
        "name": "Andrew Dai",
        "type": "Researcher"
      },
      {
        "name": "Hongkun Yu",
        "type": "Researcher"
      },
      {
        "name": "Slav Petrov",
        "type": "Researcher"
      },
      {
        "name": "Ed H. Chi",
        "type": "Researcher"
      },
      {
        "name": "Jeff Dean",
        "type": "Researcher"
      },
      {
        "name": "Denny Zhou",
        "type": "Researcher"
      },
      {
        "name": "Jason Wei",
        "type": "Researcher"
      },
      {
        "name": "Robin Rombach",
        "type": "Researcher"
      },
      {
        "name": "Andreas Blattmann",
        "type": "Researcher"
      },
      {
        "name": "Dominik Lorenz",
        "type": "Researcher"
      },
      {
        "name": "Patrick Esser",
        "type": "Researcher"
      },
      {
        "name": "Björn Ommer",
        "type": "Researcher"
      },
      {
        "name": "Romain Lopez",
        "type": "Researcher"
      },
      {
        "name": "Pierre Boyeau",
        "type": "Researcher"
      },
      {
        "name": "N. Yosef",
        "type": "Researcher"
      },
      {
        "name": "Michael I. Jordan",
        "type": "Researcher"
      },
      {
        "name": "J. Regier",
        "type": "Researcher"
      },
      {
        "name": "Phillip Isola",
        "type": "Researcher"
      },
      {
        "name": "Alexei A. Efros",
        "type": "Researcher"
      },
      {
        "name": "Oliver Wang",
        "type": "Researcher"
      },
      {
        "name": "Pei Sun",
        "type": "Researcher"
      },
      {
        "name": "Henrik Kretzschmar",
        "type": "Researcher"
      },
      {
        "name": "Xerxes Dotiwalla",
        "type": "Researcher"
      },
      {
        "name": "Aurelien Chouard",
        "type": "Researcher"
      },
      {
        "name": "Vijaysai Patnaik",
        "type": "Researcher"
      },
      {
        "name": "Paul Tsui",
        "type": "Researcher"
      },
      {
        "name": "James Guo",
        "type": "Researcher"
      },
      {
        "name": "Yin Zhou",
        "type": "Researcher"
      },
      {
        "name": "Yuning Chai",
        "type": "Researcher"
      },
      {
        "name": "Benjamin Caine",
        "type": "Researcher"
      },
      {
        "name": "Vijay Vasudevan",
        "type": "Researcher"
      },
      {
        "name": "Wei Han",
        "type": "Researcher"
      },
      {
        "name": "Jiquan Ngiam",
        "type": "Researcher"
      },
      {
        "name": "Hang Zhao",
        "type": "Researcher"
      },
      {
        "name": "Aleksei Timofeev",
        "type": "Researcher"
      },
      {
        "name": "Scott Ettinger",
        "type": "Researcher"
      },
      {
        "name": "Maxim Krivokon",
        "type": "Researcher"
      },
      {
        "name": "Amy Gao",
        "type": "Researcher"
      },
      {
        "name": "Aditya Joshi",
        "type": "Researcher"
      },
      {
        "name": "Sheng Zhao",
        "type": "Researcher"
      },
      {
        "name": "Shuyang Cheng",
        "type": "Researcher"
      },
      {
        "name": "Yu Zhang",
        "type": "Researcher"
      },
      {
        "name": "Zhifeng Chen",
        "type": "Researcher"
      },
      {
        "name": "Gilad Cohen",
        "type": "Researcher"
      },
      {
        "name": "Raja Giryes",
        "type": "Researcher"
      },
      {
        "name": "Zekai Zhang",
        "type": "Researcher"
      },
      {
        "name": "Xiao Li",
        "type": "Researcher"
      },
      {
        "name": "Xiang Li",
        "type": "Researcher"
      },
      {
        "name": "Lianghe Shi",
        "type": "Researcher"
      },
      {
        "name": "Meng Wu",
        "type": "Researcher"
      },
      {
        "name": "Molei Tao",
        "type": "Researcher"
      },
      {
        "name": "Qing Qu",
        "type": "Researcher"
      },
      {
        "name": "Donglin Yang",
        "type": "Researcher"
      },
      {
        "name": "Yongxing Zhang",
        "type": "Researcher"
      },
      {
        "name": "Xin Yu",
        "type": "Researcher"
      },
      {
        "name": "Liang Hou",
        "type": "Researcher"
      },
      {
        "name": "Xin Tao",
        "type": "Researcher"
      },
      {
        "name": "Xiaojuan Qi",
        "type": "Researcher"
      },
      {
        "name": "Renjie Liao",
        "type": "Researcher"
      },
      {
        "name": "Ramón Calvo-González",
        "type": "Researcher"
      },
      {
        "name": "François Fleuret",
        "type": "Researcher"
      },
      {
        "name": "Yao Teng",
        "type": "Researcher"
      },
      {
        "name": "Minxuan Lin",
        "type": "Researcher"
      },
      {
        "name": "Xian Liu",
        "type": "Researcher"
      },
      {
        "name": "Shuai Wang",
        "type": "Researcher"
      },
      {
        "name": "Xihui Liu",
        "type": "Researcher"
      },
      {
        "name": "Nicolas Sereyjol-Garros",
        "type": "Researcher"
      },
      {
        "name": "Ellington Kirby",
        "type": "Researcher"
      },
      {
        "name": "Victor Letzelter",
        "type": "Researcher"
      },
      {
        "name": "Victor Besnier",
        "type": "Researcher"
      },
      {
        "name": "Nermin Samet",
        "type": "Researcher"
      },
      {
        "name": "Gao Huang",
        "type": "Researcher"
      },
      {
        "name": "Zhuang Liu",
        "type": "Researcher"
      },
      {
        "name": "Laurens van der Maaten",
        "type": "Researcher"
      },
      {
        "name": "Kilian Q. Weinberger",
        "type": "Researcher"
      },
      {
        "name": "Zhongyu Zeng",
        "type": "Researcher"
      },
      {
        "name": "Xuan Peng",
        "type": "Researcher"
      },
      {
        "name": "J. B. Awotunde",
        "type": "Researcher"
      },
      {
        "name": "Korede Israel Adeyanju",
        "type": "Researcher"
      },
      {
        "name": "Kehinde Elisha Akerele",
        "type": "Researcher"
      },
      {
        "name": "Oluwatobi Akinlade",
        "type": "Researcher"
      },
      {
        "name": "S. Folorunso",
        "type": "Researcher"
      },
      {
        "name": "S. Ajagbe",
        "type": "Researcher"
      },
      {
        "name": "Camillo Lugaresi",
        "type": "Researcher"
      },
      {
        "name": "Jiuqiang Tang",
        "type": "Researcher"
      },
      {
        "name": "Hadon Nash",
        "type": "Researcher"
      },
      {
        "name": "Chris McClanahan",
        "type": "Researcher"
      },
      {
        "name": "Esha Uboweja",
        "type": "Researcher"
      },
      {
        "name": "Michael Hays",
        "type": "Researcher"
      },
      {
        "name": "Fan Zhang",
        "type": "Researcher"
      },
      {
        "name": "Chuo-Ling Chang",
        "type": "Researcher"
      },
      {
        "name": "Ming Guang Yong",
        "type": "Researcher"
      },
      {
        "name": "Juhyun Lee",
        "type": "Researcher"
      },
      {
        "name": "Wan-Teh Chang",
        "type": "Researcher"
      },
      {
        "name": "Wei Hua",
        "type": "Researcher"
      },
      {
        "name": "Manfred Georg",
        "type": "Researcher"
      },
      {
        "name": "Matthias Grundmann",
        "type": "Researcher"
      },
      {
        "name": "Cem Keskin",
        "type": "Researcher"
      },
      {
        "name": "Mustafa Furkan Kıraç",
        "type": "Researcher"
      },
      {
        "name": "Yunus Emre Kara",
        "type": "Researcher"
      },
      {
        "name": "L. Akarun",
        "type": "Researcher"
      },
      {
        "name": "S. P. Priyal",
        "type": "Researcher"
      },
      {
        "name": "P. Bora",
        "type": "Researcher"
      },
      {
        "name": "Octavian Dudas",
        "type": "Researcher"
      },
      {
        "name": "C. Nandra",
        "type": "Researcher"
      },
      {
        "name": "C. Mocan",
        "type": "Researcher"
      },
      {
        "name": "D. Gorgan",
        "type": "Researcher"
      },
      {
        "name": "Avinash Dhiran",
        "type": "Researcher"
      },
      {
        "name": "Anurag Kumbhare",
        "type": "Researcher"
      },
      {
        "name": "Achal Patil",
        "type": "Researcher"
      },
      {
        "name": "Mrugank Vichare",
        "type": "Researcher"
      },
      {
        "name": "Dhananjay Patel",
        "type": "Researcher"
      },
      {
        "name": "Saransh Mishra",
        "type": "Researcher"
      },
      {
        "name": "Pavan Nair",
        "type": "Researcher"
      },
      {
        "name": "Pushpalatha M",
        "type": "Researcher"
      },
      {
        "name": "Poornima S",
        "type": "Researcher"
      },
      {
        "name": "Jeff Donahue",
        "type": "Researcher"
      },
      {
        "name": "Trevor Darrell",
        "type": "Researcher"
      },
      {
        "name": "Jitendra Malik",
        "type": "Researcher"
      },
      {
        "name": "Shansong Liu",
        "type": "Researcher"
      },
      {
        "name": "Atin Sakkeer Hussain",
        "type": "Researcher"
      },
      {
        "name": "Qilong Wu",
        "type": "Researcher"
      },
      {
        "name": "Chenshuo Sun",
        "type": "Researcher"
      },
      {
        "name": "Ying Shan",
        "type": "Researcher"
      },
      {
        "name": "Jiahang Tu",
        "type": "Researcher"
      },
      {
        "name": "Ye Li",
        "type": "Researcher"
      },
      {
        "name": "Yiming Wu",
        "type": "Researcher"
      },
      {
        "name": "Hanbin Zhao",
        "type": "Researcher"
      },
      {
        "name": "Chao Zhang",
        "type": "Researcher"
      },
      {
        "name": "Hui Qian",
        "type": "Researcher"
      },
      {
        "name": "Haotian Lv",
        "type": "Researcher"
      },
      {
        "name": "Yuhui Zhang",
        "type": "Researcher"
      },
      {
        "name": "Jiangbo Dai",
        "type": "Researcher"
      },
      {
        "name": "Hanli Wu",
        "type": "Researcher"
      },
      {
        "name": "Jiaji Wang",
        "type": "Researcher"
      },
      {
        "name": "Dawei Wang",
        "type": "Researcher"
      },
      {
        "name": "Mingxin Li",
        "type": "Researcher"
      },
      {
        "name": "Yanzhao Zhang",
        "type": "Researcher"
      },
      {
        "name": "Dingkun Long",
        "type": "Researcher"
      },
      {
        "name": "Keqin Chen",
        "type": "Researcher"
      },
      {
        "name": "Sibo Song",
        "type": "Researcher"
      },
      {
        "name": "Shuai Bai",
        "type": "Researcher"
      },
      {
        "name": "Zhibo Yang",
        "type": "Researcher"
      },
      {
        "name": "Pengjun Xie",
        "type": "Researcher"
      },
      {
        "name": "An Yang",
        "type": "Researcher"
      },
      {
        "name": "Dayiheng Liu",
        "type": "Researcher"
      },
      {
        "name": "Jingren Zhou",
        "type": "Researcher"
      },
      {
        "name": "Junyang Lin",
        "type": "Researcher"
      },
      {
        "name": "Mingyue Chen",
        "type": "Researcher"
      },
      {
        "name": "Xin Liao",
        "type": "Researcher"
      },
      {
        "name": "Han Fang",
        "type": "Researcher"
      },
      {
        "name": "Jinlin Guo",
        "type": "Researcher"
      },
      {
        "name": "Yanxiang Chen",
        "type": "Researcher"
      },
      {
        "name": "Xiaoshuai Wu",
        "type": "Researcher"
      },
      {
        "name": "A. Dempster",
        "type": "Researcher"
      },
      {
        "name": "N. Laird",
        "type": "Researcher"
      },
      {
        "name": "D. Rubin",
        "type": "Researcher"
      },
      {
        "name": "L. Maaten",
        "type": "Researcher"
      },
      {
        "name": "Jacy Reese Anthis",
        "type": "Researcher"
      },
      {
        "name": "Ryan Liu",
        "type": "Researcher"
      },
      {
        "name": "Sean M. Richardson",
        "type": "Researcher"
      },
      {
        "name": "Austin C. Kozlowski",
        "type": "Researcher"
      },
      {
        "name": "Bernard Koch",
        "type": "Researcher"
      },
      {
        "name": "James Evans",
        "type": "Researcher"
      },
      {
        "name": "Erik Brynjolfsson",
        "type": "Researcher"
      },
      {
        "name": "Zhiwen Xiao",
        "type": "Researcher"
      },
      {
        "name": "Huagang Tong",
        "type": "Researcher"
      },
      {
        "name": "Runqian Wang",
        "type": "Researcher"
      },
      {
        "name": "Ibomoiye Domor Mienye",
        "type": "Researcher"
      },
      {
        "name": "Theo G. Swart",
        "type": "Researcher"
      },
      {
        "name": "Jusheng Zhang",
        "type": "Researcher"
      },
      {
        "name": "Zimeng Huang",
        "type": "Researcher"
      },
      {
        "name": "Yijia Fan",
        "type": "Researcher"
      },
      {
        "name": "Ningyuan Liu",
        "type": "Researcher"
      },
      {
        "name": "Mingyan Li",
        "type": "Researcher"
      },
      {
        "name": "Zhuojie Yang",
        "type": "Researcher"
      },
      {
        "name": "Jiawei Yao",
        "type": "Researcher"
      },
      {
        "name": "Jian Wang",
        "type": "Researcher"
      },
      {
        "name": "Keze Wang",
        "type": "Researcher"
      },
      {
        "name": "Olaf Ronneberger",
        "type": "Researcher"
      },
      {
        "name": "Philipp Fischer",
        "type": "Researcher"
      },
      {
        "name": "Thomas Brox",
        "type": "Researcher"
      },
      {
        "name": "Tianwei Yin",
        "type": "Researcher"
      },
      {
        "name": "Michaël Gharbi",
        "type": "Researcher"
      },
      {
        "name": "Taesung Park",
        "type": "Researcher"
      },
      {
        "name": "Fredo Durand",
        "type": "Researcher"
      },
      {
        "name": "William T. Freeman",
        "type": "Researcher"
      },
      {
        "name": "Axel Sauer",
        "type": "Researcher"
      },
      {
        "name": "Frederic Boesel",
        "type": "Researcher"
      },
      {
        "name": "Tim Dockhorn",
        "type": "Researcher"
      },
      {
        "name": "Takuya Akiba",
        "type": "Researcher"
      },
      {
        "name": "Makoto Shing",
        "type": "Researcher"
      },
      {
        "name": "Yujin Tang",
        "type": "Researcher"
      },
      {
        "name": "Qi Sun",
        "type": "Researcher"
      },
      {
        "name": "David Ha",
        "type": "Researcher"
      },
      {
        "name": "Qiang Zhang",
        "type": "Researcher"
      },
      {
        "name": "Xun Huang",
        "type": "Researcher"
      },
      {
        "name": "Zinan Guo",
        "type": "Researcher"
      },
      {
        "name": "Yanze Wu",
        "type": "Researcher"
      },
      {
        "name": "Zhuowei Chen",
        "type": "Researcher"
      },
      {
        "name": "Lang Chen",
        "type": "Researcher"
      },
      {
        "name": "Peng Zhang",
        "type": "Researcher"
      },
      {
        "name": "Qian He",
        "type": "Researcher"
      },
      {
        "name": "Jonathan Ho",
        "type": "Researcher"
      },
      {
        "name": "Ajay Jain",
        "type": "Researcher"
      },
      {
        "name": "Pieter Abbeel",
        "type": "Researcher"
      },
      {
        "name": "Weijie Kong",
        "type": "Researcher"
      },
      {
        "name": "Qi Tian",
        "type": "Researcher"
      },
      {
        "name": "Zijian Zhang",
        "type": "Researcher"
      },
      {
        "name": "Rox Min",
        "type": "Researcher"
      },
      {
        "name": "Zuozhuo Dai",
        "type": "Researcher"
      },
      {
        "name": "Jin Zhou",
        "type": "Researcher"
      },
      {
        "name": "Jiangfeng Xiong",
        "type": "Researcher"
      },
      {
        "name": "Xin Li",
        "type": "Researcher"
      },
      {
        "name": "Bo Wu",
        "type": "Researcher"
      },
      {
        "name": "Jianwei Zhang",
        "type": "Researcher"
      },
      {
        "name": "Kathrina Wu",
        "type": "Researcher"
      },
      {
        "name": "Qin Lin",
        "type": "Researcher"
      },
      {
        "name": "Junkun Yuan",
        "type": "Researcher"
      },
      {
        "name": "Yanxin Long",
        "type": "Researcher"
      },
      {
        "name": "Aladdin Wang",
        "type": "Researcher"
      },
      {
        "name": "Andong Wang",
        "type": "Researcher"
      },
      {
        "name": "Changlin Li",
        "type": "Researcher"
      },
      {
        "name": "Duojun Huang",
        "type": "Researcher"
      },
      {
        "name": "Fang Yang",
        "type": "Researcher"
      },
      {
        "name": "Hao Tan",
        "type": "Researcher"
      },
      {
        "name": "Hongmei Wang",
        "type": "Researcher"
      },
      {
        "name": "Jacob Song",
        "type": "Researcher"
      },
      {
        "name": "Jiawang Bai",
        "type": "Researcher"
      },
      {
        "name": "Jianbing Wu",
        "type": "Researcher"
      },
      {
        "name": "Jinbao Xue",
        "type": "Researcher"
      },
      {
        "name": "Joey Wang",
        "type": "Researcher"
      },
      {
        "name": "Mengyang Liu",
        "type": "Researcher"
      },
      {
        "name": "Pengyu Li",
        "type": "Researcher"
      },
      {
        "name": "Shuai Li",
        "type": "Researcher"
      },
      {
        "name": "Weiyan Wang",
        "type": "Researcher"
      },
      {
        "name": "Wenqing Yu",
        "type": "Researcher"
      },
      {
        "name": "Xinchi Deng",
        "type": "Researcher"
      },
      {
        "name": "Yi Chen",
        "type": "Researcher"
      },
      {
        "name": "Yutao Cui",
        "type": "Researcher"
      },
      {
        "name": "Yuanbo Peng",
        "type": "Researcher"
      },
      {
        "name": "Zhentao Yu",
        "type": "Researcher"
      },
      {
        "name": "Zhiyu He",
        "type": "Researcher"
      },
      {
        "name": "Zhiyong Xu",
        "type": "Researcher"
      },
      {
        "name": "Zixiang Zhou",
        "type": "Researcher"
      },
      {
        "name": "Zunnan Xu",
        "type": "Researcher"
      },
      {
        "name": "Yangyu Tao",
        "type": "Researcher"
      },
      {
        "name": "Qinglin Lu",
        "type": "Researcher"
      },
      {
        "name": "Songtao Liu",
        "type": "Researcher"
      },
      {
        "name": "Dax Zhou",
        "type": "Researcher"
      },
      {
        "name": "Yong Yang",
        "type": "Researcher"
      },
      {
        "name": "Di Wang",
        "type": "Researcher"
      },
      {
        "name": "Yuhong Liu",
        "type": "Researcher"
      },
      {
        "name": "Jie Jiang",
        "type": "Researcher"
      },
      {
        "name": "Caesar Zhong",
        "type": "Researcher"
      },
      {
        "name": "Jianwen Jiang",
        "type": "Researcher"
      },
      {
        "name": "Chao Liang",
        "type": "Researcher"
      },
      {
        "name": "Jiaqi Yang",
        "type": "Researcher"
      },
      {
        "name": "Gaojie Lin",
        "type": "Researcher"
      },
      {
        "name": "Tianyun Zhong",
        "type": "Researcher"
      },
      {
        "name": "Yanbo Zheng",
        "type": "Researcher"
      },
      {
        "name": "Zerong Zheng",
        "type": "Researcher"
      },
      {
        "name": "Jiahao Cui",
        "type": "Researcher"
      },
      {
        "name": "Hui Li",
        "type": "Researcher"
      },
      {
        "name": "Yao Yao",
        "type": "Researcher"
      },
      {
        "name": "Hao Zhu",
        "type": "Researcher"
      },
      {
        "name": "Hanlin Shang",
        "type": "Researcher"
      },
      {
        "name": "Kaihui Cheng",
        "type": "Researcher"
      },
      {
        "name": "Hang Zhou",
        "type": "Researcher"
      },
      {
        "name": "Siyu Zhu",
        "type": "Researcher"
      },
      {
        "name": "Jingdong Wang",
        "type": "Researcher"
      },
      {
        "name": "Rang Meng",
        "type": "Researcher"
      },
      {
        "name": "Xingyu Zhang",
        "type": "Researcher"
      },
      {
        "name": "Bharath Hariharan",
        "type": "Researcher"
      },
      {
        "name": "Kyunghyun Cho",
        "type": "Researcher"
      },
      {
        "name": "Bart van Merrienboer",
        "type": "Researcher"
      },
      {
        "name": "Caglar Gulcehre",
        "type": "Researcher"
      },
      {
        "name": "Dzmitry Bahdanau",
        "type": "Researcher"
      },
      {
        "name": "Fethi Bougares",
        "type": "Researcher"
      },
      {
        "name": "Holger Schwenk",
        "type": "Researcher"
      },
      {
        "name": "Erfei Cui",
        "type": "Researcher"
      },
      {
        "name": "Wenhai Wang",
        "type": "Researcher"
      },
      {
        "name": "Zhiqi Li",
        "type": "Researcher"
      },
      {
        "name": "Jiangwei Xie",
        "type": "Researcher"
      },
      {
        "name": "Haoming Zou",
        "type": "Researcher"
      },
      {
        "name": "Hanming Deng",
        "type": "Researcher"
      },
      {
        "name": "Gen Luo",
        "type": "Researcher"
      },
      {
        "name": "Lewei Lu",
        "type": "Researcher"
      },
      {
        "name": "Xizhou Zhu",
        "type": "Researcher"
      },
      {
        "name": "Jifeng Dai",
        "type": "Researcher"
      },
      {
        "name": "Shenyuan Gao",
        "type": "Researcher"
      },
      {
        "name": "Jiazhi Yang",
        "type": "Researcher"
      },
      {
        "name": "Li Chen",
        "type": "Researcher"
      },
      {
        "name": "Yihang Qiu",
        "type": "Researcher"
      },
      {
        "name": "Jun Zhang",
        "type": "Researcher"
      },
      {
        "name": "Bencheng Liao",
        "type": "Researcher"
      },
      {
        "name": "Shaoyu Chen",
        "type": "Researcher"
      },
      {
        "name": "Haoran Yin",
        "type": "Researcher"
      },
      {
        "name": "Bo Jiang",
        "type": "Researcher"
      },
      {
        "name": "Cheng Wang",
        "type": "Researcher"
      },
      {
        "name": "Xinbang Zhang",
        "type": "Researcher"
      },
      {
        "name": "Xiangyu Li",
        "type": "Researcher"
      },
      {
        "name": "Ying Zhang",
        "type": "Researcher"
      },
      {
        "name": "Qian Zhang",
        "type": "Researcher"
      },
      {
        "name": "Bingyi Kang",
        "type": "Researcher"
      },
      {
        "name": "Yang Yue",
        "type": "Researcher"
      },
      {
        "name": "Rui Lu",
        "type": "Researcher"
      },
      {
        "name": "Zhijie Lin",
        "type": "Researcher"
      },
      {
        "name": "Yang Zhao",
        "type": "Researcher"
      },
      {
        "name": "Kaixin Wang",
        "type": "Researcher"
      },
      {
        "name": "Jiashi Feng",
        "type": "Researcher"
      },
      {
        "name": "Jyh-Jing Hwang",
        "type": "Researcher"
      },
      {
        "name": "Runsheng Xu",
        "type": "Researcher"
      },
      {
        "name": "Hubert Lin",
        "type": "Researcher"
      },
      {
        "name": "Wei-Chih Hung",
        "type": "Researcher"
      },
      {
        "name": "Jingwei Ji",
        "type": "Researcher"
      },
      {
        "name": "Kristy Choi",
        "type": "Researcher"
      },
      {
        "name": "Di Huang",
        "type": "Researcher"
      },
      {
        "name": "Tong He",
        "type": "Researcher"
      },
      {
        "name": "Paul Covington",
        "type": "Researcher"
      },
      {
        "name": "Benjamin Sapp",
        "type": "Researcher"
      },
      {
        "name": "Tom B. Brown",
        "type": "Researcher"
      },
      {
        "name": "Benjamin Mann",
        "type": "Researcher"
      },
      {
        "name": "Nick Ryder",
        "type": "Researcher"
      },
      {
        "name": "Melanie Subbiah",
        "type": "Researcher"
      },
      {
        "name": "Jared Kaplan",
        "type": "Researcher"
      },
      {
        "name": "Prafulla Dhariwal",
        "type": "Researcher"
      },
      {
        "name": "Arvind Neelakantan",
        "type": "Researcher"
      },
      {
        "name": "Pranav Shyam",
        "type": "Researcher"
      },
      {
        "name": "Ariel Herbert-Voss",
        "type": "Researcher"
      },
      {
        "name": "Tom Henighan",
        "type": "Researcher"
      },
      {
        "name": "Rewon Child",
        "type": "Researcher"
      },
      {
        "name": "Daniel M. Ziegler",
        "type": "Researcher"
      },
      {
        "name": "Jeffrey Wu",
        "type": "Researcher"
      },
      {
        "name": "Clemens Winter",
        "type": "Researcher"
      },
      {
        "name": "Christopher Hesse",
        "type": "Researcher"
      },
      {
        "name": "Mark Chen",
        "type": "Researcher"
      },
      {
        "name": "Eric Sigler",
        "type": "Researcher"
      },
      {
        "name": "Mateusz Litwin",
        "type": "Researcher"
      },
      {
        "name": "Scott Gray",
        "type": "Researcher"
      },
      {
        "name": "Benjamin Chess",
        "type": "Researcher"
      },
      {
        "name": "Christopher Berner",
        "type": "Researcher"
      },
      {
        "name": "Sam McCandlish",
        "type": "Researcher"
      },
      {
        "name": "Dario Amodei",
        "type": "Researcher"
      },
      {
        "name": "Zhe Chen",
        "type": "Researcher"
      },
      {
        "name": "Weiyun Wang",
        "type": "Researcher"
      },
      {
        "name": "Yue Cao",
        "type": "Researcher"
      },
      {
        "name": "Yangzhou Liu",
        "type": "Researcher"
      },
      {
        "name": "Zhangwei Gao",
        "type": "Researcher"
      },
      {
        "name": "Jinguo Zhu",
        "type": "Researcher"
      },
      {
        "name": "Shenglong Ye",
        "type": "Researcher"
      },
      {
        "name": "Hao Tian",
        "type": "Researcher"
      },
      {
        "name": "Zhaoyang Liu",
        "type": "Researcher"
      },
      {
        "name": "Lixin Gu",
        "type": "Researcher"
      },
      {
        "name": "Xuehui Wang",
        "type": "Researcher"
      },
      {
        "name": "Qingyun Li",
        "type": "Researcher"
      },
      {
        "name": "Yiming Ren",
        "type": "Researcher"
      },
      {
        "name": "Zixuan Chen",
        "type": "Researcher"
      },
      {
        "name": "Jiapeng Luo",
        "type": "Researcher"
      },
      {
        "name": "Jiahao Wang",
        "type": "Researcher"
      },
      {
        "name": "Tan Jiang",
        "type": "Researcher"
      },
      {
        "name": "Conghui He",
        "type": "Researcher"
      },
      {
        "name": "Xingcheng Zhang",
        "type": "Researcher"
      },
      {
        "name": "Han Lv",
        "type": "Researcher"
      },
      {
        "name": "Yi Wang",
        "type": "Researcher"
      },
      {
        "name": "Wenqi Shao",
        "type": "Researcher"
      },
      {
        "name": "Pei Chu",
        "type": "Researcher"
      },
      {
        "name": "Zhongying Tu",
        "type": "Researcher"
      },
      {
        "name": "Zhiyong Wu",
        "type": "Researcher"
      },
      {
        "name": "Huipeng Deng",
        "type": "Researcher"
      },
      {
        "name": "Jiaye Ge",
        "type": "Researcher"
      },
      {
        "name": "Kai Chen",
        "type": "Researcher"
      },
      {
        "name": "Kaipeng Zhang",
        "type": "Researcher"
      },
      {
        "name": "Limin Wang",
        "type": "Researcher"
      },
      {
        "name": "Tong Lu",
        "type": "Researcher"
      },
      {
        "name": "Dahua Lin",
        "type": "Researcher"
      },
      {
        "name": "Yu Qiao",
        "type": "Researcher"
      },
      {
        "name": "Yuchen Duan",
        "type": "Researcher"
      },
      {
        "name": "Weijie Su",
        "type": "Researcher"
      },
      {
        "name": "Jie Shao",
        "type": "Researcher"
      },
      {
        "name": "Xingguang Wei",
        "type": "Researcher"
      },
      {
        "name": "Hongjie Zhang",
        "type": "Researcher"
      },
      {
        "name": "Haomin Wang",
        "type": "Researcher"
      },
      {
        "name": "Weiye Xu",
        "type": "Researcher"
      },
      {
        "name": "Hao Li",
        "type": "Researcher"
      },
      {
        "name": "Songze Li",
        "type": "Researcher"
      },
      {
        "name": "Yinan He",
        "type": "Researcher"
      },
      {
        "name": "Junjun He",
        "type": "Researcher"
      },
      {
        "name": "Yingtong Xiong",
        "type": "Researcher"
      },
      {
        "name": "Wenwen Qu",
        "type": "Researcher"
      },
      {
        "name": "Peng Sun",
        "type": "Researcher"
      },
      {
        "name": "Penglong Jiao",
        "type": "Researcher"
      },
      {
        "name": "Lijun Wu",
        "type": "Researcher"
      },
      {
        "name": "Haodong Duan",
        "type": "Researcher"
      },
      {
        "name": "Xinyu Fang",
        "type": "Researcher"
      },
      {
        "name": "Junming Yang",
        "type": "Researcher"
      },
      {
        "name": "Xiangyu Zhao",
        "type": "Researcher"
      },
      {
        "name": "Yuxuan Qiao",
        "type": "Researcher"
      },
      {
        "name": "Mo Li",
        "type": "Researcher"
      },
      {
        "name": "Amit Agarwal",
        "type": "Researcher"
      },
      {
        "name": "Lin Chen",
        "type": "Researcher"
      },
      {
        "name": "Yuan Liu",
        "type": "Researcher"
      },
      {
        "name": "Yubo Ma",
        "type": "Researcher"
      },
      {
        "name": "Hailong Sun",
        "type": "Researcher"
      },
      {
        "name": "Yifan Zhang",
        "type": "Researcher"
      },
      {
        "name": "Tack Hwa Wong",
        "type": "Researcher"
      },
      {
        "name": "Peiheng Zhou",
        "type": "Researcher"
      },
      {
        "name": "Xiaozhe Li",
        "type": "Researcher"
      },
      {
        "name": "Chaoyou Fu",
        "type": "Researcher"
      },
      {
        "name": "Junbo Cui",
        "type": "Researcher"
      },
      {
        "name": "Jixuan Chen",
        "type": "Researcher"
      },
      {
        "name": "Enxin Song",
        "type": "Researcher"
      },
      {
        "name": "Song Mao",
        "type": "Researcher"
      },
      {
        "name": "Shengyuan Ding",
        "type": "Researcher"
      },
      {
        "name": "Tianhao Liang",
        "type": "Researcher"
      },
      {
        "name": "Zicheng Zhang",
        "type": "Researcher"
      },
      {
        "name": "Xiaoyi Dong",
        "type": "Researcher"
      },
      {
        "name": "Yuhang Zang",
        "type": "Researcher"
      },
      {
        "name": "Pan Zhang",
        "type": "Researcher"
      },
      {
        "name": "Jiaqi Wang",
        "type": "Researcher"
      },
      {
        "name": "Guowei Xu",
        "type": "Researcher"
      },
      {
        "name": "Peng Jin",
        "type": "Researcher"
      },
      {
        "name": "Ziang Wu",
        "type": "Researcher"
      },
      {
        "name": "Yibing Song",
        "type": "Researcher"
      },
      {
        "name": "Lichao Sun",
        "type": "Researcher"
      },
      {
        "name": "Li Yuan",
        "type": "Researcher"
      },
      {
        "name": "Hengjun Pu",
        "type": "Researcher"
      },
      {
        "name": "Long Cui",
        "type": "Researcher"
      },
      {
        "name": "Linglin Jing",
        "type": "Researcher"
      },
      {
        "name": "Zhaokai Wang",
        "type": "Researcher"
      },
      {
        "name": "Ganlin Yang",
        "type": "Researcher"
      },
      {
        "name": "Qi Wei",
        "type": "Researcher"
      },
      {
        "name": "Jinhui Yin",
        "type": "Researcher"
      },
      {
        "name": "Wenhao Li",
        "type": "Researcher"
      },
      {
        "name": "Guanzhou Chen",
        "type": "Researcher"
      },
      {
        "name": "Zichen Ding",
        "type": "Researcher"
      },
      {
        "name": "Changyao Tian",
        "type": "Researcher"
      },
      {
        "name": "Zhenyu Wu",
        "type": "Researcher"
      },
      {
        "name": "Jingjing Xie",
        "type": "Researcher"
      },
      {
        "name": "Zehao Li",
        "type": "Researcher"
      },
      {
        "name": "Bowen Yang",
        "type": "Researcher"
      },
      {
        "name": "Zhi Hou",
        "type": "Researcher"
      },
      {
        "name": "Haoran Hao",
        "type": "Researcher"
      },
      {
        "name": "Tianyi Zhang",
        "type": "Researcher"
      },
      {
        "name": "Bin Fu",
        "type": "Researcher"
      },
      {
        "name": "Biqing Qi",
        "type": "Researcher"
      },
      {
        "name": "Qipeng Guo",
        "type": "Researcher"
      },
      {
        "name": "Wenwei Zhang",
        "type": "Researcher"
      },
      {
        "name": "Songyang Zhang",
        "type": "Researcher"
      },
      {
        "name": "Maosong Cao",
        "type": "Researcher"
      },
      {
        "name": "Junyao Lin",
        "type": "Researcher"
      },
      {
        "name": "Kexian Tang",
        "type": "Researcher"
      },
      {
        "name": "Jianfei Gao",
        "type": "Researcher"
      },
      {
        "name": "Haian Huang",
        "type": "Researcher"
      },
      {
        "name": "Yuzhe Gu",
        "type": "Researcher"
      },
      {
        "name": "Chengqi Lyu",
        "type": "Researcher"
      },
      {
        "name": "Huanze Tang",
        "type": "Researcher"
      },
      {
        "name": "Rui Wang",
        "type": "Researcher"
      },
      {
        "name": "Haijun Lv",
        "type": "Researcher"
      },
      {
        "name": "Bowen Zhou",
        "type": "Researcher"
      },
      {
        "name": "John Schulman",
        "type": "Researcher"
      },
      {
        "name": "Filip Wolski",
        "type": "Researcher"
      },
      {
        "name": "Oleg Klimov",
        "type": "Researcher"
      },
      {
        "name": "Yangguang Li",
        "type": "Researcher"
      },
      {
        "name": "Jiaheng Liu",
        "type": "Researcher"
      },
      {
        "name": "Zeyue Xue",
        "type": "Researcher"
      },
      {
        "name": "Yu Gao",
        "type": "Researcher"
      },
      {
        "name": "Fangyuan Kong",
        "type": "Researcher"
      },
      {
        "name": "Lingting Zhu",
        "type": "Researcher"
      },
      {
        "name": "Mengzhao Chen",
        "type": "Researcher"
      },
      {
        "name": "Qiushan Guo",
        "type": "Researcher"
      },
      {
        "name": "Weilin Huang",
        "type": "Researcher"
      },
      {
        "name": "Haoyuan Guo",
        "type": "Researcher"
      },
      {
        "name": "Tuyen Hoang",
        "type": "Researcher"
      },
      {
        "name": "Lu Jiang",
        "type": "Researcher"
      },
      {
        "name": "Huixia Li",
        "type": "Researcher"
      },
      {
        "name": "Jiashi Li",
        "type": "Researcher"
      },
      {
        "name": "Liang Li",
        "type": "Researcher"
      },
      {
        "name": "Xiaojie Li",
        "type": "Researcher"
      },
      {
        "name": "Xunsong Li",
        "type": "Researcher"
      },
      {
        "name": "Yifu Li",
        "type": "Researcher"
      },
      {
        "name": "Jiawei Liu",
        "type": "Researcher"
      },
      {
        "name": "Shu Liu",
        "type": "Researcher"
      },
      {
        "name": "Xiaonan Nie",
        "type": "Researcher"
      },
      {
        "name": "Zhiwu Qing",
        "type": "Researcher"
      },
      {
        "name": "Li Sun",
        "type": "Researcher"
      },
      {
        "name": "Zhi Tian",
        "type": "Researcher"
      },
      {
        "name": "Sen Wang",
        "type": "Researcher"
      },
      {
        "name": "Guoqiang Wei",
        "type": "Researcher"
      },
      {
        "name": "Guohong Wu",
        "type": "Researcher"
      },
      {
        "name": "Ruiqi Xia",
        "type": "Researcher"
      },
      {
        "name": "Fei Xiao",
        "type": "Researcher"
      },
      {
        "name": "Xuefeng Xiao",
        "type": "Researcher"
      },
      {
        "name": "Jiangqiao Yan",
        "type": "Researcher"
      },
      {
        "name": "Ceyuan Yang",
        "type": "Researcher"
      },
      {
        "name": "Jianchao Yang",
        "type": "Researcher"
      },
      {
        "name": "Runkai Yang",
        "type": "Researcher"
      },
      {
        "name": "Tao Yang",
        "type": "Researcher"
      },
      {
        "name": "Yihang Yang",
        "type": "Researcher"
      },
      {
        "name": "Zilyu Ye",
        "type": "Researcher"
      },
      {
        "name": "Xuejiao Zeng",
        "type": "Researcher"
      },
      {
        "name": "Yan Zeng",
        "type": "Researcher"
      },
      {
        "name": "Heng Zhang",
        "type": "Researcher"
      },
      {
        "name": "Xiaozheng Zheng",
        "type": "Researcher"
      },
      {
        "name": "Peihao Zhu",
        "type": "Researcher"
      },
      {
        "name": "Jiaxin Zou",
        "type": "Researcher"
      },
      {
        "name": "Feilong Zuo",
        "type": "Researcher"
      },
      {
        "name": "Guibin Chen",
        "type": "Researcher"
      },
      {
        "name": "Dixuan Lin",
        "type": "Researcher"
      },
      {
        "name": "Jiangping Yang",
        "type": "Researcher"
      },
      {
        "name": "Chunze Lin",
        "type": "Researcher"
      },
      {
        "name": "Junchen Zhu",
        "type": "Researcher"
      },
      {
        "name": "Mingyuan Fan",
        "type": "Researcher"
      },
      {
        "name": "Hao Zhang",
        "type": "Researcher"
      },
      {
        "name": "Sheng Chen",
        "type": "Researcher"
      },
      {
        "name": "Zheng Chen",
        "type": "Researcher"
      },
      {
        "name": "Chengcheng Ma",
        "type": "Researcher"
      },
      {
        "name": "Weiming Xiong",
        "type": "Researcher"
      },
      {
        "name": "Wei Wang",
        "type": "Researcher"
      },
      {
        "name": "Nuo Pang",
        "type": "Researcher"
      },
      {
        "name": "Kang Kang",
        "type": "Researcher"
      },
      {
        "name": "Zhiheng Xu",
        "type": "Researcher"
      },
      {
        "name": "Yuzhe Jin",
        "type": "Researcher"
      },
      {
        "name": "Yupeng Liang",
        "type": "Researcher"
      },
      {
        "name": "Yubing Song",
        "type": "Researcher"
      },
      {
        "name": "Peng Zhao",
        "type": "Researcher"
      },
      {
        "name": "Boyuan Xu",
        "type": "Researcher"
      },
      {
        "name": "Di Qiu",
        "type": "Researcher"
      },
      {
        "name": "Debang Li",
        "type": "Researcher"
      },
      {
        "name": "Zhengcong Fei",
        "type": "Researcher"
      },
      {
        "name": "Yahui Zhou",
        "type": "Researcher"
      },
      {
        "name": "Yibin Wang",
        "type": "Researcher"
      },
      {
        "name": "Cheng Jin",
        "type": "Researcher"
      },
      {
        "name": "OpenAI",
        "type": "Researcher"
      },
      {
        "name": "Josh Achiam",
        "type": "Researcher"
      },
      {
        "name": "Steven Adler",
        "type": "Researcher"
      },
      {
        "name": "Lama Ahmad",
        "type": "Researcher"
      },
      {
        "name": "Ilge Akkaya",
        "type": "Researcher"
      },
      {
        "name": "Florencia Leoni Aleman",
        "type": "Researcher"
      },
      {
        "name": "Diogo Almeida",
        "type": "Researcher"
      },
      {
        "name": "Janko Altenschmidt",
        "type": "Researcher"
      },
      {
        "name": "Sam Altman",
        "type": "Researcher"
      },
      {
        "name": "Shyamal Anadkat",
        "type": "Researcher"
      },
      {
        "name": "Red Avila",
        "type": "Researcher"
      },
      {
        "name": "Igor Babuschkin",
        "type": "Researcher"
      },
      {
        "name": "Suchir Balaji",
        "type": "Researcher"
      },
      {
        "name": "Valerie Balcom",
        "type": "Researcher"
      },
      {
        "name": "Paul Baltescu",
        "type": "Researcher"
      },
      {
        "name": "Haiming Bao",
        "type": "Researcher"
      },
      {
        "name": "Mohammad Bavarian",
        "type": "Researcher"
      },
      {
        "name": "Jeff Belgum",
        "type": "Researcher"
      },
      {
        "name": "Irwan Bello",
        "type": "Researcher"
      },
      {
        "name": "Jake Berdine",
        "type": "Researcher"
      },
      {
        "name": "Gabriel Bernadett-Shapiro",
        "type": "Researcher"
      },
      {
        "name": "Lenny Bogdonoff",
        "type": "Researcher"
      },
      {
        "name": "Oleg Boiko",
        "type": "Researcher"
      },
      {
        "name": "Madelaine Boyd",
        "type": "Researcher"
      },
      {
        "name": "Anna-Luisa Brakman",
        "type": "Researcher"
      },
      {
        "name": "Greg Brockman",
        "type": "Researcher"
      },
      {
        "name": "Tim Brooks",
        "type": "Researcher"
      },
      {
        "name": "Miles Brundage",
        "type": "Researcher"
      },
      {
        "name": "Kevin Button",
        "type": "Researcher"
      },
      {
        "name": "Trevor Cai",
        "type": "Researcher"
      },
      {
        "name": "Rosie Campbell",
        "type": "Researcher"
      },
      {
        "name": "Andrew Cann",
        "type": "Researcher"
      },
      {
        "name": "Brittany Carey",
        "type": "Researcher"
      },
      {
        "name": "Chelsea Carlson",
        "type": "Researcher"
      },
      {
        "name": "Rory Carmichael",
        "type": "Researcher"
      },
      {
        "name": "Brooke Chan",
        "type": "Researcher"
      },
      {
        "name": "Che Chang",
        "type": "Researcher"
      },
      {
        "name": "Fotis Chantzis",
        "type": "Researcher"
      },
      {
        "name": "Derek Chen",
        "type": "Researcher"
      },
      {
        "name": "Sully Chen",
        "type": "Researcher"
      },
      {
        "name": "Ruby Chen",
        "type": "Researcher"
      },
      {
        "name": "Jason Chen",
        "type": "Researcher"
      },
      {
        "name": "Ben Chess",
        "type": "Researcher"
      },
      {
        "name": "Chester Cho",
        "type": "Researcher"
      },
      {
        "name": "Casey Chu",
        "type": "Researcher"
      },
      {
        "name": "Dave Cummings",
        "type": "Researcher"
      },
      {
        "name": "Jeremiah Currier",
        "type": "Researcher"
      },
      {
        "name": "Yunxing Dai",
        "type": "Researcher"
      },
      {
        "name": "Cory Decareaux",
        "type": "Researcher"
      },
      {
        "name": "Thomas Degry",
        "type": "Researcher"
      },
      {
        "name": "Noah Deutsch",
        "type": "Researcher"
      },
      {
        "name": "Damien Deville",
        "type": "Researcher"
      },
      {
        "name": "Arka Dhar",
        "type": "Researcher"
      },
      {
        "name": "David Dohan",
        "type": "Researcher"
      },
      {
        "name": "Steve Dowling",
        "type": "Researcher"
      },
      {
        "name": "Sheila Dunning",
        "type": "Researcher"
      },
      {
        "name": "Adrien Ecoffet",
        "type": "Researcher"
      },
      {
        "name": "Atty Eleti",
        "type": "Researcher"
      },
      {
        "name": "Tyna Eloundou",
        "type": "Researcher"
      },
      {
        "name": "David Farhi",
        "type": "Researcher"
      },
      {
        "name": "Liam Fedus",
        "type": "Researcher"
      },
      {
        "name": "Niko Felix",
        "type": "Researcher"
      },
      {
        "name": "Simón Posada Fishman",
        "type": "Researcher"
      },
      {
        "name": "Juston Forte",
        "type": "Researcher"
      },
      {
        "name": "Isabella Fulford",
        "type": "Researcher"
      },
      {
        "name": "Leo Gao",
        "type": "Researcher"
      },
      {
        "name": "Elie Georges",
        "type": "Researcher"
      },
      {
        "name": "Christian Gibson",
        "type": "Researcher"
      },
      {
        "name": "Vik Goel",
        "type": "Researcher"
      },
      {
        "name": "Tarun Gogineni",
        "type": "Researcher"
      },
      {
        "name": "Rapha Gontijo-Lopes",
        "type": "Researcher"
      },
      {
        "name": "Jonathan Gordon",
        "type": "Researcher"
      },
      {
        "name": "Morgan Grafstein",
        "type": "Researcher"
      },
      {
        "name": "Ryan Greene",
        "type": "Researcher"
      },
      {
        "name": "Joshua Gross",
        "type": "Researcher"
      },
      {
        "name": "Yufei Guo",
        "type": "Researcher"
      },
      {
        "name": "Jesse Han",
        "type": "Researcher"
      },
      {
        "name": "Jeff Harris",
        "type": "Researcher"
      },
      {
        "name": "Yuchen He",
        "type": "Researcher"
      },
      {
        "name": "Mike Heaton",
        "type": "Researcher"
      },
      {
        "name": "Johannes Heidecke",
        "type": "Researcher"
      },
      {
        "name": "Chris Hesse",
        "type": "Researcher"
      },
      {
        "name": "Alan Hickey",
        "type": "Researcher"
      },
      {
        "name": "Wade Hickey",
        "type": "Researcher"
      },
      {
        "name": "Peter Hoeschele",
        "type": "Researcher"
      },
      {
        "name": "Brandon Houghton",
        "type": "Researcher"
      },
      {
        "name": "Kenny Hsu",
        "type": "Researcher"
      },
      {
        "name": "Shengli Hu",
        "type": "Researcher"
      },
      {
        "name": "Xin Hu",
        "type": "Researcher"
      },
      {
        "name": "Joost Huizinga",
        "type": "Researcher"
      },
      {
        "name": "Shantanu Jain",
        "type": "Researcher"
      },
      {
        "name": "Shawn Jain",
        "type": "Researcher"
      },
      {
        "name": "Joanne Jang",
        "type": "Researcher"
      },
      {
        "name": "Angela Jiang",
        "type": "Researcher"
      },
      {
        "name": "Roger Jiang",
        "type": "Researcher"
      },
      {
        "name": "Haozhun Jin",
        "type": "Researcher"
      },
      {
        "name": "Denny Jin",
        "type": "Researcher"
      },
      {
        "name": "Shino Jomoto",
        "type": "Researcher"
      },
      {
        "name": "Billie Jonn",
        "type": "Researcher"
      },
      {
        "name": "Heewoo Jun",
        "type": "Researcher"
      },
      {
        "name": "Tomer Kaftan",
        "type": "Researcher"
      },
      {
        "name": "Łukasz Kaiser",
        "type": "Researcher"
      },
      {
        "name": "Ali Kamali",
        "type": "Researcher"
      },
      {
        "name": "Ingmar Kanitscheider",
        "type": "Researcher"
      },
      {
        "name": "Nitish Shirish Keskar",
        "type": "Researcher"
      },
      {
        "name": "Tabarak Khan",
        "type": "Researcher"
      },
      {
        "name": "Logan Kilpatrick",
        "type": "Researcher"
      },
      {
        "name": "Christina Kim",
        "type": "Researcher"
      },
      {
        "name": "Yongjik Kim",
        "type": "Researcher"
      },
      {
        "name": "Jan Hendrik Kirchner",
        "type": "Researcher"
      },
      {
        "name": "Jamie Kiros",
        "type": "Researcher"
      },
      {
        "name": "Matt Knight",
        "type": "Researcher"
      },
      {
        "name": "Daniel Kokotajlo",
        "type": "Researcher"
      },
      {
        "name": "Łukasz Kondraciuk",
        "type": "Researcher"
      },
      {
        "name": "Andrew Kondrich",
        "type": "Researcher"
      },
      {
        "name": "Aris Konstantinidis",
        "type": "Researcher"
      },
      {
        "name": "Kyle Kosic",
        "type": "Researcher"
      },
      {
        "name": "Vishal Kuo",
        "type": "Researcher"
      },
      {
        "name": "Michael Lampe",
        "type": "Researcher"
      },
      {
        "name": "Ikai Lan",
        "type": "Researcher"
      },
      {
        "name": "Teddy Lee",
        "type": "Researcher"
      },
      {
        "name": "Jan Leike",
        "type": "Researcher"
      },
      {
        "name": "Jade Leung",
        "type": "Researcher"
      },
      {
        "name": "Daniel Levy",
        "type": "Researcher"
      },
      {
        "name": "Chak Ming Li",
        "type": "Researcher"
      },
      {
        "name": "Rachel Lim",
        "type": "Researcher"
      },
      {
        "name": "Molly Lin",
        "type": "Researcher"
      },
      {
        "name": "Stephanie Lin",
        "type": "Researcher"
      },
      {
        "name": "Theresa Lopez",
        "type": "Researcher"
      },
      {
        "name": "Ryan Lowe",
        "type": "Researcher"
      },
      {
        "name": "Patricia Lue",
        "type": "Researcher"
      },
      {
        "name": "Anna Makanju",
        "type": "Researcher"
      },
      {
        "name": "Kim Malfacini",
        "type": "Researcher"
      },
      {
        "name": "Sam Manning",
        "type": "Researcher"
      },
      {
        "name": "Todor Markov",
        "type": "Researcher"
      },
      {
        "name": "Yaniv Markovski",
        "type": "Researcher"
      },
      {
        "name": "Bianca Martin",
        "type": "Researcher"
      },
      {
        "name": "Katie Mayer",
        "type": "Researcher"
      },
      {
        "name": "Andrew Mayne",
        "type": "Researcher"
      },
      {
        "name": "Bob McGrew",
        "type": "Researcher"
      },
      {
        "name": "Scott Mayer McKinney",
        "type": "Researcher"
      },
      {
        "name": "Christine McLeavey",
        "type": "Researcher"
      },
      {
        "name": "Paul McMillan",
        "type": "Researcher"
      },
      {
        "name": "Jake McNeil",
        "type": "Researcher"
      },
      {
        "name": "David Medina",
        "type": "Researcher"
      },
      {
        "name": "Aalok Mehta",
        "type": "Researcher"
      },
      {
        "name": "Jacob Menick",
        "type": "Researcher"
      },
      {
        "name": "Luke Metz",
        "type": "Researcher"
      },
      {
        "name": "Andrey Mishchenko",
        "type": "Researcher"
      },
      {
        "name": "Vinnie Monaco",
        "type": "Researcher"
      },
      {
        "name": "Evan Morikawa",
        "type": "Researcher"
      },
      {
        "name": "Daniel Mossing",
        "type": "Researcher"
      },
      {
        "name": "Tong Mu",
        "type": "Researcher"
      },
      {
        "name": "Mira Murati",
        "type": "Researcher"
      },
      {
        "name": "Oleg Murk",
        "type": "Researcher"
      },
      {
        "name": "David Mély",
        "type": "Researcher"
      },
      {
        "name": "Ashvin Nair",
        "type": "Researcher"
      },
      {
        "name": "Reiichiro Nakano",
        "type": "Researcher"
      },
      {
        "name": "Rajeev Nayak",
        "type": "Researcher"
      },
      {
        "name": "Richard Ngo",
        "type": "Researcher"
      },
      {
        "name": "Hyeonwoo Noh",
        "type": "Researcher"
      },
      {
        "name": "Long Ouyang",
        "type": "Researcher"
      },
      {
        "name": "Cullen O'Keefe",
        "type": "Researcher"
      },
      {
        "name": "Jakub Pachocki",
        "type": "Researcher"
      },
      {
        "name": "Alex Paino",
        "type": "Researcher"
      },
      {
        "name": "Joe Palermo",
        "type": "Researcher"
      },
      {
        "name": "Ashley Pantuliano",
        "type": "Researcher"
      },
      {
        "name": "Giambattista Parascandolo",
        "type": "Researcher"
      },
      {
        "name": "Joel Parish",
        "type": "Researcher"
      },
      {
        "name": "Emy Parparita",
        "type": "Researcher"
      },
      {
        "name": "Alex Passos",
        "type": "Researcher"
      },
      {
        "name": "Mikhail Pavlov",
        "type": "Researcher"
      },
      {
        "name": "Andrew Peng",
        "type": "Researcher"
      },
      {
        "name": "Adam Perelman",
        "type": "Researcher"
      },
      {
        "name": "Filipe de Avila Belbute Peres",
        "type": "Researcher"
      },
      {
        "name": "Michael Petrov",
        "type": "Researcher"
      },
      {
        "name": "Henrique Ponde de Oliveira Pinto",
        "type": "Researcher"
      },
      {
        "name": "Michael",
        "type": "Researcher"
      },
      {
        "name": "Pokorny",
        "type": "Researcher"
      },
      {
        "name": "Michelle Pokrass",
        "type": "Researcher"
      },
      {
        "name": "Vitchyr H. Pong",
        "type": "Researcher"
      },
      {
        "name": "Tolly Powell",
        "type": "Researcher"
      },
      {
        "name": "Alethea Power",
        "type": "Researcher"
      },
      {
        "name": "Boris Power",
        "type": "Researcher"
      },
      {
        "name": "Elizabeth Proehl",
        "type": "Researcher"
      },
      {
        "name": "Raul Puri",
        "type": "Researcher"
      },
      {
        "name": "Jack Rae",
        "type": "Researcher"
      },
      {
        "name": "Cameron Raymond",
        "type": "Researcher"
      },
      {
        "name": "Francis Real",
        "type": "Researcher"
      },
      {
        "name": "Kendra Rimbach",
        "type": "Researcher"
      },
      {
        "name": "Carl Ross",
        "type": "Researcher"
      },
      {
        "name": "Bob Rotsted",
        "type": "Researcher"
      },
      {
        "name": "Henri Roussez",
        "type": "Researcher"
      },
      {
        "name": "Mario Saltarelli",
        "type": "Researcher"
      },
      {
        "name": "Ted Sanders",
        "type": "Researcher"
      },
      {
        "name": "Shibani Santurkar",
        "type": "Researcher"
      },
      {
        "name": "Heather Schmidt",
        "type": "Researcher"
      },
      {
        "name": "David Schnurr",
        "type": "Researcher"
      },
      {
        "name": "Daniel Selsam",
        "type": "Researcher"
      },
      {
        "name": "Kyla Sheppard",
        "type": "Researcher"
      },
      {
        "name": "Toki Sherbakov",
        "type": "Researcher"
      },
      {
        "name": "Jessica Shieh",
        "type": "Researcher"
      },
      {
        "name": "Sarah Shoker",
        "type": "Researcher"
      },
      {
        "name": "Szymon Sidor",
        "type": "Researcher"
      },
      {
        "name": "Maddie Simens",
        "type": "Researcher"
      },
      {
        "name": "Jordan Sitkin",
        "type": "Researcher"
      },
      {
        "name": "Katarina Slama",
        "type": "Researcher"
      },
      {
        "name": "Ian Sohl",
        "type": "Researcher"
      },
      {
        "name": "Benjamin Sokolowsky",
        "type": "Researcher"
      },
      {
        "name": "Yang Song",
        "type": "Researcher"
      },
      {
        "name": "Natalie Staudacher",
        "type": "Researcher"
      },
      {
        "name": "Felipe Petroski Such",
        "type": "Researcher"
      },
      {
        "name": "Natalie Summers",
        "type": "Researcher"
      },
      {
        "name": "Jie Tang",
        "type": "Researcher"
      },
      {
        "name": "Nikolas Tezak",
        "type": "Researcher"
      },
      {
        "name": "Madeleine B. Thompson",
        "type": "Researcher"
      },
      {
        "name": "Phil Tillet",
        "type": "Researcher"
      },
      {
        "name": "Amin Tootoonchian",
        "type": "Researcher"
      },
      {
        "name": "Elizabeth Tseng",
        "type": "Researcher"
      },
      {
        "name": "Preston Tuggle",
        "type": "Researcher"
      },
      {
        "name": "Nick Turley",
        "type": "Researcher"
      },
      {
        "name": "Jerry Tworek",
        "type": "Researcher"
      },
      {
        "name": "Juan Felipe Cerón Uribe",
        "type": "Researcher"
      },
      {
        "name": "Andrea Vallone",
        "type": "Researcher"
      },
      {
        "name": "Arun Vijayvergiya",
        "type": "Researcher"
      },
      {
        "name": "Chelsea Voss",
        "type": "Researcher"
      },
      {
        "name": "Carroll Wainwright",
        "type": "Researcher"
      },
      {
        "name": "Justin Jay Wang",
        "type": "Researcher"
      },
      {
        "name": "Alvin Wang",
        "type": "Researcher"
      },
      {
        "name": "Ben Wang",
        "type": "Researcher"
      },
      {
        "name": "Jonathan Ward",
        "type": "Researcher"
      },
      {
        "name": "CJ Weinmann",
        "type": "Researcher"
      },
      {
        "name": "Akila Welihinda",
        "type": "Researcher"
      },
      {
        "name": "Peter Welinder",
        "type": "Researcher"
      },
      {
        "name": "Jiayi Weng",
        "type": "Researcher"
      },
      {
        "name": "Lilian Weng",
        "type": "Researcher"
      },
      {
        "name": "Matt Wiethoff",
        "type": "Researcher"
      },
      {
        "name": "Dave Willner",
        "type": "Researcher"
      },
      {
        "name": "Samuel Wolrich",
        "type": "Researcher"
      },
      {
        "name": "Hannah Wong",
        "type": "Researcher"
      },
      {
        "name": "Lauren Workman",
        "type": "Researcher"
      },
      {
        "name": "Sherwin Wu",
        "type": "Researcher"
      },
      {
        "name": "Jeff Wu",
        "type": "Researcher"
      },
      {
        "name": "Michael Wu",
        "type": "Researcher"
      },
      {
        "name": "Kai Xiao",
        "type": "Researcher"
      },
      {
        "name": "Tao Xu",
        "type": "Researcher"
      },
      {
        "name": "Sarah Yoo",
        "type": "Researcher"
      },
      {
        "name": "Kevin Yu",
        "type": "Researcher"
      },
      {
        "name": "Qiming Yuan",
        "type": "Researcher"
      },
      {
        "name": "Wojciech Zaremba",
        "type": "Researcher"
      },
      {
        "name": "Rowan Zellers",
        "type": "Researcher"
      },
      {
        "name": "Chong Zhang",
        "type": "Researcher"
      },
      {
        "name": "Marvin Zhang",
        "type": "Researcher"
      },
      {
        "name": "Shengjia Zhao",
        "type": "Researcher"
      },
      {
        "name": "Tianhao Zheng",
        "type": "Researcher"
      },
      {
        "name": "Juntang Zhuang",
        "type": "Researcher"
      },
      {
        "name": "William Zhuk",
        "type": "Researcher"
      },
      {
        "name": "Hugo Touvron",
        "type": "Researcher"
      },
      {
        "name": "Thibaut Lavril",
        "type": "Researcher"
      },
      {
        "name": "Gautier Izacard",
        "type": "Researcher"
      },
      {
        "name": "Xavier Martinet",
        "type": "Researcher"
      },
      {
        "name": "Marie-Anne Lachaux",
        "type": "Researcher"
      },
      {
        "name": "Timothée Lacroix",
        "type": "Researcher"
      },
      {
        "name": "Baptiste Rozière",
        "type": "Researcher"
      },
      {
        "name": "Naman Goyal",
        "type": "Researcher"
      },
      {
        "name": "Eric Hambro",
        "type": "Researcher"
      },
      {
        "name": "Faisal Azhar",
        "type": "Researcher"
      },
      {
        "name": "Aurelien Rodriguez",
        "type": "Researcher"
      },
      {
        "name": "Armand Joulin",
        "type": "Researcher"
      },
      {
        "name": "Edouard Grave",
        "type": "Researcher"
      },
      {
        "name": "Guillaume Lample",
        "type": "Researcher"
      },
      {
        "name": "Dale Schuurmans",
        "type": "Researcher"
      },
      {
        "name": "Maarten Bosma",
        "type": "Researcher"
      },
      {
        "name": "Brian Ichter",
        "type": "Researcher"
      },
      {
        "name": "Fei Xia",
        "type": "Researcher"
      },
      {
        "name": "Ed Chi",
        "type": "Researcher"
      },
      {
        "name": "Quoc Le",
        "type": "Researcher"
      },
      {
        "name": "Karl Cobbe",
        "type": "Researcher"
      },
      {
        "name": "Vineet Kosaraju",
        "type": "Researcher"
      },
      {
        "name": "Matthias Plappert",
        "type": "Researcher"
      },
      {
        "name": "Jacob Hilton",
        "type": "Researcher"
      },
      {
        "name": "K. Luger",
        "type": "Researcher"
      },
      {
        "name": "A. Mäder",
        "type": "Researcher"
      },
      {
        "name": "R. K. Richmond",
        "type": "Researcher"
      },
      {
        "name": "D. Sargent",
        "type": "Researcher"
      },
      {
        "name": "T. Richmond",
        "type": "Researcher"
      },
      {
        "name": "Albert Gu",
        "type": "Researcher"
      },
      {
        "name": "Tri Dao",
        "type": "Researcher"
      },
      {
        "name": "Stephen M. Mount",
        "type": "Researcher"
      },
      {
        "name": "F. Crick",
        "type": "Researcher"
      },
      {
        "name": "Ilya Loshchilov",
        "type": "Researcher"
      },
      {
        "name": "Frank Hutter",
        "type": "Researcher"
      },
      {
        "name": "Azalia Mirhoseini",
        "type": "Researcher"
      },
      {
        "name": "Krzysztof Maziarz",
        "type": "Researcher"
      },
      {
        "name": "Andy Davis",
        "type": "Researcher"
      },
      {
        "name": "J. Ziv",
        "type": "Researcher"
      },
      {
        "name": "A. Lempel",
        "type": "Researcher"
      },
      {
        "name": "Stephen Merity",
        "type": "Researcher"
      },
      {
        "name": "Caiming Xiong",
        "type": "Researcher"
      },
      {
        "name": "James Bradbury",
        "type": "Researcher"
      },
      {
        "name": "Richard Socher",
        "type": "Researcher"
      },
      {
        "name": "Dan Hendrycks",
        "type": "Researcher"
      },
      {
        "name": "Collin Burns",
        "type": "Researcher"
      },
      {
        "name": "Steven Basart",
        "type": "Researcher"
      },
      {
        "name": "Andy Zou",
        "type": "Researcher"
      },
      {
        "name": "Mantas Mazeika",
        "type": "Researcher"
      },
      {
        "name": "Dawn Song",
        "type": "Researcher"
      },
      {
        "name": "Jacob Steinhardt",
        "type": "Researcher"
      },
      {
        "name": "Hunter Lightman",
        "type": "Researcher"
      },
      {
        "name": "Yura Burda",
        "type": "Researcher"
      },
      {
        "name": "Harri Edwards",
        "type": "Researcher"
      },
      {
        "name": "Bowen Baker",
        "type": "Researcher"
      },
      {
        "name": "David Rein",
        "type": "Researcher"
      },
      {
        "name": "Betty Li Hou",
        "type": "Researcher"
      },
      {
        "name": "Asa Cooper Stickland",
        "type": "Researcher"
      },
      {
        "name": "Jackson Petty",
        "type": "Researcher"
      },
      {
        "name": "Richard Yuanzhe Pang",
        "type": "Researcher"
      },
      {
        "name": "Julien Dirani",
        "type": "Researcher"
      },
      {
        "name": "Julian Michael",
        "type": "Researcher"
      },
      {
        "name": "Samuel R. Bowman",
        "type": "Researcher"
      },
      {
        "name": "Dmitry Lepikhin",
        "type": "Researcher"
      },
      {
        "name": "HyoukJoong Lee",
        "type": "Researcher"
      },
      {
        "name": "Yuanzhong Xu",
        "type": "Researcher"
      },
      {
        "name": "Dehao Chen",
        "type": "Researcher"
      },
      {
        "name": "Orhan Firat",
        "type": "Researcher"
      },
      {
        "name": "Maxim Krikun",
        "type": "Researcher"
      },
      {
        "name": "Radford M. Neal",
        "type": "Researcher"
      },
      {
        "name": "L. Breiman",
        "type": "Researcher"
      },
      {
        "name": "Guan Wang",
        "type": "Researcher"
      },
      {
        "name": "Yu Sun",
        "type": "Researcher"
      },
      {
        "name": "Jianxin Wang",
        "type": "Researcher"
      },
      {
        "name": "Mostafa Mehdipour-Ghazi",
        "type": "Researcher"
      },
      {
        "name": "B. Yanikoglu",
        "type": "Researcher"
      },
      {
        "name": "E. Aptoula",
        "type": "Researcher"
      },
      {
        "name": "Sue Han Lee",
        "type": "Researcher"
      },
      {
        "name": "H. Goëau",
        "type": "Researcher"
      },
      {
        "name": "P. Bonnet",
        "type": "Researcher"
      },
      {
        "name": "A. Joly",
        "type": "Researcher"
      },
      {
        "name": "Haiyan Zhang",
        "type": "Researcher"
      },
      {
        "name": "Jose Carranza-Rojas",
        "type": "Researcher"
      },
      {
        "name": "Hervé Goeau",
        "type": "Researcher"
      },
      {
        "name": "Erick Mata-Montero",
        "type": "Researcher"
      },
      {
        "name": "Mark Sandler",
        "type": "Researcher"
      },
      {
        "name": "Andrew Howard",
        "type": "Researcher"
      },
      {
        "name": "Menglong Zhu",
        "type": "Researcher"
      },
      {
        "name": "Andrey Zhmoginov",
        "type": "Researcher"
      },
      {
        "name": "Liang-Chieh Chen",
        "type": "Researcher"
      },
      {
        "name": "H. Brendan McMahan",
        "type": "Researcher"
      },
      {
        "name": "Eider Moore",
        "type": "Researcher"
      },
      {
        "name": "Daniel Ramage",
        "type": "Researcher"
      },
      {
        "name": "Seth Hampson",
        "type": "Researcher"
      },
      {
        "name": "Blaise Agüera y Arcas",
        "type": "Researcher"
      },
      {
        "name": "Fuzhen Zhuang",
        "type": "Researcher"
      },
      {
        "name": "Zhiyuan Qi",
        "type": "Researcher"
      },
      {
        "name": "Keyu Duan",
        "type": "Researcher"
      },
      {
        "name": "Dongbo Xi",
        "type": "Researcher"
      },
      {
        "name": "Yongchun Zhu",
        "type": "Researcher"
      },
      {
        "name": "Hengshu Zhu",
        "type": "Researcher"
      },
      {
        "name": "Hui Xiong",
        "type": "Researcher"
      },
      {
        "name": "Qing He",
        "type": "Researcher"
      },
      {
        "name": "A. Khan",
        "type": "Researcher"
      },
      {
        "name": "Wadii Boulila",
        "type": "Researcher"
      },
      {
        "name": "G. A. Sampedro",
        "type": "Researcher"
      },
      {
        "name": "Sidra Abbas",
        "type": "Researcher"
      },
      {
        "name": "Chitapong Wechtaisong",
        "type": "Researcher"
      },
      {
        "name": "Tesfahunegn Minwuyelet Mengistu",
        "type": "Researcher"
      },
      {
        "name": "Taewoon Kim",
        "type": "Researcher"
      },
      {
        "name": "Jenn-Wei Lin",
        "type": "Researcher"
      },
      {
        "name": "A. Alamer",
        "type": "Researcher"
      },
      {
        "name": "Manel Khazri Khlifi",
        "type": "Researcher"
      },
      {
        "name": "I. Farah",
        "type": "Researcher"
      },
      {
        "name": "Anwesha Mukherjee",
        "type": "Researcher"
      },
      {
        "name": "Rajkumar Buyya",
        "type": "Researcher"
      },
      {
        "name": "Alexander Kirillov",
        "type": "Researcher"
      },
      {
        "name": "Eric Mintun",
        "type": "Researcher"
      },
      {
        "name": "Nikhila Ravi",
        "type": "Researcher"
      },
      {
        "name": "Hanzi Mao",
        "type": "Researcher"
      },
      {
        "name": "Chloe Rolland",
        "type": "Researcher"
      },
      {
        "name": "Laura Gustafson",
        "type": "Researcher"
      },
      {
        "name": "Tete Xiao",
        "type": "Researcher"
      },
      {
        "name": "Spencer Whitehead",
        "type": "Researcher"
      },
      {
        "name": "Wan-Yen Lo",
        "type": "Researcher"
      },
      {
        "name": "Haotian Liu",
        "type": "Researcher"
      },
      {
        "name": "Qingyang Wu",
        "type": "Researcher"
      },
      {
        "name": "Yong Jae Lee",
        "type": "Researcher"
      },
      {
        "name": "Yuheng Li",
        "type": "Researcher"
      },
      {
        "name": "Zhiyuan You",
        "type": "Researcher"
      },
      {
        "name": "Jinjin Gu",
        "type": "Researcher"
      },
      {
        "name": "Xin Cai",
        "type": "Researcher"
      },
      {
        "name": "Zheyuan Li",
        "type": "Researcher"
      },
      {
        "name": "Kaiwen Zhu",
        "type": "Researcher"
      },
      {
        "name": "Chao Dong",
        "type": "Researcher"
      },
      {
        "name": "Tianfan Xue",
        "type": "Researcher"
      },
      {
        "name": "Xintong Zhang",
        "type": "Researcher"
      },
      {
        "name": "Zhi Gao",
        "type": "Researcher"
      },
      {
        "name": "Bofei Zhang",
        "type": "Researcher"
      },
      {
        "name": "Pengxiang Li",
        "type": "Researcher"
      },
      {
        "name": "Xiaowen Zhang",
        "type": "Researcher"
      },
      {
        "name": "Yang Liu",
        "type": "Researcher"
      },
      {
        "name": "Tao Yuan",
        "type": "Researcher"
      },
      {
        "name": "Yuwei Wu",
        "type": "Researcher"
      },
      {
        "name": "Yunde Jia",
        "type": "Researcher"
      },
      {
        "name": "Song-Chun Zhu",
        "type": "Researcher"
      },
      {
        "name": "Qing Li",
        "type": "Researcher"
      },
      {
        "name": "Zhangquan Chen",
        "type": "Researcher"
      },
      {
        "name": "Manyuan Zhang",
        "type": "Researcher"
      },
      {
        "name": "Xinlei Yu",
        "type": "Researcher"
      },
      {
        "name": "Xufang Luo",
        "type": "Researcher"
      },
      {
        "name": "Mingze Sun",
        "type": "Researcher"
      },
      {
        "name": "Zihao Pan",
        "type": "Researcher"
      },
      {
        "name": "Yan Feng",
        "type": "Researcher"
      },
      {
        "name": "Ruqi Huang",
        "type": "Researcher"
      },
      {
        "name": "Tiancheng Gu",
        "type": "Researcher"
      },
      {
        "name": "Kaichen Zhang",
        "type": "Researcher"
      },
      {
        "name": "Yueyi Zhang",
        "type": "Researcher"
      },
      {
        "name": "Weidong Cai",
        "type": "Researcher"
      },
      {
        "name": "Lidong Bing",
        "type": "Researcher"
      },
      {
        "name": "Keming Wu",
        "type": "Researcher"
      },
      {
        "name": "Zuhao Yang",
        "type": "Researcher"
      },
      {
        "name": "Kairui Hu",
        "type": "Researcher"
      },
      {
        "name": "Bin Wang",
        "type": "Researcher"
      },
      {
        "name": "Xingxuan Li",
        "type": "Researcher"
      },
      {
        "name": "Ranjan Sapkota",
        "type": "Researcher"
      },
      {
        "name": "Yang Cao",
        "type": "Researcher"
      },
      {
        "name": "Konstantinos I. Roumeliotis",
        "type": "Researcher"
      },
      {
        "name": "Manoj Karkee",
        "type": "Researcher"
      },
      {
        "name": "Kohei Sendai",
        "type": "Researcher"
      },
      {
        "name": "Maxime Alvarez",
        "type": "Researcher"
      },
      {
        "name": "Tatsuya Matsushima",
        "type": "Researcher"
      },
      {
        "name": "Yutaka Matsuo",
        "type": "Researcher"
      },
      {
        "name": "Yusuke Iwasawa",
        "type": "Researcher"
      },
      {
        "name": "Shuhan Tan",
        "type": "Researcher"
      },
      {
        "name": "Philipp Krahenbuhl",
        "type": "Researcher"
      },
      {
        "name": "Zheng Xiong",
        "type": "Researcher"
      },
      {
        "name": "Kang Li",
        "type": "Researcher"
      },
      {
        "name": "Zilin Wang",
        "type": "Researcher"
      },
      {
        "name": "Matthew Jackson",
        "type": "Researcher"
      },
      {
        "name": "Jakob Foerster",
        "type": "Researcher"
      },
      {
        "name": "Shimon Whiteson",
        "type": "Researcher"
      },
      {
        "name": "Yifan Ye",
        "type": "Researcher"
      },
      {
        "name": "Jiaqi Ma",
        "type": "Researcher"
      },
      {
        "name": "Jun Cen",
        "type": "Researcher"
      },
      {
        "name": "Zhihe Lu",
        "type": "Researcher"
      },
      {
        "name": "Zhaohu Xing",
        "type": "Researcher"
      },
      {
        "name": "Tian Ye",
        "type": "Researcher"
      },
      {
        "name": "Yijun Yang",
        "type": "Researcher"
      },
      {
        "name": "D. Cai",
        "type": "Researcher"
      },
      {
        "name": "Baowen Gai",
        "type": "Researcher"
      },
      {
        "name": "Xiao-Jian Wu",
        "type": "Researcher"
      },
      {
        "name": "Feng Gao",
        "type": "Researcher"
      },
      {
        "name": "Lei Zhu",
        "type": "Researcher"
      },
      {
        "name": "Hai Liu",
        "type": "Researcher"
      },
      {
        "name": "Yu Song",
        "type": "Researcher"
      },
      {
        "name": "Tingting Liu",
        "type": "Researcher"
      },
      {
        "name": "Zhaoli Zhang",
        "type": "Researcher"
      },
      {
        "name": "Xiaolan Yang",
        "type": "Researcher"
      },
      {
        "name": "Neal N. Xiong",
        "type": "Researcher"
      },
      {
        "name": "Honghu Chu",
        "type": "Researcher"
      },
      {
        "name": "Jiahao Gai",
        "type": "Researcher"
      },
      {
        "name": "Weiwei Chen",
        "type": "Researcher"
      },
      {
        "name": "Jun Ma",
        "type": "Researcher"
      },
      {
        "name": "Yinjun Jia",
        "type": "Researcher"
      },
      {
        "name": "Bowen Gao",
        "type": "Researcher"
      },
      {
        "name": "Jiaxin Tan",
        "type": "Researcher"
      },
      {
        "name": "Jiqing Zheng",
        "type": "Researcher"
      },
      {
        "name": "Xin Hong",
        "type": "Researcher"
      },
      {
        "name": "Wenyu Zhu",
        "type": "Researcher"
      },
      {
        "name": "Haichuan Tan",
        "type": "Researcher"
      },
      {
        "name": "Yuan Xiao",
        "type": "Researcher"
      },
      {
        "name": "Liping Tan",
        "type": "Researcher"
      },
      {
        "name": "Hongyi Cai",
        "type": "Researcher"
      },
      {
        "name": "Yanwen Huang",
        "type": "Researcher"
      },
      {
        "name": "Zhiheng Deng",
        "type": "Researcher"
      },
      {
        "name": "Xiangwei Wu",
        "type": "Researcher"
      },
      {
        "name": "Yue Jin",
        "type": "Researcher"
      },
      {
        "name": "Yafei Yuan",
        "type": "Researcher"
      },
      {
        "name": "Jiekang Tian",
        "type": "Researcher"
      },
      {
        "name": "Wei He",
        "type": "Researcher"
      },
      {
        "name": "Weiying Ma",
        "type": "Researcher"
      },
      {
        "name": "Chuangye Yan",
        "type": "Researcher"
      },
      {
        "name": "Yanyan Lan",
        "type": "Researcher"
      },
      {
        "name": "Guodong Fan",
        "type": "Researcher"
      },
      {
        "name": "Shengning Zhou",
        "type": "Researcher"
      },
      {
        "name": "Zhen Hua",
        "type": "Researcher"
      },
      {
        "name": "Jinjiang Li",
        "type": "Researcher"
      },
      {
        "name": "Jingchun Zhou",
        "type": "Researcher"
      },
      {
        "name": "Jiahua Dong",
        "type": "Researcher"
      },
      {
        "name": "Yu-Xiong Wang",
        "type": "Researcher"
      },
      {
        "name": "Hasi Hays",
        "type": "Researcher"
      },
      {
        "name": "Qiao Sun",
        "type": "Researcher"
      },
      {
        "name": "Xianbang Wang",
        "type": "Researcher"
      },
      {
        "name": "Zhicheng Jiang",
        "type": "Researcher"
      },
      {
        "name": "Hanhong Zhao",
        "type": "Researcher"
      },
      {
        "name": "Susie Lu",
        "type": "Researcher"
      },
      {
        "name": "Tianhong Li",
        "type": "Researcher"
      },
      {
        "name": "Peter Potaptchik",
        "type": "Researcher"
      },
      {
        "name": "Adhi Saravanan",
        "type": "Researcher"
      },
      {
        "name": "Abbas Mammadov",
        "type": "Researcher"
      },
      {
        "name": "Alvaro Prat",
        "type": "Researcher"
      },
      {
        "name": "Michael S. Albergo",
        "type": "Researcher"
      },
      {
        "name": "Yee Whye Teh",
        "type": "Researcher"
      },
      {
        "name": "Yinan Huang",
        "type": "Researcher"
      },
      {
        "name": "Hans Hao-Hsun Hsu",
        "type": "Researcher"
      },
      {
        "name": "Junran Wang",
        "type": "Researcher"
      },
      {
        "name": "Bo Dai",
        "type": "Researcher"
      },
      {
        "name": "Pan Li",
        "type": "Researcher"
      },
      {
        "name": "Mingyang Deng",
        "type": "Researcher"
      },
      {
        "name": "He Li",
        "type": "Researcher"
      },
      {
        "name": "Yilun Du",
        "type": "Researcher"
      },
      {
        "name": "Ting Chen",
        "type": "Researcher"
      },
      {
        "name": "Mohammad Norouzi",
        "type": "Researcher"
      },
      {
        "name": "Chubin Chen",
        "type": "Researcher"
      },
      {
        "name": "Sujie Hu",
        "type": "Researcher"
      },
      {
        "name": "Jiashu Zhu",
        "type": "Researcher"
      },
      {
        "name": "Meiqi Wu",
        "type": "Researcher"
      },
      {
        "name": "Jintao Chen",
        "type": "Researcher"
      },
      {
        "name": "Yanxun Li",
        "type": "Researcher"
      },
      {
        "name": "Nisha Huang",
        "type": "Researcher"
      },
      {
        "name": "Chengyu Fang",
        "type": "Researcher"
      },
      {
        "name": "Xiaokun Feng",
        "type": "Researcher"
      },
      {
        "name": "Chen Zhu",
        "type": "Researcher"
      },
      {
        "name": "Bingze Song",
        "type": "Researcher"
      },
      {
        "name": "Fangyuan Mao",
        "type": "Researcher"
      },
      {
        "name": "Kaiqi Huang",
        "type": "Researcher"
      },
      {
        "name": "Ziteng Wang",
        "type": "Researcher"
      },
      {
        "name": "Bingda Tang",
        "type": "Researcher"
      },
      {
        "name": "Ellis Brown",
        "type": "Researcher"
      },
      {
        "name": "Jihan Yang",
        "type": "Researcher"
      },
      {
        "name": "Rob Fergus",
        "type": "Researcher"
      },
      {
        "name": "Yann LeCun",
        "type": "Researcher"
      },
      {
        "name": "Jingtong Yue",
        "type": "Researcher"
      },
      {
        "name": "Ziqi Huang",
        "type": "Researcher"
      },
      {
        "name": "Kaixin Zhu",
        "type": "Researcher"
      },
      {
        "name": "Daili Hua",
        "type": "Researcher"
      },
      {
        "name": "Bozhou Li",
        "type": "Researcher"
      },
      {
        "name": "Chengzhuo Tong",
        "type": "Researcher"
      },
      {
        "name": "Yuran Wang",
        "type": "Researcher"
      },
      {
        "name": "Xinyi Huang",
        "type": "Researcher"
      },
      {
        "name": "Yifan Dai",
        "type": "Researcher"
      },
      {
        "name": "Zixiang Zhang",
        "type": "Researcher"
      },
      {
        "name": "Yifan Yang",
        "type": "Researcher"
      },
      {
        "name": "Zhou Liu",
        "type": "Researcher"
      },
      {
        "name": "Hao Liang",
        "type": "Researcher"
      },
      {
        "name": "Xiaochen Ma",
        "type": "Researcher"
      },
      {
        "name": "Ruichuan An",
        "type": "Researcher"
      },
      {
        "name": "Tianyi Bai",
        "type": "Researcher"
      },
      {
        "name": "Hongcheng Gao",
        "type": "Researcher"
      },
      {
        "name": "Junbo Niu",
        "type": "Researcher"
      },
      {
        "name": "Yang Shi",
        "type": "Researcher"
      },
      {
        "name": "Xinlong Chen",
        "type": "Researcher"
      },
      {
        "name": "Yue Ding",
        "type": "Researcher"
      },
      {
        "name": "Kai Zeng",
        "type": "Researcher"
      },
      {
        "name": "Yiwen Tang",
        "type": "Researcher"
      },
      {
        "name": "Wentao Zhang",
        "type": "Researcher"
      },
      {
        "name": "Qingyu Shi",
        "type": "Researcher"
      },
      {
        "name": "Size Wu",
        "type": "Researcher"
      },
      {
        "name": "Jinbin Bai",
        "type": "Researcher"
      },
      {
        "name": "Kaidong Yu",
        "type": "Researcher"
      },
      {
        "name": "Yujing Wang",
        "type": "Researcher"
      },
      {
        "name": "Yunhai Tong",
        "type": "Researcher"
      },
      {
        "name": "Xiangtai Li",
        "type": "Researcher"
      },
      {
        "name": "Xuelong Li",
        "type": "Researcher"
      },
      {
        "name": "Guanfang Dong",
        "type": "Researcher"
      },
      {
        "name": "Luke Schultz",
        "type": "Researcher"
      },
      {
        "name": "Negar Hassanpour",
        "type": "Researcher"
      },
      {
        "name": "Chao Gao",
        "type": "Researcher"
      },
      {
        "name": "Alex Nichol",
        "type": "Researcher"
      },
      {
        "name": "Maxime Oquab",
        "type": "Researcher"
      },
      {
        "name": "Timothée Darcet",
        "type": "Researcher"
      },
      {
        "name": "Théo Moutakanni",
        "type": "Researcher"
      },
      {
        "name": "Huy Vo",
        "type": "Researcher"
      },
      {
        "name": "Marc Szafraniec",
        "type": "Researcher"
      },
      {
        "name": "Vasil Khalidov",
        "type": "Researcher"
      },
      {
        "name": "Pierre Fernandez",
        "type": "Researcher"
      },
      {
        "name": "Daniel Haziza",
        "type": "Researcher"
      },
      {
        "name": "Francisco Massa",
        "type": "Researcher"
      },
      {
        "name": "Alaaeldin El-Nouby",
        "type": "Researcher"
      },
      {
        "name": "Mahmoud Assran",
        "type": "Researcher"
      },
      {
        "name": "Nicolas Ballas",
        "type": "Researcher"
      },
      {
        "name": "Wojciech Galuba",
        "type": "Researcher"
      },
      {
        "name": "Russell Howes",
        "type": "Researcher"
      },
      {
        "name": "Po-Yao Huang",
        "type": "Researcher"
      },
      {
        "name": "Shang-Wen Li",
        "type": "Researcher"
      },
      {
        "name": "Ishan Misra",
        "type": "Researcher"
      },
      {
        "name": "Michael Rabbat",
        "type": "Researcher"
      },
      {
        "name": "Vasu Sharma",
        "type": "Researcher"
      },
      {
        "name": "Gabriel Synnaeve",
        "type": "Researcher"
      },
      {
        "name": "Hu Xu",
        "type": "Researcher"
      },
      {
        "name": "Hervé Jegou",
        "type": "Researcher"
      },
      {
        "name": "Julien Mairal",
        "type": "Researcher"
      },
      {
        "name": "Patrick Labatut",
        "type": "Researcher"
      },
      {
        "name": "Piotr Bojanowski",
        "type": "Researcher"
      },
      {
        "name": "Zehong Ma",
        "type": "Researcher"
      },
      {
        "name": "Ruihan Xu",
        "type": "Researcher"
      },
      {
        "name": "Shiliang Zhang",
        "type": "Researcher"
      },
      {
        "name": "Shanshan Zhao",
        "type": "Researcher"
      },
      {
        "name": "Xinjie Zhang",
        "type": "Researcher"
      },
      {
        "name": "Jintao Guo",
        "type": "Researcher"
      },
      {
        "name": "Jiakui Hu",
        "type": "Researcher"
      },
      {
        "name": "Lunhao Duan",
        "type": "Researcher"
      },
      {
        "name": "Minghao Fu",
        "type": "Researcher"
      },
      {
        "name": "Yong Xien Chng",
        "type": "Researcher"
      },
      {
        "name": "Guo-Hua Wang",
        "type": "Researcher"
      },
      {
        "name": "Jana Zeller",
        "type": "Researcher"
      },
      {
        "name": "Thaddäus Wiedemer",
        "type": "Researcher"
      },
      {
        "name": "Fanfei Li",
        "type": "Researcher"
      },
      {
        "name": "Thomas Klein",
        "type": "Researcher"
      },
      {
        "name": "Prasanna Mayilvahanan",
        "type": "Researcher"
      },
      {
        "name": "Matthias Bethge",
        "type": "Researcher"
      },
      {
        "name": "Felix Wichmann",
        "type": "Researcher"
      },
      {
        "name": "Ryan Cotterell",
        "type": "Researcher"
      },
      {
        "name": "Wieland Brendel",
        "type": "Researcher"
      },
      {
        "name": "Letian Zhang",
        "type": "Researcher"
      },
      {
        "name": "Sucheng Ren",
        "type": "Researcher"
      },
      {
        "name": "Yanqing Liu",
        "type": "Researcher"
      },
      {
        "name": "Xianhang Li",
        "type": "Researcher"
      },
      {
        "name": "Yuyin Zhou",
        "type": "Researcher"
      },
      {
        "name": "Huaxiu Yao",
        "type": "Researcher"
      },
      {
        "name": "Zeyu Zheng",
        "type": "Researcher"
      },
      {
        "name": "Guilin Liu",
        "type": "Researcher"
      },
      {
        "name": "Zhiding Yu",
        "type": "Researcher"
      },
      {
        "name": "Cihang Xie",
        "type": "Researcher"
      },
      {
        "name": "Tomas Mikolov",
        "type": "Researcher"
      },
      {
        "name": "Greg Corrado",
        "type": "Researcher"
      },
      {
        "name": "Jeffrey Dean",
        "type": "Researcher"
      },
      {
        "name": "Jeffrey Pennington",
        "type": "Researcher"
      },
      {
        "name": "Christopher D. Manning",
        "type": "Researcher"
      },
      {
        "name": "Matthew E. Peters",
        "type": "Researcher"
      },
      {
        "name": "Mark Neumann",
        "type": "Researcher"
      },
      {
        "name": "Mohit Iyyer",
        "type": "Researcher"
      },
      {
        "name": "Matt Gardner",
        "type": "Researcher"
      },
      {
        "name": "Christopher Clark",
        "type": "Researcher"
      },
      {
        "name": "Luke Zettlemoyer",
        "type": "Researcher"
      },
      {
        "name": "Quentin Fournier",
        "type": "Researcher"
      },
      {
        "name": "Robert M. Vernon",
        "type": "Researcher"
      },
      {
        "name": "Almer M. van der Sloot",
        "type": "Researcher"
      },
      {
        "name": "Benjamin Schulz",
        "type": "Researcher"
      },
      {
        "name": "Sarath Chandar",
        "type": "Researcher"
      },
      {
        "name": "C. Langmead",
        "type": "Researcher"
      },
      {
        "name": "Scott Friedman",
        "type": "Researcher"
      },
      {
        "name": "Sonja Schmer-Galunder",
        "type": "Researcher"
      },
      {
        "name": "Anthony Chen",
        "type": "Researcher"
      },
      {
        "name": "Jeffrey Rye",
        "type": "Researcher"
      },
      {
        "name": "Alexander C. Li",
        "type": "Researcher"
      },
      {
        "name": "Ananya Kumar",
        "type": "Researcher"
      },
      {
        "name": "Deepak Pathak",
        "type": "Researcher"
      },
      {
        "name": "Y. Sun",
        "type": "Researcher"
      },
      {
        "name": "Yinqiu Liu",
        "type": "Researcher"
      },
      {
        "name": "Shaoyong Guo",
        "type": "Researcher"
      },
      {
        "name": "Xuesong Qiu",
        "type": "Researcher"
      },
      {
        "name": "Jiewei Chen",
        "type": "Researcher"
      },
      {
        "name": "Jiakai Hao",
        "type": "Researcher"
      },
      {
        "name": "Dusist Niyato",
        "type": "Researcher"
      },
      {
        "name": "R. Child",
        "type": "Researcher"
      },
      {
        "name": "D. Luan",
        "type": "Researcher"
      },
      {
        "name": "Aaron Grattafiori",
        "type": "Researcher"
      },
      {
        "name": "Abhimanyu Dubey",
        "type": "Researcher"
      },
      {
        "name": "Abhinav Jauhri",
        "type": "Researcher"
      },
      {
        "name": "Abhinav Pandey",
        "type": "Researcher"
      },
      {
        "name": "Abhishek Kadian",
        "type": "Researcher"
      },
      {
        "name": "Ahmad Al-Dahle",
        "type": "Researcher"
      },
      {
        "name": "Aiesha Letman",
        "type": "Researcher"
      },
      {
        "name": "Akhil Mathur",
        "type": "Researcher"
      },
      {
        "name": "Alan Schelten",
        "type": "Researcher"
      },
      {
        "name": "Alex Vaughan",
        "type": "Researcher"
      },
      {
        "name": "Amy Yang",
        "type": "Researcher"
      },
      {
        "name": "Angela Fan",
        "type": "Researcher"
      },
      {
        "name": "Anirudh Goyal",
        "type": "Researcher"
      },
      {
        "name": "Anthony Hartshorn",
        "type": "Researcher"
      },
      {
        "name": "Aobo Yang",
        "type": "Researcher"
      },
      {
        "name": "Archi Mitra",
        "type": "Researcher"
      },
      {
        "name": "Archie Sravankumar",
        "type": "Researcher"
      },
      {
        "name": "Artem Korenev",
        "type": "Researcher"
      },
      {
        "name": "Arthur Hinsvark",
        "type": "Researcher"
      },
      {
        "name": "Arun Rao",
        "type": "Researcher"
      },
      {
        "name": "Aston Zhang",
        "type": "Researcher"
      },
      {
        "name": "Austen Gregerson",
        "type": "Researcher"
      },
      {
        "name": "Ava Spataru",
        "type": "Researcher"
      },
      {
        "name": "Baptiste Roziere",
        "type": "Researcher"
      },
      {
        "name": "Bethany Biron",
        "type": "Researcher"
      },
      {
        "name": "Binh Tang",
        "type": "Researcher"
      },
      {
        "name": "Bobbie Chern",
        "type": "Researcher"
      },
      {
        "name": "Charlotte Caucheteux",
        "type": "Researcher"
      },
      {
        "name": "Chaya Nayak",
        "type": "Researcher"
      },
      {
        "name": "Chloe Bi",
        "type": "Researcher"
      },
      {
        "name": "Chris Marra",
        "type": "Researcher"
      },
      {
        "name": "Chris McConnell",
        "type": "Researcher"
      },
      {
        "name": "Christian Keller",
        "type": "Researcher"
      },
      {
        "name": "Christophe Touret",
        "type": "Researcher"
      },
      {
        "name": "Chunyang Wu",
        "type": "Researcher"
      },
      {
        "name": "Corinne Wong",
        "type": "Researcher"
      },
      {
        "name": "Cristian Canton Ferrer",
        "type": "Researcher"
      },
      {
        "name": "Cyrus Nikolaidis",
        "type": "Researcher"
      },
      {
        "name": "Damien Allonsius",
        "type": "Researcher"
      },
      {
        "name": "Daniel Song",
        "type": "Researcher"
      },
      {
        "name": "Danielle Pintz",
        "type": "Researcher"
      },
      {
        "name": "Danny Livshits",
        "type": "Researcher"
      },
      {
        "name": "Danny Wyatt",
        "type": "Researcher"
      },
      {
        "name": "David Esiobu",
        "type": "Researcher"
      },
      {
        "name": "Dhruv Choudhary",
        "type": "Researcher"
      },
      {
        "name": "Dhruv Mahajan",
        "type": "Researcher"
      },
      {
        "name": "Diego Garcia-Olano",
        "type": "Researcher"
      },
      {
        "name": "Diego Perino",
        "type": "Researcher"
      },
      {
        "name": "Dieuwke Hupkes",
        "type": "Researcher"
      },
      {
        "name": "Egor Lakomkin",
        "type": "Researcher"
      },
      {
        "name": "Ehab AlBadawy",
        "type": "Researcher"
      },
      {
        "name": "Elina Lobanova",
        "type": "Researcher"
      },
      {
        "name": "Emily Dinan",
        "type": "Researcher"
      },
      {
        "name": "Eric Michael Smith",
        "type": "Researcher"
      },
      {
        "name": "Filip Radenovic",
        "type": "Researcher"
      },
      {
        "name": "Francisco Guzmán",
        "type": "Researcher"
      },
      {
        "name": "Frank Zhang",
        "type": "Researcher"
      },
      {
        "name": "Gabrielle Lee",
        "type": "Researcher"
      },
      {
        "name": "Georgia Lewis Anderson",
        "type": "Researcher"
      },
      {
        "name": "Govind Thattai",
        "type": "Researcher"
      },
      {
        "name": "Graeme Nail",
        "type": "Researcher"
      },
      {
        "name": "Gregoire Mialon",
        "type": "Researcher"
      },
      {
        "name": "Guan Pang",
        "type": "Researcher"
      },
      {
        "name": "Guillem Cucurell",
        "type": "Researcher"
      },
      {
        "name": "Hailey Nguyen",
        "type": "Researcher"
      },
      {
        "name": "Hannah Korevaar",
        "type": "Researcher"
      },
      {
        "name": "Iliyan Zarov",
        "type": "Researcher"
      },
      {
        "name": "Imanol Arrieta Ibarra",
        "type": "Researcher"
      },
      {
        "name": "Isabel Kloumann",
        "type": "Researcher"
      },
      {
        "name": "Ivan Evtimov",
        "type": "Researcher"
      },
      {
        "name": "Jack Zhang",
        "type": "Researcher"
      },
      {
        "name": "Jade Copet",
        "type": "Researcher"
      },
      {
        "name": "Jaewon Lee",
        "type": "Researcher"
      },
      {
        "name": "Jan Geffert",
        "type": "Researcher"
      },
      {
        "name": "Jana Vranes",
        "type": "Researcher"
      },
      {
        "name": "Jason Park",
        "type": "Researcher"
      },
      {
        "name": "Jay Mahadeokar",
        "type": "Researcher"
      },
      {
        "name": "Jeet Shah",
        "type": "Researcher"
      },
      {
        "name": "Jelmer van der Linde",
        "type": "Researcher"
      },
      {
        "name": "Jennifer Billock",
        "type": "Researcher"
      },
      {
        "name": "Jenny Hong",
        "type": "Researcher"
      },
      {
        "name": "Jenya Lee",
        "type": "Researcher"
      },
      {
        "name": "Jeremy Fu",
        "type": "Researcher"
      },
      {
        "name": "Jianfeng Chi",
        "type": "Researcher"
      },
      {
        "name": "Jianyu Huang",
        "type": "Researcher"
      },
      {
        "name": "Jiawen Liu",
        "type": "Researcher"
      },
      {
        "name": "Jie Wang",
        "type": "Researcher"
      },
      {
        "name": "Jiecao Yu",
        "type": "Researcher"
      },
      {
        "name": "Joanna Bitton",
        "type": "Researcher"
      },
      {
        "name": "Joe Spisak",
        "type": "Researcher"
      },
      {
        "name": "Jongsoo Park",
        "type": "Researcher"
      },
      {
        "name": "Joseph Rocca",
        "type": "Researcher"
      },
      {
        "name": "Joshua Johnstun",
        "type": "Researcher"
      },
      {
        "name": "Joshua Saxe",
        "type": "Researcher"
      },
      {
        "name": "Junteng Jia",
        "type": "Researcher"
      },
      {
        "name": "Kalyan Vasuden Alwala",
        "type": "Researcher"
      },
      {
        "name": "Karthik Prasad",
        "type": "Researcher"
      },
      {
        "name": "Kartikeya Upasani",
        "type": "Researcher"
      },
      {
        "name": "Kate Plawiak",
        "type": "Researcher"
      },
      {
        "name": "Ke Li",
        "type": "Researcher"
      },
      {
        "name": "Kenneth Heafield",
        "type": "Researcher"
      },
      {
        "name": "Kevin Stone",
        "type": "Researcher"
      },
      {
        "name": "Khalid El-Arini",
        "type": "Researcher"
      },
      {
        "name": "Krithika Iyer",
        "type": "Researcher"
      },
      {
        "name": "Kshitiz Malik",
        "type": "Researcher"
      },
      {
        "name": "Kuenley Chiu",
        "type": "Researcher"
      },
      {
        "name": "Kunal Bhalla",
        "type": "Researcher"
      },
      {
        "name": "Kushal Lakhotia",
        "type": "Researcher"
      },
      {
        "name": "Lauren Rantala-Yeary",
        "type": "Researcher"
      },
      {
        "name": "Lawrence Chen",
        "type": "Researcher"
      },
      {
        "name": "Liang Tan",
        "type": "Researcher"
      },
      {
        "name": "Liz Jenkins",
        "type": "Researcher"
      },
      {
        "name": "Louis Martin",
        "type": "Researcher"
      },
      {
        "name": "Lovish Madaan",
        "type": "Researcher"
      },
      {
        "name": "Lubo Malo",
        "type": "Researcher"
      },
      {
        "name": "Lukas Blecher",
        "type": "Researcher"
      },
      {
        "name": "Lukas Landzaat",
        "type": "Researcher"
      },
      {
        "name": "Luke de Oliveira",
        "type": "Researcher"
      },
      {
        "name": "Madeline Muzzi",
        "type": "Researcher"
      },
      {
        "name": "Mahesh Pasupuleti",
        "type": "Researcher"
      },
      {
        "name": "Mannat Singh",
        "type": "Researcher"
      },
      {
        "name": "Manohar Paluri",
        "type": "Researcher"
      },
      {
        "name": "Marcin Kardas",
        "type": "Researcher"
      },
      {
        "name": "Maria Tsimpoukelli",
        "type": "Researcher"
      },
      {
        "name": "Mathew Oldham",
        "type": "Researcher"
      },
      {
        "name": "Mathieu Rita",
        "type": "Researcher"
      },
      {
        "name": "Maya Pavlova",
        "type": "Researcher"
      },
      {
        "name": "Melanie Kambadur",
        "type": "Researcher"
      },
      {
        "name": "Mike Lewis",
        "type": "Researcher"
      },
      {
        "name": "Min Si",
        "type": "Researcher"
      },
      {
        "name": "Mitesh Kumar Singh",
        "type": "Researcher"
      },
      {
        "name": "Mona Hassan",
        "type": "Researcher"
      },
      {
        "name": "Narjes Torabi",
        "type": "Researcher"
      },
      {
        "name": "Nikolay Bashlykov",
        "type": "Researcher"
      },
      {
        "name": "Nikolay Bogoychev",
        "type": "Researcher"
      },
      {
        "name": "Niladri Chatterji",
        "type": "Researcher"
      },
      {
        "name": "Ning Zhang",
        "type": "Researcher"
      },
      {
        "name": "Olivier Duchenne",
        "type": "Researcher"
      },
      {
        "name": "Onur Çelebi",
        "type": "Researcher"
      },
      {
        "name": "Patrick Alrassy",
        "type": "Researcher"
      },
      {
        "name": "Pengchuan Zhang",
        "type": "Researcher"
      },
      {
        "name": "Pengwei Li",
        "type": "Researcher"
      },
      {
        "name": "Petar Vasic",
        "type": "Researcher"
      },
      {
        "name": "Peter Weng",
        "type": "Researcher"
      },
      {
        "name": "Prajjwal Bhargava",
        "type": "Researcher"
      },
      {
        "name": "Pratik Dubal",
        "type": "Researcher"
      },
      {
        "name": "Praveen Krishnan",
        "type": "Researcher"
      },
      {
        "name": "Punit Singh Koura",
        "type": "Researcher"
      },
      {
        "name": "Puxin Xu",
        "type": "Researcher"
      },
      {
        "name": "Qingxiao Dong",
        "type": "Researcher"
      },
      {
        "name": "Ragavan Srinivasan",
        "type": "Researcher"
      },
      {
        "name": "Raj Ganapathy",
        "type": "Researcher"
      },
      {
        "name": "Ramon Calderer",
        "type": "Researcher"
      },
      {
        "name": "Ricardo Silveira Cabral",
        "type": "Researcher"
      },
      {
        "name": "Robert Stojnic",
        "type": "Researcher"
      },
      {
        "name": "Roberta Raileanu",
        "type": "Researcher"
      },
      {
        "name": "Rohan Maheswari",
        "type": "Researcher"
      },
      {
        "name": "Rohit Girdhar",
        "type": "Researcher"
      },
      {
        "name": "Rohit Patel",
        "type": "Researcher"
      },
      {
        "name": "Romain Sauvestre",
        "type": "Researcher"
      },
      {
        "name": "Ronnie Polidoro",
        "type": "Researcher"
      },
      {
        "name": "Roshan Sumbaly",
        "type": "Researcher"
      },
      {
        "name": "Ross Taylor",
        "type": "Researcher"
      },
      {
        "name": "Ruan Silva",
        "type": "Researcher"
      },
      {
        "name": "Rui Hou",
        "type": "Researcher"
      },
      {
        "name": "Saghar Hosseini",
        "type": "Researcher"
      },
      {
        "name": "Sahana Chennabasappa",
        "type": "Researcher"
      },
      {
        "name": "Sanjay Singh",
        "type": "Researcher"
      },
      {
        "name": "Sean Bell",
        "type": "Researcher"
      },
      {
        "name": "Seohyun Sonia Kim",
        "type": "Researcher"
      },
      {
        "name": "Sergey Edunov",
        "type": "Researcher"
      },
      {
        "name": "Shaoliang Nie",
        "type": "Researcher"
      },
      {
        "name": "Sharath Raparthy",
        "type": "Researcher"
      },
      {
        "name": "Sheng Shen",
        "type": "Researcher"
      },
      {
        "name": "Shengye Wan",
        "type": "Researcher"
      },
      {
        "name": "Shruti Bhosale",
        "type": "Researcher"
      },
      {
        "name": "Shun Zhang",
        "type": "Researcher"
      },
      {
        "name": "Simon Vandenhende",
        "type": "Researcher"
      },
      {
        "name": "Soumya Batra",
        "type": "Researcher"
      },
      {
        "name": "Spencer Whitman",
        "type": "Researcher"
      },
      {
        "name": "Sten Sootla",
        "type": "Researcher"
      },
      {
        "name": "Stephane Collot",
        "type": "Researcher"
      },
      {
        "name": "Suchin Gururangan",
        "type": "Researcher"
      },
      {
        "name": "Sydney Borodinsky",
        "type": "Researcher"
      },
      {
        "name": "Tamar Herman",
        "type": "Researcher"
      },
      {
        "name": "Tara Fowler",
        "type": "Researcher"
      },
      {
        "name": "Tarek Sheasha",
        "type": "Researcher"
      },
      {
        "name": "Thomas Georgiou",
        "type": "Researcher"
      },
      {
        "name": "Thomas Scialom",
        "type": "Researcher"
      },
      {
        "name": "Tobias Speckbacher",
        "type": "Researcher"
      },
      {
        "name": "Todor Mihaylov",
        "type": "Researcher"
      },
      {
        "name": "Tong Xiao",
        "type": "Researcher"
      },
      {
        "name": "Ujjwal Karn",
        "type": "Researcher"
      },
      {
        "name": "Vedanuj Goswami",
        "type": "Researcher"
      },
      {
        "name": "Vibhor Gupta",
        "type": "Researcher"
      },
      {
        "name": "Vignesh Ramanathan",
        "type": "Researcher"
      },
      {
        "name": "Viktor Kerkez",
        "type": "Researcher"
      },
      {
        "name": "Vincent Gonguet",
        "type": "Researcher"
      },
      {
        "name": "Virginie Do",
        "type": "Researcher"
      },
      {
        "name": "Vish Vogeti",
        "type": "Researcher"
      },
      {
        "name": "Vítor Albiero",
        "type": "Researcher"
      },
      {
        "name": "Vladan Petrovic",
        "type": "Researcher"
      },
      {
        "name": "Weiwei Chu",
        "type": "Researcher"
      },
      {
        "name": "Wenhan Xiong",
        "type": "Researcher"
      },
      {
        "name": "Wenyin Fu",
        "type": "Researcher"
      },
      {
        "name": "Whitney Meers",
        "type": "Researcher"
      },
      {
        "name": "Xiaodong Wang",
        "type": "Researcher"
      },
      {
        "name": "Xiaofang Wang",
        "type": "Researcher"
      },
      {
        "name": "Xiaoqing Ellen Tan",
        "type": "Researcher"
      },
      {
        "name": "Xide Xia",
        "type": "Researcher"
      },
      {
        "name": "Xinfeng Xie",
        "type": "Researcher"
      },
      {
        "name": "Xuchao Jia",
        "type": "Researcher"
      },
      {
        "name": "Xuewei Wang",
        "type": "Researcher"
      },
      {
        "name": "Yaelle Goldschlag",
        "type": "Researcher"
      },
      {
        "name": "Yashesh Gaur",
        "type": "Researcher"
      },
      {
        "name": "Yasmine Babaei",
        "type": "Researcher"
      },
      {
        "name": "Yi Wen",
        "type": "Researcher"
      },
      {
        "name": "Yiwen Song",
        "type": "Researcher"
      },
      {
        "name": "Yuchen Zhang",
        "type": "Researcher"
      },
      {
        "name": "Yue Li",
        "type": "Researcher"
      },
      {
        "name": "Yuning Mao",
        "type": "Researcher"
      },
      {
        "name": "Zacharie Delpierre Coudert",
        "type": "Researcher"
      },
      {
        "name": "Zheng Yan",
        "type": "Researcher"
      },
      {
        "name": "Zhengxing Chen",
        "type": "Researcher"
      },
      {
        "name": "Zoe Papakipos",
        "type": "Researcher"
      },
      {
        "name": "Aaditya Singh",
        "type": "Researcher"
      },
      {
        "name": "Aayushi Srivastava",
        "type": "Researcher"
      },
      {
        "name": "Abha Jain",
        "type": "Researcher"
      },
      {
        "name": "Adam Kelsey",
        "type": "Researcher"
      },
      {
        "name": "Adam Shajnfeld",
        "type": "Researcher"
      },
      {
        "name": "Adithya Gangidi",
        "type": "Researcher"
      },
      {
        "name": "Adolfo Victoria",
        "type": "Researcher"
      },
      {
        "name": "Ahuva Goldstand",
        "type": "Researcher"
      },
      {
        "name": "Ajay Menon",
        "type": "Researcher"
      },
      {
        "name": "Ajay Sharma",
        "type": "Researcher"
      },
      {
        "name": "Alex Boesenberg",
        "type": "Researcher"
      },
      {
        "name": "Alexei Baevski",
        "type": "Researcher"
      },
      {
        "name": "Allie Feinstein",
        "type": "Researcher"
      },
      {
        "name": "Amanda Kallet",
        "type": "Researcher"
      },
      {
        "name": "Amit Sangani",
        "type": "Researcher"
      },
      {
        "name": "Amos Teo",
        "type": "Researcher"
      },
      {
        "name": "Anam Yunus",
        "type": "Researcher"
      },
      {
        "name": "Andrei Lupu",
        "type": "Researcher"
      },
      {
        "name": "Andres Alvarado",
        "type": "Researcher"
      },
      {
        "name": "Andrew Caples",
        "type": "Researcher"
      },
      {
        "name": "Andrew Gu",
        "type": "Researcher"
      },
      {
        "name": "Andrew Ho",
        "type": "Researcher"
      },
      {
        "name": "Andrew Poulton",
        "type": "Researcher"
      },
      {
        "name": "Andrew Ryan",
        "type": "Researcher"
      },
      {
        "name": "Ankit Ramchandani",
        "type": "Researcher"
      },
      {
        "name": "Annie Dong",
        "type": "Researcher"
      },
      {
        "name": "Annie Franco",
        "type": "Researcher"
      },
      {
        "name": "Anuj Goyal",
        "type": "Researcher"
      },
      {
        "name": "Aparajita Saraf",
        "type": "Researcher"
      },
      {
        "name": "Arkabandhu Chowdhury",
        "type": "Researcher"
      },
      {
        "name": "Ashley Gabriel",
        "type": "Researcher"
      },
      {
        "name": "Ashwin Bharambe",
        "type": "Researcher"
      },
      {
        "name": "Assaf Eisenman",
        "type": "Researcher"
      },
      {
        "name": "Azadeh Yazdan",
        "type": "Researcher"
      },
      {
        "name": "Beau James",
        "type": "Researcher"
      },
      {
        "name": "Ben Maurer",
        "type": "Researcher"
      },
      {
        "name": "Benjamin Leonhardi",
        "type": "Researcher"
      },
      {
        "name": "Bernie Huang",
        "type": "Researcher"
      },
      {
        "name": "Beth Loyd",
        "type": "Researcher"
      },
      {
        "name": "Beto De Paola",
        "type": "Researcher"
      },
      {
        "name": "Bhargavi Paranjape",
        "type": "Researcher"
      },
      {
        "name": "Bing Liu",
        "type": "Researcher"
      },
      {
        "name": "Boyu Ni",
        "type": "Researcher"
      },
      {
        "name": "Braden Hancock",
        "type": "Researcher"
      },
      {
        "name": "Bram Wasti",
        "type": "Researcher"
      },
      {
        "name": "Brandon Spence",
        "type": "Researcher"
      },
      {
        "name": "Brani Stojkovic",
        "type": "Researcher"
      },
      {
        "name": "Brian Gamido",
        "type": "Researcher"
      },
      {
        "name": "Britt Montalvo",
        "type": "Researcher"
      },
      {
        "name": "Carl Parker",
        "type": "Researcher"
      },
      {
        "name": "Carly Burton",
        "type": "Researcher"
      },
      {
        "name": "Catalina Mejia",
        "type": "Researcher"
      },
      {
        "name": "Ce Liu",
        "type": "Researcher"
      },
      {
        "name": "Changhan Wang",
        "type": "Researcher"
      },
      {
        "name": "Changkyu Kim",
        "type": "Researcher"
      },
      {
        "name": "Chao Zhou",
        "type": "Researcher"
      },
      {
        "name": "Chester Hu",
        "type": "Researcher"
      },
      {
        "name": "Ching-Hsiang Chu",
        "type": "Researcher"
      },
      {
        "name": "Chris Cai",
        "type": "Researcher"
      },
      {
        "name": "Chris Tindal",
        "type": "Researcher"
      },
      {
        "name": "Christoph Feichtenhofer",
        "type": "Researcher"
      },
      {
        "name": "Cynthia Gao",
        "type": "Researcher"
      },
      {
        "name": "Damon Civin",
        "type": "Researcher"
      },
      {
        "name": "Dana Beaty",
        "type": "Researcher"
      },
      {
        "name": "Daniel Kreymer",
        "type": "Researcher"
      },
      {
        "name": "Daniel Li",
        "type": "Researcher"
      },
      {
        "name": "David Adkins",
        "type": "Researcher"
      },
      {
        "name": "David Xu",
        "type": "Researcher"
      },
      {
        "name": "Davide Testuggine",
        "type": "Researcher"
      },
      {
        "name": "Delia David",
        "type": "Researcher"
      },
      {
        "name": "Devi Parikh",
        "type": "Researcher"
      },
      {
        "name": "Diana Liskovich",
        "type": "Researcher"
      },
      {
        "name": "Didem Foss",
        "type": "Researcher"
      },
      {
        "name": "Dingkang Wang",
        "type": "Researcher"
      },
      {
        "name": "Duc Le",
        "type": "Researcher"
      },
      {
        "name": "Dustin Holland",
        "type": "Researcher"
      },
      {
        "name": "Edward Dowling",
        "type": "Researcher"
      },
      {
        "name": "Eissa Jamil",
        "type": "Researcher"
      },
      {
        "name": "Elaine Montgomery",
        "type": "Researcher"
      },
      {
        "name": "Eleonora Presani",
        "type": "Researcher"
      },
      {
        "name": "Emily Hahn",
        "type": "Researcher"
      },
      {
        "name": "Emily Wood",
        "type": "Researcher"
      },
      {
        "name": "Eric-Tuan Le",
        "type": "Researcher"
      },
      {
        "name": "Erik Brinkman",
        "type": "Researcher"
      },
      {
        "name": "Esteban Arcaute",
        "type": "Researcher"
      },
      {
        "name": "Evan Dunbar",
        "type": "Researcher"
      },
      {
        "name": "Evan Smothers",
        "type": "Researcher"
      },
      {
        "name": "Fei Sun",
        "type": "Researcher"
      },
      {
        "name": "Felix Kreuk",
        "type": "Researcher"
      },
      {
        "name": "Filippos Kokkinos",
        "type": "Researcher"
      },
      {
        "name": "Firat Ozgenel",
        "type": "Researcher"
      },
      {
        "name": "Francesco Caggioni",
        "type": "Researcher"
      },
      {
        "name": "Frank Kanayet",
        "type": "Researcher"
      },
      {
        "name": "Frank Seide",
        "type": "Researcher"
      },
      {
        "name": "Gabriela Medina Florez",
        "type": "Researcher"
      },
      {
        "name": "Gabriella Schwarz",
        "type": "Researcher"
      },
      {
        "name": "Gada Badeer",
        "type": "Researcher"
      },
      {
        "name": "Georgia Swee",
        "type": "Researcher"
      },
      {
        "name": "Gil Halpern",
        "type": "Researcher"
      },
      {
        "name": "Grant Herman",
        "type": "Researcher"
      },
      {
        "name": "Grigory Sizov",
        "type": "Researcher"
      },
      {
        "name": "Guangyi",
        "type": "Researcher"
      },
      {
        "name": "Zhang",
        "type": "Researcher"
      },
      {
        "name": "Guna Lakshminarayanan",
        "type": "Researcher"
      },
      {
        "name": "Hakan Inan",
        "type": "Researcher"
      },
      {
        "name": "Hamid Shojanazeri",
        "type": "Researcher"
      },
      {
        "name": "Han Zou",
        "type": "Researcher"
      },
      {
        "name": "Hannah Wang",
        "type": "Researcher"
      },
      {
        "name": "Hanwen Zha",
        "type": "Researcher"
      },
      {
        "name": "Haroun Habeeb",
        "type": "Researcher"
      },
      {
        "name": "Harrison Rudolph",
        "type": "Researcher"
      },
      {
        "name": "Helen Suk",
        "type": "Researcher"
      },
      {
        "name": "Henry Aspegren",
        "type": "Researcher"
      },
      {
        "name": "Hunter Goldman",
        "type": "Researcher"
      },
      {
        "name": "Hongyuan Zhan",
        "type": "Researcher"
      },
      {
        "name": "Ibrahim Damlaj",
        "type": "Researcher"
      },
      {
        "name": "Igor Molybog",
        "type": "Researcher"
      },
      {
        "name": "Igor Tufanov",
        "type": "Researcher"
      },
      {
        "name": "Ilias Leontiadis",
        "type": "Researcher"
      },
      {
        "name": "Irina-Elena Veliche",
        "type": "Researcher"
      },
      {
        "name": "Itai Gat",
        "type": "Researcher"
      },
      {
        "name": "Jake Weissman",
        "type": "Researcher"
      },
      {
        "name": "James Geboski",
        "type": "Researcher"
      },
      {
        "name": "James Kohli",
        "type": "Researcher"
      },
      {
        "name": "Janice Lam",
        "type": "Researcher"
      },
      {
        "name": "Japhet Asher",
        "type": "Researcher"
      },
      {
        "name": "Jean-Baptiste Gaya",
        "type": "Researcher"
      },
      {
        "name": "Jeff Marcus",
        "type": "Researcher"
      },
      {
        "name": "Jeff Tang",
        "type": "Researcher"
      },
      {
        "name": "Jennifer Chan",
        "type": "Researcher"
      },
      {
        "name": "Jenny Zhen",
        "type": "Researcher"
      },
      {
        "name": "Jeremy Reizenstein",
        "type": "Researcher"
      },
      {
        "name": "Jeremy Teboul",
        "type": "Researcher"
      },
      {
        "name": "Jessica Zhong",
        "type": "Researcher"
      },
      {
        "name": "Jian Jin",
        "type": "Researcher"
      },
      {
        "name": "Jingyi Yang",
        "type": "Researcher"
      },
      {
        "name": "Joe Cummings",
        "type": "Researcher"
      },
      {
        "name": "Jon Carvill",
        "type": "Researcher"
      },
      {
        "name": "Jon Shepard",
        "type": "Researcher"
      },
      {
        "name": "Jonathan McPhie",
        "type": "Researcher"
      },
      {
        "name": "Jonathan Torres",
        "type": "Researcher"
      },
      {
        "name": "Josh Ginsburg",
        "type": "Researcher"
      },
      {
        "name": "Junjie Wang",
        "type": "Researcher"
      },
      {
        "name": "Kai Wu",
        "type": "Researcher"
      },
      {
        "name": "Kam Hou U",
        "type": "Researcher"
      },
      {
        "name": "Karan Saxena",
        "type": "Researcher"
      },
      {
        "name": "Kartikay Khandelwal",
        "type": "Researcher"
      },
      {
        "name": "Katayoun Zand",
        "type": "Researcher"
      },
      {
        "name": "Kathy Matosich",
        "type": "Researcher"
      },
      {
        "name": "Kaushik Veeraraghavan",
        "type": "Researcher"
      },
      {
        "name": "Kelly Michelena",
        "type": "Researcher"
      },
      {
        "name": "Keqian Li",
        "type": "Researcher"
      },
      {
        "name": "Kiran Jagadeesh",
        "type": "Researcher"
      },
      {
        "name": "Kun Huang",
        "type": "Researcher"
      },
      {
        "name": "Kunal Chawla",
        "type": "Researcher"
      },
      {
        "name": "Kyle Huang",
        "type": "Researcher"
      },
      {
        "name": "Lailin Chen",
        "type": "Researcher"
      },
      {
        "name": "Lakshya Garg",
        "type": "Researcher"
      },
      {
        "name": "Lavender A",
        "type": "Researcher"
      },
      {
        "name": "Leandro Silva",
        "type": "Researcher"
      },
      {
        "name": "Lee Bell",
        "type": "Researcher"
      },
      {
        "name": "Liangpeng Guo",
        "type": "Researcher"
      },
      {
        "name": "Licheng Yu",
        "type": "Researcher"
      },
      {
        "name": "Liron Moshkovich",
        "type": "Researcher"
      },
      {
        "name": "Luca Wehrstedt",
        "type": "Researcher"
      },
      {
        "name": "Madian Khabsa",
        "type": "Researcher"
      },
      {
        "name": "Manav Avalani",
        "type": "Researcher"
      },
      {
        "name": "Manish Bhatt",
        "type": "Researcher"
      },
      {
        "name": "Martynas Mankus",
        "type": "Researcher"
      },
      {
        "name": "Matan Hasson",
        "type": "Researcher"
      },
      {
        "name": "Matthew Lennie",
        "type": "Researcher"
      },
      {
        "name": "Matthias Reso",
        "type": "Researcher"
      },
      {
        "name": "Maxim Groshev",
        "type": "Researcher"
      },
      {
        "name": "Maxim Naumov",
        "type": "Researcher"
      },
      {
        "name": "Maya Lathi",
        "type": "Researcher"
      },
      {
        "name": "Meghan Keneally",
        "type": "Researcher"
      },
      {
        "name": "Miao Liu",
        "type": "Researcher"
      },
      {
        "name": "Michael L. Seltzer",
        "type": "Researcher"
      },
      {
        "name": "Michal Valko",
        "type": "Researcher"
      },
      {
        "name": "Michelle Restrepo",
        "type": "Researcher"
      },
      {
        "name": "Mihir Patel",
        "type": "Researcher"
      },
      {
        "name": "Mik Vyatskov",
        "type": "Researcher"
      },
      {
        "name": "Mikayel Samvelyan",
        "type": "Researcher"
      },
      {
        "name": "Mike Clark",
        "type": "Researcher"
      },
      {
        "name": "Mike Macey",
        "type": "Researcher"
      },
      {
        "name": "Mike Wang",
        "type": "Researcher"
      },
      {
        "name": "Miquel Jubert Hermoso",
        "type": "Researcher"
      },
      {
        "name": "Mo Metanat",
        "type": "Researcher"
      },
      {
        "name": "Mohammad Rastegari",
        "type": "Researcher"
      },
      {
        "name": "Munish Bansal",
        "type": "Researcher"
      },
      {
        "name": "Nandhini Santhanam",
        "type": "Researcher"
      },
      {
        "name": "Natascha Parks",
        "type": "Researcher"
      },
      {
        "name": "Natasha White",
        "type": "Researcher"
      },
      {
        "name": "Navyata Bawa",
        "type": "Researcher"
      },
      {
        "name": "Nayan Singhal",
        "type": "Researcher"
      },
      {
        "name": "Nick Egebo",
        "type": "Researcher"
      },
      {
        "name": "Nicolas Usunier",
        "type": "Researcher"
      },
      {
        "name": "Nikhil Mehta",
        "type": "Researcher"
      },
      {
        "name": "Nikolay Pavlovich Laptev",
        "type": "Researcher"
      },
      {
        "name": "Ning Dong",
        "type": "Researcher"
      },
      {
        "name": "Norman Cheng",
        "type": "Researcher"
      },
      {
        "name": "Oleg Chernoguz",
        "type": "Researcher"
      },
      {
        "name": "Olivia Hart",
        "type": "Researcher"
      },
      {
        "name": "Omkar Salpekar",
        "type": "Researcher"
      },
      {
        "name": "Ozlem Kalinli",
        "type": "Researcher"
      },
      {
        "name": "Parkin Kent",
        "type": "Researcher"
      },
      {
        "name": "Parth Parekh",
        "type": "Researcher"
      },
      {
        "name": "Paul Saab",
        "type": "Researcher"
      },
      {
        "name": "Pavan Balaji",
        "type": "Researcher"
      },
      {
        "name": "Pedro Rittner",
        "type": "Researcher"
      },
      {
        "name": "Philip Bontrager",
        "type": "Researcher"
      },
      {
        "name": "Pierre Roux",
        "type": "Researcher"
      },
      {
        "name": "Piotr Dollar",
        "type": "Researcher"
      },
      {
        "name": "Polina Zvyagina",
        "type": "Researcher"
      },
      {
        "name": "Prashant Ratanchandani",
        "type": "Researcher"
      },
      {
        "name": "Pritish Yuvraj",
        "type": "Researcher"
      },
      {
        "name": "Qian Liang",
        "type": "Researcher"
      },
      {
        "name": "Rachad Alao",
        "type": "Researcher"
      },
      {
        "name": "Rachel Rodriguez",
        "type": "Researcher"
      },
      {
        "name": "Rafi Ayub",
        "type": "Researcher"
      },
      {
        "name": "Raghotham Murthy",
        "type": "Researcher"
      },
      {
        "name": "Raghu Nayani",
        "type": "Researcher"
      },
      {
        "name": "Rahul Mitra",
        "type": "Researcher"
      },
      {
        "name": "Rangaprabhu Parthasarathy",
        "type": "Researcher"
      },
      {
        "name": "Raymond Li",
        "type": "Researcher"
      },
      {
        "name": "Rebekkah Hogan",
        "type": "Researcher"
      },
      {
        "name": "Robin Battey",
        "type": "Researcher"
      },
      {
        "name": "Rocky Wang",
        "type": "Researcher"
      },
      {
        "name": "Russ Howes",
        "type": "Researcher"
      },
      {
        "name": "Ruty Rinott",
        "type": "Researcher"
      },
      {
        "name": "Sachin Mehta",
        "type": "Researcher"
      },
      {
        "name": "Sachin Siby",
        "type": "Researcher"
      },
      {
        "name": "Sai Jayesh Bondu",
        "type": "Researcher"
      },
      {
        "name": "Samyak Datta",
        "type": "Researcher"
      },
      {
        "name": "Sara Chugh",
        "type": "Researcher"
      },
      {
        "name": "Sara Hunt",
        "type": "Researcher"
      },
      {
        "name": "Sargun Dhillon",
        "type": "Researcher"
      },
      {
        "name": "Sasha Sidorov",
        "type": "Researcher"
      },
      {
        "name": "Satadru Pan",
        "type": "Researcher"
      },
      {
        "name": "Saurabh Mahajan",
        "type": "Researcher"
      },
      {
        "name": "Saurabh Verma",
        "type": "Researcher"
      },
      {
        "name": "Seiji Yamamoto",
        "type": "Researcher"
      },
      {
        "name": "Sharadh Ramaswamy",
        "type": "Researcher"
      },
      {
        "name": "Shaun Lindsay",
        "type": "Researcher"
      },
      {
        "name": "Sheng Feng",
        "type": "Researcher"
      },
      {
        "name": "Shenghao Lin",
        "type": "Researcher"
      },
      {
        "name": "Shengxin Cindy Zha",
        "type": "Researcher"
      },
      {
        "name": "Shishir Patil",
        "type": "Researcher"
      },
      {
        "name": "Shiva Shankar",
        "type": "Researcher"
      },
      {
        "name": "Shuqiang Zhang",
        "type": "Researcher"
      },
      {
        "name": "Sinong Wang",
        "type": "Researcher"
      },
      {
        "name": "Sneha Agarwal",
        "type": "Researcher"
      },
      {
        "name": "Soji Sajuyigbe",
        "type": "Researcher"
      },
      {
        "name": "Soumith Chintala",
        "type": "Researcher"
      },
      {
        "name": "Stephanie Max",
        "type": "Researcher"
      },
      {
        "name": "Stephen Chen",
        "type": "Researcher"
      },
      {
        "name": "Steve Kehoe",
        "type": "Researcher"
      },
      {
        "name": "Steve Satterfield",
        "type": "Researcher"
      },
      {
        "name": "Sudarshan Govindaprasad",
        "type": "Researcher"
      },
      {
        "name": "Sumit Gupta",
        "type": "Researcher"
      },
      {
        "name": "Summer Deng",
        "type": "Researcher"
      },
      {
        "name": "Sungmin Cho",
        "type": "Researcher"
      },
      {
        "name": "Sunny Virk",
        "type": "Researcher"
      },
      {
        "name": "Suraj Subramanian",
        "type": "Researcher"
      },
      {
        "name": "Sy Choudhury",
        "type": "Researcher"
      },
      {
        "name": "Sydney Goldman",
        "type": "Researcher"
      },
      {
        "name": "Tal Remez",
        "type": "Researcher"
      },
      {
        "name": "Tamar Glaser",
        "type": "Researcher"
      },
      {
        "name": "Tamara Best",
        "type": "Researcher"
      },
      {
        "name": "Thilo Koehler",
        "type": "Researcher"
      },
      {
        "name": "Thomas Robinson",
        "type": "Researcher"
      },
      {
        "name": "Tianhe Li",
        "type": "Researcher"
      },
      {
        "name": "Tianjun Zhang",
        "type": "Researcher"
      },
      {
        "name": "Tim Matthews",
        "type": "Researcher"
      },
      {
        "name": "Timothy Chou",
        "type": "Researcher"
      },
      {
        "name": "Tzook Shaked",
        "type": "Researcher"
      },
      {
        "name": "Varun Vontimitta",
        "type": "Researcher"
      },
      {
        "name": "Victoria Ajayi",
        "type": "Researcher"
      },
      {
        "name": "Victoria Montanez",
        "type": "Researcher"
      },
      {
        "name": "Vijai Mohan",
        "type": "Researcher"
      },
      {
        "name": "Vinay Satish Kumar",
        "type": "Researcher"
      },
      {
        "name": "Vishal Mangla",
        "type": "Researcher"
      },
      {
        "name": "Vlad Ionescu",
        "type": "Researcher"
      },
      {
        "name": "Vlad Poenaru",
        "type": "Researcher"
      },
      {
        "name": "Vlad Tiberiu Mihailescu",
        "type": "Researcher"
      },
      {
        "name": "Vladimir Ivanov",
        "type": "Researcher"
      },
      {
        "name": "Wenchen Wang",
        "type": "Researcher"
      },
      {
        "name": "Wenwen Jiang",
        "type": "Researcher"
      },
      {
        "name": "Wes Bouaziz",
        "type": "Researcher"
      },
      {
        "name": "Will Constable",
        "type": "Researcher"
      },
      {
        "name": "Xiaocheng Tang",
        "type": "Researcher"
      },
      {
        "name": "Xiaojian Wu",
        "type": "Researcher"
      },
      {
        "name": "Xiaolan Wang",
        "type": "Researcher"
      },
      {
        "name": "Xilun Wu",
        "type": "Researcher"
      },
      {
        "name": "Xinbo Gao",
        "type": "Researcher"
      },
      {
        "name": "Yaniv Kleinman",
        "type": "Researcher"
      },
      {
        "name": "Yanjun Chen",
        "type": "Researcher"
      },
      {
        "name": "Ye Hu",
        "type": "Researcher"
      },
      {
        "name": "Ye Jia",
        "type": "Researcher"
      },
      {
        "name": "Ye Qi",
        "type": "Researcher"
      },
      {
        "name": "Yenda Li",
        "type": "Researcher"
      },
      {
        "name": "Yilin Zhang",
        "type": "Researcher"
      },
      {
        "name": "Yossi Adi",
        "type": "Researcher"
      },
      {
        "name": "Youngjin Nam",
        "type": "Researcher"
      },
      {
        "name": "Yu",
        "type": "Researcher"
      },
      {
        "name": "Wang",
        "type": "Researcher"
      },
      {
        "name": "Yu Zhao",
        "type": "Researcher"
      },
      {
        "name": "Yuchen Hao",
        "type": "Researcher"
      },
      {
        "name": "Yundi Qian",
        "type": "Researcher"
      },
      {
        "name": "Yunlu Li",
        "type": "Researcher"
      },
      {
        "name": "Yuzi He",
        "type": "Researcher"
      },
      {
        "name": "Zach Rait",
        "type": "Researcher"
      },
      {
        "name": "Zachary DeVito",
        "type": "Researcher"
      },
      {
        "name": "Zef Rosnbrick",
        "type": "Researcher"
      },
      {
        "name": "Zhaoduo Wen",
        "type": "Researcher"
      },
      {
        "name": "Zhenyu Yang",
        "type": "Researcher"
      },
      {
        "name": "Zhiwei Zhao",
        "type": "Researcher"
      },
      {
        "name": "Zhiyu Ma",
        "type": "Researcher"
      },
      {
        "name": "Tianyi Li",
        "type": "Researcher"
      },
      {
        "name": "Mingda Chen",
        "type": "Researcher"
      },
      {
        "name": "Bowei Guo",
        "type": "Researcher"
      },
      {
        "name": "Zhiqiang Shen",
        "type": "Researcher"
      },
      {
        "name": "Siyan Zhao",
        "type": "Researcher"
      },
      {
        "name": "Mengchen Liu",
        "type": "Researcher"
      },
      {
        "name": "Jing Huang",
        "type": "Researcher"
      },
      {
        "name": "Chenyu Wang",
        "type": "Researcher"
      },
      {
        "name": "Bo Liu",
        "type": "Researcher"
      },
      {
        "name": "Yuandong Tian",
        "type": "Researcher"
      },
      {
        "name": "Aditya Grover",
        "type": "Researcher"
      },
      {
        "name": "Feiyu Chen",
        "type": "Researcher"
      },
      {
        "name": "Marianne Arriola",
        "type": "Researcher"
      },
      {
        "name": "Yair Schiff",
        "type": "Researcher"
      },
      {
        "name": "Hao Phung",
        "type": "Researcher"
      },
      {
        "name": "Aaron Gokaslan",
        "type": "Researcher"
      },
      {
        "name": "Volodymyr Kuleshov",
        "type": "Researcher"
      },
      {
        "name": "Chenghao Fan",
        "type": "Researcher"
      },
      {
        "name": "Wen Heng",
        "type": "Researcher"
      },
      {
        "name": "Sichen Liu",
        "type": "Researcher"
      },
      {
        "name": "Yuxuan Song",
        "type": "Researcher"
      },
      {
        "name": "Jing Su",
        "type": "Researcher"
      },
      {
        "name": "Xiaoye Qu",
        "type": "Researcher"
      },
      {
        "name": "Kai Shen",
        "type": "Researcher"
      },
      {
        "name": "Wei Wei",
        "type": "Researcher"
      },
      {
        "name": "Zhilin Yang",
        "type": "Researcher"
      },
      {
        "name": "Zihang Dai",
        "type": "Researcher"
      },
      {
        "name": "Yiming Yang",
        "type": "Researcher"
      },
      {
        "name": "Jaime Carbonell",
        "type": "Researcher"
      },
      {
        "name": "Ruslan Salakhutdinov",
        "type": "Researcher"
      },
      {
        "name": "N. Cambridge",
        "type": "Researcher"
      },
      {
        "name": "Jinze Bai",
        "type": "Researcher"
      },
      {
        "name": "Yunfei Chu",
        "type": "Researcher"
      },
      {
        "name": "Zeyu Cui",
        "type": "Researcher"
      },
      {
        "name": "Kai Dang",
        "type": "Researcher"
      },
      {
        "name": "Xiaodong Deng",
        "type": "Researcher"
      },
      {
        "name": "Yang Fan",
        "type": "Researcher"
      },
      {
        "name": "Wenbin Ge",
        "type": "Researcher"
      },
      {
        "name": "Yu Han",
        "type": "Researcher"
      },
      {
        "name": "Fei Huang",
        "type": "Researcher"
      },
      {
        "name": "Binyuan Hui",
        "type": "Researcher"
      },
      {
        "name": "Luo Ji",
        "type": "Researcher"
      },
      {
        "name": "Mei Li",
        "type": "Researcher"
      },
      {
        "name": "Runji Lin",
        "type": "Researcher"
      },
      {
        "name": "Gao Liu",
        "type": "Researcher"
      },
      {
        "name": "Chengqiang Lu",
        "type": "Researcher"
      },
      {
        "name": "Keming Lu",
        "type": "Researcher"
      },
      {
        "name": "Jianxin Ma",
        "type": "Researcher"
      },
      {
        "name": "Rui Men",
        "type": "Researcher"
      },
      {
        "name": "Xingzhang Ren",
        "type": "Researcher"
      },
      {
        "name": "Xuancheng Ren",
        "type": "Researcher"
      },
      {
        "name": "Chuanqi Tan",
        "type": "Researcher"
      },
      {
        "name": "Sinan Tan",
        "type": "Researcher"
      },
      {
        "name": "Jianhong Tu",
        "type": "Researcher"
      },
      {
        "name": "Peng Wang",
        "type": "Researcher"
      },
      {
        "name": "Shijie Wang",
        "type": "Researcher"
      },
      {
        "name": "Shengguang Wu",
        "type": "Researcher"
      },
      {
        "name": "Benfeng Xu",
        "type": "Researcher"
      },
      {
        "name": "Jin Xu",
        "type": "Researcher"
      },
      {
        "name": "Hao Yang",
        "type": "Researcher"
      },
      {
        "name": "Jian Yang",
        "type": "Researcher"
      },
      {
        "name": "Shusheng Yang",
        "type": "Researcher"
      },
      {
        "name": "Yang Yao",
        "type": "Researcher"
      },
      {
        "name": "Bowen Yu",
        "type": "Researcher"
      },
      {
        "name": "Hongyi Yuan",
        "type": "Researcher"
      },
      {
        "name": "Zheng Yuan",
        "type": "Researcher"
      },
      {
        "name": "Xingxuan Zhang",
        "type": "Researcher"
      },
      {
        "name": "Yichang Zhang",
        "type": "Researcher"
      },
      {
        "name": "Zhenru Zhang",
        "type": "Researcher"
      },
      {
        "name": "Chang Zhou",
        "type": "Researcher"
      },
      {
        "name": "Xiaohuan Zhou",
        "type": "Researcher"
      },
      {
        "name": "Tianhang Zhu",
        "type": "Researcher"
      },
      {
        "name": "Jonas Gehring",
        "type": "Researcher"
      },
      {
        "name": "Fabian Gloeckle",
        "type": "Researcher"
      },
      {
        "name": "Jingyu Liu",
        "type": "Researcher"
      },
      {
        "name": "Jérémy Rapin",
        "type": "Researcher"
      },
      {
        "name": "Artyom Kozhevnikov",
        "type": "Researcher"
      },
      {
        "name": "Alexandre Défossez",
        "type": "Researcher"
      },
      {
        "name": "Yicun Yang",
        "type": "Researcher"
      },
      {
        "name": "Cong Wang",
        "type": "Researcher"
      },
      {
        "name": "Shaobo Wang",
        "type": "Researcher"
      },
      {
        "name": "Zichen Wen",
        "type": "Researcher"
      },
      {
        "name": "Hanlin Xu",
        "type": "Researcher"
      },
      {
        "name": "Heli Ben-Hamu",
        "type": "Researcher"
      },
      {
        "name": "Marton Havasi",
        "type": "Researcher"
      },
      {
        "name": "David Lopez-Paz",
        "type": "Researcher"
      },
      {
        "name": "Brian Karrer",
        "type": "Researcher"
      },
      {
        "name": "Yaron Lipman",
        "type": "Researcher"
      },
      {
        "name": "John Nguyen",
        "type": "Researcher"
      },
      {
        "name": "Tariq Berrada",
        "type": "Researcher"
      },
      {
        "name": "Ricky T. Q. Chen",
        "type": "Researcher"
      },
      {
        "name": "Niels Mündler",
        "type": "Researcher"
      },
      {
        "name": "Jasper Dekoninck",
        "type": "Researcher"
      },
      {
        "name": "Martin Vechev",
        "type": "Researcher"
      },
      {
        "name": "Julianna Piskorz",
        "type": "Researcher"
      },
      {
        "name": "Cristina Pinneri",
        "type": "Researcher"
      },
      {
        "name": "Alvaro Correia",
        "type": "Researcher"
      },
      {
        "name": "Motasem Alfarra",
        "type": "Researcher"
      },
      {
        "name": "Risheek Garrepalli",
        "type": "Researcher"
      },
      {
        "name": "Christos Louizos",
        "type": "Researcher"
      },
      {
        "name": "Chang Yang",
        "type": "Researcher"
      },
      {
        "name": "Chuang Zhou",
        "type": "Researcher"
      },
      {
        "name": "Yilin Xiao",
        "type": "Researcher"
      },
      {
        "name": "Su Dong",
        "type": "Researcher"
      },
      {
        "name": "Luyao Zhuang",
        "type": "Researcher"
      },
      {
        "name": "Yujing Zhang",
        "type": "Researcher"
      },
      {
        "name": "Zhu Wang",
        "type": "Researcher"
      },
      {
        "name": "Zijin Hong",
        "type": "Researcher"
      },
      {
        "name": "Zhishang Xiang",
        "type": "Researcher"
      },
      {
        "name": "Shengyuan Chen",
        "type": "Researcher"
      },
      {
        "name": "Huachi Zhou",
        "type": "Researcher"
      },
      {
        "name": "Qinggang Zhang",
        "type": "Researcher"
      },
      {
        "name": "Ninghao Liu",
        "type": "Researcher"
      },
      {
        "name": "Xinrun Wang",
        "type": "Researcher"
      },
      {
        "name": "Yi Chang",
        "type": "Researcher"
      },
      {
        "name": "Xiao Huang",
        "type": "Researcher"
      },
      {
        "name": "Hao Lu",
        "type": "Researcher"
      },
      {
        "name": "Haoyuan Huang",
        "type": "Researcher"
      },
      {
        "name": "Yulin Zhou",
        "type": "Researcher"
      },
      {
        "name": "Chen Li",
        "type": "Researcher"
      },
      {
        "name": "Ningxin Zhu",
        "type": "Researcher"
      },
      {
        "name": "Yu Cheng",
        "type": "Researcher"
      },
      {
        "name": "Jiuan Zhou",
        "type": "Researcher"
      },
      {
        "name": "Yongkang Hu",
        "type": "Researcher"
      },
      {
        "name": "Yihang Chen",
        "type": "Researcher"
      },
      {
        "name": "Huichi Zhou",
        "type": "Researcher"
      },
      {
        "name": "Mingang Chen",
        "type": "Researcher"
      },
      {
        "name": "Zhizhong Zhang",
        "type": "Researcher"
      },
      {
        "name": "Kun Shao",
        "type": "Researcher"
      },
      {
        "name": "Yuan Xie",
        "type": "Researcher"
      },
      {
        "name": "Zhaoxia Yin",
        "type": "Researcher"
      },
      {
        "name": "Qirui Mi",
        "type": "Researcher"
      },
      {
        "name": "Zhijian Ma",
        "type": "Researcher"
      },
      {
        "name": "Mengyue Yang",
        "type": "Researcher"
      },
      {
        "name": "Haoxuan Li",
        "type": "Researcher"
      },
      {
        "name": "Yisen Wang",
        "type": "Researcher"
      },
      {
        "name": "Haifeng Zhang",
        "type": "Researcher"
      },
      {
        "name": "Jun Wang",
        "type": "Researcher"
      },
      {
        "name": "Xiaoyu Tao",
        "type": "Researcher"
      },
      {
        "name": "Ze Guo",
        "type": "Researcher"
      },
      {
        "name": "Zexuan Yan",
        "type": "Researcher"
      },
      {
        "name": "Heng Pan",
        "type": "Researcher"
      },
      {
        "name": "Ailing Zeng",
        "type": "Researcher"
      },
      {
        "name": "Chengfei Cai",
        "type": "Researcher"
      },
      {
        "name": "Heung-Yeung Shum",
        "type": "Researcher"
      },
      {
        "name": "Xinhua Zhang",
        "type": "Researcher"
      },
      {
        "name": "Boshi Liu",
        "type": "Researcher"
      },
      {
        "name": "Yikuang Yuluo",
        "type": "Researcher"
      },
      {
        "name": "Yinhan Zhang",
        "type": "Researcher"
      },
      {
        "name": "Runtao Liu",
        "type": "Researcher"
      },
      {
        "name": "Zhiyuan Qin",
        "type": "Researcher"
      },
      {
        "name": "Shanhui Mo",
        "type": "Researcher"
      },
      {
        "name": "Xinyao Liao",
        "type": "Researcher"
      },
      {
        "name": "Xianfang Zeng",
        "type": "Researcher"
      },
      {
        "name": "Ziye Song",
        "type": "Researcher"
      },
      {
        "name": "Zhoujie Fu",
        "type": "Researcher"
      },
      {
        "name": "Gang Yu",
        "type": "Researcher"
      },
      {
        "name": "Zeyu Zhu",
        "type": "Researcher"
      },
      {
        "name": "Kevin Qinghong Lin",
        "type": "Researcher"
      },
      {
        "name": "Mike Zheng Shou",
        "type": "Researcher"
      },
      {
        "name": "Yiyang Chen",
        "type": "Researcher"
      },
      {
        "name": "Xiujun Ma",
        "type": "Researcher"
      },
      {
        "name": "DeepSeek-AI",
        "type": "Researcher"
      },
      {
        "name": "Daya Guo",
        "type": "Researcher"
      },
      {
        "name": "Dejian Yang",
        "type": "Researcher"
      },
      {
        "name": "Haowei Zhang",
        "type": "Researcher"
      },
      {
        "name": "Junxiao Song",
        "type": "Researcher"
      },
      {
        "name": "Peiyi Wang",
        "type": "Researcher"
      },
      {
        "name": "Qihao Zhu",
        "type": "Researcher"
      },
      {
        "name": "Runxin Xu",
        "type": "Researcher"
      },
      {
        "name": "Ruoyu Zhang",
        "type": "Researcher"
      },
      {
        "name": "Shirong Ma",
        "type": "Researcher"
      },
      {
        "name": "Xiao Bi",
        "type": "Researcher"
      },
      {
        "name": "Xiaokang Zhang",
        "type": "Researcher"
      },
      {
        "name": "Yu Wu",
        "type": "Researcher"
      },
      {
        "name": "Z. F. Wu",
        "type": "Researcher"
      },
      {
        "name": "Zhibin Gou",
        "type": "Researcher"
      },
      {
        "name": "Zhihong Shao",
        "type": "Researcher"
      },
      {
        "name": "Zhuoshu Li",
        "type": "Researcher"
      },
      {
        "name": "Ziyi Gao",
        "type": "Researcher"
      },
      {
        "name": "Aixin Liu",
        "type": "Researcher"
      },
      {
        "name": "Bing Xue",
        "type": "Researcher"
      },
      {
        "name": "Bochao Wu",
        "type": "Researcher"
      },
      {
        "name": "Bei Feng",
        "type": "Researcher"
      },
      {
        "name": "Chengda Lu",
        "type": "Researcher"
      },
      {
        "name": "Chenggang Zhao",
        "type": "Researcher"
      },
      {
        "name": "Chengqi Deng",
        "type": "Researcher"
      },
      {
        "name": "Chenyu Zhang",
        "type": "Researcher"
      },
      {
        "name": "Chong Ruan",
        "type": "Researcher"
      },
      {
        "name": "Deli Chen",
        "type": "Researcher"
      },
      {
        "name": "Dongjie Ji",
        "type": "Researcher"
      },
      {
        "name": "Erhang Li",
        "type": "Researcher"
      },
      {
        "name": "Fangyun Lin",
        "type": "Researcher"
      },
      {
        "name": "Fucong Dai",
        "type": "Researcher"
      },
      {
        "name": "Fuli Luo",
        "type": "Researcher"
      },
      {
        "name": "Guangbo Hao",
        "type": "Researcher"
      },
      {
        "name": "Guanting Chen",
        "type": "Researcher"
      },
      {
        "name": "Guowei Li",
        "type": "Researcher"
      },
      {
        "name": "H. Zhang",
        "type": "Researcher"
      },
      {
        "name": "Han Bao",
        "type": "Researcher"
      },
      {
        "name": "Hanwei Xu",
        "type": "Researcher"
      },
      {
        "name": "Haocheng Wang",
        "type": "Researcher"
      },
      {
        "name": "Honghui Ding",
        "type": "Researcher"
      },
      {
        "name": "Huajian Xin",
        "type": "Researcher"
      },
      {
        "name": "Huazuo Gao",
        "type": "Researcher"
      },
      {
        "name": "Hui Qu",
        "type": "Researcher"
      },
      {
        "name": "Jianzhong Guo",
        "type": "Researcher"
      },
      {
        "name": "Jiawei Wang",
        "type": "Researcher"
      },
      {
        "name": "Jingchang Chen",
        "type": "Researcher"
      },
      {
        "name": "Jingyang Yuan",
        "type": "Researcher"
      },
      {
        "name": "Junjie Qiu",
        "type": "Researcher"
      },
      {
        "name": "Junlong Li",
        "type": "Researcher"
      },
      {
        "name": "J. L. Cai",
        "type": "Researcher"
      },
      {
        "name": "Jiaqi Ni",
        "type": "Researcher"
      },
      {
        "name": "Jian Liang",
        "type": "Researcher"
      },
      {
        "name": "Jin Chen",
        "type": "Researcher"
      },
      {
        "name": "Kai Dong",
        "type": "Researcher"
      },
      {
        "name": "Kai Hu",
        "type": "Researcher"
      },
      {
        "name": "Kaige Gao",
        "type": "Researcher"
      },
      {
        "name": "Kang Guan",
        "type": "Researcher"
      },
      {
        "name": "Kexin Huang",
        "type": "Researcher"
      },
      {
        "name": "Kuai Yu",
        "type": "Researcher"
      },
      {
        "name": "Lean Wang",
        "type": "Researcher"
      },
      {
        "name": "Lecong Zhang",
        "type": "Researcher"
      },
      {
        "name": "Liang Zhao",
        "type": "Researcher"
      },
      {
        "name": "Litong Wang",
        "type": "Researcher"
      },
      {
        "name": "Liyue Zhang",
        "type": "Researcher"
      },
      {
        "name": "Lei Xu",
        "type": "Researcher"
      },
      {
        "name": "Leyi Xia",
        "type": "Researcher"
      },
      {
        "name": "Mingchuan Zhang",
        "type": "Researcher"
      },
      {
        "name": "Minghua Zhang",
        "type": "Researcher"
      },
      {
        "name": "Minghui Tang",
        "type": "Researcher"
      },
      {
        "name": "Meng Li",
        "type": "Researcher"
      },
      {
        "name": "Miaojun Wang",
        "type": "Researcher"
      },
      {
        "name": "Mingming Li",
        "type": "Researcher"
      },
      {
        "name": "Ning Tian",
        "type": "Researcher"
      },
      {
        "name": "Panpan Huang",
        "type": "Researcher"
      },
      {
        "name": "Qiancheng Wang",
        "type": "Researcher"
      },
      {
        "name": "Qiushi Du",
        "type": "Researcher"
      },
      {
        "name": "Ruiqi Ge",
        "type": "Researcher"
      },
      {
        "name": "Ruisong Zhang",
        "type": "Researcher"
      },
      {
        "name": "Ruizhe Pan",
        "type": "Researcher"
      },
      {
        "name": "Runji Wang",
        "type": "Researcher"
      },
      {
        "name": "R. J. Chen",
        "type": "Researcher"
      },
      {
        "name": "R. L. Jin",
        "type": "Researcher"
      },
      {
        "name": "Ruyi Chen",
        "type": "Researcher"
      },
      {
        "name": "Shanghao Lu",
        "type": "Researcher"
      },
      {
        "name": "Shangyan Zhou",
        "type": "Researcher"
      },
      {
        "name": "Shanhuang Chen",
        "type": "Researcher"
      },
      {
        "name": "Shengfeng Ye",
        "type": "Researcher"
      },
      {
        "name": "Shiyu Wang",
        "type": "Researcher"
      },
      {
        "name": "Shuiping Yu",
        "type": "Researcher"
      },
      {
        "name": "Shunfeng Zhou",
        "type": "Researcher"
      },
      {
        "name": "Shuting Pan",
        "type": "Researcher"
      },
      {
        "name": "S. S. Li",
        "type": "Researcher"
      },
      {
        "name": "Shuang Zhou",
        "type": "Researcher"
      },
      {
        "name": "Shaoqing Wu",
        "type": "Researcher"
      },
      {
        "name": "Tao Yun",
        "type": "Researcher"
      },
      {
        "name": "Tian Pei",
        "type": "Researcher"
      },
      {
        "name": "Tianyu Sun",
        "type": "Researcher"
      },
      {
        "name": "T. Wang",
        "type": "Researcher"
      },
      {
        "name": "Wanjia Zhao",
        "type": "Researcher"
      },
      {
        "name": "Wen Liu",
        "type": "Researcher"
      },
      {
        "name": "Wenjun Gao",
        "type": "Researcher"
      },
      {
        "name": "Wenqin Yu",
        "type": "Researcher"
      },
      {
        "name": "W. L. Xiao",
        "type": "Researcher"
      },
      {
        "name": "Wei An",
        "type": "Researcher"
      },
      {
        "name": "Xiaodong Liu",
        "type": "Researcher"
      },
      {
        "name": "Xiaohan Wang",
        "type": "Researcher"
      },
      {
        "name": "Xiaokang Chen",
        "type": "Researcher"
      },
      {
        "name": "Xiaotao Nie",
        "type": "Researcher"
      },
      {
        "name": "Xin Liu",
        "type": "Researcher"
      },
      {
        "name": "Xin Xie",
        "type": "Researcher"
      },
      {
        "name": "Xingchao Liu",
        "type": "Researcher"
      },
      {
        "name": "Xinyu Yang",
        "type": "Researcher"
      },
      {
        "name": "Xinyuan Li",
        "type": "Researcher"
      },
      {
        "name": "Xuecheng Su",
        "type": "Researcher"
      },
      {
        "name": "Xuheng Lin",
        "type": "Researcher"
      },
      {
        "name": "X. Q. Li",
        "type": "Researcher"
      },
      {
        "name": "Xiangyue Jin",
        "type": "Researcher"
      },
      {
        "name": "Xiaojin Shen",
        "type": "Researcher"
      },
      {
        "name": "Xiaosha Chen",
        "type": "Researcher"
      },
      {
        "name": "Xiaowen Sun",
        "type": "Researcher"
      },
      {
        "name": "Xiaoxiang Wang",
        "type": "Researcher"
      },
      {
        "name": "Xinnan Song",
        "type": "Researcher"
      },
      {
        "name": "Xinyi Zhou",
        "type": "Researcher"
      },
      {
        "name": "Xianzu Wang",
        "type": "Researcher"
      },
      {
        "name": "Xinxia Shan",
        "type": "Researcher"
      },
      {
        "name": "Y. K. Li",
        "type": "Researcher"
      },
      {
        "name": "Y. Q. Wang",
        "type": "Researcher"
      },
      {
        "name": "Y. X. Wei",
        "type": "Researcher"
      },
      {
        "name": "Yang Zhang",
        "type": "Researcher"
      },
      {
        "name": "Yanhong Xu",
        "type": "Researcher"
      },
      {
        "name": "Yao Li",
        "type": "Researcher"
      },
      {
        "name": "Yao Zhao",
        "type": "Researcher"
      },
      {
        "name": "Yaofeng Sun",
        "type": "Researcher"
      },
      {
        "name": "Yaohui Wang",
        "type": "Researcher"
      },
      {
        "name": "Yi Yu",
        "type": "Researcher"
      },
      {
        "name": "Yichao Zhang",
        "type": "Researcher"
      },
      {
        "name": "Yifan Shi",
        "type": "Researcher"
      },
      {
        "name": "Yiliang Xiong",
        "type": "Researcher"
      },
      {
        "name": "Ying He",
        "type": "Researcher"
      },
      {
        "name": "Yishi Piao",
        "type": "Researcher"
      },
      {
        "name": "Yisong Wang",
        "type": "Researcher"
      },
      {
        "name": "Yixuan Tan",
        "type": "Researcher"
      },
      {
        "name": "Yiyang Ma",
        "type": "Researcher"
      },
      {
        "name": "Yiyuan Liu",
        "type": "Researcher"
      },
      {
        "name": "Yongqiang Guo",
        "type": "Researcher"
      },
      {
        "name": "Yuan Ou",
        "type": "Researcher"
      },
      {
        "name": "Yuduan Wang",
        "type": "Researcher"
      },
      {
        "name": "Yue Gong",
        "type": "Researcher"
      },
      {
        "name": "Yuheng Zou",
        "type": "Researcher"
      },
      {
        "name": "Yujia He",
        "type": "Researcher"
      },
      {
        "name": "Yunfan Xiong",
        "type": "Researcher"
      },
      {
        "name": "Yuxiang Luo",
        "type": "Researcher"
      },
      {
        "name": "Yuxiang You",
        "type": "Researcher"
      },
      {
        "name": "Yuxuan Liu",
        "type": "Researcher"
      },
      {
        "name": "Yuyang Zhou",
        "type": "Researcher"
      },
      {
        "name": "Y. X. Zhu",
        "type": "Researcher"
      },
      {
        "name": "Yaohui Li",
        "type": "Researcher"
      },
      {
        "name": "Yi Zheng",
        "type": "Researcher"
      },
      {
        "name": "Yuchen Zhu",
        "type": "Researcher"
      },
      {
        "name": "Yunxian Ma",
        "type": "Researcher"
      },
      {
        "name": "Ying Tang",
        "type": "Researcher"
      },
      {
        "name": "Yukun Zha",
        "type": "Researcher"
      },
      {
        "name": "Yuting Yan",
        "type": "Researcher"
      },
      {
        "name": "Z. Z. Ren",
        "type": "Researcher"
      },
      {
        "name": "Zehui Ren",
        "type": "Researcher"
      },
      {
        "name": "Zhangli Sha",
        "type": "Researcher"
      },
      {
        "name": "Zhe Fu",
        "type": "Researcher"
      },
      {
        "name": "Zhean Xu",
        "type": "Researcher"
      },
      {
        "name": "Zhengyan Zhang",
        "type": "Researcher"
      },
      {
        "name": "Zhicheng Ma",
        "type": "Researcher"
      },
      {
        "name": "Zhigang Yan",
        "type": "Researcher"
      },
      {
        "name": "Zhiyu Wu",
        "type": "Researcher"
      },
      {
        "name": "Zihui Gu",
        "type": "Researcher"
      },
      {
        "name": "Zijia Zhu",
        "type": "Researcher"
      },
      {
        "name": "Zijun Liu",
        "type": "Researcher"
      },
      {
        "name": "Zilin Li",
        "type": "Researcher"
      },
      {
        "name": "Ziwei Xie",
        "type": "Researcher"
      },
      {
        "name": "Ziyang Song",
        "type": "Researcher"
      },
      {
        "name": "Zizheng Pan",
        "type": "Researcher"
      },
      {
        "name": "Zhen Huang",
        "type": "Researcher"
      },
      {
        "name": "Zhipeng Xu",
        "type": "Researcher"
      },
      {
        "name": "Zhongyu Zhang",
        "type": "Researcher"
      },
      {
        "name": "Zhen Zhang",
        "type": "Researcher"
      },
      {
        "name": "Marah Abdin",
        "type": "Researcher"
      },
      {
        "name": "Jyoti Aneja",
        "type": "Researcher"
      },
      {
        "name": "Hany Awadalla",
        "type": "Researcher"
      },
      {
        "name": "Ahmed Awadallah",
        "type": "Researcher"
      },
      {
        "name": "Ammar Ahmad Awan",
        "type": "Researcher"
      },
      {
        "name": "Nguyen Bach",
        "type": "Researcher"
      },
      {
        "name": "Amit Bahree",
        "type": "Researcher"
      },
      {
        "name": "Arash Bakhtiari",
        "type": "Researcher"
      },
      {
        "name": "Jianmin Bao",
        "type": "Researcher"
      },
      {
        "name": "Harkirat Behl",
        "type": "Researcher"
      },
      {
        "name": "Alon Benhaim",
        "type": "Researcher"
      },
      {
        "name": "Misha Bilenko",
        "type": "Researcher"
      },
      {
        "name": "Johan Bjorck",
        "type": "Researcher"
      },
      {
        "name": "Sébastien Bubeck",
        "type": "Researcher"
      },
      {
        "name": "Martin Cai",
        "type": "Researcher"
      },
      {
        "name": "Qin Cai",
        "type": "Researcher"
      },
      {
        "name": "Vishrav Chaudhary",
        "type": "Researcher"
      },
      {
        "name": "Dong Chen",
        "type": "Researcher"
      },
      {
        "name": "Dongdong Chen",
        "type": "Researcher"
      },
      {
        "name": "Weizhu Chen",
        "type": "Researcher"
      },
      {
        "name": "Yen-Chun Chen",
        "type": "Researcher"
      },
      {
        "name": "Yi-Ling Chen",
        "type": "Researcher"
      },
      {
        "name": "Hao Cheng",
        "type": "Researcher"
      },
      {
        "name": "Parul Chopra",
        "type": "Researcher"
      },
      {
        "name": "Xiyang Dai",
        "type": "Researcher"
      },
      {
        "name": "Matthew Dixon",
        "type": "Researcher"
      },
      {
        "name": "Ronen Eldan",
        "type": "Researcher"
      },
      {
        "name": "Victor Fragoso",
        "type": "Researcher"
      },
      {
        "name": "Jianfeng Gao",
        "type": "Researcher"
      },
      {
        "name": "Mei Gao",
        "type": "Researcher"
      },
      {
        "name": "Min Gao",
        "type": "Researcher"
      },
      {
        "name": "Amit Garg",
        "type": "Researcher"
      },
      {
        "name": "Allie Del Giorno",
        "type": "Researcher"
      },
      {
        "name": "Abhishek Goswami",
        "type": "Researcher"
      },
      {
        "name": "Suriya Gunasekar",
        "type": "Researcher"
      },
      {
        "name": "Emman Haider",
        "type": "Researcher"
      },
      {
        "name": "Junheng Hao",
        "type": "Researcher"
      },
      {
        "name": "Russell J. Hewett",
        "type": "Researcher"
      },
      {
        "name": "Wenxiang Hu",
        "type": "Researcher"
      },
      {
        "name": "Jamie Huynh",
        "type": "Researcher"
      },
      {
        "name": "Dan Iter",
        "type": "Researcher"
      },
      {
        "name": "Sam Ade Jacobs",
        "type": "Researcher"
      },
      {
        "name": "Mojan Javaheripi",
        "type": "Researcher"
      },
      {
        "name": "Nikos Karampatziakis",
        "type": "Researcher"
      },
      {
        "name": "Piero Kauffmann",
        "type": "Researcher"
      },
      {
        "name": "Mahoud Khademi",
        "type": "Researcher"
      },
      {
        "name": "Dongwoo Kim",
        "type": "Researcher"
      },
      {
        "name": "Young Jin Kim",
        "type": "Researcher"
      },
      {
        "name": "Lev Kurilenko",
        "type": "Researcher"
      },
      {
        "name": "James R. Lee",
        "type": "Researcher"
      },
      {
        "name": "Yin Tat Lee",
        "type": "Researcher"
      },
      {
        "name": "Yuanzhi Li",
        "type": "Researcher"
      },
      {
        "name": "Yunsheng Li",
        "type": "Researcher"
      },
      {
        "name": "Chen Liang",
        "type": "Researcher"
      },
      {
        "name": "Lars Liden",
        "type": "Researcher"
      },
      {
        "name": "Xihui Lin",
        "type": "Researcher"
      },
      {
        "name": "Zeqi Lin",
        "type": "Researcher"
      },
      {
        "name": "Liyuan Liu",
        "type": "Researcher"
      },
      {
        "name": "Weishung Liu",
        "type": "Researcher"
      },
      {
        "name": "Chong Luo",
        "type": "Researcher"
      },
      {
        "name": "Piyush Madan",
        "type": "Researcher"
      },
      {
        "name": "Ali Mahmoudzadeh",
        "type": "Researcher"
      },
      {
        "name": "David Majercak",
        "type": "Researcher"
      },
      {
        "name": "Matt Mazzola",
        "type": "Researcher"
      },
      {
        "name": "Caio César Teodoro Mendes",
        "type": "Researcher"
      },
      {
        "name": "Arindam Mitra",
        "type": "Researcher"
      },
      {
        "name": "Hardik Modi",
        "type": "Researcher"
      },
      {
        "name": "Anh Nguyen",
        "type": "Researcher"
      },
      {
        "name": "Brandon Norick",
        "type": "Researcher"
      },
      {
        "name": "Barun Patra",
        "type": "Researcher"
      },
      {
        "name": "Daniel Perez-Becker",
        "type": "Researcher"
      },
      {
        "name": "Thomas Portet",
        "type": "Researcher"
      },
      {
        "name": "Reid Pryzant",
        "type": "Researcher"
      },
      {
        "name": "Heyang Qin",
        "type": "Researcher"
      },
      {
        "name": "Marko Radmilac",
        "type": "Researcher"
      },
      {
        "name": "Liliang Ren",
        "type": "Researcher"
      },
      {
        "name": "Gustavo de Rosa",
        "type": "Researcher"
      },
      {
        "name": "Corby Rosset",
        "type": "Researcher"
      },
      {
        "name": "Sambudha Roy",
        "type": "Researcher"
      },
      {
        "name": "Olatunji Ruwase",
        "type": "Researcher"
      },
      {
        "name": "Olli Saarikivi",
        "type": "Researcher"
      },
      {
        "name": "Amin Saied",
        "type": "Researcher"
      },
      {
        "name": "Adil Salim",
        "type": "Researcher"
      },
      {
        "name": "Michael Santacroce",
        "type": "Researcher"
      },
      {
        "name": "Shital Shah",
        "type": "Researcher"
      },
      {
        "name": "Ning Shang",
        "type": "Researcher"
      },
      {
        "name": "Hiteshi Sharma",
        "type": "Researcher"
      },
      {
        "name": "Yelong Shen",
        "type": "Researcher"
      },
      {
        "name": "Swadheen Shukla",
        "type": "Researcher"
      },
      {
        "name": "Xia Song",
        "type": "Researcher"
      },
      {
        "name": "Masahiro Tanaka",
        "type": "Researcher"
      },
      {
        "name": "Andrea Tupini",
        "type": "Researcher"
      },
      {
        "name": "Praneetha Vaddamanu",
        "type": "Researcher"
      },
      {
        "name": "Chunyu Wang",
        "type": "Researcher"
      },
      {
        "name": "Guanhua Wang",
        "type": "Researcher"
      },
      {
        "name": "Lijuan Wang",
        "type": "Researcher"
      },
      {
        "name": "Shuohang Wang",
        "type": "Researcher"
      },
      {
        "name": "Xin Wang",
        "type": "Researcher"
      },
      {
        "name": "Yu Wang",
        "type": "Researcher"
      },
      {
        "name": "Rachel Ward",
        "type": "Researcher"
      },
      {
        "name": "Wen Wen",
        "type": "Researcher"
      },
      {
        "name": "Philipp Witte",
        "type": "Researcher"
      },
      {
        "name": "Haiping Wu",
        "type": "Researcher"
      },
      {
        "name": "Xiaoxia Wu",
        "type": "Researcher"
      },
      {
        "name": "Michael Wyatt",
        "type": "Researcher"
      },
      {
        "name": "Bin Xiao",
        "type": "Researcher"
      },
      {
        "name": "Can Xu",
        "type": "Researcher"
      },
      {
        "name": "Jiahang Xu",
        "type": "Researcher"
      },
      {
        "name": "Weijian Xu",
        "type": "Researcher"
      },
      {
        "name": "Jilong Xue",
        "type": "Researcher"
      },
      {
        "name": "Sonali Yadav",
        "type": "Researcher"
      },
      {
        "name": "Fan Yang",
        "type": "Researcher"
      },
      {
        "name": "Jianwei Yang",
        "type": "Researcher"
      },
      {
        "name": "Ziyi Yang",
        "type": "Researcher"
      },
      {
        "name": "Donghan Yu",
        "type": "Researcher"
      },
      {
        "name": "Lu Yuan",
        "type": "Researcher"
      },
      {
        "name": "Chenruidong Zhang",
        "type": "Researcher"
      },
      {
        "name": "Cyril Zhang",
        "type": "Researcher"
      },
      {
        "name": "Jianwen Zhang",
        "type": "Researcher"
      },
      {
        "name": "Li Lyna Zhang",
        "type": "Researcher"
      },
      {
        "name": "Yi Zhang",
        "type": "Researcher"
      },
      {
        "name": "Yue Zhang",
        "type": "Researcher"
      },
      {
        "name": "Yunan Zhang",
        "type": "Researcher"
      },
      {
        "name": "Xiren Zhou",
        "type": "Researcher"
      },
      {
        "name": "Yuanhan Zhang",
        "type": "Researcher"
      },
      {
        "name": "Dong Guo",
        "type": "Researcher"
      },
      {
        "name": "Renrui Zhang",
        "type": "Researcher"
      },
      {
        "name": "Feng Li",
        "type": "Researcher"
      },
      {
        "name": "Peiyuan Zhang",
        "type": "Researcher"
      },
      {
        "name": "Yanwei Li",
        "type": "Researcher"
      },
      {
        "name": "Weichen Liu",
        "type": "Researcher"
      },
      {
        "name": "Qiyao Xue",
        "type": "Researcher"
      },
      {
        "name": "Haoming Wang",
        "type": "Researcher"
      },
      {
        "name": "Xiangyu Yin",
        "type": "Researcher"
      },
      {
        "name": "Boyuan Yang",
        "type": "Researcher"
      },
      {
        "name": "Wei Gao",
        "type": "Researcher"
      },
      {
        "name": "Yuxi Xiao",
        "type": "Researcher"
      },
      {
        "name": "Longfei Li",
        "type": "Researcher"
      },
      {
        "name": "Shen Yan",
        "type": "Researcher"
      },
      {
        "name": "Xinhang Liu",
        "type": "Researcher"
      },
      {
        "name": "Sida Peng",
        "type": "Researcher"
      },
      {
        "name": "Yunchao Wei",
        "type": "Researcher"
      },
      {
        "name": "Xiaowei Zhou",
        "type": "Researcher"
      },
      {
        "name": "Mingrui Wu",
        "type": "Researcher"
      },
      {
        "name": "Zhaozhi Wang",
        "type": "Researcher"
      },
      {
        "name": "Fangjinhua Wang",
        "type": "Researcher"
      },
      {
        "name": "Jiaolong Yang",
        "type": "Researcher"
      },
      {
        "name": "Marc Pollefeys",
        "type": "Researcher"
      },
      {
        "name": "Tong Zhang",
        "type": "Researcher"
      },
      {
        "name": "Meng Cao",
        "type": "Researcher"
      },
      {
        "name": "Xingyu Li",
        "type": "Researcher"
      },
      {
        "name": "Xue Liu",
        "type": "Researcher"
      },
      {
        "name": "Xiaodan Liang",
        "type": "Researcher"
      },
      {
        "name": "Joseph Redmon",
        "type": "Researcher"
      },
      {
        "name": "Santosh Divvala",
        "type": "Researcher"
      },
      {
        "name": "Ali Farhadi",
        "type": "Researcher"
      },
      {
        "name": "Zhichao Sun",
        "type": "Researcher"
      },
      {
        "name": "Yepeng Liu",
        "type": "Researcher"
      },
      {
        "name": "Zhiling Su",
        "type": "Researcher"
      },
      {
        "name": "Huachao Zhu",
        "type": "Researcher"
      },
      {
        "name": "Yuliang Gu",
        "type": "Researcher"
      },
      {
        "name": "Yuda Zou",
        "type": "Researcher"
      },
      {
        "name": "Zelong Liu",
        "type": "Researcher"
      },
      {
        "name": "Gui-Song Xia",
        "type": "Researcher"
      },
      {
        "name": "Bo Du",
        "type": "Researcher"
      },
      {
        "name": "Yongchao Xu",
        "type": "Researcher"
      },
      {
        "name": "Shenghao Fu",
        "type": "Researcher"
      },
      {
        "name": "Yukun Su",
        "type": "Researcher"
      },
      {
        "name": "Fengyun Rao",
        "type": "Researcher"
      },
      {
        "name": "Jing Lyu",
        "type": "Researcher"
      },
      {
        "name": "Xiaohua Xie",
        "type": "Researcher"
      },
      {
        "name": "Wei-Shi Zheng",
        "type": "Researcher"
      },
      {
        "name": "Jiwan Chung",
        "type": "Researcher"
      },
      {
        "name": "Junhyeok Kim",
        "type": "Researcher"
      },
      {
        "name": "Siyeol Kim",
        "type": "Researcher"
      },
      {
        "name": "Jaeyoung Lee",
        "type": "Researcher"
      },
      {
        "name": "Min Soo Kim",
        "type": "Researcher"
      },
      {
        "name": "Youngjae Yu",
        "type": "Researcher"
      },
      {
        "name": "Jiazhe Wei",
        "type": "Researcher"
      },
      {
        "name": "Ken Li",
        "type": "Researcher"
      },
      {
        "name": "Tianyu Lao",
        "type": "Researcher"
      },
      {
        "name": "Haofan Wang",
        "type": "Researcher"
      },
      {
        "name": "Liang Wang",
        "type": "Researcher"
      },
      {
        "name": "Caifeng Shan",
        "type": "Researcher"
      },
      {
        "name": "Chenyang Si",
        "type": "Researcher"
      },
      {
        "name": "Jianhua Han",
        "type": "Researcher"
      },
      {
        "name": "Meng Tian",
        "type": "Researcher"
      },
      {
        "name": "Jiangtong Zhu",
        "type": "Researcher"
      },
      {
        "name": "Fan He",
        "type": "Researcher"
      },
      {
        "name": "Huixin Zhang",
        "type": "Researcher"
      },
      {
        "name": "Sitong Guo",
        "type": "Researcher"
      },
      {
        "name": "Dechang Zhu",
        "type": "Researcher"
      },
      {
        "name": "Hao Tang",
        "type": "Researcher"
      },
      {
        "name": "Pei Xu",
        "type": "Researcher"
      },
      {
        "name": "Yuze Guo",
        "type": "Researcher"
      },
      {
        "name": "Minzhe Niu",
        "type": "Researcher"
      },
      {
        "name": "Haojie Zhu",
        "type": "Researcher"
      },
      {
        "name": "Qichao Dong",
        "type": "Researcher"
      },
      {
        "name": "Xuechao Yan",
        "type": "Researcher"
      },
      {
        "name": "Siyuan Dong",
        "type": "Researcher"
      },
      {
        "name": "Lu Hou",
        "type": "Researcher"
      },
      {
        "name": "Qingqiu Huang",
        "type": "Researcher"
      },
      {
        "name": "Hang Xu",
        "type": "Researcher"
      },
      {
        "name": "R. S. Sutton",
        "type": "Researcher"
      },
      {
        "name": "A. Barto",
        "type": "Researcher"
      },
      {
        "name": "Jonathan J. Hunt",
        "type": "Researcher"
      },
      {
        "name": "Alexander Pritzel",
        "type": "Researcher"
      },
      {
        "name": "Nicolas Heess",
        "type": "Researcher"
      },
      {
        "name": "Tom Erez",
        "type": "Researcher"
      },
      {
        "name": "Yuval Tassa",
        "type": "Researcher"
      },
      {
        "name": "Shenzhi Wang",
        "type": "Researcher"
      },
      {
        "name": "Le Yu",
        "type": "Researcher"
      },
      {
        "name": "Chang Gao",
        "type": "Researcher"
      },
      {
        "name": "Chujie Zheng",
        "type": "Researcher"
      },
      {
        "name": "Shixuan Liu",
        "type": "Researcher"
      },
      {
        "name": "Xionghui Chen",
        "type": "Researcher"
      },
      {
        "name": "Jianxin Yang",
        "type": "Researcher"
      },
      {
        "name": "Yuqiong Liu",
        "type": "Researcher"
      },
      {
        "name": "Andrew Zhao",
        "type": "Researcher"
      },
      {
        "name": "Shiji Song",
        "type": "Researcher"
      },
      {
        "name": "Ganqu Cui",
        "type": "Researcher"
      },
      {
        "name": "Jiacheng Chen",
        "type": "Researcher"
      },
      {
        "name": "Lifan Yuan",
        "type": "Researcher"
      },
      {
        "name": "Zhi Wang",
        "type": "Researcher"
      },
      {
        "name": "Yuxin Zuo",
        "type": "Researcher"
      },
      {
        "name": "Haozhan Li",
        "type": "Researcher"
      },
      {
        "name": "Yuchen Fan",
        "type": "Researcher"
      },
      {
        "name": "Huayu Chen",
        "type": "Researcher"
      },
      {
        "name": "Weize Chen",
        "type": "Researcher"
      },
      {
        "name": "Zhiyuan Liu",
        "type": "Researcher"
      },
      {
        "name": "Hao Peng",
        "type": "Researcher"
      },
      {
        "name": "Lei Bai",
        "type": "Researcher"
      },
      {
        "name": "Jianhao Yan",
        "type": "Researcher"
      },
      {
        "name": "Yafu Li",
        "type": "Researcher"
      },
      {
        "name": "Zican Hu",
        "type": "Researcher"
      },
      {
        "name": "Komal Kumar",
        "type": "Researcher"
      },
      {
        "name": "Tajamul Ashraf",
        "type": "Researcher"
      },
      {
        "name": "Omkar Thawakar",
        "type": "Researcher"
      },
      {
        "name": "Rao Muhammad Anwer",
        "type": "Researcher"
      },
      {
        "name": "Hisham Cholakkal",
        "type": "Researcher"
      },
      {
        "name": "Mubarak Shah",
        "type": "Researcher"
      },
      {
        "name": "Ming-Hsuan Yang",
        "type": "Researcher"
      },
      {
        "name": "Phillip H. S. Torr",
        "type": "Researcher"
      },
      {
        "name": "Fahad Shahbaz Khan",
        "type": "Researcher"
      },
      {
        "name": "Salman Khan",
        "type": "Researcher"
      },
      {
        "name": "Nicolas Le Roux",
        "type": "Researcher"
      },
      {
        "name": "Jonathan Lebensold",
        "type": "Researcher"
      },
      {
        "name": "Arnaud Bergeron",
        "type": "Researcher"
      },
      {
        "name": "Joshua Greaves",
        "type": "Researcher"
      },
      {
        "name": "Alex Fréchette",
        "type": "Researcher"
      },
      {
        "name": "Carolyne Pelletier",
        "type": "Researcher"
      },
      {
        "name": "Eric Thibodeau-Laufer",
        "type": "Researcher"
      },
      {
        "name": "Sándor Toth",
        "type": "Researcher"
      },
      {
        "name": "Sam Work",
        "type": "Researcher"
      },
      {
        "name": "Evan Shelhamer",
        "type": "Researcher"
      },
      {
        "name": "Jonathan Long",
        "type": "Researcher"
      },
      {
        "name": "Florinel-Alin Croitoru",
        "type": "Researcher"
      },
      {
        "name": "Vlad Hondru",
        "type": "Researcher"
      },
      {
        "name": "Radu Tudor Ionescu",
        "type": "Researcher"
      },
      {
        "name": "Meng-Hao Guo",
        "type": "Researcher"
      },
      {
        "name": "Cheng-Ze Lu",
        "type": "Researcher"
      },
      {
        "name": "Qibin Hou",
        "type": "Researcher"
      },
      {
        "name": "Zhengning Liu",
        "type": "Researcher"
      },
      {
        "name": "Ming-Ming Cheng",
        "type": "Researcher"
      },
      {
        "name": "Shi-Min Hu",
        "type": "Researcher"
      },
      {
        "name": "Jiaming Zhang",
        "type": "Researcher"
      },
      {
        "name": "Huayao Liu",
        "type": "Researcher"
      },
      {
        "name": "Kailun Yang",
        "type": "Researcher"
      },
      {
        "name": "Xinxin Hu",
        "type": "Researcher"
      },
      {
        "name": "Ruiping Liu",
        "type": "Researcher"
      },
      {
        "name": "Rainer Stiefelhagen",
        "type": "Researcher"
      },
      {
        "name": "Lei Ke",
        "type": "Researcher"
      },
      {
        "name": "Mingqiao Ye",
        "type": "Researcher"
      },
      {
        "name": "Martin Danelljan",
        "type": "Researcher"
      },
      {
        "name": "Yifan Liu",
        "type": "Researcher"
      },
      {
        "name": "Yu-Wing Tai",
        "type": "Researcher"
      },
      {
        "name": "Chi-Keung Tang",
        "type": "Researcher"
      },
      {
        "name": "Fisher Yu",
        "type": "Researcher"
      },
      {
        "name": "Jiacong Xu",
        "type": "Researcher"
      },
      {
        "name": "Zixiang Xiong",
        "type": "Researcher"
      },
      {
        "name": "Shankar P. Bhattacharyya",
        "type": "Researcher"
      },
      {
        "name": "Baosong Yang",
        "type": "Researcher"
      },
      {
        "name": "Bo Zheng",
        "type": "Researcher"
      },
      {
        "name": "Chengpeng Li",
        "type": "Researcher"
      },
      {
        "name": "Chengyuan Li",
        "type": "Researcher"
      },
      {
        "name": "Guanting Dong",
        "type": "Researcher"
      },
      {
        "name": "Haoran Wei",
        "type": "Researcher"
      },
      {
        "name": "Huan Lin",
        "type": "Researcher"
      },
      {
        "name": "Jialong Tang",
        "type": "Researcher"
      },
      {
        "name": "Jialin Wang",
        "type": "Researcher"
      },
      {
        "name": "Jinzheng He",
        "type": "Researcher"
      },
      {
        "name": "Kexin Yang",
        "type": "Researcher"
      },
      {
        "name": "Mingfeng Xue",
        "type": "Researcher"
      },
      {
        "name": "Na Ni",
        "type": "Researcher"
      },
      {
        "name": "Pei Zhang",
        "type": "Researcher"
      },
      {
        "name": "Ru Peng",
        "type": "Researcher"
      },
      {
        "name": "Ruize Gao",
        "type": "Researcher"
      },
      {
        "name": "Tianhao Li",
        "type": "Researcher"
      },
      {
        "name": "Tianyu Liu",
        "type": "Researcher"
      },
      {
        "name": "Xinyu Zhang",
        "type": "Researcher"
      },
      {
        "name": "Xipin Wei",
        "type": "Researcher"
      },
      {
        "name": "Xuejing Liu",
        "type": "Researcher"
      },
      {
        "name": "Yu Wan",
        "type": "Researcher"
      },
      {
        "name": "Zhifang Guo",
        "type": "Researcher"
      },
      {
        "name": "Zhihao Fan",
        "type": "Researcher"
      },
      {
        "name": "Shuang Zeng",
        "type": "Researcher"
      },
      {
        "name": "Xinyuan Chang",
        "type": "Researcher"
      },
      {
        "name": "Mengwei Xie",
        "type": "Researcher"
      },
      {
        "name": "Xinran Liu",
        "type": "Researcher"
      },
      {
        "name": "Yifan Bai",
        "type": "Researcher"
      },
      {
        "name": "Zheng Pan",
        "type": "Researcher"
      },
      {
        "name": "Mu Xu",
        "type": "Researcher"
      },
      {
        "name": "Xing Wei",
        "type": "Researcher"
      },
      {
        "name": "Ning Guo",
        "type": "Researcher"
      },
      {
        "name": "Zewei Zhou",
        "type": "Researcher"
      },
      {
        "name": "Tianhui Cai",
        "type": "Researcher"
      },
      {
        "name": "Seth Z. Zhao",
        "type": "Researcher"
      },
      {
        "name": "Yun Zhang",
        "type": "Researcher"
      },
      {
        "name": "Zhiyu Huang",
        "type": "Researcher"
      },
      {
        "name": "Bolei Zhou",
        "type": "Researcher"
      },
      {
        "name": "Haohan Chi",
        "type": "Researcher"
      },
      {
        "name": "Huan-ang Gao",
        "type": "Researcher"
      },
      {
        "name": "Ziming Liu",
        "type": "Researcher"
      },
      {
        "name": "Jianing Liu",
        "type": "Researcher"
      },
      {
        "name": "Chenyu Liu",
        "type": "Researcher"
      },
      {
        "name": "Jinwei Li",
        "type": "Researcher"
      },
      {
        "name": "Kaisen Yang",
        "type": "Researcher"
      },
      {
        "name": "Yangcheng Yu",
        "type": "Researcher"
      },
      {
        "name": "Zeda Wang",
        "type": "Researcher"
      },
      {
        "name": "Wenyi Li",
        "type": "Researcher"
      },
      {
        "name": "Leichen Wang",
        "type": "Researcher"
      },
      {
        "name": "Xingtao Hu",
        "type": "Researcher"
      },
      {
        "name": "Hao Sun",
        "type": "Researcher"
      },
      {
        "name": "Mengmeng Yang",
        "type": "Researcher"
      },
      {
        "name": "Jinyu Miao",
        "type": "Researcher"
      },
      {
        "name": "Yining Shi",
        "type": "Researcher"
      },
      {
        "name": "He Zhe Lim",
        "type": "Researcher"
      },
      {
        "name": "Li Liu",
        "type": "Researcher"
      },
      {
        "name": "Tianbao Zhou",
        "type": "Researcher"
      },
      {
        "name": "Huang Yu",
        "type": "Researcher"
      },
      {
        "name": "Yifei Hu",
        "type": "Researcher"
      },
      {
        "name": "Guang Li",
        "type": "Researcher"
      },
      {
        "name": "Zhenyu Lin",
        "type": "Researcher"
      },
      {
        "name": "Zhiwei Xiong",
        "type": "Researcher"
      },
      {
        "name": "Xinhai Zhao",
        "type": "Researcher"
      },
      {
        "name": "Anqing Jiang",
        "type": "Researcher"
      },
      {
        "name": "Yiru Wang",
        "type": "Researcher"
      },
      {
        "name": "Zhigang Sun",
        "type": "Researcher"
      },
      {
        "name": "Shuo Wang",
        "type": "Researcher"
      },
      {
        "name": "Yuwen Heng",
        "type": "Researcher"
      },
      {
        "name": "Shichen Tang",
        "type": "Researcher"
      },
      {
        "name": "Lijuan Zhu",
        "type": "Researcher"
      },
      {
        "name": "Jinhao Chai",
        "type": "Researcher"
      },
      {
        "name": "Jijun Wang",
        "type": "Researcher"
      },
      {
        "name": "Zichong Gu",
        "type": "Researcher"
      },
      {
        "name": "Hao Jiang",
        "type": "Researcher"
      },
      {
        "name": "Yingyan Li",
        "type": "Researcher"
      },
      {
        "name": "Shuyao Shang",
        "type": "Researcher"
      },
      {
        "name": "Weisong Liu",
        "type": "Researcher"
      },
      {
        "name": "Bing Zhan",
        "type": "Researcher"
      },
      {
        "name": "Haochen Wang",
        "type": "Researcher"
      },
      {
        "name": "Yuntao Chen",
        "type": "Researcher"
      },
      {
        "name": "Xiaoman Wang",
        "type": "Researcher"
      },
      {
        "name": "Yasong An",
        "type": "Researcher"
      },
      {
        "name": "Chufeng Tang",
        "type": "Researcher"
      },
      {
        "name": "Lue Fan",
        "type": "Researcher"
      },
      {
        "name": "Bernhard Kerbl",
        "type": "Researcher"
      },
      {
        "name": "Georgios Kopanas",
        "type": "Researcher"
      },
      {
        "name": "Thomas Leimkühler",
        "type": "Researcher"
      },
      {
        "name": "George Drettakis",
        "type": "Researcher"
      },
      {
        "name": "Johannes L. Schönberger",
        "type": "Researcher"
      },
      {
        "name": "Jan-Michael Frahm",
        "type": "Researcher"
      },
      {
        "name": "Zhenxin Li",
        "type": "Researcher"
      },
      {
        "name": "Wenhao Yao",
        "type": "Researcher"
      },
      {
        "name": "Zi Wang",
        "type": "Researcher"
      },
      {
        "name": "Xinglong Sun",
        "type": "Researcher"
      },
      {
        "name": "Joshua Chen",
        "type": "Researcher"
      },
      {
        "name": "Nadine Chang",
        "type": "Researcher"
      },
      {
        "name": "Maying Shen",
        "type": "Researcher"
      },
      {
        "name": "Zuxuan Wu",
        "type": "Researcher"
      },
      {
        "name": "Shiyi Lan",
        "type": "Researcher"
      },
      {
        "name": "Jose M. Alvarez",
        "type": "Researcher"
      },
      {
        "name": "Lan Feng",
        "type": "Researcher"
      },
      {
        "name": "Yang Gao",
        "type": "Researcher"
      },
      {
        "name": "Quanyi Li",
        "type": "Researcher"
      },
      {
        "name": "Wuyang Li",
        "type": "Researcher"
      },
      {
        "name": "Sichao Liu",
        "type": "Researcher"
      },
      {
        "name": "Maciej K. Wozniak",
        "type": "Researcher"
      },
      {
        "name": "Lianhang Liu",
        "type": "Researcher"
      },
      {
        "name": "Yixi Cai",
        "type": "Researcher"
      },
      {
        "name": "Patric Jensfelt",
        "type": "Researcher"
      },
      {
        "name": "Junnan Li",
        "type": "Researcher"
      },
      {
        "name": "Dongxu Li",
        "type": "Researcher"
      },
      {
        "name": "Silvio Savarese",
        "type": "Researcher"
      },
      {
        "name": "Steven Hoi",
        "type": "Researcher"
      },
      {
        "name": "Maximilian Nickel",
        "type": "Researcher"
      },
      {
        "name": "Matt Le",
        "type": "Researcher"
      },
      {
        "name": "Cheng Chi",
        "type": "Researcher"
      },
      {
        "name": "Zhenjia Xu",
        "type": "Researcher"
      },
      {
        "name": "Siyuan Feng",
        "type": "Researcher"
      },
      {
        "name": "Eric Cousineau",
        "type": "Researcher"
      },
      {
        "name": "Benjamin Burchfiel",
        "type": "Researcher"
      },
      {
        "name": "Russ Tedrake",
        "type": "Researcher"
      },
      {
        "name": "Shuran Song",
        "type": "Researcher"
      },
      {
        "name": "Fanlong Zeng",
        "type": "Researcher"
      },
      {
        "name": "Wensheng Gan",
        "type": "Researcher"
      },
      {
        "name": "Zezheng Huai",
        "type": "Researcher"
      },
      {
        "name": "Hechang Chen",
        "type": "Researcher"
      },
      {
        "name": "Yongheng Wang",
        "type": "Researcher"
      },
      {
        "name": "Ning Liu",
        "type": "Researcher"
      },
      {
        "name": "Philip S. Yu",
        "type": "Researcher"
      },
      {
        "name": "Xue Yang",
        "type": "Researcher"
      },
      {
        "name": "Maoqing Yao",
        "type": "Researcher"
      },
      {
        "name": "Dapeng Zhang",
        "type": "Researcher"
      },
      {
        "name": "Jing Sun",
        "type": "Researcher"
      },
      {
        "name": "Chenghui Hu",
        "type": "Researcher"
      },
      {
        "name": "Xiaoyan Wu",
        "type": "Researcher"
      },
      {
        "name": "Zhenlong Yuan",
        "type": "Researcher"
      },
      {
        "name": "Rui Zhou",
        "type": "Researcher"
      },
      {
        "name": "Fei Shen",
        "type": "Researcher"
      },
      {
        "name": "Qingguo Zhou",
        "type": "Researcher"
      },
      {
        "name": "Xiangyu Yue",
        "type": "Researcher"
      },
      {
        "name": "Peter Albert",
        "type": "Researcher"
      },
      {
        "name": "Amjad Almahairi",
        "type": "Researcher"
      },
      {
        "name": "Dan Bikel",
        "type": "Researcher"
      },
      {
        "name": "Moya Chen",
        "type": "Researcher"
      },
      {
        "name": "Guillem Cucurull",
        "type": "Researcher"
      },
      {
        "name": "Jude Fernandes",
        "type": "Researcher"
      },
      {
        "name": "Brian Fuller",
        "type": "Researcher"
      },
      {
        "name": "Yinghai Lu",
        "type": "Researcher"
      },
      {
        "name": "Pushkar Mishra",
        "type": "Researcher"
      },
      {
        "name": "Yixin Nie",
        "type": "Researcher"
      },
      {
        "name": "Rashi Rungta",
        "type": "Researcher"
      },
      {
        "name": "Kalyan Saladi",
        "type": "Researcher"
      },
      {
        "name": "Ranjan Subramanian",
        "type": "Researcher"
      },
      {
        "name": "Adina Williams",
        "type": "Researcher"
      },
      {
        "name": "Jian Xiang Kuan",
        "type": "Researcher"
      },
      {
        "name": "Weifan Guan",
        "type": "Researcher"
      },
      {
        "name": "Qinghao Hu",
        "type": "Researcher"
      },
      {
        "name": "Aosheng Li",
        "type": "Researcher"
      },
      {
        "name": "Jian Cheng",
        "type": "Researcher"
      },
      {
        "name": "Yuechen Luo",
        "type": "Researcher"
      },
      {
        "name": "Zhiyi Lai",
        "type": "Researcher"
      },
      {
        "name": "Lei Yang",
        "type": "Researcher"
      },
      {
        "name": "Qimao Chen",
        "type": "Researcher"
      },
      {
        "name": "Jiaxin Liu",
        "type": "Researcher"
      },
      {
        "name": "Zhi-xin Yang",
        "type": "Researcher"
      },
      {
        "name": "Ruiyang Hao",
        "type": "Researcher"
      },
      {
        "name": "Haibao Yu",
        "type": "Researcher"
      },
      {
        "name": "Jiaru Zhong",
        "type": "Researcher"
      },
      {
        "name": "Chuanye Wang",
        "type": "Researcher"
      },
      {
        "name": "Yiming Kan",
        "type": "Researcher"
      },
      {
        "name": "Wenxian Yang",
        "type": "Researcher"
      },
      {
        "name": "Siqi Fan",
        "type": "Researcher"
      },
      {
        "name": "Huilin Yin",
        "type": "Researcher"
      },
      {
        "name": "Jianing Qiu",
        "type": "Researcher"
      },
      {
        "name": "Yao Mu",
        "type": "Researcher"
      },
      {
        "name": "Jiankai Sun",
        "type": "Researcher"
      },
      {
        "name": "Walter Zimmer",
        "type": "Researcher"
      },
      {
        "name": "Dandan Zhang",
        "type": "Researcher"
      },
      {
        "name": "Shanghang Zhang",
        "type": "Researcher"
      },
      {
        "name": "Mac Schwager",
        "type": "Researcher"
      },
      {
        "name": "Zaiqing Nie",
        "type": "Researcher"
      },
      {
        "name": "Weixing Chen",
        "type": "Researcher"
      },
      {
        "name": "Yongjie Bai",
        "type": "Researcher"
      },
      {
        "name": "Guanbin Li",
        "type": "Researcher"
      },
      {
        "name": "Wen Gao",
        "type": "Researcher"
      },
      {
        "name": "Liang Lin",
        "type": "Researcher"
      },
      {
        "name": "Yueen Ma",
        "type": "Researcher"
      },
      {
        "name": "Zixing Song",
        "type": "Researcher"
      },
      {
        "name": "Yuzheng Zhuang",
        "type": "Researcher"
      },
      {
        "name": "Jianye Hao",
        "type": "Researcher"
      },
      {
        "name": "Irwin King",
        "type": "Researcher"
      },
      {
        "name": "Chaojun Ni",
        "type": "Researcher"
      },
      {
        "name": "Xueyang Zhang",
        "type": "Researcher"
      },
      {
        "name": "Yida Wang",
        "type": "Researcher"
      },
      {
        "name": "Xinze Chen",
        "type": "Researcher"
      },
      {
        "name": "Boyuan Wang",
        "type": "Researcher"
      },
      {
        "name": "Youyi Zhang",
        "type": "Researcher"
      },
      {
        "name": "Wenjun Mei",
        "type": "Researcher"
      },
      {
        "name": "Fanqing Meng",
        "type": "Researcher"
      },
      {
        "name": "Jiaqi Liao",
        "type": "Researcher"
      },
      {
        "name": "Xinyu Tan",
        "type": "Researcher"
      },
      {
        "name": "Quanfeng Lu",
        "type": "Researcher"
      },
      {
        "name": "Dianqi Li",
        "type": "Researcher"
      },
      {
        "name": "Yu Li",
        "type": "Researcher"
      },
      {
        "name": "Keyu Zhao",
        "type": "Researcher"
      },
      {
        "name": "Likai Ma",
        "type": "Researcher"
      },
      {
        "name": "Jiahe Liu",
        "type": "Researcher"
      },
      {
        "name": "Tianhui Liu",
        "type": "Researcher"
      },
      {
        "name": "Yuwei Du",
        "type": "Researcher"
      },
      {
        "name": "Siqi Guo",
        "type": "Researcher"
      },
      {
        "name": "Yuming Lin",
        "type": "Researcher"
      },
      {
        "name": "Kaiwen Zhang",
        "type": "Researcher"
      },
      {
        "name": "Zhenyu Tang",
        "type": "Researcher"
      },
      {
        "name": "Xiaotao Hu",
        "type": "Researcher"
      },
      {
        "name": "Xingang Pan",
        "type": "Researcher"
      },
      {
        "name": "Xiaoyang Guo",
        "type": "Researcher"
      },
      {
        "name": "Jingwei Huang",
        "type": "Researcher"
      },
      {
        "name": "Xiao-Xiao Long",
        "type": "Researcher"
      },
      {
        "name": "Xun Cao",
        "type": "Researcher"
      },
      {
        "name": "Wei Yin",
        "type": "Researcher"
      },
      {
        "name": "Yinhan Liu",
        "type": "Researcher"
      },
      {
        "name": "Myle Ott",
        "type": "Researcher"
      },
      {
        "name": "Jingfei Du",
        "type": "Researcher"
      },
      {
        "name": "Mandar Joshi",
        "type": "Researcher"
      },
      {
        "name": "Danqi Chen",
        "type": "Researcher"
      },
      {
        "name": "Omer Levy",
        "type": "Researcher"
      },
      {
        "name": "Veselin Stoyanov",
        "type": "Researcher"
      },
      {
        "name": "Yongliang Wu",
        "type": "Researcher"
      },
      {
        "name": "Shiji Zhou",
        "type": "Researcher"
      },
      {
        "name": "Mingzhuo Yang",
        "type": "Researcher"
      },
      {
        "name": "Lianzhe Wang",
        "type": "Researcher"
      },
      {
        "name": "Heng Chang",
        "type": "Researcher"
      },
      {
        "name": "Wenbo Zhu",
        "type": "Researcher"
      },
      {
        "name": "Xinting Hu",
        "type": "Researcher"
      },
      {
        "name": "Xiao Zhou",
        "type": "Researcher"
      },
      {
        "name": "Xu Yang",
        "type": "Researcher"
      },
      {
        "name": "Xingjun Ma",
        "type": "Researcher"
      },
      {
        "name": "Yifeng Gao",
        "type": "Researcher"
      },
      {
        "name": "Yixu Wang",
        "type": "Researcher"
      },
      {
        "name": "Ruofan Wang",
        "type": "Researcher"
      },
      {
        "name": "Ye Sun",
        "type": "Researcher"
      },
      {
        "name": "Hengyuan Xu",
        "type": "Researcher"
      },
      {
        "name": "Yunhao Chen",
        "type": "Researcher"
      },
      {
        "name": "Yunhan Zhao",
        "type": "Researcher"
      },
      {
        "name": "Hanxun Huang",
        "type": "Researcher"
      },
      {
        "name": "Yige Li",
        "type": "Researcher"
      },
      {
        "name": "Xiang Zheng",
        "type": "Researcher"
      },
      {
        "name": "Yang Bai",
        "type": "Researcher"
      },
      {
        "name": "Henghui Ding",
        "type": "Researcher"
      },
      {
        "name": "Xipeng Qiu",
        "type": "Researcher"
      },
      {
        "name": "Jingfeng Zhang",
        "type": "Researcher"
      },
      {
        "name": "Yiming Li",
        "type": "Researcher"
      },
      {
        "name": "Jun Sun",
        "type": "Researcher"
      },
      {
        "name": "Jindong Gu",
        "type": "Researcher"
      },
      {
        "name": "Baoyuan Wu",
        "type": "Researcher"
      },
      {
        "name": "Siheng Chen",
        "type": "Researcher"
      },
      {
        "name": "Min Gong",
        "type": "Researcher"
      },
      {
        "name": "Tongliang Liu",
        "type": "Researcher"
      },
      {
        "name": "Shirui Pan",
        "type": "Researcher"
      },
      {
        "name": "Yinpeng Dong",
        "type": "Researcher"
      },
      {
        "name": "Ruoxi Jia",
        "type": "Researcher"
      },
      {
        "name": "Shi-jie Ma",
        "type": "Researcher"
      },
      {
        "name": "Neil Gong",
        "type": "Researcher"
      },
      {
        "name": "Chaowei Xiao",
        "type": "Researcher"
      },
      {
        "name": "Sarah Erfani",
        "type": "Researcher"
      },
      {
        "name": "Masashi Sugiyama",
        "type": "Researcher"
      },
      {
        "name": "Dacheng Tao",
        "type": "Researcher"
      },
      {
        "name": "James Bailey",
        "type": "Researcher"
      },
      {
        "name": "Yu-Gang Jiang",
        "type": "Researcher"
      },
      {
        "name": "Junying Wang",
        "type": "Researcher"
      },
      {
        "name": "Farong Wen",
        "type": "Researcher"
      },
      {
        "name": "Yijin Guo",
        "type": "Researcher"
      },
      {
        "name": "Ziheng Jia",
        "type": "Researcher"
      },
      {
        "name": "Jiahao Xiao",
        "type": "Researcher"
      },
      {
        "name": "Ye Shen",
        "type": "Researcher"
      },
      {
        "name": "Yushuo Zheng",
        "type": "Researcher"
      },
      {
        "name": "Xiaorong Zhu",
        "type": "Researcher"
      },
      {
        "name": "Yalun Wu",
        "type": "Researcher"
      },
      {
        "name": "Ziheng Jiao",
        "type": "Researcher"
      },
      {
        "name": "Wei Sun",
        "type": "Researcher"
      },
      {
        "name": "Zijian Chen",
        "type": "Researcher"
      },
      {
        "name": "Kaiwei Zhang",
        "type": "Researcher"
      },
      {
        "name": "Kang Fu",
        "type": "Researcher"
      },
      {
        "name": "Yuqin Cao",
        "type": "Researcher"
      },
      {
        "name": "Ming Hu",
        "type": "Researcher"
      },
      {
        "name": "Yue Zhou",
        "type": "Researcher"
      },
      {
        "name": "Xuemei Zhou",
        "type": "Researcher"
      },
      {
        "name": "Juntai Cao",
        "type": "Researcher"
      },
      {
        "name": "Wei Zhou",
        "type": "Researcher"
      },
      {
        "name": "Jinyu Cao",
        "type": "Researcher"
      },
      {
        "name": "Ronghui Li",
        "type": "Researcher"
      },
      {
        "name": "Donghao Zhou",
        "type": "Researcher"
      },
      {
        "name": "Yuan Tian",
        "type": "Researcher"
      },
      {
        "name": "Xiangyang Zhu",
        "type": "Researcher"
      },
      {
        "name": "Chun-yuan Li",
        "type": "Researcher"
      },
      {
        "name": "Haoning Wu",
        "type": "Researcher"
      },
      {
        "name": "Yu Zhou",
        "type": "Researcher"
      },
      {
        "name": "Hui Liu",
        "type": "Researcher"
      },
      {
        "name": "Lin Zhang",
        "type": "Researcher"
      },
      {
        "name": "Zesheng Wang",
        "type": "Researcher"
      },
      {
        "name": "Huiyu Duan",
        "type": "Researcher"
      },
      {
        "name": "Yingjie Zhou",
        "type": "Researcher"
      },
      {
        "name": "Xiongkuo Min",
        "type": "Researcher"
      },
      {
        "name": "Qi Jia",
        "type": "Researcher"
      },
      {
        "name": "Dongzhan Zhou",
        "type": "Researcher"
      },
      {
        "name": "Wenlong Zhang",
        "type": "Researcher"
      },
      {
        "name": "Jiezhang Cao",
        "type": "Researcher"
      },
      {
        "name": "Guangtao Zhai",
        "type": "Researcher"
      },
      {
        "name": "Yangyang Guo",
        "type": "Researcher"
      },
      {
        "name": "Fangkai Jiao",
        "type": "Researcher"
      },
      {
        "name": "Liqiang Nie",
        "type": "Researcher"
      },
      {
        "name": "Mohan Kankanhalli",
        "type": "Researcher"
      },
      {
        "name": "Zheqi He",
        "type": "Researcher"
      },
      {
        "name": "Xi Yang",
        "type": "Researcher"
      },
      {
        "name": "Jinmiao Zhao",
        "type": "Researcher"
      },
      {
        "name": "Chuang Yu",
        "type": "Researcher"
      },
      {
        "name": "Zelin Shi",
        "type": "Researcher"
      },
      {
        "name": "Yunpeng Liu",
        "type": "Researcher"
      },
      {
        "name": "Yingdi Zhang",
        "type": "Researcher"
      },
      {
        "name": "Waleed Khalid",
        "type": "Researcher"
      },
      {
        "name": "Dmitry Ignatov",
        "type": "Researcher"
      },
      {
        "name": "Radu Timofte",
        "type": "Researcher"
      },
      {
        "name": "Yu Tian",
        "type": "Researcher"
      },
      {
        "name": "Zhongheng Yang",
        "type": "Researcher"
      },
      {
        "name": "Chenshi Liu",
        "type": "Researcher"
      },
      {
        "name": "Yiyun Su",
        "type": "Researcher"
      },
      {
        "name": "Ziwei Hong",
        "type": "Researcher"
      },
      {
        "name": "Zexi Gong",
        "type": "Researcher"
      },
      {
        "name": "Jingyuan Xu",
        "type": "Researcher"
      },
      {
        "name": "Yang Lu",
        "type": "Researcher"
      },
      {
        "name": "Haoyang Zhou",
        "type": "Researcher"
      },
      {
        "name": "Erzhi Wang",
        "type": "Researcher"
      },
      {
        "name": "Gongfa Li",
        "type": "Researcher"
      },
      {
        "name": "Tongjian Yu",
        "type": "Researcher"
      },
      {
        "name": "Jinhui Yi",
        "type": "Researcher"
      },
      {
        "name": "Gina Lopez",
        "type": "Researcher"
      },
      {
        "name": "S. Hadir",
        "type": "Researcher"
      },
      {
        "name": "Jan Weyler",
        "type": "Researcher"
      },
      {
        "name": "Lasse Klingbeil",
        "type": "Researcher"
      },
      {
        "name": "Marion Deichmann",
        "type": "Researcher"
      },
      {
        "name": "Juergen Gall",
        "type": "Researcher"
      },
      {
        "name": "S. J. Seidel",
        "type": "Researcher"
      },
      {
        "name": "Isaac Robinson",
        "type": "Researcher"
      },
      {
        "name": "Peter Robicheaux",
        "type": "Researcher"
      },
      {
        "name": "Matvei Popov",
        "type": "Researcher"
      },
      {
        "name": "Neehar Peri",
        "type": "Researcher"
      },
      {
        "name": "Zhou Wang",
        "type": "Researcher"
      },
      {
        "name": "A. Bovik",
        "type": "Researcher"
      },
      {
        "name": "H. Sheikh",
        "type": "Researcher"
      },
      {
        "name": "Eero P. Simoncelli",
        "type": "Researcher"
      },
      {
        "name": "Fengrui Tian",
        "type": "Researcher"
      },
      {
        "name": "Yulun Wu",
        "type": "Researcher"
      },
      {
        "name": "Yingying Li",
        "type": "Researcher"
      },
      {
        "name": "Shenlong Wang",
        "type": "Researcher"
      },
      {
        "name": "Ning Yu",
        "type": "Researcher"
      },
      {
        "name": "Yaoyao Liu",
        "type": "Researcher"
      },
      {
        "name": "Yuxue Yang",
        "type": "Researcher"
      },
      {
        "name": "Ziqi Shi",
        "type": "Researcher"
      },
      {
        "name": "Junran Peng",
        "type": "Researcher"
      },
      {
        "name": "Feng Wang",
        "type": "Researcher"
      },
      {
        "name": "William Peebles",
        "type": "Researcher"
      },
      {
        "name": "Jianlin Su",
        "type": "Researcher"
      },
      {
        "name": "Yu Lu",
        "type": "Researcher"
      },
      {
        "name": "Shengfeng Pan",
        "type": "Researcher"
      },
      {
        "name": "Ahmed Murtadha",
        "type": "Researcher"
      },
      {
        "name": "Bo Wen",
        "type": "Researcher"
      },
      {
        "name": "Yunfeng Liu",
        "type": "Researcher"
      },
      {
        "name": "Sifan Tu",
        "type": "Researcher"
      },
      {
        "name": "Xin Zhou",
        "type": "Researcher"
      },
      {
        "name": "Dingkang Liang",
        "type": "Researcher"
      },
      {
        "name": "Xingyu Jiang",
        "type": "Researcher"
      },
      {
        "name": "Yumeng Zhang",
        "type": "Researcher"
      },
      {
        "name": "Xiaofan Li",
        "type": "Researcher"
      },
      {
        "name": "Xiang Bai",
        "type": "Researcher"
      },
      {
        "name": "Philip Lenz",
        "type": "Researcher"
      },
      {
        "name": "C. Stiller",
        "type": "Researcher"
      },
      {
        "name": "R. Urtasun",
        "type": "Researcher"
      },
      {
        "name": "S. Umeyama",
        "type": "Researcher"
      },
      {
        "name": "Run Wang",
        "type": "Researcher"
      },
      {
        "name": "Chaoyi Zhou",
        "type": "Researcher"
      },
      {
        "name": "Amir Salarpour",
        "type": "Researcher"
      },
      {
        "name": "Xi Liu",
        "type": "Researcher"
      },
      {
        "name": "Zhi-Qi Cheng",
        "type": "Researcher"
      },
      {
        "name": "Feng Luo",
        "type": "Researcher"
      },
      {
        "name": "Mert D. Pesé",
        "type": "Researcher"
      },
      {
        "name": "Siyu Huang",
        "type": "Researcher"
      },
      {
        "name": "Xingbang Hao",
        "type": "Researcher"
      },
      {
        "name": "Guigang Zhang",
        "type": "Researcher"
      },
      {
        "name": "Shang Ma",
        "type": "Researcher"
      },
      {
        "name": "Nikhil Keetha",
        "type": "Researcher"
      },
      {
        "name": "Norman Müller",
        "type": "Researcher"
      },
      {
        "name": "Johannes Schönberger",
        "type": "Researcher"
      },
      {
        "name": "Lorenzo Porzi",
        "type": "Researcher"
      },
      {
        "name": "Tobias Fischer",
        "type": "Researcher"
      },
      {
        "name": "Arno Knapitsch",
        "type": "Researcher"
      },
      {
        "name": "Duncan Zauss",
        "type": "Researcher"
      },
      {
        "name": "Ethan Weber",
        "type": "Researcher"
      },
      {
        "name": "Nelson Antunes",
        "type": "Researcher"
      },
      {
        "name": "Jonathon Luiten",
        "type": "Researcher"
      },
      {
        "name": "Manuel Lopez-Antequera",
        "type": "Researcher"
      },
      {
        "name": "Samuel Rota Bulò",
        "type": "Researcher"
      },
      {
        "name": "Christian Richardt",
        "type": "Researcher"
      },
      {
        "name": "Sebastian Scherer",
        "type": "Researcher"
      },
      {
        "name": "Peter Kontschieder",
        "type": "Researcher"
      },
      {
        "name": "Team Seedream",
        "type": "Researcher"
      },
      {
        "name": "Yunpeng Chen",
        "type": "Researcher"
      },
      {
        "name": "Lixue Gong",
        "type": "Researcher"
      },
      {
        "name": "Meng Guo",
        "type": "Researcher"
      },
      {
        "name": "Zhiyao Guo",
        "type": "Researcher"
      },
      {
        "name": "Xiaoxia Hou",
        "type": "Researcher"
      },
      {
        "name": "Yixuan Huang",
        "type": "Researcher"
      },
      {
        "name": "Xiaowen Jian",
        "type": "Researcher"
      },
      {
        "name": "Huafeng Kuang",
        "type": "Researcher"
      },
      {
        "name": "Zhichao Lai",
        "type": "Researcher"
      },
      {
        "name": "Fanshi Li",
        "type": "Researcher"
      },
      {
        "name": "Xiaochen Lian",
        "type": "Researcher"
      },
      {
        "name": "Chao Liao",
        "type": "Researcher"
      },
      {
        "name": "Liyang Liu",
        "type": "Researcher"
      },
      {
        "name": "Yanzuo Lu",
        "type": "Researcher"
      },
      {
        "name": "Zhengxiong Luo",
        "type": "Researcher"
      },
      {
        "name": "Tongtong Ou",
        "type": "Researcher"
      },
      {
        "name": "Guang Shi",
        "type": "Researcher"
      },
      {
        "name": "Yichun Shi",
        "type": "Researcher"
      },
      {
        "name": "Shiqi Sun",
        "type": "Researcher"
      },
      {
        "name": "Xun Wang",
        "type": "Researcher"
      },
      {
        "name": "Ye Wang",
        "type": "Researcher"
      },
      {
        "name": "Guofeng Wu",
        "type": "Researcher"
      },
      {
        "name": "Wenxu Wu",
        "type": "Researcher"
      },
      {
        "name": "Yonghui Wu",
        "type": "Researcher"
      },
      {
        "name": "Xin Xia",
        "type": "Researcher"
      },
      {
        "name": "Shuang Xu",
        "type": "Researcher"
      },
      {
        "name": "Xin Yan",
        "type": "Researcher"
      },
      {
        "name": "Zhonghua Zhai",
        "type": "Researcher"
      },
      {
        "name": "Chenlin Zhang",
        "type": "Researcher"
      },
      {
        "name": "Qi Zhang",
        "type": "Researcher"
      },
      {
        "name": "Yuwei Zhang",
        "type": "Researcher"
      },
      {
        "name": "Shijia Zhao",
        "type": "Researcher"
      },
      {
        "name": "Wenliang Zhao",
        "type": "Researcher"
      },
      {
        "name": "Wenjia Zhu",
        "type": "Researcher"
      },
      {
        "name": "Junyan Ye",
        "type": "Researcher"
      },
      {
        "name": "Dongzhi Jiang",
        "type": "Researcher"
      },
      {
        "name": "Zihao Wang",
        "type": "Researcher"
      },
      {
        "name": "Leqi Zhu",
        "type": "Researcher"
      },
      {
        "name": "Zhenghao Hu",
        "type": "Researcher"
      },
      {
        "name": "Zilong Huang",
        "type": "Researcher"
      },
      {
        "name": "Jun He",
        "type": "Researcher"
      },
      {
        "name": "Zhiyuan Yan",
        "type": "Researcher"
      },
      {
        "name": "Jinghua Yu",
        "type": "Researcher"
      },
      {
        "name": "Hongsheng Li",
        "type": "Researcher"
      },
      {
        "name": "Weijia Li",
        "type": "Researcher"
      },
      {
        "name": "Yi Xin",
        "type": "Researcher"
      },
      {
        "name": "Qi Qin",
        "type": "Researcher"
      },
      {
        "name": "Siqi Luo",
        "type": "Researcher"
      },
      {
        "name": "Juncheng Yan",
        "type": "Researcher"
      },
      {
        "name": "Yan Tai",
        "type": "Researcher"
      },
      {
        "name": "Jiayi Lei",
        "type": "Researcher"
      },
      {
        "name": "Yuewen Cao",
        "type": "Researcher"
      },
      {
        "name": "Keqi Wang",
        "type": "Researcher"
      },
      {
        "name": "Qian Yu",
        "type": "Researcher"
      },
      {
        "name": "Dengyang Jiang",
        "type": "Researcher"
      },
      {
        "name": "Yuandong Pu",
        "type": "Researcher"
      },
      {
        "name": "Haoxing Chen",
        "type": "Researcher"
      },
      {
        "name": "Le Zhuo",
        "type": "Researcher"
      },
      {
        "name": "Tianbin Li",
        "type": "Researcher"
      },
      {
        "name": "Jin Ye",
        "type": "Researcher"
      },
      {
        "name": "Bo Zhang",
        "type": "Researcher"
      },
      {
        "name": "Chang Xu",
        "type": "Researcher"
      },
      {
        "name": "Yihao Liu",
        "type": "Researcher"
      },
      {
        "name": "NextStep Team",
        "type": "Researcher"
      },
      {
        "name": "Chunrui Han",
        "type": "Researcher"
      },
      {
        "name": "Guopeng Li",
        "type": "Researcher"
      },
      {
        "name": "Jingwei Wu",
        "type": "Researcher"
      },
      {
        "name": "Quan Sun",
        "type": "Researcher"
      },
      {
        "name": "Yan Cai",
        "type": "Researcher"
      },
      {
        "name": "Yuang Peng",
        "type": "Researcher"
      },
      {
        "name": "Zheng Ge",
        "type": "Researcher"
      },
      {
        "name": "Deyu Zhou",
        "type": "Researcher"
      },
      {
        "name": "Haomiao Tang",
        "type": "Researcher"
      },
      {
        "name": "Hongyu Zhou",
        "type": "Researcher"
      },
      {
        "name": "Kenkun Liu",
        "type": "Researcher"
      },
      {
        "name": "Ailin Huang",
        "type": "Researcher"
      },
      {
        "name": "Changxin Miao",
        "type": "Researcher"
      },
      {
        "name": "Deshan Sun",
        "type": "Researcher"
      },
      {
        "name": "En Yu",
        "type": "Researcher"
      },
      {
        "name": "Fukun Yin",
        "type": "Researcher"
      },
      {
        "name": "Hao Nie",
        "type": "Researcher"
      },
      {
        "name": "Haoran Lv",
        "type": "Researcher"
      },
      {
        "name": "Hanpeng Hu",
        "type": "Researcher"
      },
      {
        "name": "Jia Wang",
        "type": "Researcher"
      },
      {
        "name": "Jian Zhou",
        "type": "Researcher"
      },
      {
        "name": "Jianjian Sun",
        "type": "Researcher"
      },
      {
        "name": "Kaijun Tan",
        "type": "Researcher"
      },
      {
        "name": "Kang An",
        "type": "Researcher"
      },
      {
        "name": "Kangheng Lin",
        "type": "Researcher"
      },
      {
        "name": "Mei Chen",
        "type": "Researcher"
      },
      {
        "name": "Peng Xing",
        "type": "Researcher"
      },
      {
        "name": "Shiyu Liu",
        "type": "Researcher"
      },
      {
        "name": "Shutao Xia",
        "type": "Researcher"
      },
      {
        "name": "Tianhao You",
        "type": "Researcher"
      },
      {
        "name": "Wei Ji",
        "type": "Researcher"
      },
      {
        "name": "Xin Han",
        "type": "Researcher"
      },
      {
        "name": "Xuelin Zhang",
        "type": "Researcher"
      },
      {
        "name": "Yana Wei",
        "type": "Researcher"
      },
      {
        "name": "Yanming Xu",
        "type": "Researcher"
      },
      {
        "name": "Yimin Jiang",
        "type": "Researcher"
      },
      {
        "name": "Yingming Wang",
        "type": "Researcher"
      },
      {
        "name": "Yucheng Han",
        "type": "Researcher"
      },
      {
        "name": "Ziyang Meng",
        "type": "Researcher"
      },
      {
        "name": "Binxing Jiao",
        "type": "Researcher"
      },
      {
        "name": "Daxin Jiang",
        "type": "Researcher"
      },
      {
        "name": "Yibo Zhu",
        "type": "Researcher"
      },
      {
        "name": "Xu Jiang",
        "type": "Researcher"
      },
      {
        "name": "Carroll L. Wainwright",
        "type": "Researcher"
      },
      {
        "name": "Alex Ray",
        "type": "Researcher"
      },
      {
        "name": "Fraser Kelton",
        "type": "Researcher"
      },
      {
        "name": "Luke Miller",
        "type": "Researcher"
      },
      {
        "name": "Paul Christiano",
        "type": "Researcher"
      },
      {
        "name": "Bowen Jin",
        "type": "Researcher"
      },
      {
        "name": "Hansi Zeng",
        "type": "Researcher"
      },
      {
        "name": "Zhenrui Yue",
        "type": "Researcher"
      },
      {
        "name": "Sercan Arik",
        "type": "Researcher"
      },
      {
        "name": "Dong Wang",
        "type": "Researcher"
      },
      {
        "name": "Hamed Zamani",
        "type": "Researcher"
      },
      {
        "name": "Jiawei Han",
        "type": "Researcher"
      },
      {
        "name": "Karan Singhal",
        "type": "Researcher"
      },
      {
        "name": "Tao Tu",
        "type": "Researcher"
      },
      {
        "name": "Juraj Gottweis",
        "type": "Researcher"
      },
      {
        "name": "R. Sayres",
        "type": "Researcher"
      },
      {
        "name": "Ellery Wulczyn",
        "type": "Researcher"
      },
      {
        "name": "Mohamed Amin",
        "type": "Researcher"
      },
      {
        "name": "Kevin Clark",
        "type": "Researcher"
      },
      {
        "name": "Stephen R. Pfohl",
        "type": "Researcher"
      },
      {
        "name": "Heather Cole-Lewis",
        "type": "Researcher"
      },
      {
        "name": "Darlene Neal",
        "type": "Researcher"
      },
      {
        "name": "Q. Rashid",
        "type": "Researcher"
      },
      {
        "name": "Mike Schaekermann",
        "type": "Researcher"
      },
      {
        "name": "Amy Wang",
        "type": "Researcher"
      },
      {
        "name": "Dev Dash",
        "type": "Researcher"
      },
      {
        "name": "Jonathan H. Chen",
        "type": "Researcher"
      },
      {
        "name": "Nigam H. Shah",
        "type": "Researcher"
      },
      {
        "name": "Sami Lachgar",
        "type": "Researcher"
      },
      {
        "name": "P. Mansfield",
        "type": "Researcher"
      },
      {
        "name": "Sushant Prakash",
        "type": "Researcher"
      },
      {
        "name": "Bradley Green",
        "type": "Researcher"
      },
      {
        "name": "Ewa Dominowska",
        "type": "Researcher"
      },
      {
        "name": "Nenad Tomašev",
        "type": "Researcher"
      },
      {
        "name": "Yun Liu",
        "type": "Researcher"
      },
      {
        "name": "Renee Wong",
        "type": "Researcher"
      },
      {
        "name": "Christopher Semturs",
        "type": "Researcher"
      },
      {
        "name": "S. Mahdavi",
        "type": "Researcher"
      },
      {
        "name": "Joelle K. Barral",
        "type": "Researcher"
      },
      {
        "name": "Dale R. Webster",
        "type": "Researcher"
      },
      {
        "name": "G. Corrado",
        "type": "Researcher"
      },
      {
        "name": "Yossi Matias",
        "type": "Researcher"
      },
      {
        "name": "A. Karthikesalingam",
        "type": "Researcher"
      },
      {
        "name": "Vivek Natarajan",
        "type": "Researcher"
      },
      {
        "name": "Tianzhe Chu",
        "type": "Researcher"
      },
      {
        "name": "Yuexiang Zhai",
        "type": "Researcher"
      },
      {
        "name": "Sergey Levine",
        "type": "Researcher"
      },
      {
        "name": "Yi Ma",
        "type": "Researcher"
      },
      {
        "name": "Peijia Lin",
        "type": "Researcher"
      },
      {
        "name": "Pin Chen",
        "type": "Researcher"
      },
      {
        "name": "Rui Jiao",
        "type": "Researcher"
      },
      {
        "name": "Qing Mo",
        "type": "Researcher"
      },
      {
        "name": "Jianhuan Cen",
        "type": "Researcher"
      },
      {
        "name": "Wenbing Huang",
        "type": "Researcher"
      },
      {
        "name": "Dan Huang",
        "type": "Researcher"
      },
      {
        "name": "Yutong Lu",
        "type": "Researcher"
      },
      {
        "name": "Wenqiang Sun",
        "type": "Researcher"
      },
      {
        "name": "Haiyu Zhang",
        "type": "Researcher"
      },
      {
        "name": "Haoyuan Wang",
        "type": "Researcher"
      },
      {
        "name": "Junta Wu",
        "type": "Researcher"
      },
      {
        "name": "Zehan Wang",
        "type": "Researcher"
      },
      {
        "name": "Zhenwei Wang",
        "type": "Researcher"
      },
      {
        "name": "Yunhong Wang",
        "type": "Researcher"
      },
      {
        "name": "Tengfei Wang",
        "type": "Researcher"
      },
      {
        "name": "Chunchao Guo",
        "type": "Researcher"
      },
      {
        "name": "Jianfeng Xiang",
        "type": "Researcher"
      },
      {
        "name": "Xiaoxue Chen",
        "type": "Researcher"
      },
      {
        "name": "Sicheng Xu",
        "type": "Researcher"
      },
      {
        "name": "Ruicheng Wang",
        "type": "Researcher"
      },
      {
        "name": "Zelong Lv",
        "type": "Researcher"
      },
      {
        "name": "Yu Deng",
        "type": "Researcher"
      },
      {
        "name": "Hongyuan Zhu",
        "type": "Researcher"
      },
      {
        "name": "Yue Dong",
        "type": "Researcher"
      },
      {
        "name": "Nicholas Jing Yuan",
        "type": "Researcher"
      },
      {
        "name": "Basile Terver",
        "type": "Researcher"
      },
      {
        "name": "Tsung-Yen Yang",
        "type": "Researcher"
      },
      {
        "name": "Jean Ponce",
        "type": "Researcher"
      },
      {
        "name": "Adrien Bardes",
        "type": "Researcher"
      },
      {
        "name": "Guangyi Zhang",
        "type": "Researcher"
      },
      {
        "name": "Hanlei Li",
        "type": "Researcher"
      },
      {
        "name": "Yunlong Cai",
        "type": "Researcher"
      },
      {
        "name": "Qiyu Hu",
        "type": "Researcher"
      },
      {
        "name": "Guanding Yu",
        "type": "Researcher"
      },
      {
        "name": "Zhijing Qin",
        "type": "Researcher"
      },
      {
        "name": "Wenjun Lin",
        "type": "Researcher"
      },
      {
        "name": "Jensen Zhang",
        "type": "Researcher"
      },
      {
        "name": "Kaitong Cai",
        "type": "Researcher"
      },
      {
        "name": "Qianqian Wang",
        "type": "Researcher"
      },
      {
        "name": "Yifei Zhang",
        "type": "Researcher"
      },
      {
        "name": "Aleksander Holynski",
        "type": "Researcher"
      },
      {
        "name": "Angjoo Kanazawa",
        "type": "Researcher"
      },
      {
        "name": "Xuanchi Ren",
        "type": "Researcher"
      },
      {
        "name": "Tianchang Shen",
        "type": "Researcher"
      },
      {
        "name": "Jiahui Huang",
        "type": "Researcher"
      },
      {
        "name": "Huan Ling",
        "type": "Researcher"
      },
      {
        "name": "Yifan Lu",
        "type": "Researcher"
      },
      {
        "name": "Merlin Nimier-David",
        "type": "Researcher"
      },
      {
        "name": "Thomas Müller",
        "type": "Researcher"
      },
      {
        "name": "Alexander Keller",
        "type": "Researcher"
      },
      {
        "name": "Sanja Fidler",
        "type": "Researcher"
      },
      {
        "name": "Jun Gao",
        "type": "Researcher"
      },
      {
        "name": "Shangzhan Zhang",
        "type": "Researcher"
      },
      {
        "name": "Jianyuan Wang",
        "type": "Researcher"
      },
      {
        "name": "Yinghao Xu",
        "type": "Researcher"
      },
      {
        "name": "Nan Xue",
        "type": "Researcher"
      },
      {
        "name": "Christian Rupprecht",
        "type": "Researcher"
      },
      {
        "name": "Yujun Shen",
        "type": "Researcher"
      },
      {
        "name": "Gordon Wetzstein",
        "type": "Researcher"
      },
      {
        "name": "Luigi Piccinelli",
        "type": "Researcher"
      },
      {
        "name": "Christos Sakaridis",
        "type": "Researcher"
      },
      {
        "name": "Yung-Hsu Yang",
        "type": "Researcher"
      },
      {
        "name": "Mattia Segu",
        "type": "Researcher"
      },
      {
        "name": "Siyuan Li",
        "type": "Researcher"
      },
      {
        "name": "Wim Abbeloos",
        "type": "Researcher"
      },
      {
        "name": "Luc Van Gool",
        "type": "Researcher"
      },
      {
        "name": "Shuo Xing",
        "type": "Researcher"
      },
      {
        "name": "Chengyuan Qian",
        "type": "Researcher"
      },
      {
        "name": "Yuping Wang",
        "type": "Researcher"
      },
      {
        "name": "Hongyuan Hua",
        "type": "Researcher"
      },
      {
        "name": "Kexin Tian",
        "type": "Researcher"
      },
      {
        "name": "Yang Zhou",
        "type": "Researcher"
      },
      {
        "name": "Zhengzhong Tu",
        "type": "Researcher"
      },
      {
        "name": "Transformer",
        "type": "AIModel"
      },
      {
        "name": "WMT 2014 English-to-German translation task",
        "type": "Dataset"
      },
      {
        "name": "WMT 2014 English-to-French translation task",
        "type": "Dataset"
      },
      {
        "name": "English constituency parsing",
        "type": "Dataset"
      },
      {
        "name": "BLEU",
        "type": "Metric"
      },
      {
        "name": "residual learning framework",
        "type": "AIModel"
      },
      {
        "name": "VGG nets",
        "type": "AIModel"
      },
      {
        "name": "ImageNet",
        "type": "Dataset"
      },
      {
        "name": "CIFAR-10",
        "type": "Dataset"
      },
      {
        "name": "COCO object detection dataset",
        "type": "Dataset"
      },
      {
        "name": "error",
        "type": "Metric"
      },
      {
        "name": "relative improvement",
        "type": "Metric"
      },
      {
        "name": "Adam",
        "type": "AIModel"
      },
      {
        "name": "AdaMax",
        "type": "AIModel"
      },
      {
        "name": "Dropout",
        "type": "AIModel"
      },
      {
        "name": "Inception Architecture",
        "type": "AIModel"
      },
      {
        "name": "ILSVRC 2012 classification challenge validation set",
        "type": "Dataset"
      },
      {
        "name": "top-1 error",
        "type": "Metric"
      },
      {
        "name": "top-5 error",
        "type": "Metric"
      },
      {
        "name": "InstaDrive",
        "type": "AIModel"
      },
      {
        "name": "Instance Flow Guider",
        "type": "AIModel"
      },
      {
        "name": "Spatial Geometric Aligner",
        "type": "AIModel"
      },
      {
        "name": "nuScenes",
        "type": "Dataset"
      },
      {
        "name": "CARLA",
        "type": "Dataset"
      },
      {
        "name": "video generation quality",
        "type": "Metric"
      },
      {
        "name": "safety evaluation",
        "type": "Metric"
      },
      {
        "name": "CNC-VLM",
        "type": "AIModel"
      },
      {
        "name": "CNC fault detection dataset",
        "type": "Dataset"
      },
      {
        "name": "accuracy",
        "type": "Metric"
      },
      {
        "name": "F1-score",
        "type": "Metric"
      },
      {
        "name": "mamba segmentation",
        "type": "AIModel"
      },
      {
        "name": "four-point laser metric calibration",
        "type": "Metric"
      },
      {
        "name": "DiffusionEngine",
        "type": "AIModel"
      },
      {
        "name": "deep convolutional neural network",
        "type": "AIModel"
      },
      {
        "name": "ConvNet models",
        "type": "AIModel"
      },
      {
        "name": "ImageNet Challenge 2014",
        "type": "Dataset"
      },
      {
        "name": "other datasets",
        "type": "Dataset"
      },
      {
        "name": "Region Proposal Network (RPN)",
        "type": "AIModel"
      },
      {
        "name": "Fast R-CNN",
        "type": "AIModel"
      },
      {
        "name": "SPPnet",
        "type": "AIModel"
      },
      {
        "name": "VGG-16",
        "type": "AIModel"
      },
      {
        "name": "PASCAL VOC 2007",
        "type": "Dataset"
      },
      {
        "name": "PASCAL VOC 2012",
        "type": "Dataset"
      },
      {
        "name": "MS COCO",
        "type": "Dataset"
      },
      {
        "name": "ILSVRC",
        "type": "Dataset"
      },
      {
        "name": "COCO 2015",
        "type": "Dataset"
      },
      {
        "name": "frame rate",
        "type": "Metric"
      },
      {
        "name": "object detection accuracy",
        "type": "Metric"
      },
      {
        "name": "Federated Learning Optimal Transport (FLOT)",
        "type": "AIModel"
      },
      {
        "name": "GTSRB",
        "type": "Dataset"
      },
      {
        "name": "KBTS",
        "type": "Dataset"
      },
      {
        "name": "CIFAR10",
        "type": "Dataset"
      },
      {
        "name": "EMNIST",
        "type": "Dataset"
      },
      {
        "name": "scalability",
        "type": "Metric"
      },
      {
        "name": "WarmGait",
        "type": "AIModel"
      },
      {
        "name": "Taylor Finite Difference (TFD)",
        "type": "AIModel"
      },
      {
        "name": "thermal array sensors",
        "type": "Dataset"
      },
      {
        "name": "average recognition accuracy",
        "type": "Metric"
      },
      {
        "name": "NPSSL",
        "type": "AIModel"
      },
      {
        "name": "Duke dataset",
        "type": "Dataset"
      },
      {
        "name": "Unsupervised Domain Adaptation",
        "type": "AIModel"
      },
      {
        "name": "Noise Perception Self-Paced Learning",
        "type": "AIModel"
      },
      {
        "name": "stochastic variational inference and learning algorithm",
        "type": "AIModel"
      },
      {
        "name": "reparameterization of the variational lower bound",
        "type": "AIModel"
      },
      {
        "name": "approximate inference model",
        "type": "AIModel"
      },
      {
        "name": "i.i.d. datasets",
        "type": "Dataset"
      },
      {
        "name": "variational lower bound",
        "type": "Metric"
      },
      {
        "name": "AdaGrad",
        "type": "AIModel"
      },
      {
        "name": "Online Learning",
        "type": "Dataset"
      },
      {
        "name": "Stochastic Optimization",
        "type": "Dataset"
      },
      {
        "name": "Convergence Rate",
        "type": "Metric"
      },
      {
        "name": "Recurrent neural networks (RNNs)",
        "type": "AIModel"
      },
      {
        "name": "Connectionist Temporal Classification",
        "type": "AIModel"
      },
      {
        "name": "Long Short-term Memory RNN",
        "type": "AIModel"
      },
      {
        "name": "deep recurrent neural networks",
        "type": "AIModel"
      },
      {
        "name": "deep Long Short-term Memory RNNs",
        "type": "AIModel"
      },
      {
        "name": "deep feedforward networks",
        "type": "AIModel"
      },
      {
        "name": "TIMIT phoneme recognition benchmark",
        "type": "Dataset"
      },
      {
        "name": "test set error of 17.7%",
        "type": "Metric"
      },
      {
        "name": "PBD",
        "type": "AIModel"
      },
      {
        "name": "GAN",
        "type": "AIModel"
      },
      {
        "name": "seven benchmarks",
        "type": "Dataset"
      },
      {
        "name": "reconstruction loss",
        "type": "Metric"
      },
      {
        "name": "AdamW",
        "type": "AIModel"
      },
      {
        "name": "face mask detection model",
        "type": "AIModel"
      },
      {
        "name": "Engram",
        "type": "AIModel"
      },
      {
        "name": "Mixture-of-Experts (MoE)",
        "type": "AIModel"
      },
      {
        "name": "MMLU",
        "type": "Dataset"
      },
      {
        "name": "CMMLU",
        "type": "Dataset"
      },
      {
        "name": "BBH",
        "type": "Dataset"
      },
      {
        "name": "ARC-Challenge",
        "type": "Dataset"
      },
      {
        "name": "HumanEval",
        "type": "Dataset"
      },
      {
        "name": "MATH",
        "type": "Dataset"
      },
      {
        "name": "Multi-Query NIAH",
        "type": "Dataset"
      },
      {
        "name": "Neural Hierarchical Interpolation for Time Series Forecasting (N-HiTS)",
        "type": "AIModel"
      },
      {
        "name": "Network-Based Expansion Analysis for Interpretable Time Series Forecasting (N-BEATS)",
        "type": "AIModel"
      },
      {
        "name": "long short-term memory (LSTM)",
        "type": "AIModel"
      },
      {
        "name": "two headwater streams in Georgia and North Carolina, USA",
        "type": "Dataset"
      },
      {
        "name": "Multi-Quantile Loss",
        "type": "Metric"
      },
      {
        "name": "95th percentile prediction uncertainty (95 PPU)",
        "type": "Metric"
      },
      {
        "name": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model",
        "type": "AIPaper"
      },
      {
        "name": "NASA-IBM geospatial foundation model",
        "type": "AIModel"
      },
      {
        "name": "harmonized Landsat and Sentinel-2 data",
        "type": "Dataset"
      },
      {
        "name": "Inception",
        "type": "AIModel"
      },
      {
        "name": "GoogLeNet",
        "type": "AIModel"
      },
      {
        "name": "ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014)",
        "type": "Dataset"
      },
      {
        "name": "Batch Normalization",
        "type": "AIModel"
      },
      {
        "name": "top-5 validation error",
        "type": "Metric"
      },
      {
        "name": "test error",
        "type": "Metric"
      },
      {
        "name": "Diffusion Transformers (DiT)",
        "type": "AIModel"
      },
      {
        "name": "Representation Autoencoders (RAEs)",
        "type": "AIModel"
      },
      {
        "name": "VAE",
        "type": "AIModel"
      },
      {
        "name": "DINO",
        "type": "AIModel"
      },
      {
        "name": "SigLIP",
        "type": "AIModel"
      },
      {
        "name": "MAE",
        "type": "AIModel"
      },
      {
        "name": "DDT head",
        "type": "AIModel"
      },
      {
        "name": "FID",
        "type": "Metric"
      },
      {
        "name": "C2S-Scale",
        "type": "AIModel"
      },
      {
        "name": "Cell2Sentence (C2S) framework",
        "type": "AIModel"
      },
      {
        "name": "Large Language Models (LLMs)",
        "type": "AIModel"
      },
      {
        "name": "single-cell foundation models (scFMs)",
        "type": "AIModel"
      },
      {
        "name": "corpus comprising over one billion tokens of transcriptomic data, biological text, and metadata",
        "type": "Dataset"
      },
      {
        "name": "human cell models",
        "type": "Dataset"
      },
      {
        "name": "predictive and generative capabilities",
        "type": "Metric"
      },
      {
        "name": "performance in perturbation response prediction, natural language interpretation, and complex biological reasoning",
        "type": "Metric"
      },
      {
        "name": "DI",
        "type": "AIModel"
      },
      {
        "name": "DiffPure",
        "type": "AIModel"
      },
      {
        "name": "Google Cloud Vision",
        "type": "AIModel"
      },
      {
        "name": "Lp constraint",
        "type": "Metric"
      },
      {
        "name": "imperceptibility metrics",
        "type": "Metric"
      },
      {
        "name": "finer-grained measures",
        "type": "Metric"
      },
      {
        "name": "user study",
        "type": "Metric"
      },
      {
        "name": "HybridVisionNet",
        "type": "AIModel"
      },
      {
        "name": "OGNet",
        "type": "AIModel"
      },
      {
        "name": "YOLO-OG",
        "type": "AIModel"
      },
      {
        "name": "Dish-10",
        "type": "Dataset"
      },
      {
        "name": "Dish-20",
        "type": "Dataset"
      },
      {
        "name": "mean Average Precision (mAP)",
        "type": "Metric"
      },
      {
        "name": "Unified Text-to-Text Transformer",
        "type": "AIModel"
      },
      {
        "name": "Colossal Clean Crawled Corpus",
        "type": "Dataset"
      },
      {
        "name": "dozens of language understanding tasks",
        "type": "Dataset"
      },
      {
        "name": "summarization",
        "type": "Metric"
      },
      {
        "name": "question answering",
        "type": "Metric"
      },
      {
        "name": "text classification",
        "type": "Metric"
      },
      {
        "name": "GANs",
        "type": "AIModel"
      },
      {
        "name": "Two Time-Scale Update Rule",
        "type": "AIModel"
      },
      {
        "name": "KITTI",
        "type": "Dataset"
      },
      {
        "name": "novel 3D detection and tracking metrics",
        "type": "Metric"
      },
      {
        "name": "lidar based detection and tracking",
        "type": "AIModel"
      },
      {
        "name": "image based detection and tracking",
        "type": "AIModel"
      },
      {
        "name": "classic modular pipeline",
        "type": "AIModel"
      },
      {
        "name": "end-to-end model trained via imitation learning",
        "type": "AIModel"
      },
      {
        "name": "end-to-end model trained via reinforcement learning",
        "type": "AIModel"
      },
      {
        "name": "controlled scenarios of increasing difficulty",
        "type": "Dataset"
      },
      {
        "name": "metrics provided by CARLA",
        "type": "Metric"
      },
      {
        "name": "Sora",
        "type": "AIModel"
      },
      {
        "name": "MNIST",
        "type": "Dataset"
      },
      {
        "name": "text-to-video generation",
        "type": "AIModel"
      },
      {
        "name": "world modeling",
        "type": "Metric"
      },
      {
        "name": "OmniNWM",
        "type": "AIModel"
      },
      {
        "name": "existing models",
        "type": "AIModel"
      },
      {
        "name": "video generation",
        "type": "Metric"
      },
      {
        "name": "control accuracy",
        "type": "Metric"
      },
      {
        "name": "long-horizon stability",
        "type": "Metric"
      },
      {
        "name": "ConsisDrive",
        "type": "AIModel"
      },
      {
        "name": "Instance-Masked Attention",
        "type": "AIModel"
      },
      {
        "name": "Instance-Masked Loss",
        "type": "AIModel"
      },
      {
        "name": "UniDriveDreamer",
        "type": "AIModel"
      },
      {
        "name": "LiDAR-specific variational autoencoder (VAE)",
        "type": "AIModel"
      },
      {
        "name": "video VAE",
        "type": "AIModel"
      },
      {
        "name": "Unified Latent Anchoring (ULA)",
        "type": "AIModel"
      },
      {
        "name": "diffusion transformer",
        "type": "AIModel"
      },
      {
        "name": "multi-camera video",
        "type": "Dataset"
      },
      {
        "name": "LiDAR sequence",
        "type": "Dataset"
      },
      {
        "name": "LiDAR generation",
        "type": "Metric"
      },
      {
        "name": "MAD-LTX",
        "type": "AIModel"
      },
      {
        "name": "SVD",
        "type": "AIModel"
      },
      {
        "name": "LTX",
        "type": "AIModel"
      },
      {
        "name": "autonomous driving",
        "type": "Dataset"
      },
      {
        "name": "driving domains",
        "type": "Dataset"
      },
      {
        "name": "structured motion",
        "type": "Metric"
      },
      {
        "name": "physically consistent interactions",
        "type": "Metric"
      },
      {
        "name": "photorealistic, temporally coherent videos",
        "type": "Metric"
      },
      {
        "name": "text, ego, and object controls",
        "type": "Metric"
      },
      {
        "name": "REPA",
        "type": "AIModel"
      },
      {
        "name": "iREPA",
        "type": "AIModel"
      },
      {
        "name": "REPA-E",
        "type": "AIModel"
      },
      {
        "name": "Meanflow",
        "type": "AIModel"
      },
      {
        "name": "JiT",
        "type": "AIModel"
      },
      {
        "name": "ImageNet-1K",
        "type": "Dataset"
      },
      {
        "name": "ImageNet-1K accuracy",
        "type": "Metric"
      },
      {
        "name": "SCB-DETR",
        "type": "AIModel"
      },
      {
        "name": "baseline model",
        "type": "AIModel"
      },
      {
        "name": "SCBehavior",
        "type": "Dataset"
      },
      {
        "name": "AP50",
        "type": "Metric"
      },
      {
        "name": "ultrasound-cardiac-feature-net (UCF-Net)",
        "type": "AIModel"
      },
      {
        "name": "filtered integral quasi-super-twisting algorithm (FIQSTA)",
        "type": "AIModel"
      },
      {
        "name": "proportional (P) controller",
        "type": "AIModel"
      },
      {
        "name": "sliding mode controller",
        "type": "AIModel"
      },
      {
        "name": "super-twisting algorithm (STA)",
        "type": "AIModel"
      },
      {
        "name": "integral quasi-STA",
        "type": "AIModel"
      },
      {
        "name": "cardiac phantom",
        "type": "Dataset"
      },
      {
        "name": "parasternal short axis",
        "type": "Dataset"
      },
      {
        "name": "parasternal long axis",
        "type": "Dataset"
      },
      {
        "name": "subcostal",
        "type": "Dataset"
      },
      {
        "name": "apical four chambers views",
        "type": "Dataset"
      },
      {
        "name": "trajectory passing through the main views",
        "type": "Dataset"
      },
      {
        "name": "BioTune",
        "type": "AIModel"
      },
      {
        "name": "AutoRGN",
        "type": "AIModel"
      },
      {
        "name": "LoRA",
        "type": "AIModel"
      },
      {
        "name": "nine image classification datasets",
        "type": "Dataset"
      },
      {
        "name": "medical imaging",
        "type": "Dataset"
      },
      {
        "name": "efficiency",
        "type": "Metric"
      },
      {
        "name": "VGG-16 net",
        "type": "AIModel"
      },
      {
        "name": "NUS dataset",
        "type": "Dataset"
      },
      {
        "name": "PASCAL",
        "type": "Dataset"
      },
      {
        "name": "SUN",
        "type": "Dataset"
      },
      {
        "name": "Deformable Parts Model",
        "type": "AIModel"
      },
      {
        "name": "bounding box detection",
        "type": "Metric"
      },
      {
        "name": "segmentation detection",
        "type": "Metric"
      },
      {
        "name": "FANet",
        "type": "AIModel"
      },
      {
        "name": "Multi-Scale Frequency Feature Enhancement Module (MSFFEM)",
        "type": "AIModel"
      },
      {
        "name": "Channel Attention-based RoI Enhancement Module (CAREM)",
        "type": "AIModel"
      },
      {
        "name": "AI-TOD",
        "type": "Dataset"
      },
      {
        "name": "VisDrone2019",
        "type": "Dataset"
      },
      {
        "name": "DOTA-v1.5",
        "type": "Dataset"
      },
      {
        "name": "detection performance",
        "type": "Metric"
      },
      {
        "name": "probabilistic models",
        "type": "AIModel"
      },
      {
        "name": "auto-encoders",
        "type": "AIModel"
      },
      {
        "name": "manifold learning",
        "type": "AIModel"
      },
      {
        "name": "deep networks",
        "type": "AIModel"
      },
      {
        "name": "Stacked Denoising Autoencoders",
        "type": "AIModel"
      },
      {
        "name": "SDXL-Lightning",
        "type": "AIModel"
      },
      {
        "name": "SDXL",
        "type": "AIModel"
      },
      {
        "name": "UNet",
        "type": "AIModel"
      },
      {
        "name": "EchoMimic",
        "type": "AIModel"
      },
      {
        "name": "various public datasets",
        "type": "Dataset"
      },
      {
        "name": "our collected dataset",
        "type": "Dataset"
      },
      {
        "name": "quantitative evaluations",
        "type": "Metric"
      },
      {
        "name": "qualitative evaluations",
        "type": "Metric"
      },
      {
        "name": "GenAD",
        "type": "AIModel"
      },
      {
        "name": "state-of-the-art performance",
        "type": "Metric"
      },
      {
        "name": "Ovis",
        "type": "AIModel"
      },
      {
        "name": "Multimodal Large Language Models (MLLMs)",
        "type": "AIModel"
      },
      {
        "name": "LLM",
        "type": "AIModel"
      },
      {
        "name": "vision transformer",
        "type": "AIModel"
      },
      {
        "name": "MLP",
        "type": "AIModel"
      },
      {
        "name": "Qwen-VL-Plus",
        "type": "AIModel"
      },
      {
        "name": "various multimodal benchmarks",
        "type": "Dataset"
      },
      {
        "name": "empirical evaluations",
        "type": "Metric"
      },
      {
        "name": "VideoReward",
        "type": "AIModel"
      },
      {
        "name": "Flow-DPO",
        "type": "AIModel"
      },
      {
        "name": "Flow-RWR",
        "type": "AIModel"
      },
      {
        "name": "Flow-NRG",
        "type": "AIModel"
      },
      {
        "name": "large-scale human preference dataset",
        "type": "Dataset"
      },
      {
        "name": "supervised fine-tuning methods",
        "type": "AIModel"
      },
      {
        "name": "test set error",
        "type": "Metric"
      },
      {
        "name": "Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks",
        "type": "AIPaper"
      },
      {
        "name": "Recurrent Neural Networks",
        "type": "AIModel"
      },
      {
        "name": "bidirectional LSTM",
        "type": "AIModel"
      },
      {
        "name": "other neural network architectures",
        "type": "AIModel"
      },
      {
        "name": "machine learning",
        "type": "AIModel"
      },
      {
        "name": "artificial synapses",
        "type": "AIModel"
      },
      {
        "name": "flexible sensors",
        "type": "Dataset"
      },
      {
        "name": "human activities",
        "type": "Dataset"
      },
      {
        "name": "artificial sensory organs",
        "type": "Dataset"
      },
      {
        "name": "soft robotics",
        "type": "Dataset"
      },
      {
        "name": "data analysis",
        "type": "Metric"
      },
      {
        "name": "intelligent decision-making",
        "type": "Metric"
      },
      {
        "name": "speech-to-text BCI",
        "type": "AIModel"
      },
      {
        "name": "50-word vocabulary",
        "type": "Dataset"
      },
      {
        "name": "125,000-word vocabulary",
        "type": "Dataset"
      },
      {
        "name": "word error rate",
        "type": "Metric"
      },
      {
        "name": "words per minute",
        "type": "Metric"
      },
      {
        "name": "deep-learning methods",
        "type": "AIModel"
      },
      {
        "name": "backpropagation algorithm",
        "type": "AIModel"
      },
      {
        "name": "continual backpropagation algorithm",
        "type": "AIModel"
      },
      {
        "name": "plasticity",
        "type": "Metric"
      },
      {
        "name": "keyword-spotting network",
        "type": "AIModel"
      },
      {
        "name": "MLPerf recurrent neural-network transducer (RNNT)",
        "type": "AIModel"
      },
      {
        "name": "speech-recognition tasks",
        "type": "Dataset"
      },
      {
        "name": "energy efficiency",
        "type": "Metric"
      },
      {
        "name": "TOPS/W chip-sustained performance",
        "type": "Metric"
      },
      {
        "name": "LiteToken",
        "type": "AIModel"
      },
      {
        "name": "BPE tokenizers",
        "type": "AIModel"
      },
      {
        "name": "commonly used tokenizers",
        "type": "Dataset"
      },
      {
        "name": "token fragmentation",
        "type": "Metric"
      },
      {
        "name": "parameters",
        "type": "Metric"
      },
      {
        "name": "robustness to noisy or misspelled inputs",
        "type": "Metric"
      },
      {
        "name": "overall performance",
        "type": "Metric"
      },
      {
        "name": "MeKi",
        "type": "AIModel"
      },
      {
        "name": "dense LLM baselines",
        "type": "AIModel"
      },
      {
        "name": "edge devices",
        "type": "Dataset"
      },
      {
        "name": "inference speed",
        "type": "Metric"
      },
      {
        "name": "Gengram",
        "type": "AIModel"
      },
      {
        "name": "genomic foundation models (GFMs)",
        "type": "AIModel"
      },
      {
        "name": "functional genomics tasks",
        "type": "Dataset"
      },
      {
        "name": "L$^3$",
        "type": "AIModel"
      },
      {
        "name": "dense models",
        "type": "AIModel"
      },
      {
        "name": "iso-sparse MoEs",
        "type": "AIModel"
      },
      {
        "name": "language modeling",
        "type": "Dataset"
      },
      {
        "name": "downstream tasks",
        "type": "Dataset"
      },
      {
        "name": "speed",
        "type": "Metric"
      },
      {
        "name": "quality",
        "type": "Metric"
      },
      {
        "name": "LongCat-Flash-Lite",
        "type": "AIModel"
      },
      {
        "name": "agentic and coding domains",
        "type": "Dataset"
      },
      {
        "name": "Convolutional Neural Network (CNN)",
        "type": "AIModel"
      },
      {
        "name": "Lasso",
        "type": "AIModel"
      },
      {
        "name": "LifeCLEF plant identification challenge",
        "type": "Dataset"
      },
      {
        "name": "FedMicro-IDA",
        "type": "AIModel"
      },
      {
        "name": "MaleVis",
        "type": "Dataset"
      },
      {
        "name": "detection and classification performance",
        "type": "Metric"
      },
      {
        "name": "external attention-based transformers",
        "type": "AIModel"
      },
      {
        "name": "large language models",
        "type": "AIModel"
      },
      {
        "name": "large vision models",
        "type": "AIModel"
      },
      {
        "name": "multimodal large language models",
        "type": "AIModel"
      },
      {
        "name": "tailored models for agricultural question-answering",
        "type": "AIModel"
      },
      {
        "name": "robotic automation",
        "type": "AIModel"
      },
      {
        "name": "advanced image analysis from remote sensing and spectral data",
        "type": "AIModel"
      },
      {
        "name": "traditional models",
        "type": "AIModel"
      },
      {
        "name": "Web of Science",
        "type": "Dataset"
      },
      {
        "name": "arXiv",
        "type": "Dataset"
      },
      {
        "name": "bibliometric analysis",
        "type": "Metric"
      },
      {
        "name": "linear-nonlinear (LN) models",
        "type": "AIModel"
      },
      {
        "name": "convolutional neural networks (CNNs)",
        "type": "AIModel"
      },
      {
        "name": "marmoset and salamander retinas datasets",
        "type": "Dataset"
      },
      {
        "name": "predictive performance",
        "type": "Metric"
      },
      {
        "name": "cross-stimulus generalization",
        "type": "Metric"
      },
      {
        "name": "SIFT",
        "type": "AIModel"
      },
      {
        "name": "Image matching dataset",
        "type": "Dataset"
      },
      {
        "name": "Accuracy",
        "type": "Metric"
      },
      {
        "name": "LLaVA-OneVision-1.5",
        "type": "AIPaper"
      },
      {
        "name": "LLaVA-OneVision-1.5-8B",
        "type": "AIModel"
      },
      {
        "name": "LLaVA-OneVision-1.5-4B",
        "type": "AIModel"
      },
      {
        "name": "Qwen2.5-VL-7B",
        "type": "AIModel"
      },
      {
        "name": "Qwen2.5-VL-3B",
        "type": "AIModel"
      },
      {
        "name": "LLaVA-OneVision-1.5-Mid-Traning",
        "type": "Dataset"
      },
      {
        "name": "LLaVA-OneVision-1.5-Instruct",
        "type": "Dataset"
      },
      {
        "name": "27 benchmarks",
        "type": "Metric"
      },
      {
        "name": "Vision-Language-Action (VLA) models",
        "type": "AIModel"
      },
      {
        "name": "large language models (LLMs)",
        "type": "AIModel"
      },
      {
        "name": "vision-language models (VLMs)",
        "type": "AIModel"
      },
      {
        "name": "publicly available datasets",
        "type": "Dataset"
      },
      {
        "name": "evaluation benchmarks",
        "type": "Metric"
      },
      {
        "name": "teacher model",
        "type": "AIModel"
      },
      {
        "name": "pretrained state-of-the-art vision foundation models",
        "type": "AIModel"
      },
      {
        "name": "human-aligned models",
        "type": "AIModel"
      },
      {
        "name": "dataset of human judgements spanning multiple levels of semantic abstractions",
        "type": "Dataset"
      },
      {
        "name": "Vision Transformer (ViT)",
        "type": "AIModel"
      },
      {
        "name": "CIFAR-100",
        "type": "Dataset"
      },
      {
        "name": "VTAB",
        "type": "Dataset"
      },
      {
        "name": "state-of-the-art convolutional networks",
        "type": "AIModel"
      },
      {
        "name": "CLIP",
        "type": "AIModel"
      },
      {
        "name": "ResNet-50",
        "type": "AIModel"
      },
      {
        "name": "400 million (image, text) pairs",
        "type": "Dataset"
      },
      {
        "name": "over 30 different existing computer vision datasets",
        "type": "Dataset"
      },
      {
        "name": "Generative Adversarial Network (GAN)",
        "type": "AIModel"
      },
      {
        "name": "MeanFlow (MF)",
        "type": "AIModel"
      },
      {
        "name": "improved MeanFlow (iMF)",
        "type": "AIModel"
      },
      {
        "name": "ImageNet 256×256",
        "type": "Dataset"
      },
      {
        "name": "pixel-space diffusion and consistency models",
        "type": "AIModel"
      },
      {
        "name": "ImageNet-256",
        "type": "Dataset"
      },
      {
        "name": "ImageNet-512",
        "type": "Dataset"
      },
      {
        "name": "DiT",
        "type": "AIModel"
      },
      {
        "name": "SVG-T2I",
        "type": "AIModel"
      },
      {
        "name": "SVG (Self-supervised representations for Visual Generation)",
        "type": "AIModel"
      },
      {
        "name": "GenEval",
        "type": "Dataset"
      },
      {
        "name": "DPG-Bench",
        "type": "Dataset"
      },
      {
        "name": "0.75",
        "type": "Metric"
      },
      {
        "name": "85.78",
        "type": "Metric"
      },
      {
        "name": "PixelDiT",
        "type": "AIModel"
      },
      {
        "name": "Diffusion Transformers (DiTs)",
        "type": "AIModel"
      },
      {
        "name": "ImageNet 256x256",
        "type": "Dataset"
      },
      {
        "name": "DPG-bench",
        "type": "Dataset"
      },
      {
        "name": "pixel generative models",
        "type": "AIModel"
      },
      {
        "name": "latent diffusion models",
        "type": "AIModel"
      },
      {
        "name": "TUNA",
        "type": "AIModel"
      },
      {
        "name": "VAE encoder",
        "type": "AIModel"
      },
      {
        "name": "representation encoder",
        "type": "AIModel"
      },
      {
        "name": "multimodal understanding and generation benchmarks",
        "type": "Dataset"
      },
      {
        "name": "performance",
        "type": "Metric"
      },
      {
        "name": "BERT",
        "type": "AIModel"
      },
      {
        "name": "GLUE",
        "type": "Dataset"
      },
      {
        "name": "MultiNLI",
        "type": "Dataset"
      },
      {
        "name": "SQuAD v1.1",
        "type": "Dataset"
      },
      {
        "name": "SQuAD v2.0",
        "type": "Dataset"
      },
      {
        "name": "GLUE score",
        "type": "Metric"
      },
      {
        "name": "MultiNLI accuracy",
        "type": "Metric"
      },
      {
        "name": "SQuAD v1.1 question answering Test F1",
        "type": "Metric"
      },
      {
        "name": "SQuAD v2.0 Test F1",
        "type": "Metric"
      },
      {
        "name": "Diffusion language models (DLMs)",
        "type": "AIModel"
      },
      {
        "name": "autoregressive (AR) models",
        "type": "AIModel"
      },
      {
        "name": "AR coder",
        "type": "AIModel"
      },
      {
        "name": "1.7B DLM",
        "type": "AIModel"
      },
      {
        "name": "1B-parameter DLM",
        "type": "AIModel"
      },
      {
        "name": "10B unique Python tokens",
        "type": "Dataset"
      },
      {
        "name": "HellaSwag",
        "type": "Dataset"
      },
      {
        "name": "validation cross-entropy",
        "type": "Metric"
      },
      {
        "name": "DreamOn",
        "type": "AIModel"
      },
      {
        "name": "Diffusion Language Models (DLMs)",
        "type": "AIModel"
      },
      {
        "name": "Dream-Coder-7B",
        "type": "AIModel"
      },
      {
        "name": "DiffuCoder-7B",
        "type": "AIModel"
      },
      {
        "name": "HumanEval-Infilling",
        "type": "Dataset"
      },
      {
        "name": "SantaCoder-FIM",
        "type": "Dataset"
      },
      {
        "name": "FLEX",
        "type": "AIModel"
      },
      {
        "name": "AIME25",
        "type": "Dataset"
      },
      {
        "name": "USPTO50k",
        "type": "Dataset"
      },
      {
        "name": "ProteinGym",
        "type": "Dataset"
      },
      {
        "name": "mathematical reasoning",
        "type": "Metric"
      },
      {
        "name": "chemical retrosynthesis",
        "type": "Metric"
      },
      {
        "name": "protein fitness prediction",
        "type": "Metric"
      },
      {
        "name": "Agent-R1",
        "type": "AIModel"
      },
      {
        "name": "Multihop QA benchmark tasks",
        "type": "Dataset"
      },
      {
        "name": "KITTI dataset",
        "type": "Dataset"
      },
      {
        "name": "video generation foundation models",
        "type": "AIModel"
      },
      {
        "name": "video diffusion models",
        "type": "AIModel"
      },
      {
        "name": "open-source video generation models",
        "type": "AIModel"
      },
      {
        "name": "pretrained video generation models",
        "type": "AIModel"
      },
      {
        "name": "single-condition generation",
        "type": "AIModel"
      },
      {
        "name": "multi-condition generation",
        "type": "AIModel"
      },
      {
        "name": "universal controllable generation",
        "type": "AIModel"
      },
      {
        "name": "Awesome-Controllable-Video-Generation",
        "type": "Dataset"
      },
      {
        "name": "Autonomous Driving Systems (ADS)",
        "type": "AIModel"
      },
      {
        "name": "multi-label classification method",
        "type": "AIModel"
      },
      {
        "name": "Alpamayo-R1 (AR1)",
        "type": "AIModel"
      },
      {
        "name": "Chain of Causation (CoC) dataset",
        "type": "Dataset"
      },
      {
        "name": "Cosmos-Reason",
        "type": "AIModel"
      },
      {
        "name": "trajectory-only baseline",
        "type": "AIModel"
      },
      {
        "name": "planning accuracy",
        "type": "Metric"
      },
      {
        "name": "close encounter rate",
        "type": "Metric"
      },
      {
        "name": "reasoning quality",
        "type": "Metric"
      },
      {
        "name": "reasoning-action consistency",
        "type": "Metric"
      },
      {
        "name": "Ego3D-VLM",
        "type": "AIModel"
      },
      {
        "name": "GPT-4o",
        "type": "AIModel"
      },
      {
        "name": "Gemini1.5-Pro",
        "type": "AIModel"
      },
      {
        "name": "InternVL3",
        "type": "AIModel"
      },
      {
        "name": "Qwen2.5-VL",
        "type": "AIModel"
      },
      {
        "name": "Ego3D-Bench",
        "type": "Dataset"
      },
      {
        "name": "multi-choice QA",
        "type": "Metric"
      },
      {
        "name": "absolute distance estimation",
        "type": "Metric"
      },
      {
        "name": "Rex-Omni",
        "type": "AIModel"
      },
      {
        "name": "YOLO",
        "type": "AIModel"
      },
      {
        "name": "DETR",
        "type": "AIModel"
      },
      {
        "name": "Grounding DINO",
        "type": "AIModel"
      },
      {
        "name": "COCO",
        "type": "Dataset"
      },
      {
        "name": "LVIS",
        "type": "Dataset"
      },
      {
        "name": "Deep Q-Network (DQN)",
        "type": "AIModel"
      },
      {
        "name": "Atari 2600 games",
        "type": "Dataset"
      },
      {
        "name": "human-level performance",
        "type": "Metric"
      },
      {
        "name": "asynchronous gradient descent",
        "type": "AIModel"
      },
      {
        "name": "asynchronous actor-critic",
        "type": "AIModel"
      },
      {
        "name": "Atari domain",
        "type": "Dataset"
      },
      {
        "name": "continuous motor control problems",
        "type": "Dataset"
      },
      {
        "name": "random 3D mazes",
        "type": "Dataset"
      },
      {
        "name": "training time",
        "type": "Metric"
      },
      {
        "name": "RefineNet",
        "type": "AIModel"
      },
      {
        "name": "intersection-over-union",
        "type": "Metric"
      },
      {
        "name": "LightEMMA",
        "type": "AIModel"
      },
      {
        "name": "Vision-Language Models (VLMs)",
        "type": "AIModel"
      },
      {
        "name": "computational metrics",
        "type": "Metric"
      },
      {
        "name": "ReCogDrive",
        "type": "AIModel"
      },
      {
        "name": "NAVSIM",
        "type": "Dataset"
      },
      {
        "name": "Bench2Drive",
        "type": "Dataset"
      },
      {
        "name": "DriveBench",
        "type": "Dataset"
      },
      {
        "name": "Diffusion Group Relative Policy Optimization (DiffGRPO)",
        "type": "AIModel"
      },
      {
        "name": "pseudo-simulation",
        "type": "AIModel"
      },
      {
        "name": "3D Gaussian Splatting",
        "type": "AIModel"
      },
      {
        "name": "real datasets",
        "type": "Dataset"
      },
      {
        "name": "closed-loop simulation",
        "type": "Metric"
      },
      {
        "name": "open-loop evaluation",
        "type": "Metric"
      },
      {
        "name": "R^2",
        "type": "Metric"
      },
      {
        "name": "DriveMoE",
        "type": "AIModel"
      },
      {
        "name": "Drive-π₀",
        "type": "AIModel"
      },
      {
        "name": "state-of-the-art (SOTA) performance",
        "type": "Metric"
      },
      {
        "name": "Vision-Language-Action (VLA) paradigms",
        "type": "AIModel"
      },
      {
        "name": "multimodal large language models (MLLM)",
        "type": "AIModel"
      },
      {
        "name": "VLA for Autonomous Driving (VLA4AD)",
        "type": "AIModel"
      },
      {
        "name": "over 20 representative models",
        "type": "AIModel"
      },
      {
        "name": "existing datasets and benchmarks",
        "type": "Dataset"
      },
      {
        "name": "driving safety",
        "type": "Metric"
      },
      {
        "name": "explanation quality",
        "type": "Metric"
      },
      {
        "name": "Sora model",
        "type": "AIModel"
      },
      {
        "name": "General world models",
        "type": "AIModel"
      },
      {
        "name": "autonomous-driving world models",
        "type": "AIModel"
      },
      {
        "name": "world models deployed within autonomous agents",
        "type": "AIModel"
      },
      {
        "name": "GPT-4",
        "type": "AIModel"
      },
      {
        "name": "multimodal machine learning",
        "type": "AIModel"
      },
      {
        "name": "state-of-the-art methods",
        "type": "AIModel"
      },
      {
        "name": "multimodal learning",
        "type": "AIModel"
      },
      {
        "name": "datasets covered in multimodal learning research",
        "type": "Dataset"
      },
      {
        "name": "multimodal generative models",
        "type": "AIModel"
      },
      {
        "name": "multimodal foundation models",
        "type": "AIModel"
      },
      {
        "name": "Any-to-Text",
        "type": "Dataset"
      },
      {
        "name": "Any-to-Vision",
        "type": "Dataset"
      },
      {
        "name": "Any-to-Any",
        "type": "Dataset"
      },
      {
        "name": "new theoretical framework for decision making in the tourism industry",
        "type": "AIModel"
      },
      {
        "name": "Intelligent chatbots",
        "type": "AIModel"
      },
      {
        "name": "generative artificial intelligence (GAI) tools",
        "type": "AIModel"
      },
      {
        "name": "existing responsive AI instruments",
        "type": "AIModel"
      },
      {
        "name": "tourism and hospitality scenarios",
        "type": "Dataset"
      },
      {
        "name": "Squeeze-and-Excitation (SE) block",
        "type": "AIModel"
      },
      {
        "name": "SENet",
        "type": "AIModel"
      },
      {
        "name": "ILSVRC 2017",
        "type": "Dataset"
      },
      {
        "name": "EfficientNets",
        "type": "AIModel"
      },
      {
        "name": "EfficientNet-B7",
        "type": "AIModel"
      },
      {
        "name": "MobileNets",
        "type": "AIModel"
      },
      {
        "name": "ResNet",
        "type": "AIModel"
      },
      {
        "name": "Flowers",
        "type": "Dataset"
      },
      {
        "name": "top-1 accuracy",
        "type": "Metric"
      },
      {
        "name": "Light-X",
        "type": "AIModel"
      },
      {
        "name": "Light-Syn",
        "type": "AIModel"
      },
      {
        "name": "Light-Syn dataset",
        "type": "Dataset"
      },
      {
        "name": "baseline methods",
        "type": "AIModel"
      },
      {
        "name": "prior video relighting methods",
        "type": "AIModel"
      },
      {
        "name": "DriveLaW",
        "type": "AIModel"
      },
      {
        "name": "DriveLaW-Video",
        "type": "AIModel"
      },
      {
        "name": "DriveLaW-Act",
        "type": "AIModel"
      },
      {
        "name": "FVD",
        "type": "Metric"
      },
      {
        "name": "DVGT",
        "type": "AIModel"
      },
      {
        "name": "OpenScene",
        "type": "Dataset"
      },
      {
        "name": "Waymo",
        "type": "Dataset"
      },
      {
        "name": "DDAD",
        "type": "Dataset"
      },
      {
        "name": "ControlNet",
        "type": "AIModel"
      },
      {
        "name": "Stable Diffusion",
        "type": "AIModel"
      },
      {
        "name": "small (<50k) and large (>1m) datasets",
        "type": "Dataset"
      },
      {
        "name": "Flan-PaLM 540B",
        "type": "AIModel"
      },
      {
        "name": "PaLM 540B",
        "type": "AIModel"
      },
      {
        "name": "Flan-T5",
        "type": "AIModel"
      },
      {
        "name": "PaLM 62B",
        "type": "AIModel"
      },
      {
        "name": "TyDiQA",
        "type": "Dataset"
      },
      {
        "name": "MGSM",
        "type": "Dataset"
      },
      {
        "name": "five-shot MMLU",
        "type": "Metric"
      },
      {
        "name": "latent diffusion models (LDMs)",
        "type": "AIModel"
      },
      {
        "name": "diffusion models (DMs)",
        "type": "AIModel"
      },
      {
        "name": "image inpainting",
        "type": "Dataset"
      },
      {
        "name": "unconditional image generation",
        "type": "Dataset"
      },
      {
        "name": "semantic scene synthesis",
        "type": "Dataset"
      },
      {
        "name": "super-resolution",
        "type": "Dataset"
      },
      {
        "name": "visual fidelity",
        "type": "Metric"
      },
      {
        "name": "computational requirements",
        "type": "Metric"
      },
      {
        "name": "Variational Autoencoder (VAE)",
        "type": "AIModel"
      },
      {
        "name": "VGG network",
        "type": "AIModel"
      },
      {
        "name": "PSNR",
        "type": "Metric"
      },
      {
        "name": "SSIM",
        "type": "Metric"
      },
      {
        "name": "new dataset of human perceptual similarity judgments",
        "type": "Dataset"
      },
      {
        "name": "Waymo Open Dataset",
        "type": "Dataset"
      },
      {
        "name": "diversity metric",
        "type": "Metric"
      },
      {
        "name": "Data augmentation",
        "type": "Dataset"
      },
      {
        "name": "face images generation",
        "type": "Dataset"
      },
      {
        "name": "two-layer ReLU denoising autoencoder (DAE)",
        "type": "AIModel"
      },
      {
        "name": "unconditional and text-to-image diffusion models",
        "type": "AIModel"
      },
      {
        "name": "representation-based method for detecting memorization",
        "type": "AIModel"
      },
      {
        "name": "training-free editing technique",
        "type": "AIModel"
      },
      {
        "name": "Stable Velocity",
        "type": "AIModel"
      },
      {
        "name": "Stable Velocity Matching (StableVM)",
        "type": "AIModel"
      },
      {
        "name": "Variance-Aware Representation Alignment (VA-REPA)",
        "type": "AIModel"
      },
      {
        "name": "Stable Velocity Sampling (StableVS)",
        "type": "AIModel"
      },
      {
        "name": "SD3.5",
        "type": "AIModel"
      },
      {
        "name": "Flux",
        "type": "AIModel"
      },
      {
        "name": "Qwen-Image",
        "type": "AIModel"
      },
      {
        "name": "Wan2.2",
        "type": "AIModel"
      },
      {
        "name": "training efficiency",
        "type": "Metric"
      },
      {
        "name": "sample quality",
        "type": "Metric"
      },
      {
        "name": "FlatDINO",
        "type": "AIModel"
      },
      {
        "name": "DiT-XL",
        "type": "AIModel"
      },
      {
        "name": "DINOv2",
        "type": "AIModel"
      },
      {
        "name": "gFID",
        "type": "Metric"
      },
      {
        "name": "One-Dimensional Diffusion Video Autoencoder (One-DVA)",
        "type": "AIModel"
      },
      {
        "name": "3D-CNN VAEs",
        "type": "AIModel"
      },
      {
        "name": "REPA-G",
        "type": "AIModel"
      },
      {
        "name": "Dense Convolutional Network (DenseNet)",
        "type": "AIModel"
      },
      {
        "name": "SVHN",
        "type": "Dataset"
      },
      {
        "name": "computation",
        "type": "Metric"
      },
      {
        "name": "Convolutional Neural Network",
        "type": "AIModel"
      },
      {
        "name": "Transfer Learning",
        "type": "AIModel"
      },
      {
        "name": "staged adaptive fine-tuning approach",
        "type": "AIModel"
      },
      {
        "name": "DenseNet-121",
        "type": "AIModel"
      },
      {
        "name": "Cholec80",
        "type": "Dataset"
      },
      {
        "name": "CATARACTS",
        "type": "Dataset"
      },
      {
        "name": "mean average precision (mAP)",
        "type": "Metric"
      },
      {
        "name": "MediaPipe",
        "type": "AIModel"
      },
      {
        "name": "Multi-layered Randomized Decision Forests",
        "type": "AIModel"
      },
      {
        "name": "geometry based normalizations",
        "type": "AIModel"
      },
      {
        "name": "Krawtchouk moments",
        "type": "AIModel"
      },
      {
        "name": "machine learning model",
        "type": "AIModel"
      },
      {
        "name": "user's hand signals",
        "type": "Dataset"
      },
      {
        "name": "MediaPipe and a fully connected neural network (FCNN)",
        "type": "AIModel"
      },
      {
        "name": "American Sign Language (ASL) dataset",
        "type": "Dataset"
      },
      {
        "name": "fast recognition",
        "type": "Metric"
      },
      {
        "name": "R-CNN",
        "type": "AIModel"
      },
      {
        "name": "OverFeat",
        "type": "AIModel"
      },
      {
        "name": "PASCAL VOC",
        "type": "Dataset"
      },
      {
        "name": "VOC 2012",
        "type": "Dataset"
      },
      {
        "name": "ILSVRC2013",
        "type": "Dataset"
      },
      {
        "name": "MuMu-LLaMA",
        "type": "AIModel"
      },
      {
        "name": "SuPLoRA",
        "type": "AIModel"
      },
      {
        "name": "supertype-subtype concept hierarchy",
        "type": "AIModel"
      },
      {
        "name": "group-wise suppression method",
        "type": "AIModel"
      },
      {
        "name": "standard diffusion regularization",
        "type": "AIModel"
      },
      {
        "name": "benchmark",
        "type": "Dataset"
      },
      {
        "name": "Multi-modal Chain and Global Attention Network (MCGA-Net)",
        "type": "AIModel"
      },
      {
        "name": "DCGAN-based data augmentation strategy",
        "type": "AIModel"
      },
      {
        "name": "Multi-modal Chain Feature Fusion (MCFF)",
        "type": "AIModel"
      },
      {
        "name": "Global Attention Mechanism (GAM)",
        "type": "AIModel"
      },
      {
        "name": "GPR images",
        "type": "Dataset"
      },
      {
        "name": "Precision",
        "type": "Metric"
      },
      {
        "name": "Recall",
        "type": "Metric"
      },
      {
        "name": "mAP@50",
        "type": "Metric"
      },
      {
        "name": "Qwen3-VL-Embedding",
        "type": "AIModel"
      },
      {
        "name": "Qwen3-VL-Reranker",
        "type": "AIModel"
      },
      {
        "name": "Qwen3-VL",
        "type": "AIModel"
      },
      {
        "name": "MMEB-V2",
        "type": "Dataset"
      },
      {
        "name": "FPSMark",
        "type": "AIModel"
      },
      {
        "name": "intrinsic signal localization network",
        "type": "AIModel"
      },
      {
        "name": "partial screen-shooting scenarios",
        "type": "Dataset"
      },
      {
        "name": "extraction accuracy",
        "type": "Metric"
      },
      {
        "name": "EM algorithm",
        "type": "AIModel"
      },
      {
        "name": "t-SNE",
        "type": "AIModel"
      },
      {
        "name": "LLM social simulations",
        "type": "AIModel"
      },
      {
        "name": "social science datasets",
        "type": "Dataset"
      },
      {
        "name": "empirical comparisons",
        "type": "Metric"
      },
      {
        "name": "FCLFD",
        "type": "AIModel"
      },
      {
        "name": "CST",
        "type": "AIModel"
      },
      {
        "name": "AWS",
        "type": "AIModel"
      },
      {
        "name": "WISDM",
        "type": "Dataset"
      },
      {
        "name": "PAMAP2",
        "type": "Dataset"
      },
      {
        "name": "F1",
        "type": "Metric"
      },
      {
        "name": "Dispersive Loss",
        "type": "AIModel"
      },
      {
        "name": "Knowledge-Aware Bayesian Bandits (KABB)",
        "type": "AIModel"
      },
      {
        "name": "multi-agent systems",
        "type": "Dataset"
      },
      {
        "name": "cost-performance balance",
        "type": "Metric"
      },
      {
        "name": "U-Net",
        "type": "AIModel"
      },
      {
        "name": "sliding-window convolutional network",
        "type": "AIModel"
      },
      {
        "name": "ISBI challenge for segmentation of neuronal structures in electron microscopic stacks",
        "type": "Dataset"
      },
      {
        "name": "ISBI cell tracking challenge 2015",
        "type": "Dataset"
      },
      {
        "name": "DMD2",
        "type": "AIModel"
      },
      {
        "name": "Distribution Matching Distillation (DMD)",
        "type": "AIModel"
      },
      {
        "name": "ImageNet-64x64",
        "type": "Dataset"
      },
      {
        "name": "COCO 2014",
        "type": "Dataset"
      },
      {
        "name": "Latent Adversarial Diffusion Distillation (LADD)",
        "type": "AIModel"
      },
      {
        "name": "adversarial diffusion distillation (ADD)",
        "type": "AIModel"
      },
      {
        "name": "Stable Diffusion 3 (8B)",
        "type": "AIModel"
      },
      {
        "name": "SD3-Turbo",
        "type": "AIModel"
      },
      {
        "name": "state-of-the-art text-to-image generators",
        "type": "AIModel"
      },
      {
        "name": "evolutionary approach",
        "type": "AIModel"
      },
      {
        "name": "Japanese LLM with Math reasoning capabilities",
        "type": "AIModel"
      },
      {
        "name": "culturally-aware Japanese VLM",
        "type": "AIModel"
      },
      {
        "name": "Japanese LLM benchmarks",
        "type": "Dataset"
      },
      {
        "name": "bidirectional diffusion transformer",
        "type": "AIModel"
      },
      {
        "name": "autoregressive transformer",
        "type": "AIModel"
      },
      {
        "name": "distribution matching distillation (DMD)",
        "type": "AIModel"
      },
      {
        "name": "50-step diffusion model",
        "type": "AIModel"
      },
      {
        "name": "4-step generator",
        "type": "AIModel"
      },
      {
        "name": "VBench-Long",
        "type": "Dataset"
      },
      {
        "name": "total score",
        "type": "Metric"
      },
      {
        "name": "PuLID",
        "type": "AIModel"
      },
      {
        "name": "Lightning T2I branch",
        "type": "AIModel"
      },
      {
        "name": "standard diffusion branch",
        "type": "AIModel"
      },
      {
        "name": "diffusion probabilistic models",
        "type": "AIModel"
      },
      {
        "name": "LSUN",
        "type": "Dataset"
      },
      {
        "name": "Inception score",
        "type": "Metric"
      },
      {
        "name": "FID score",
        "type": "Metric"
      },
      {
        "name": "ProgressiveGAN",
        "type": "AIModel"
      },
      {
        "name": "HunyuanVideo",
        "type": "AIModel"
      },
      {
        "name": "Runway Gen-3",
        "type": "AIModel"
      },
      {
        "name": "Luma 1.6",
        "type": "AIModel"
      },
      {
        "name": "three top-performing Chinese video generative models",
        "type": "AIModel"
      },
      {
        "name": "Loopy",
        "type": "AIModel"
      },
      {
        "name": "audio-driven portrait diffusion models",
        "type": "AIModel"
      },
      {
        "name": "OmniHuman",
        "type": "AIModel"
      },
      {
        "name": "existing end-to-end audio-driven methods",
        "type": "AIModel"
      },
      {
        "name": "highly realistic human video generation",
        "type": "Metric"
      },
      {
        "name": "more realistic videos",
        "type": "Metric"
      },
      {
        "name": "greater flexibility in inputs",
        "type": "Metric"
      },
      {
        "name": "Hallo",
        "type": "AIModel"
      },
      {
        "name": "Hallo2",
        "type": "AIModel"
      },
      {
        "name": "HDTF",
        "type": "Dataset"
      },
      {
        "name": "CelebV",
        "type": "Dataset"
      },
      {
        "name": "Wild",
        "type": "Dataset"
      },
      {
        "name": "EchoMimicV2",
        "type": "AIModel"
      },
      {
        "name": "Audio-Pose Dynamic Harmonization",
        "type": "AIModel"
      },
      {
        "name": "Pose Sampling",
        "type": "AIModel"
      },
      {
        "name": "Audio Diffusion",
        "type": "AIModel"
      },
      {
        "name": "Head Partial Attention",
        "type": "AIModel"
      },
      {
        "name": "Phase-specific Denoising Loss",
        "type": "AIModel"
      },
      {
        "name": "half-body data",
        "type": "Dataset"
      },
      {
        "name": "headshot data",
        "type": "Dataset"
      },
      {
        "name": "novel benchmark for evaluating the effectiveness of half-body human animation",
        "type": "Dataset"
      },
      {
        "name": "Feature Pyramid Network (FPN)",
        "type": "AIModel"
      },
      {
        "name": "Faster R-CNN",
        "type": "AIModel"
      },
      {
        "name": "COCO detection benchmark",
        "type": "Dataset"
      },
      {
        "name": "state-of-the-art single-model results",
        "type": "Metric"
      },
      {
        "name": "RNN Encoder-Decoder",
        "type": "AIModel"
      },
      {
        "name": "statistical machine translation system",
        "type": "AIModel"
      },
      {
        "name": "log-linear model",
        "type": "AIModel"
      },
      {
        "name": "DriveMLM",
        "type": "AIModel"
      },
      {
        "name": "Autopilot",
        "type": "AIModel"
      },
      {
        "name": "Apollo",
        "type": "AIModel"
      },
      {
        "name": "CARLA Town05 Long",
        "type": "Dataset"
      },
      {
        "name": "multimodal LLM (MLLM)",
        "type": "AIModel"
      },
      {
        "name": "Vista",
        "type": "AIModel"
      },
      {
        "name": "most advanced general-purpose video generator",
        "type": "AIModel"
      },
      {
        "name": "best-performing driving world model",
        "type": "AIModel"
      },
      {
        "name": "multiple datasets",
        "type": "Dataset"
      },
      {
        "name": "DiffusionDrive",
        "type": "AIModel"
      },
      {
        "name": "vanilla diffusion policy",
        "type": "AIModel"
      },
      {
        "name": "PDMS",
        "type": "Metric"
      },
      {
        "name": "ResNet-34",
        "type": "AIModel"
      },
      {
        "name": "diffusion-based video generation models",
        "type": "AIModel"
      },
      {
        "name": "2D simulation testbed for object movement and collisions",
        "type": "Dataset"
      },
      {
        "name": "physical laws adherence",
        "type": "Metric"
      },
      {
        "name": "EMMA",
        "type": "AIModel"
      },
      {
        "name": "Gemini",
        "type": "AIModel"
      },
      {
        "name": "Waymo Open Motion Dataset (WOMD)",
        "type": "Dataset"
      },
      {
        "name": "Waymo Open Dataset (WOD)",
        "type": "Dataset"
      },
      {
        "name": "state-of-the-art performance in motion planning",
        "type": "Metric"
      },
      {
        "name": "competitive results",
        "type": "Metric"
      },
      {
        "name": "GPT-3",
        "type": "AIModel"
      },
      {
        "name": "translation, question-answering, and cloze tasks",
        "type": "Dataset"
      },
      {
        "name": "few-shot performance",
        "type": "Metric"
      },
      {
        "name": "InternVL 2.5",
        "type": "AIModel"
      },
      {
        "name": "InternVL 2.0",
        "type": "AIModel"
      },
      {
        "name": "Claude-3.5-Sonnet",
        "type": "AIModel"
      },
      {
        "name": "MMMU benchmark",
        "type": "Dataset"
      },
      {
        "name": "Chain-of-Thought (CoT) reasoning",
        "type": "Metric"
      },
      {
        "name": "InternVL3-78B",
        "type": "AIModel"
      },
      {
        "name": "MMMU",
        "type": "Dataset"
      },
      {
        "name": "ChatGPT-4o",
        "type": "AIModel"
      },
      {
        "name": "Claude 3.5 Sonnet",
        "type": "AIModel"
      },
      {
        "name": "Gemini 2.5 Pro",
        "type": "AIModel"
      },
      {
        "name": "VLMEvalKit",
        "type": "AIModel"
      },
      {
        "name": "OpenVLM Leaderboard",
        "type": "Metric"
      },
      {
        "name": "LLaVA-CoT",
        "type": "AIPaper"
      },
      {
        "name": "LLaVA-CoT-100k",
        "type": "Dataset"
      },
      {
        "name": "Gemini-1.5-pro",
        "type": "AIModel"
      },
      {
        "name": "GPT-4o-mini",
        "type": "AIModel"
      },
      {
        "name": "Llama-3.2-90B-Vision-Instruct",
        "type": "AIModel"
      },
      {
        "name": "multimodal reasoning benchmarks",
        "type": "Metric"
      },
      {
        "name": "InternVL 3.5",
        "type": "AIPaper"
      },
      {
        "name": "Cascade Reinforcement Learning (Cascade RL) framework",
        "type": "AIModel"
      },
      {
        "name": "Visual Resolution Router (ViR)",
        "type": "AIModel"
      },
      {
        "name": "Decoupled Vision-Language Deployment (DvD) strategy",
        "type": "AIModel"
      },
      {
        "name": "InternVL3.5-241B-A28B",
        "type": "AIModel"
      },
      {
        "name": "GPT-5",
        "type": "AIModel"
      },
      {
        "name": "MathVista",
        "type": "Dataset"
      },
      {
        "name": "overall reasoning performance",
        "type": "Metric"
      },
      {
        "name": "inference speedup",
        "type": "Metric"
      },
      {
        "name": "Proximal Policy Optimization (PPO)",
        "type": "AIModel"
      },
      {
        "name": "Trust Region Policy Optimization (TRPO)",
        "type": "AIModel"
      },
      {
        "name": "simulated robotic locomotion",
        "type": "Dataset"
      },
      {
        "name": "Atari game playing",
        "type": "Dataset"
      },
      {
        "name": "sample complexity",
        "type": "Metric"
      },
      {
        "name": "Flow-GRPO",
        "type": "AIPaper"
      },
      {
        "name": "SD3.5-M",
        "type": "AIModel"
      },
      {
        "name": "DanceGRPO",
        "type": "AIPaper"
      },
      {
        "name": "Group Relative Policy Optimization (GRPO)",
        "type": "AIModel"
      },
      {
        "name": "DDPO",
        "type": "AIModel"
      },
      {
        "name": "DPOK",
        "type": "AIModel"
      },
      {
        "name": "diffusion models",
        "type": "AIModel"
      },
      {
        "name": "rectified flows",
        "type": "AIModel"
      },
      {
        "name": "HPS-v2.1",
        "type": "Dataset"
      },
      {
        "name": "CLIP Score",
        "type": "Metric"
      },
      {
        "name": "VideoAlign",
        "type": "Dataset"
      },
      {
        "name": "Seedance 1.0",
        "type": "AIModel"
      },
      {
        "name": "state-of-the-art video generation models",
        "type": "AIModel"
      },
      {
        "name": "multi-source data curation",
        "type": "Dataset"
      },
      {
        "name": "prompt following",
        "type": "Metric"
      },
      {
        "name": "motion plausibility",
        "type": "Metric"
      },
      {
        "name": "visual quality",
        "type": "Metric"
      },
      {
        "name": "spatiotemporal fluidity",
        "type": "Metric"
      },
      {
        "name": "structural stability",
        "type": "Metric"
      },
      {
        "name": "instruction adherence",
        "type": "Metric"
      },
      {
        "name": "narrative coherence",
        "type": "Metric"
      },
      {
        "name": "subject representation",
        "type": "Metric"
      },
      {
        "name": "SkyReels-V2",
        "type": "AIModel"
      },
      {
        "name": "Multi-modal Large Language Model (MLLM)",
        "type": "AIModel"
      },
      {
        "name": "SkyCaptioner-V1",
        "type": "AIModel"
      },
      {
        "name": "Multi-stage Pretraining",
        "type": "AIModel"
      },
      {
        "name": "Reinforcement Learning",
        "type": "AIModel"
      },
      {
        "name": "Diffusion Forcing Framework",
        "type": "AIModel"
      },
      {
        "name": "Supervised Fine-Tuning (SFT)",
        "type": "AIModel"
      },
      {
        "name": "video data",
        "type": "Dataset"
      },
      {
        "name": "human-annotated and synthetic distortion data",
        "type": "Dataset"
      },
      {
        "name": "prompt adherence",
        "type": "Metric"
      },
      {
        "name": "motion dynamics",
        "type": "Metric"
      },
      {
        "name": "duration",
        "type": "Metric"
      },
      {
        "name": "temporal visual quality",
        "type": "Metric"
      },
      {
        "name": "resolution",
        "type": "Metric"
      },
      {
        "name": "shot-aware generation",
        "type": "Metric"
      },
      {
        "name": "realistic long-form synthesis",
        "type": "Metric"
      },
      {
        "name": "professional film-style generation",
        "type": "Metric"
      },
      {
        "name": "dynamic artifacts",
        "type": "Metric"
      },
      {
        "name": "UnifiedReward",
        "type": "AIModel"
      },
      {
        "name": "Direct Preference Optimization (DPO)",
        "type": "AIModel"
      },
      {
        "name": "Transformer-based model",
        "type": "AIModel"
      },
      {
        "name": "simulated bar exam",
        "type": "Dataset"
      },
      {
        "name": "factuality",
        "type": "Metric"
      },
      {
        "name": "adherence to desired behavior",
        "type": "Metric"
      },
      {
        "name": "LLaMA",
        "type": "AIPaper"
      },
      {
        "name": "LLaMA-13B",
        "type": "AIModel"
      },
      {
        "name": "LLaMA-65B",
        "type": "AIModel"
      },
      {
        "name": "GPT-3 (175B)",
        "type": "AIModel"
      },
      {
        "name": "Chinchilla-70B",
        "type": "AIModel"
      },
      {
        "name": "PaLM-540B",
        "type": "AIModel"
      },
      {
        "name": "benchmarks",
        "type": "Metric"
      },
      {
        "name": "chain of thought prompting",
        "type": "AIModel"
      },
      {
        "name": "GSM8K",
        "type": "Dataset"
      },
      {
        "name": "verifiers",
        "type": "AIModel"
      },
      {
        "name": "transformer models",
        "type": "AIModel"
      },
      {
        "name": "finetuning baseline",
        "type": "AIModel"
      },
      {
        "name": "test performance",
        "type": "Metric"
      },
      {
        "name": "Mamba",
        "type": "AIModel"
      },
      {
        "name": "Mamba-3B",
        "type": "AIModel"
      },
      {
        "name": "linear attention",
        "type": "AIModel"
      },
      {
        "name": "gated convolution",
        "type": "AIModel"
      },
      {
        "name": "recurrent models",
        "type": "AIModel"
      },
      {
        "name": "structured state space models (SSMs)",
        "type": "AIModel"
      },
      {
        "name": "language",
        "type": "Dataset"
      },
      {
        "name": "audio",
        "type": "Dataset"
      },
      {
        "name": "genomics",
        "type": "Dataset"
      },
      {
        "name": "SGDR",
        "type": "AIModel"
      },
      {
        "name": "EEG recordings dataset",
        "type": "Dataset"
      },
      {
        "name": "downsampled ImageNet",
        "type": "Dataset"
      },
      {
        "name": "error rate",
        "type": "Metric"
      },
      {
        "name": "Sparsely-Gated Mixture-of-Experts layer (MoE)",
        "type": "AIModel"
      },
      {
        "name": "machine translation",
        "type": "Dataset"
      },
      {
        "name": "large language modeling benchmarks",
        "type": "Dataset"
      },
      {
        "name": "machine translation benchmarks",
        "type": "Dataset"
      },
      {
        "name": "computational efficiency",
        "type": "Metric"
      },
      {
        "name": "pointer sentinel mixture architecture",
        "type": "AIModel"
      },
      {
        "name": "pointer sentinel-LSTM model",
        "type": "AIModel"
      },
      {
        "name": "standard softmax LSTM",
        "type": "AIModel"
      },
      {
        "name": "Penn Treebank",
        "type": "Dataset"
      },
      {
        "name": "WikiText corpus",
        "type": "Dataset"
      },
      {
        "name": "perplexity",
        "type": "Metric"
      },
      {
        "name": "massive multitask language understanding test",
        "type": "Dataset"
      },
      {
        "name": "multitask accuracy",
        "type": "Metric"
      },
      {
        "name": "process-supervised model",
        "type": "AIModel"
      },
      {
        "name": "PRM800K",
        "type": "Dataset"
      },
      {
        "name": "GPQA",
        "type": "Dataset"
      },
      {
        "name": "GShard",
        "type": "AIModel"
      },
      {
        "name": "multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts",
        "type": "AIModel"
      },
      {
        "name": "XLA compiler",
        "type": "AIModel"
      },
      {
        "name": "prior art",
        "type": "AIModel"
      },
      {
        "name": "translation from 100 languages to English",
        "type": "Dataset"
      },
      {
        "name": "model quality",
        "type": "Metric"
      },
      {
        "name": "computation cost",
        "type": "Metric"
      },
      {
        "name": "ease of programming",
        "type": "Metric"
      },
      {
        "name": "efficient implementation on parallel devices",
        "type": "Metric"
      },
      {
        "name": "Probability Distributions",
        "type": "AIModel"
      },
      {
        "name": "Linear Models for Regression",
        "type": "AIModel"
      },
      {
        "name": "Linear Models for Classification",
        "type": "AIModel"
      },
      {
        "name": "Neural Networks",
        "type": "AIModel"
      },
      {
        "name": "Kernel Methods",
        "type": "AIModel"
      },
      {
        "name": "Sparse Kernel Machines",
        "type": "AIModel"
      },
      {
        "name": "Graphical Models",
        "type": "AIModel"
      },
      {
        "name": "Mixture Models and EM",
        "type": "AIModel"
      },
      {
        "name": "Approximate Inference",
        "type": "AIModel"
      },
      {
        "name": "Sampling Methods",
        "type": "AIModel"
      },
      {
        "name": "Continuous Latent Variables",
        "type": "AIModel"
      },
      {
        "name": "Sequential Data",
        "type": "AIModel"
      },
      {
        "name": "Combining Models",
        "type": "AIModel"
      },
      {
        "name": "deep convolutional neural networks",
        "type": "AIModel"
      },
      {
        "name": "shallow networks",
        "type": "AIModel"
      },
      {
        "name": "deep models",
        "type": "AIModel"
      },
      {
        "name": "VGG16",
        "type": "AIModel"
      },
      {
        "name": "PlantVillage",
        "type": "Dataset"
      },
      {
        "name": "deep neural networks",
        "type": "AIModel"
      },
      {
        "name": "26-layer deep learning model consisting of 8 residual building blocks",
        "type": "AIModel"
      },
      {
        "name": "BJFU100 dataset",
        "type": "Dataset"
      },
      {
        "name": "recognition rate",
        "type": "Metric"
      },
      {
        "name": "herbarium images",
        "type": "Dataset"
      },
      {
        "name": "big dataset with thousands of species from herbaria",
        "type": "Dataset"
      },
      {
        "name": "different datasets from different herbaria",
        "type": "Dataset"
      },
      {
        "name": "MobileNetV2",
        "type": "AIModel"
      },
      {
        "name": "SSDLite",
        "type": "AIModel"
      },
      {
        "name": "Mobile DeepLabv3",
        "type": "AIModel"
      },
      {
        "name": "Imagenet",
        "type": "Dataset"
      },
      {
        "name": "VOC",
        "type": "Dataset"
      },
      {
        "name": "multiply-adds (MAdd)",
        "type": "Metric"
      },
      {
        "name": "number of parameters",
        "type": "Metric"
      },
      {
        "name": "Federated Learning",
        "type": "AIModel"
      },
      {
        "name": "five different model architectures",
        "type": "AIModel"
      },
      {
        "name": "four datasets",
        "type": "Dataset"
      },
      {
        "name": "synchronized stochastic gradient descent",
        "type": "AIModel"
      },
      {
        "name": "Amazon Reviews",
        "type": "Dataset"
      },
      {
        "name": "Reuters-21578",
        "type": "Dataset"
      },
      {
        "name": "Office-31",
        "type": "Dataset"
      },
      {
        "name": "federated deep neural network (FDNN)",
        "type": "AIModel"
      },
      {
        "name": "Flipkart dataset",
        "type": "Dataset"
      },
      {
        "name": "Federated learning (FL)",
        "type": "AIModel"
      },
      {
        "name": "Internet of Things (IoT)",
        "type": "Dataset"
      },
      {
        "name": "Wireless Sensor Networks (WSNs)",
        "type": "Dataset"
      },
      {
        "name": "privacy-preserving federated learning",
        "type": "AIModel"
      },
      {
        "name": "malware detection models",
        "type": "AIModel"
      },
      {
        "name": "Long Short-Term Memory Network",
        "type": "AIModel"
      },
      {
        "name": "centralized federated learning framework",
        "type": "AIModel"
      },
      {
        "name": "decentralized federated learning framework",
        "type": "AIModel"
      },
      {
        "name": "crop yield prediction",
        "type": "Dataset"
      },
      {
        "name": "prediction accuracy",
        "type": "Metric"
      },
      {
        "name": "precision",
        "type": "Metric"
      },
      {
        "name": "recall",
        "type": "Metric"
      },
      {
        "name": "F1-Score",
        "type": "Metric"
      },
      {
        "name": "Segment Anything (SA) project",
        "type": "AIPaper"
      },
      {
        "name": "Segment Anything Model (SAM)",
        "type": "AIModel"
      },
      {
        "name": "SA-1B",
        "type": "Dataset"
      },
      {
        "name": "zero-shot performance",
        "type": "Metric"
      },
      {
        "name": "LLaVA",
        "type": "AIModel"
      },
      {
        "name": "Science QA",
        "type": "Dataset"
      },
      {
        "name": "synthetic multimodal instruction-following dataset",
        "type": "Dataset"
      },
      {
        "name": "relative score",
        "type": "Metric"
      },
      {
        "name": "CLIP-ViT-L-336px",
        "type": "AIModel"
      },
      {
        "name": "MLP projection",
        "type": "AIModel"
      },
      {
        "name": "academic-task-oriented VQA data",
        "type": "Dataset"
      },
      {
        "name": "11 benchmarks",
        "type": "Dataset"
      },
      {
        "name": "state-of-the-art",
        "type": "Metric"
      },
      {
        "name": "DepictQA-Wild",
        "type": "AIModel"
      },
      {
        "name": "Vision Language Models (VLMs)",
        "type": "AIModel"
      },
      {
        "name": "VLM-based Image Quality Assessment (IQA)",
        "type": "AIModel"
      },
      {
        "name": "traditional score-based methods",
        "type": "AIModel"
      },
      {
        "name": "GPT-4V",
        "type": "AIModel"
      },
      {
        "name": "DQ-495K",
        "type": "Dataset"
      },
      {
        "name": "distortion identification",
        "type": "Metric"
      },
      {
        "name": "instant rating",
        "type": "Metric"
      },
      {
        "name": "reasoning tasks",
        "type": "Metric"
      },
      {
        "name": "Chain-of-Focus (CoF)",
        "type": "AIModel"
      },
      {
        "name": "MM-CoF",
        "type": "Dataset"
      },
      {
        "name": "V* benchmark",
        "type": "Dataset"
      },
      {
        "name": "outcome accuracies",
        "type": "Metric"
      },
      {
        "name": "formats",
        "type": "Metric"
      },
      {
        "name": "3DThinker",
        "type": "AIModel"
      },
      {
        "name": "VGGT",
        "type": "AIModel"
      },
      {
        "name": "multiple benchmarks",
        "type": "Dataset"
      },
      {
        "name": "UniME-V2",
        "type": "AIModel"
      },
      {
        "name": "MLLM-as-a-Judge mechanism",
        "type": "AIModel"
      },
      {
        "name": "UniME-V2-Reranker",
        "type": "AIModel"
      },
      {
        "name": "MMEB benchmark",
        "type": "Dataset"
      },
      {
        "name": "retrieval tasks",
        "type": "Dataset"
      },
      {
        "name": "OpenMMReasoner",
        "type": "AIPaper"
      },
      {
        "name": "Qwen2.5-VL-7B-Instruct",
        "type": "AIModel"
      },
      {
        "name": "874K-sample cold-start dataset",
        "type": "Dataset"
      },
      {
        "name": "74K-sample dataset",
        "type": "Dataset"
      },
      {
        "name": "nine multimodal reasoning benchmarks",
        "type": "Metric"
      },
      {
        "name": "agentic AI",
        "type": "AIModel"
      },
      {
        "name": "autonomous vehicles",
        "type": "Dataset"
      },
      {
        "name": "medical and industrial robotics",
        "type": "Dataset"
      },
      {
        "name": "precision agriculture",
        "type": "Dataset"
      },
      {
        "name": "humanoid robotics",
        "type": "Dataset"
      },
      {
        "name": "augmented reality",
        "type": "Dataset"
      },
      {
        "name": "agentic adaptation",
        "type": "Metric"
      },
      {
        "name": "cross-embodiment planning",
        "type": "Metric"
      },
      {
        "name": "Asynchronous Action Chunk Correction (A2C2)",
        "type": "AIModel"
      },
      {
        "name": "Real Time Chunking (RTC)",
        "type": "AIModel"
      },
      {
        "name": "dynamic Kinetix task suite (12 tasks)",
        "type": "Dataset"
      },
      {
        "name": "LIBERO Spatial",
        "type": "Dataset"
      },
      {
        "name": "success rate",
        "type": "Metric"
      },
      {
        "name": "Latent-CoT-Drive (LCDrive)",
        "type": "AIModel"
      },
      {
        "name": "large-scale end-to-end driving benchmark",
        "type": "Dataset"
      },
      {
        "name": "trajectory quality",
        "type": "Metric"
      },
      {
        "name": "non-reasoning baselines",
        "type": "AIModel"
      },
      {
        "name": "text-reasoning baselines",
        "type": "AIModel"
      },
      {
        "name": "HyperVLA",
        "type": "AIPaper"
      },
      {
        "name": "OpenVLA",
        "type": "AIModel"
      },
      {
        "name": "hypernetwork (HN)-based architecture",
        "type": "AIModel"
      },
      {
        "name": "vision foundation models",
        "type": "AIModel"
      },
      {
        "name": "large-scale robotic data",
        "type": "Dataset"
      },
      {
        "name": "inference costs",
        "type": "Metric"
      },
      {
        "name": "number of activated parameters",
        "type": "Metric"
      },
      {
        "name": "TEAM-VLA",
        "type": "AIModel"
      },
      {
        "name": "LIBERO",
        "type": "Dataset"
      },
      {
        "name": "task success rate",
        "type": "Metric"
      },
      {
        "name": "SegMamba-V2",
        "type": "AIPaper"
      },
      {
        "name": "CRC-2000",
        "type": "Dataset"
      },
      {
        "name": "three other large-scale 3D medical image segmentation datasets",
        "type": "Dataset"
      },
      {
        "name": "TransSIL",
        "type": "AIModel"
      },
      {
        "name": "CUB200-2011",
        "type": "Dataset"
      },
      {
        "name": "NABirds",
        "type": "Dataset"
      },
      {
        "name": "CBRFormer",
        "type": "AIModel"
      },
      {
        "name": "Deep contrastive learning enables genome-wide virtual screening",
        "type": "AIPaper"
      },
      {
        "name": "DrugCLIP",
        "type": "AIModel"
      },
      {
        "name": "GenomeScreenDB",
        "type": "Dataset"
      },
      {
        "name": "AlphaFold2",
        "type": "AIModel"
      },
      {
        "name": "docking",
        "type": "AIModel"
      },
      {
        "name": "hit rate",
        "type": "Metric"
      },
      {
        "name": "LLaVA-based semantic feature modulation diffusion model",
        "type": "AIModel"
      },
      {
        "name": "underwater image enhancement",
        "type": "Dataset"
      },
      {
        "name": "3DGS-Drag",
        "type": "AIModel"
      },
      {
        "name": "diffusion guidance",
        "type": "AIModel"
      },
      {
        "name": "autoregressive transformers",
        "type": "AIModel"
      },
      {
        "name": "bidirectional encoders for representation learning",
        "type": "AIModel"
      },
      {
        "name": "Vision Transformers",
        "type": "AIModel"
      },
      {
        "name": "cross-modal attention",
        "type": "AIModel"
      },
      {
        "name": "standard datasets",
        "type": "Dataset"
      },
      {
        "name": "performance benchmarks",
        "type": "Metric"
      },
      {
        "name": "BiFlow",
        "type": "AIModel"
      },
      {
        "name": "Normalizing Flows",
        "type": "AIModel"
      },
      {
        "name": "TARFlow",
        "type": "AIModel"
      },
      {
        "name": "generation quality",
        "type": "Metric"
      },
      {
        "name": "sampling speed",
        "type": "Metric"
      },
      {
        "name": "pixel MeanFlow",
        "type": "AIModel"
      },
      {
        "name": "Meta Flow Maps (MFMs)",
        "type": "AIModel"
      },
      {
        "name": "consistency models",
        "type": "AIModel"
      },
      {
        "name": "flow maps",
        "type": "AIModel"
      },
      {
        "name": "Best-of-1000",
        "type": "AIModel"
      },
      {
        "name": "Sequential Flow Matching",
        "type": "AIModel"
      },
      {
        "name": "flow-matching models",
        "type": "AIModel"
      },
      {
        "name": "forecasting tasks",
        "type": "Dataset"
      },
      {
        "name": "decision-making tasks",
        "type": "Dataset"
      },
      {
        "name": "state estimation tasks",
        "type": "Dataset"
      },
      {
        "name": "Drifting Models",
        "type": "AIModel"
      },
      {
        "name": "SimCLR",
        "type": "AIModel"
      },
      {
        "name": "top-5 accuracy",
        "type": "Metric"
      },
      {
        "name": "AlexNet",
        "type": "AIModel"
      },
      {
        "name": "Directional Decoupling Alignment (D²-Align)",
        "type": "AIModel"
      },
      {
        "name": "DivGenBench",
        "type": "Dataset"
      },
      {
        "name": "automated reward metrics",
        "type": "Metric"
      },
      {
        "name": "quantitative metrics for both quality and diversity",
        "type": "Metric"
      },
      {
        "name": "ImagerySearch",
        "type": "AIModel"
      },
      {
        "name": "LDT-Bench",
        "type": "Dataset"
      },
      {
        "name": "VBench",
        "type": "Dataset"
      },
      {
        "name": "creative generation capabilities",
        "type": "Metric"
      },
      {
        "name": "FLUX VAE",
        "type": "AIModel"
      },
      {
        "name": "web, synthetic, and text-rendering data",
        "type": "Dataset"
      },
      {
        "name": "high-quality datasets",
        "type": "Dataset"
      },
      {
        "name": "general fidelity",
        "type": "Metric"
      },
      {
        "name": "video foundation models",
        "type": "AIModel"
      },
      {
        "name": "implicit world model",
        "type": "AIModel"
      },
      {
        "name": "video renderer",
        "type": "AIModel"
      },
      {
        "name": "world model",
        "type": "AIModel"
      },
      {
        "name": "video generation model",
        "type": "AIModel"
      },
      {
        "name": "World models",
        "type": "AIModel"
      },
      {
        "name": "visual prediction",
        "type": "Dataset"
      },
      {
        "name": "3D estimation",
        "type": "Dataset"
      },
      {
        "name": "symbol grounding",
        "type": "Dataset"
      },
      {
        "name": "RecTok",
        "type": "AIModel"
      },
      {
        "name": "visual tokenizers",
        "type": "AIModel"
      },
      {
        "name": "vision foundation models (VFMs)",
        "type": "AIModel"
      },
      {
        "name": "flow matching",
        "type": "AIModel"
      },
      {
        "name": "diffusion transformers",
        "type": "AIModel"
      },
      {
        "name": "gFID-50K",
        "type": "Dataset"
      },
      {
        "name": "RePack then Refine",
        "type": "AIPaper"
      },
      {
        "name": "Repack then Refine",
        "type": "AIModel"
      },
      {
        "name": "RePack-DiT-XL/1",
        "type": "AIModel"
      },
      {
        "name": "Vision Foundation Models",
        "type": "AIModel"
      },
      {
        "name": "Latent Diffusion Models",
        "type": "AIModel"
      },
      {
        "name": "Diffusion Transformers",
        "type": "AIModel"
      },
      {
        "name": "RePack module",
        "type": "AIModel"
      },
      {
        "name": "Latent-Guided Refiner",
        "type": "AIModel"
      },
      {
        "name": "BigGAN-deep",
        "type": "AIModel"
      },
      {
        "name": "ImageNet 128×128",
        "type": "Dataset"
      },
      {
        "name": "ImageNet 512×512",
        "type": "Dataset"
      },
      {
        "name": "ViT model (Dosovitskiy et al., 2020)",
        "type": "AIModel"
      },
      {
        "name": "OpenCLIP (Ilharco et al., 2021)",
        "type": "AIModel"
      },
      {
        "name": "automatic pipeline to build a dedicated, diverse, and curated image dataset",
        "type": "Dataset"
      },
      {
        "name": "PixelGen",
        "type": "AIModel"
      },
      {
        "name": "diffusion-based models",
        "type": "AIModel"
      },
      {
        "name": "autoregressive-based architectures",
        "type": "AIModel"
      },
      {
        "name": "hybrid approaches",
        "type": "AIModel"
      },
      {
        "name": "datasets and benchmarks",
        "type": "Dataset"
      },
      {
        "name": "MentisOculi",
        "type": "Dataset"
      },
      {
        "name": "multimodal large language models (MLLMs)",
        "type": "AIModel"
      },
      {
        "name": "unified multimodal models (UMMs)",
        "type": "AIModel"
      },
      {
        "name": "visual strategies",
        "type": "AIModel"
      },
      {
        "name": "OpenVision 3",
        "type": "AIModel"
      },
      {
        "name": "ViT encoder",
        "type": "AIModel"
      },
      {
        "name": "ViT-VAE decoder",
        "type": "AIModel"
      },
      {
        "name": "CLIP vision encoder",
        "type": "AIModel"
      },
      {
        "name": "LLaVA-1.5 framework",
        "type": "AIModel"
      },
      {
        "name": "RAE framework",
        "type": "AIModel"
      },
      {
        "name": "SeedBench",
        "type": "Dataset"
      },
      {
        "name": "POPE",
        "type": "Dataset"
      },
      {
        "name": "continuous Skip-gram model",
        "type": "AIModel"
      },
      {
        "name": "hierarchical softmax",
        "type": "AIModel"
      },
      {
        "name": "negative sampling",
        "type": "AIModel"
      },
      {
        "name": "global logbilinear regression model",
        "type": "AIModel"
      },
      {
        "name": "global matrix factorization",
        "type": "AIModel"
      },
      {
        "name": "local context window methods",
        "type": "AIModel"
      },
      {
        "name": "word analogy task",
        "type": "Dataset"
      },
      {
        "name": "similarity tasks",
        "type": "Dataset"
      },
      {
        "name": "named entity recognition",
        "type": "Dataset"
      },
      {
        "name": "75%",
        "type": "Metric"
      },
      {
        "name": "deep bidirectional language model (biLM)",
        "type": "AIModel"
      },
      {
        "name": "large text corpus",
        "type": "Dataset"
      },
      {
        "name": "textual entailment",
        "type": "Metric"
      },
      {
        "name": "sentiment analysis",
        "type": "Metric"
      },
      {
        "name": "Protein Language Models",
        "type": "AIModel"
      },
      {
        "name": "Modern models for common NLP tasks",
        "type": "AIModel"
      },
      {
        "name": "2018 Twitter data spanning 51 U.S. regions and 99 countries",
        "type": "Dataset"
      },
      {
        "name": "gender bias in word embeddings",
        "type": "Metric"
      },
      {
        "name": "statistical gender gaps in education, politics, economics, and health",
        "type": "Metric"
      },
      {
        "name": "generative classifiers",
        "type": "AIModel"
      },
      {
        "name": "diffusion-based generative classifiers",
        "type": "AIModel"
      },
      {
        "name": "autoregressive generative classifiers",
        "type": "AIModel"
      },
      {
        "name": "five standard image and text distribution shift benchmarks",
        "type": "Dataset"
      },
      {
        "name": "medical datasets",
        "type": "Dataset"
      },
      {
        "name": "satellite datasets",
        "type": "Dataset"
      },
      {
        "name": "Gaussian toy setting",
        "type": "Dataset"
      },
      {
        "name": "impact of spurious correlations",
        "type": "Metric"
      },
      {
        "name": "GPT-2",
        "type": "AIModel"
      },
      {
        "name": "WebText",
        "type": "Dataset"
      },
      {
        "name": "Llama 3",
        "type": "AIModel"
      },
      {
        "name": "Llama Guard 3",
        "type": "AIModel"
      },
      {
        "name": "autoregressive (AR) paradigm",
        "type": "AIModel"
      },
      {
        "name": "masked language models",
        "type": "AIModel"
      },
      {
        "name": "Quokka",
        "type": "AIModel"
      },
      {
        "name": "Chinchilla",
        "type": "AIModel"
      },
      {
        "name": "IGPO",
        "type": "AIModel"
      },
      {
        "name": "GRPO",
        "type": "AIModel"
      },
      {
        "name": "Math500",
        "type": "Dataset"
      },
      {
        "name": "AMC",
        "type": "Dataset"
      },
      {
        "name": "E2D2",
        "type": "AIModel"
      },
      {
        "name": "translation",
        "type": "Dataset"
      },
      {
        "name": "inference throughput",
        "type": "Metric"
      },
      {
        "name": "Stable-DiffCoder",
        "type": "AIModel"
      },
      {
        "name": "Seed-Coder",
        "type": "AIModel"
      },
      {
        "name": "Diffusion-based language models (DLLMs)",
        "type": "AIModel"
      },
      {
        "name": "code benchmarks",
        "type": "Dataset"
      },
      {
        "name": "low-resource coding languages",
        "type": "Dataset"
      },
      {
        "name": "XLNet",
        "type": "AIModel"
      },
      {
        "name": "Transformer-XL",
        "type": "AIModel"
      },
      {
        "name": "Qwen",
        "type": "AIModel"
      },
      {
        "name": "Qwen-Chat",
        "type": "AIModel"
      },
      {
        "name": "Code-Qwen",
        "type": "AIModel"
      },
      {
        "name": "Code-Qwen-Chat",
        "type": "AIModel"
      },
      {
        "name": "Math-Qwen-Chat",
        "type": "AIModel"
      },
      {
        "name": "Reinforcement Learning from Human Feedback (RLHF)",
        "type": "AIModel"
      },
      {
        "name": "code interpreter",
        "type": "Dataset"
      },
      {
        "name": "Code Llama",
        "type": "AIPaper"
      },
      {
        "name": "Code Llama - Python",
        "type": "AIModel"
      },
      {
        "name": "Code Llama - Instruct",
        "type": "AIModel"
      },
      {
        "name": "Llama 2",
        "type": "AIModel"
      },
      {
        "name": "MBPP",
        "type": "Dataset"
      },
      {
        "name": "MultiPL-E",
        "type": "Dataset"
      },
      {
        "name": "dLLM-Var",
        "type": "AIModel"
      },
      {
        "name": "dLLMs",
        "type": "AIModel"
      },
      {
        "name": "autoregressive models",
        "type": "AIModel"
      },
      {
        "name": "Llama",
        "type": "AIModel"
      },
      {
        "name": "standard benchmarks",
        "type": "Dataset"
      },
      {
        "name": "speedup",
        "type": "Metric"
      },
      {
        "name": "Set Block Decoding (SBD)",
        "type": "AIModel"
      },
      {
        "name": "next token prediction (NTP)",
        "type": "AIModel"
      },
      {
        "name": "masked token prediction (MATP)",
        "type": "AIModel"
      },
      {
        "name": "Llama-3.1 8B",
        "type": "AIModel"
      },
      {
        "name": "Qwen-3 8B",
        "type": "AIModel"
      },
      {
        "name": "discrete diffusion literature",
        "type": "Dataset"
      },
      {
        "name": "speedups",
        "type": "Metric"
      },
      {
        "name": "OneFlow",
        "type": "AIModel"
      },
      {
        "name": "autoregressive baselines",
        "type": "AIModel"
      },
      {
        "name": "diffusion-based approaches",
        "type": "AIModel"
      },
      {
        "name": "generation and understanding tasks",
        "type": "Dataset"
      },
      {
        "name": "training FLOPs",
        "type": "Metric"
      },
      {
        "name": "Large language models (LLMs)",
        "type": "AIModel"
      },
      {
        "name": "diffusion LLMs",
        "type": "AIModel"
      },
      {
        "name": "constrained decoding method for diffusion models",
        "type": "AIModel"
      },
      {
        "name": "C++ code infilling",
        "type": "Dataset"
      },
      {
        "name": "structured data extraction in JSON",
        "type": "Dataset"
      },
      {
        "name": "syntactic correctness",
        "type": "Metric"
      },
      {
        "name": "functional correctness",
        "type": "Metric"
      },
      {
        "name": "Masked Diffusion Language Models (MDLMs)",
        "type": "AIModel"
      },
      {
        "name": "Autoregressive Language Models (ARLMs)",
        "type": "AIModel"
      },
      {
        "name": "mask-agnostic loss function",
        "type": "AIModel"
      },
      {
        "name": "Large Language Model (LLM)-based agents",
        "type": "AIModel"
      },
      {
        "name": "open-sourced libraries and benchmarks",
        "type": "Dataset"
      },
      {
        "name": "self-evolving agent memory",
        "type": "Metric"
      },
      {
        "name": "Empirical-MCTS",
        "type": "AIModel"
      },
      {
        "name": "Pairwise-Experience-Evolutionary Meta-Prompting (PE-EMP)",
        "type": "AIModel"
      },
      {
        "name": "Memory Optimization Agent",
        "type": "AIModel"
      },
      {
        "name": "Monte Carlo Tree Search (MCTS)",
        "type": "AIModel"
      },
      {
        "name": "ARC-AGI-2",
        "type": "Dataset"
      },
      {
        "name": "MathArena Apex",
        "type": "Dataset"
      },
      {
        "name": "TAME",
        "type": "AIModel"
      },
      {
        "name": "Trust-Memevo",
        "type": "Dataset"
      },
      {
        "name": "trustworthiness",
        "type": "Metric"
      },
      {
        "name": "task performance",
        "type": "Metric"
      },
      {
        "name": "ProcMEM",
        "type": "AIModel"
      },
      {
        "name": "Non-Parametric PPO",
        "type": "AIModel"
      },
      {
        "name": "Skill-MDP",
        "type": "AIModel"
      },
      {
        "name": "in-domain, cross-task, and cross-agent scenarios",
        "type": "Dataset"
      },
      {
        "name": "reuse rates",
        "type": "Metric"
      },
      {
        "name": "performance gains",
        "type": "Metric"
      },
      {
        "name": "memory compression",
        "type": "Metric"
      },
      {
        "name": "agentic time series forecasting (ATSF)",
        "type": "AIModel"
      },
      {
        "name": "workflow-based design",
        "type": "AIModel"
      },
      {
        "name": "agentic reinforcement learning",
        "type": "AIModel"
      },
      {
        "name": "hybrid agentic workflow paradigm",
        "type": "AIModel"
      },
      {
        "name": "Follow-Your-Emoji-Faster",
        "type": "AIPaper"
      },
      {
        "name": "EmojiBench++",
        "type": "Dataset"
      },
      {
        "name": "animation quality",
        "type": "Metric"
      },
      {
        "name": "controllability",
        "type": "Metric"
      },
      {
        "name": "Follow-Your-Instruction",
        "type": "AIModel"
      },
      {
        "name": "MLLM-Collector",
        "type": "AIModel"
      },
      {
        "name": "MLLM-Generator",
        "type": "AIModel"
      },
      {
        "name": "MLLM-Optimizer",
        "type": "AIModel"
      },
      {
        "name": "MLLM-Planner",
        "type": "AIModel"
      },
      {
        "name": "existing baseline models",
        "type": "AIModel"
      },
      {
        "name": "2D, 3D, and 4D generative tasks",
        "type": "Dataset"
      },
      {
        "name": "HunyuanVideoT2V",
        "type": "AIModel"
      },
      {
        "name": "approximately 1M real video clips",
        "type": "Dataset"
      },
      {
        "name": "fewer than 150k curated editing pairs",
        "type": "Dataset"
      },
      {
        "name": "editing instruction following",
        "type": "Metric"
      },
      {
        "name": "editing quality",
        "type": "Metric"
      },
      {
        "name": "PaperTalker",
        "type": "AIModel"
      },
      {
        "name": "Paper2Video",
        "type": "Dataset"
      },
      {
        "name": "Meta Similarity",
        "type": "Metric"
      },
      {
        "name": "PresentArena",
        "type": "Metric"
      },
      {
        "name": "PresentQuiz",
        "type": "Metric"
      },
      {
        "name": "IP Memory",
        "type": "Metric"
      },
      {
        "name": "ContextFlow",
        "type": "AIPaper"
      },
      {
        "name": "Rectified Flow",
        "type": "AIModel"
      },
      {
        "name": "Guidance Responsiveness Metric",
        "type": "Metric"
      },
      {
        "name": "DeepSeek-R1",
        "type": "AIModel"
      },
      {
        "name": "chain-of-thought prompting",
        "type": "AIModel"
      },
      {
        "name": "conventional supervised learning on human demonstrations",
        "type": "AIModel"
      },
      {
        "name": "mathematics",
        "type": "Dataset"
      },
      {
        "name": "coding competitions",
        "type": "Dataset"
      },
      {
        "name": "STEM fields",
        "type": "Dataset"
      },
      {
        "name": "phi-3-mini",
        "type": "AIModel"
      },
      {
        "name": "Mixtral 8x7B",
        "type": "AIModel"
      },
      {
        "name": "GPT-3.5",
        "type": "AIModel"
      },
      {
        "name": "MT-bench",
        "type": "Metric"
      },
      {
        "name": "phi-2",
        "type": "AIModel"
      },
      {
        "name": "phi-3-small",
        "type": "AIModel"
      },
      {
        "name": "phi-3-medium",
        "type": "AIModel"
      },
      {
        "name": "phi-3.5-mini",
        "type": "AIModel"
      },
      {
        "name": "phi-3.5-MoE",
        "type": "AIModel"
      },
      {
        "name": "phi-3.5-Vision",
        "type": "AIModel"
      },
      {
        "name": "Llama 3.1",
        "type": "AIModel"
      },
      {
        "name": "Mixtral series",
        "type": "AIModel"
      },
      {
        "name": "Gemini-1.5-Flash",
        "type": "AIModel"
      },
      {
        "name": "LLaVA-OneVision",
        "type": "AIModel"
      },
      {
        "name": "LLaVA-NeXT blog series",
        "type": "AIPaper"
      },
      {
        "name": "single-image scenario",
        "type": "Dataset"
      },
      {
        "name": "multi-image scenario",
        "type": "Dataset"
      },
      {
        "name": "video scenario",
        "type": "Dataset"
      },
      {
        "name": "Multimodal large language models (MLLMs)",
        "type": "AIModel"
      },
      {
        "name": "existing benchmarks across text only, vision language, and embodied settings",
        "type": "Dataset"
      },
      {
        "name": "evaluation metrics and methodologies for assessing spatial reasoning ability",
        "type": "Metric"
      },
      {
        "name": "SpatialTree",
        "type": "AIModel"
      },
      {
        "name": "mainstream MLLMs",
        "type": "AIModel"
      },
      {
        "name": "capability-centric hierarchical benchmark",
        "type": "Dataset"
      },
      {
        "name": "27 sub-abilities",
        "type": "Metric"
      },
      {
        "name": "large-scale benchmark built from pedestrian-perspective videos captured with synchronized stereo cameras, LiDAR, and IMU/GPS sensors",
        "type": "Dataset"
      },
      {
        "name": "spatial reasoning questions",
        "type": "Metric"
      },
      {
        "name": "SpatialDreamer",
        "type": "AIModel"
      },
      {
        "name": "Geometric Policy Optimization (GeoPO)",
        "type": "AIModel"
      },
      {
        "name": "Multi-modal Large Language Models (MLLMs)",
        "type": "AIModel"
      },
      {
        "name": "multiple challenging benchmarks",
        "type": "Dataset"
      },
      {
        "name": "highly competitive results",
        "type": "Metric"
      },
      {
        "name": "Fast YOLO",
        "type": "AIModel"
      },
      {
        "name": "DPM",
        "type": "AIModel"
      },
      {
        "name": "Picasso Dataset",
        "type": "Dataset"
      },
      {
        "name": "People-Art Dataset",
        "type": "Dataset"
      },
      {
        "name": "mAP",
        "type": "Metric"
      },
      {
        "name": "Number GroundingDINO (NGDINO)",
        "type": "AIModel"
      },
      {
        "name": "RefDrone",
        "type": "Dataset"
      },
      {
        "name": "gRefCOCO",
        "type": "Dataset"
      },
      {
        "name": "RDAgent (referring drone annotation framework with multi-agent system)",
        "type": "AIModel"
      },
      {
        "name": "state-of-the-art REC methods",
        "type": "AIModel"
      },
      {
        "name": "WeDetect",
        "type": "AIModel"
      },
      {
        "name": "WeDetect-Uni",
        "type": "AIModel"
      },
      {
        "name": "WeDetect-Ref",
        "type": "AIModel"
      },
      {
        "name": "15 benchmarks",
        "type": "Dataset"
      },
      {
        "name": "v1",
        "type": "AIModel"
      },
      {
        "name": "multimodal mathematical reasoning benchmarks",
        "type": "Metric"
      },
      {
        "name": "PosterCopilot",
        "type": "AIModel"
      },
      {
        "name": "Large Multimodal Models (LMMs)",
        "type": "AIModel"
      },
      {
        "name": "LMM-based design model",
        "type": "AIModel"
      },
      {
        "name": "generative models",
        "type": "AIModel"
      },
      {
        "name": "Perturbed Supervised Fine-Tuning",
        "type": "AIModel"
      },
      {
        "name": "Reinforcement Learning for Visual-Reality Alignment",
        "type": "AIModel"
      },
      {
        "name": "Reinforcement Learning from Aesthetic Feedback",
        "type": "AIModel"
      },
      {
        "name": "existing methods",
        "type": "AIModel"
      },
      {
        "name": "Percept-WAM",
        "type": "AIModel"
      },
      {
        "name": "vision-language models",
        "type": "AIModel"
      },
      {
        "name": "VLA systems",
        "type": "AIModel"
      },
      {
        "name": "classical detectors and segmenters",
        "type": "AIModel"
      },
      {
        "name": "PMDS",
        "type": "Metric"
      },
      {
        "name": "Deep Q-Learning",
        "type": "AIModel"
      },
      {
        "name": "actor-critic, model-free algorithm based on the deterministic policy gradient",
        "type": "AIModel"
      },
      {
        "name": "simulated physics tasks",
        "type": "Dataset"
      },
      {
        "name": "cartpole swing-up",
        "type": "Dataset"
      },
      {
        "name": "dexterous manipulation",
        "type": "Dataset"
      },
      {
        "name": "legged locomotion",
        "type": "Dataset"
      },
      {
        "name": "car driving",
        "type": "Dataset"
      },
      {
        "name": "planning algorithm with full access to the dynamics of the domain and its derivatives",
        "type": "AIModel"
      },
      {
        "name": "Reinforcement Learning with Verifiable Rewards (RLVR)",
        "type": "AIModel"
      },
      {
        "name": "Qwen3-8B",
        "type": "AIModel"
      },
      {
        "name": "Qwen3-32B",
        "type": "AIModel"
      },
      {
        "name": "Qwen3-14B",
        "type": "AIModel"
      },
      {
        "name": "AIME'25",
        "type": "Dataset"
      },
      {
        "name": "AIME'24",
        "type": "Dataset"
      },
      {
        "name": "Clip-Cov",
        "type": "AIModel"
      },
      {
        "name": "KL-Cov",
        "type": "AIModel"
      },
      {
        "name": "Policy Gradient-like algorithms",
        "type": "AIModel"
      },
      {
        "name": "entropy",
        "type": "Metric"
      },
      {
        "name": "downstream performance",
        "type": "Metric"
      },
      {
        "name": "covariance",
        "type": "Metric"
      },
      {
        "name": "LUFFY",
        "type": "AIModel"
      },
      {
        "name": "RLVR",
        "type": "AIModel"
      },
      {
        "name": "Mixed-Policy GRPO",
        "type": "AIModel"
      },
      {
        "name": "six math benchmarks",
        "type": "Dataset"
      },
      {
        "name": "out-of-distribution tasks",
        "type": "Dataset"
      },
      {
        "name": "average gain",
        "type": "Metric"
      },
      {
        "name": "advantage",
        "type": "Metric"
      },
      {
        "name": "Fine-tuning",
        "type": "AIModel"
      },
      {
        "name": "Reinforcement learning",
        "type": "AIModel"
      },
      {
        "name": "Test-time scaling",
        "type": "AIModel"
      },
      {
        "name": "Model alignment",
        "type": "AIModel"
      },
      {
        "name": "Scalable adaptation",
        "type": "AIModel"
      },
      {
        "name": "Inference-time reasoning",
        "type": "AIModel"
      },
      {
        "name": "Pretraining",
        "type": "Dataset"
      },
      {
        "name": "Post-training",
        "type": "Dataset"
      },
      {
        "name": "Catastrophic forgetting",
        "type": "Metric"
      },
      {
        "name": "Reward hacking",
        "type": "Metric"
      },
      {
        "name": "Inference-time trade-offs",
        "type": "Metric"
      },
      {
        "name": "Tapered Off-Policy REINFORCE (TOPR)",
        "type": "AIModel"
      },
      {
        "name": "test-time accuracy",
        "type": "Metric"
      },
      {
        "name": "training data efficiency",
        "type": "Metric"
      },
      {
        "name": "REINFORCE",
        "type": "AIModel"
      },
      {
        "name": "Fully convolutional network",
        "type": "AIModel"
      },
      {
        "name": "VGG net",
        "type": "AIModel"
      },
      {
        "name": "NYUDv2",
        "type": "Dataset"
      },
      {
        "name": "SIFT Flow",
        "type": "Dataset"
      },
      {
        "name": "mean IU",
        "type": "Metric"
      },
      {
        "name": "denoising diffusion probabilistic models",
        "type": "AIModel"
      },
      {
        "name": "noise conditioned score networks",
        "type": "AIModel"
      },
      {
        "name": "stochastic differential equations",
        "type": "AIModel"
      },
      {
        "name": "variational auto-encoders",
        "type": "AIModel"
      },
      {
        "name": "generative adversarial networks",
        "type": "AIModel"
      },
      {
        "name": "energy-based models",
        "type": "AIModel"
      },
      {
        "name": "normalizing flows",
        "type": "AIModel"
      },
      {
        "name": "SegNeXt",
        "type": "AIPaper"
      },
      {
        "name": "EfficientNet-L2 w/ NAS-FPN",
        "type": "AIModel"
      },
      {
        "name": "ADE20K",
        "type": "Dataset"
      },
      {
        "name": "Cityscapes",
        "type": "Dataset"
      },
      {
        "name": "COCO-Stuff",
        "type": "Dataset"
      },
      {
        "name": "Pascal VOC",
        "type": "Dataset"
      },
      {
        "name": "Pascal Context",
        "type": "Dataset"
      },
      {
        "name": "iSAID",
        "type": "Dataset"
      },
      {
        "name": "mIoU",
        "type": "Metric"
      },
      {
        "name": "CMX",
        "type": "AIModel"
      },
      {
        "name": "Cross-Modal Feature Rectification Module (CM-FRM)",
        "type": "AIModel"
      },
      {
        "name": "Feature Fusion Module (FFM)",
        "type": "AIModel"
      },
      {
        "name": "depth",
        "type": "Dataset"
      },
      {
        "name": "thermal",
        "type": "Dataset"
      },
      {
        "name": "polarization",
        "type": "Dataset"
      },
      {
        "name": "event",
        "type": "Dataset"
      },
      {
        "name": "LiDAR",
        "type": "Dataset"
      },
      {
        "name": "RGB-Depth benchmarks",
        "type": "Dataset"
      },
      {
        "name": "RGB-Thermal",
        "type": "Dataset"
      },
      {
        "name": "RGB-Polarization",
        "type": "Dataset"
      },
      {
        "name": "RGB-LiDAR",
        "type": "Dataset"
      },
      {
        "name": "EventScape dataset",
        "type": "Dataset"
      },
      {
        "name": "RGB-Event semantic segmentation benchmark",
        "type": "Dataset"
      },
      {
        "name": "state-of-the-art performances",
        "type": "Metric"
      },
      {
        "name": "HQ-SAM",
        "type": "AIModel"
      },
      {
        "name": "dataset of 44K fine-grained masks",
        "type": "Dataset"
      },
      {
        "name": "suite of 10 diverse segmentation datasets",
        "type": "Dataset"
      },
      {
        "name": "PIDNet",
        "type": "AIModel"
      },
      {
        "name": "PIDNet-S",
        "type": "AIModel"
      },
      {
        "name": "CamVid",
        "type": "Dataset"
      },
      {
        "name": "mIOU",
        "type": "Metric"
      },
      {
        "name": "Qwen2 series",
        "type": "AIModel"
      },
      {
        "name": "Qwen2-72B",
        "type": "AIModel"
      },
      {
        "name": "Qwen2-72B-Instruct",
        "type": "AIModel"
      },
      {
        "name": "Qwen1.5",
        "type": "AIModel"
      },
      {
        "name": "MT-Bench",
        "type": "Dataset"
      },
      {
        "name": "Arena-Hard",
        "type": "Dataset"
      },
      {
        "name": "LiveCodeBench",
        "type": "Dataset"
      },
      {
        "name": "FSDrive",
        "type": "AIModel"
      },
      {
        "name": "DriveLM",
        "type": "Dataset"
      },
      {
        "name": "AutoVLA",
        "type": "AIModel"
      },
      {
        "name": "nuPlan",
        "type": "Dataset"
      },
      {
        "name": "Impromptu VLA",
        "type": "AIPaper"
      },
      {
        "name": "Impromptu VLA Dataset",
        "type": "Dataset"
      },
      {
        "name": "NeuroNCAP",
        "type": "Metric"
      },
      {
        "name": "collision rates",
        "type": "Metric"
      },
      {
        "name": "L2 accuracy",
        "type": "Metric"
      },
      {
        "name": "AgentThink",
        "type": "AIModel"
      },
      {
        "name": "DriveLMM-o1",
        "type": "Dataset"
      },
      {
        "name": "overall reasoning scores",
        "type": "Metric"
      },
      {
        "name": "answer accuracy",
        "type": "Metric"
      },
      {
        "name": "Drive-R1",
        "type": "AIModel"
      },
      {
        "name": "DriveLM-nuScenes",
        "type": "Dataset"
      },
      {
        "name": "IRL-VLA",
        "type": "AIModel"
      },
      {
        "name": "VLA architecture",
        "type": "AIModel"
      },
      {
        "name": "VLA policy",
        "type": "AIModel"
      },
      {
        "name": "reward world model",
        "type": "AIModel"
      },
      {
        "name": "PPO (Proximal Policy Optimization)",
        "type": "AIModel"
      },
      {
        "name": "NAVSIM v2 end-to-end driving benchmark",
        "type": "Dataset"
      },
      {
        "name": "CVPR2025 Autonomous Grand Challenge",
        "type": "Dataset"
      },
      {
        "name": "1st runner up",
        "type": "Metric"
      },
      {
        "name": "DriveVLA-W0",
        "type": "AIModel"
      },
      {
        "name": "autoregressive world model",
        "type": "AIModel"
      },
      {
        "name": "diffusion world model",
        "type": "AIModel"
      },
      {
        "name": "BEV",
        "type": "AIModel"
      },
      {
        "name": "VLA",
        "type": "AIModel"
      },
      {
        "name": "NAVSIM v1/v2 benchmark",
        "type": "Dataset"
      },
      {
        "name": "in-house dataset",
        "type": "Dataset"
      },
      {
        "name": "several established datasets",
        "type": "Dataset"
      },
      {
        "name": "GTRS (Generalized Trajectory Scoring)",
        "type": "AIModel"
      },
      {
        "name": "Navsim v2 Challenge",
        "type": "Dataset"
      },
      {
        "name": "3D Rasterization",
        "type": "AIModel"
      },
      {
        "name": "Rasterization Augmented Planning (RAP)",
        "type": "AIModel"
      },
      {
        "name": "Raster-to-Real feature-space alignment",
        "type": "AIModel"
      },
      {
        "name": "NAVSIM v1",
        "type": "Dataset"
      },
      {
        "name": "NAVSIM v2",
        "type": "Dataset"
      },
      {
        "name": "Waymo Open Dataset Vision-based E2E Driving",
        "type": "Dataset"
      },
      {
        "name": "closed-loop robustness",
        "type": "Metric"
      },
      {
        "name": "long-tail generalization",
        "type": "Metric"
      },
      {
        "name": "PRIX",
        "type": "AIModel"
      },
      {
        "name": "Context-aware Recalibration Transformer (CaRT)",
        "type": "AIModel"
      },
      {
        "name": "NavSim",
        "type": "Dataset"
      },
      {
        "name": "multimodal diffusion planners",
        "type": "AIModel"
      },
      {
        "name": "BLIP-2",
        "type": "AIModel"
      },
      {
        "name": "Flamingo80B",
        "type": "AIModel"
      },
      {
        "name": "VQAv2",
        "type": "Dataset"
      },
      {
        "name": "zero-shot VQAv2",
        "type": "Metric"
      },
      {
        "name": "Flow Matching (FM)",
        "type": "AIModel"
      },
      {
        "name": "Continuous Normalizing Flows (CNFs)",
        "type": "AIModel"
      },
      {
        "name": "likelihood",
        "type": "Metric"
      },
      {
        "name": "Diffusion Policy",
        "type": "AIModel"
      },
      {
        "name": "existing state-of-the-art robot learning methods",
        "type": "AIModel"
      },
      {
        "name": "4 different robot manipulation benchmarks",
        "type": "Dataset"
      },
      {
        "name": "average improvement of 46.9%",
        "type": "Metric"
      },
      {
        "name": "LLM-based robotic models",
        "type": "AIModel"
      },
      {
        "name": "robot control",
        "type": "Dataset"
      },
      {
        "name": "perception",
        "type": "Dataset"
      },
      {
        "name": "decision-making",
        "type": "Dataset"
      },
      {
        "name": "planning",
        "type": "Dataset"
      },
      {
        "name": "dexterity intelligence",
        "type": "Metric"
      },
      {
        "name": "human-robot interaction",
        "type": "Metric"
      },
      {
        "name": "autonomy",
        "type": "Metric"
      },
      {
        "name": "Raw2Drive",
        "type": "AIModel"
      },
      {
        "name": "CARLA Leaderboard 2.0",
        "type": "Dataset"
      },
      {
        "name": "Vision Language Action (VLA) models",
        "type": "AIModel"
      },
      {
        "name": "autoregression-based methods",
        "type": "AIModel"
      },
      {
        "name": "diffusion-based methods",
        "type": "AIModel"
      },
      {
        "name": "reinforcement-based methods",
        "type": "AIModel"
      },
      {
        "name": "hybrid methods",
        "type": "AIModel"
      },
      {
        "name": "specialized methods",
        "type": "AIModel"
      },
      {
        "name": "foundational datasets",
        "type": "Dataset"
      },
      {
        "name": "simulation platforms",
        "type": "Dataset"
      },
      {
        "name": "ReSim",
        "type": "AIModel"
      },
      {
        "name": "Video2Reward",
        "type": "AIModel"
      },
      {
        "name": "diffusion transformer architecture",
        "type": "AIModel"
      },
      {
        "name": "planning performance",
        "type": "Metric"
      },
      {
        "name": "policy selection performance",
        "type": "Metric"
      },
      {
        "name": "Llama 2-Chat",
        "type": "AIModel"
      },
      {
        "name": "open-source chat models",
        "type": "AIModel"
      },
      {
        "name": "closed-source models",
        "type": "AIModel"
      },
      {
        "name": "human evaluations for helpfulness and safety",
        "type": "Metric"
      },
      {
        "name": "AdaThinkDrive",
        "type": "AIPaper"
      },
      {
        "name": "Chain of Thought (CoT)",
        "type": "AIModel"
      },
      {
        "name": "Navsim",
        "type": "Dataset"
      },
      {
        "name": "never Think baseline",
        "type": "AIModel"
      },
      {
        "name": "always Think baseline",
        "type": "AIModel"
      },
      {
        "name": "vision only baseline",
        "type": "AIModel"
      },
      {
        "name": "UniV2X framework",
        "type": "AIModel"
      },
      {
        "name": "V2X-Seq-SPD dataset",
        "type": "Dataset"
      },
      {
        "name": "End-to-End Autonomous Driving through V2X Cooperation Challenge",
        "type": "Dataset"
      },
      {
        "name": "cooperative temporal perception",
        "type": "Metric"
      },
      {
        "name": "cooperative end-to-end planning",
        "type": "Metric"
      },
      {
        "name": "Multi-modal Large Models (MLMs)",
        "type": "AIModel"
      },
      {
        "name": "World Models (WMs)",
        "type": "AIModel"
      },
      {
        "name": "embodied robots",
        "type": "AIModel"
      },
      {
        "name": "simulators",
        "type": "AIModel"
      },
      {
        "name": "embodied perception",
        "type": "Dataset"
      },
      {
        "name": "embodied interaction",
        "type": "Dataset"
      },
      {
        "name": "embodied agent",
        "type": "Dataset"
      },
      {
        "name": "sim-to-real adaptation",
        "type": "Dataset"
      },
      {
        "name": "comprehensive datasets",
        "type": "Dataset"
      },
      {
        "name": "vision-language-action models (VLAs)",
        "type": "AIModel"
      },
      {
        "name": "VLA-based control policies",
        "type": "AIModel"
      },
      {
        "name": "high-level task planners",
        "type": "AIModel"
      },
      {
        "name": "datasets",
        "type": "Dataset"
      },
      {
        "name": "DriveDreamer4D",
        "type": "AIPaper"
      },
      {
        "name": "NeRF",
        "type": "AIModel"
      },
      {
        "name": "3DGS",
        "type": "AIModel"
      },
      {
        "name": "PVG",
        "type": "AIModel"
      },
      {
        "name": "S3Gaussian",
        "type": "AIModel"
      },
      {
        "name": "Deformable-GS",
        "type": "AIModel"
      },
      {
        "name": "4DGS",
        "type": "AIModel"
      },
      {
        "name": "NTA-IoU",
        "type": "Metric"
      },
      {
        "name": "PhyGenBench",
        "type": "Dataset"
      },
      {
        "name": "PhyGenEval",
        "type": "Metric"
      },
      {
        "name": "AgentSquare",
        "type": "AIModel"
      },
      {
        "name": "Modularized LLM Agent Search (MoLAS)",
        "type": "AIModel"
      },
      {
        "name": "six benchmarks",
        "type": "Dataset"
      },
      {
        "name": "performance gain",
        "type": "Metric"
      },
      {
        "name": "CityGPT",
        "type": "AIPaper"
      },
      {
        "name": "CityInstruction",
        "type": "Dataset"
      },
      {
        "name": "SWFT",
        "type": "AIModel"
      },
      {
        "name": "ChatGLM3-6B",
        "type": "AIModel"
      },
      {
        "name": "Llama3-8B",
        "type": "AIModel"
      },
      {
        "name": "Qwen2.5-7B",
        "type": "AIModel"
      },
      {
        "name": "CityEval",
        "type": "Metric"
      },
      {
        "name": "Epona",
        "type": "AIModel"
      },
      {
        "name": "RoBERTa",
        "type": "AIModel"
      },
      {
        "name": "RACE",
        "type": "Dataset"
      },
      {
        "name": "SQuAD",
        "type": "Dataset"
      },
      {
        "name": "DoCo",
        "type": "AIModel"
      },
      {
        "name": "Machine Unlearning (MU)",
        "type": "AIModel"
      },
      {
        "name": "Text-to-image diffusion models",
        "type": "AIModel"
      },
      {
        "name": "various instances, styles, and offensive concepts",
        "type": "Dataset"
      },
      {
        "name": "generalization",
        "type": "Metric"
      },
      {
        "name": "utility",
        "type": "Metric"
      },
      {
        "name": "Vision Foundation Models (VFMs)",
        "type": "AIModel"
      },
      {
        "name": "Vision-Language Pre-training (VLP) models",
        "type": "AIModel"
      },
      {
        "name": "Diffusion Models (DMs)",
        "type": "AIModel"
      },
      {
        "name": "large-model-based Agents",
        "type": "AIModel"
      },
      {
        "name": "commonly used datasets and benchmarks for safety research",
        "type": "Dataset"
      },
      {
        "name": "Vision Large Language Models (VLLMs)",
        "type": "AIModel"
      },
      {
        "name": "LLM-Pipeline",
        "type": "AIModel"
      },
      {
        "name": "benchmark datasets",
        "type": "Dataset"
      },
      {
        "name": "benchmark evaluations",
        "type": "Metric"
      },
      {
        "name": "evaluation methods for jailbreak",
        "type": "Metric"
      },
      {
        "name": "Video-SafetyBench",
        "type": "Dataset"
      },
      {
        "name": "RJScore",
        "type": "Metric"
      },
      {
        "name": "Large Vision-Language Models (LVLMs)",
        "type": "AIModel"
      },
      {
        "name": "GGL-Net",
        "type": "AIModel"
      },
      {
        "name": "gradient supplementary module (GSM)",
        "type": "AIModel"
      },
      {
        "name": "two-way guidance fusion module (TGFM)",
        "type": "AIModel"
      },
      {
        "name": "NUAA-SIRST dataset",
        "type": "Dataset"
      },
      {
        "name": "NUDT-SIRST dataset",
        "type": "Dataset"
      },
      {
        "name": "NN-RAG",
        "type": "AIModel"
      },
      {
        "name": "LEMUR",
        "type": "Dataset"
      },
      {
        "name": "structural uniqueness",
        "type": "Metric"
      },
      {
        "name": "novel network structures",
        "type": "Metric"
      },
      {
        "name": "CenterMamba-SAM",
        "type": "AIModel"
      },
      {
        "name": "CenterMamba encoder",
        "type": "AIModel"
      },
      {
        "name": "memory-driven structural prompt generator",
        "type": "AIModel"
      },
      {
        "name": "memory-augmented multi-scale decoder",
        "type": "AIModel"
      },
      {
        "name": "public benchmarks",
        "type": "Dataset"
      },
      {
        "name": "IMobileTransformer",
        "type": "AIModel"
      },
      {
        "name": "UAV-based RGB images",
        "type": "Dataset"
      },
      {
        "name": "RF-DETR",
        "type": "AIPaper"
      },
      {
        "name": "RF-DETR (nano)",
        "type": "AIModel"
      },
      {
        "name": "RF-DETR (2x-large)",
        "type": "AIModel"
      },
      {
        "name": "D-FINE (nano)",
        "type": "AIModel"
      },
      {
        "name": "GroundingDINO (tiny)",
        "type": "AIModel"
      },
      {
        "name": "Roboflow100-VL",
        "type": "Dataset"
      },
      {
        "name": "AP",
        "type": "Metric"
      },
      {
        "name": "FreeOrbit4D",
        "type": "AIPaper"
      },
      {
        "name": "object-centric multi-view diffusion model",
        "type": "AIModel"
      },
      {
        "name": "conditional video diffusion model",
        "type": "AIModel"
      },
      {
        "name": "monocular video",
        "type": "Dataset"
      },
      {
        "name": "redirected videos",
        "type": "Dataset"
      },
      {
        "name": "faithful redirected videos",
        "type": "Metric"
      },
      {
        "name": "NeoVerse",
        "type": "AIModel"
      },
      {
        "name": "standard reconstruction and generation benchmarks",
        "type": "Dataset"
      },
      {
        "name": "DiT-XL/2",
        "type": "AIModel"
      },
      {
        "name": "ImageNet 512x512",
        "type": "Dataset"
      },
      {
        "name": "Gflops",
        "type": "Metric"
      },
      {
        "name": "RoPE",
        "type": "AIModel"
      },
      {
        "name": "RoFormer",
        "type": "AIModel"
      },
      {
        "name": "long text classification benchmark datasets",
        "type": "Dataset"
      },
      {
        "name": "Driving World Model (DWM)",
        "type": "AIModel"
      },
      {
        "name": "mainstream simulators",
        "type": "Dataset"
      },
      {
        "name": "high-impact datasets",
        "type": "Dataset"
      },
      {
        "name": "various metrics",
        "type": "Metric"
      },
      {
        "name": "FlexMap",
        "type": "AIModel"
      },
      {
        "name": "multiple configurations",
        "type": "Dataset"
      },
      {
        "name": "MapAnything",
        "type": "AIModel"
      },
      {
        "name": "diverse datasets",
        "type": "Dataset"
      },
      {
        "name": "specialist feed-forward models",
        "type": "AIModel"
      },
      {
        "name": "Seedream 4.0",
        "type": "AIPaper"
      },
      {
        "name": "Seedream 4.5",
        "type": "AIModel"
      },
      {
        "name": "VLM model",
        "type": "AIModel"
      },
      {
        "name": "billions of text-image pairs",
        "type": "Dataset"
      },
      {
        "name": "T2I",
        "type": "Metric"
      },
      {
        "name": "multimodal image editing",
        "type": "Metric"
      },
      {
        "name": "Echo-4o",
        "type": "AIModel"
      },
      {
        "name": "Bagel",
        "type": "AIModel"
      },
      {
        "name": "OmniGen2",
        "type": "AIModel"
      },
      {
        "name": "BLIP3-o",
        "type": "AIModel"
      },
      {
        "name": "Echo-4o-Image",
        "type": "Dataset"
      },
      {
        "name": "GenEval++",
        "type": "Metric"
      },
      {
        "name": "Imagine-Bench",
        "type": "Metric"
      },
      {
        "name": "Lumina-DiMOO",
        "type": "AIModel"
      },
      {
        "name": "autoregressive (AR) or hybrid AR-Diffusion paradigms",
        "type": "AIModel"
      },
      {
        "name": "NextStep-1",
        "type": "AIModel"
      },
      {
        "name": "vector quantization (VQ)",
        "type": "AIModel"
      },
      {
        "name": "flow matching head",
        "type": "AIModel"
      },
      {
        "name": "text-to-image generation tasks",
        "type": "Dataset"
      },
      {
        "name": "high-fidelity image synthesis",
        "type": "Metric"
      },
      {
        "name": "image editing",
        "type": "Metric"
      },
      {
        "name": "InstructGPT",
        "type": "AIModel"
      },
      {
        "name": "prompt distribution",
        "type": "Dataset"
      },
      {
        "name": "public NLP datasets",
        "type": "Dataset"
      },
      {
        "name": "human evaluations",
        "type": "Metric"
      },
      {
        "name": "truthfulness",
        "type": "Metric"
      },
      {
        "name": "toxic output generation",
        "type": "Metric"
      },
      {
        "name": "Search-R1",
        "type": "AIPaper"
      },
      {
        "name": "Qwen2.5-3B",
        "type": "AIModel"
      },
      {
        "name": "seven question-answering datasets",
        "type": "Dataset"
      },
      {
        "name": "Med-PaLM",
        "type": "AIModel"
      },
      {
        "name": "Med-PaLM 2",
        "type": "AIModel"
      },
      {
        "name": "MedQA",
        "type": "Dataset"
      },
      {
        "name": "MedMCQA",
        "type": "Dataset"
      },
      {
        "name": "PubMedQA",
        "type": "Dataset"
      },
      {
        "name": "MMLU clinical topics",
        "type": "Dataset"
      },
      {
        "name": "score",
        "type": "Metric"
      },
      {
        "name": "evaluation metrics",
        "type": "Metric"
      },
      {
        "name": "Supervised fine-tuning (SFT)",
        "type": "AIModel"
      },
      {
        "name": "reinforcement learning (RL)",
        "type": "AIModel"
      },
      {
        "name": "GeneralPoints",
        "type": "Dataset"
      },
      {
        "name": "V-IRL",
        "type": "Dataset"
      },
      {
        "name": "outcome-based reward",
        "type": "Metric"
      },
      {
        "name": "EquiCSP",
        "type": "AIModel"
      },
      {
        "name": "accurate structures",
        "type": "Metric"
      },
      {
        "name": "faster convergence",
        "type": "Metric"
      },
      {
        "name": "WorldPlay",
        "type": "AIModel"
      },
      {
        "name": "existing techniques",
        "type": "AIModel"
      },
      {
        "name": "diverse scenes",
        "type": "Dataset"
      },
      {
        "name": "long-term geometric consistency",
        "type": "Metric"
      },
      {
        "name": "real-time speeds",
        "type": "Metric"
      },
      {
        "name": "24 FPS",
        "type": "Metric"
      },
      {
        "name": "720p video",
        "type": "Metric"
      },
      {
        "name": "O-Voxel",
        "type": "AIModel"
      },
      {
        "name": "Sparse Compression VAE",
        "type": "AIModel"
      },
      {
        "name": "diverse public 3D asset datasets",
        "type": "Dataset"
      },
      {
        "name": "JEPA-WMs",
        "type": "AIModel"
      },
      {
        "name": "DINO-WM",
        "type": "AIModel"
      },
      {
        "name": "V-JEPA-2-AC",
        "type": "AIModel"
      },
      {
        "name": "simulated environments",
        "type": "Dataset"
      },
      {
        "name": "real-world robotic data",
        "type": "Dataset"
      },
      {
        "name": "planning success",
        "type": "Metric"
      },
      {
        "name": "PLIT",
        "type": "AIModel"
      },
      {
        "name": "hierarchical VAE",
        "type": "AIModel"
      },
      {
        "name": "rate-distortion performance",
        "type": "Metric"
      },
      {
        "name": "STORM",
        "type": "AIModel"
      },
      {
        "name": "CogACT",
        "type": "AIModel"
      },
      {
        "name": "SimplerEnv manipulation benchmark",
        "type": "Dataset"
      },
      {
        "name": "average success rate",
        "type": "Metric"
      },
      {
        "name": "Frechet Video Distance",
        "type": "Metric"
      },
      {
        "name": "CUT3R (Continuous Updating Transformer for 3D Reconstruction)",
        "type": "AIModel"
      },
      {
        "name": "various 3D/4D tasks",
        "type": "Dataset"
      },
      {
        "name": "GEN3C",
        "type": "AIModel"
      },
      {
        "name": "sparse-view novel view synthesis",
        "type": "Dataset"
      },
      {
        "name": "driving scenes",
        "type": "Dataset"
      },
      {
        "name": "monocular dynamic video",
        "type": "Dataset"
      },
      {
        "name": "camera control",
        "type": "Metric"
      },
      {
        "name": "temporal 3D consistency",
        "type": "Metric"
      },
      {
        "name": "FLARE",
        "type": "AIModel"
      },
      {
        "name": "large-scale public datasets",
        "type": "Dataset"
      },
      {
        "name": "pose estimation",
        "type": "Metric"
      },
      {
        "name": "geometry reconstruction",
        "type": "Metric"
      },
      {
        "name": "novel view synthesis",
        "type": "Metric"
      },
      {
        "name": "UniDepthV2",
        "type": "AIModel"
      },
      {
        "name": "UniDepth",
        "type": "AIModel"
      },
      {
        "name": "ten depth datasets",
        "type": "Dataset"
      },
      {
        "name": "OpenEMMA",
        "type": "AIModel"
      },
      {
        "name": "Chain-of-Thought reasoning process",
        "type": "AIModel"
      },
      {
        "name": "PASCAL VOC dataset",
        "type": "Dataset"
      },
      {
        "name": "ILSVRC2013 detection dataset",
        "type": "Dataset"
      },
      {
        "name": "cloud-only framework",
        "type": "AIModel"
      },
      {
        "name": "crop yield prediction dataset",
        "type": "Dataset"
      },
      {
        "name": "response time",
        "type": "Metric"
      },
      {
        "name": "prior VLM-based IQA models",
        "type": "AIModel"
      },
      {
        "name": "Fully Convolutional Networks",
        "type": "AIModel"
      },
      {
        "name": "RGB-Thermal datasets",
        "type": "Dataset"
      },
      {
        "name": "RGB-Polarization datasets",
        "type": "Dataset"
      },
      {
        "name": "RGB-LiDAR datasets",
        "type": "Dataset"
      },
      {
        "name": "robot manipulation benchmarks",
        "type": "Dataset"
      }
    ],
    "triples": [
      {
        "head": "Ashish Vaswani",
        "relation": "author_of",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "Noam Shazeer",
        "relation": "author_of",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "Niki Parmar",
        "relation": "author_of",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "Jakob Uszkoreit",
        "relation": "author_of",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "Llion Jones",
        "relation": "author_of",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "Aidan N. Gomez",
        "relation": "author_of",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "Lukasz Kaiser",
        "relation": "author_of",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "Illia Polosukhin",
        "relation": "author_of",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "Kaiming He",
        "relation": "author_of",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Xiangyu Zhang",
        "relation": "author_of",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Shaoqing Ren",
        "relation": "author_of",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Jian Sun",
        "relation": "author_of",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Diederik P. Kingma",
        "relation": "author_of",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "Jimmy Ba",
        "relation": "author_of",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "Sepp Hochreiter",
        "relation": "author_of",
        "tail": "Long Short-Term Memory"
      },
      {
        "head": "J. Schmidhuber",
        "relation": "author_of",
        "tail": "Long Short-Term Memory"
      },
      {
        "head": "Nitish Srivastava",
        "relation": "author_of",
        "tail": "Dropout: a simple way to prevent neural networks from overfitting"
      },
      {
        "head": "Geoffrey E. Hinton",
        "relation": "author_of",
        "tail": "Dropout: a simple way to prevent neural networks from overfitting"
      },
      {
        "head": "A. Krizhevsky",
        "relation": "author_of",
        "tail": "Dropout: a simple way to prevent neural networks from overfitting"
      },
      {
        "head": "I. Sutskever",
        "relation": "author_of",
        "tail": "Dropout: a simple way to prevent neural networks from overfitting"
      },
      {
        "head": "R. Salakhutdinov",
        "relation": "author_of",
        "tail": "Dropout: a simple way to prevent neural networks from overfitting"
      },
      {
        "head": "Christian Szegedy",
        "relation": "author_of",
        "tail": "Rethinking the Inception Architecture for Computer Vision"
      },
      {
        "head": "Vincent Vanhoucke",
        "relation": "author_of",
        "tail": "Rethinking the Inception Architecture for Computer Vision"
      },
      {
        "head": "Sergey Ioffe",
        "relation": "author_of",
        "tail": "Rethinking the Inception Architecture for Computer Vision"
      },
      {
        "head": "Jonathon Shlens",
        "relation": "author_of",
        "tail": "Rethinking the Inception Architecture for Computer Vision"
      },
      {
        "head": "Zbigniew Wojna",
        "relation": "author_of",
        "tail": "Rethinking the Inception Architecture for Computer Vision"
      },
      {
        "head": "Shaina Raza",
        "relation": "author_of",
        "tail": "A comprehensive review of recommender systems: Transitioning from theory to practice"
      },
      {
        "head": "Mizanur Rahman",
        "relation": "author_of",
        "tail": "A comprehensive review of recommender systems: Transitioning from theory to practice"
      },
      {
        "head": "Safiullah Kamawal",
        "relation": "author_of",
        "tail": "A comprehensive review of recommender systems: Transitioning from theory to practice"
      },
      {
        "head": "Armin Toroghi",
        "relation": "author_of",
        "tail": "A comprehensive review of recommender systems: Transitioning from theory to practice"
      },
      {
        "head": "Ananya Raval",
        "relation": "author_of",
        "tail": "A comprehensive review of recommender systems: Transitioning from theory to practice"
      },
      {
        "head": "F. Navah",
        "relation": "author_of",
        "tail": "A comprehensive review of recommender systems: Transitioning from theory to practice"
      },
      {
        "head": "Amirmohammad Kazemeini",
        "relation": "author_of",
        "tail": "A comprehensive review of recommender systems: Transitioning from theory to practice"
      },
      {
        "head": "Zhuoran Yang",
        "relation": "author_of",
        "tail": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation"
      },
      {
        "head": "Xi Guo",
        "relation": "author_of",
        "tail": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation"
      },
      {
        "head": "Chenjing Ding",
        "relation": "author_of",
        "tail": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation"
      },
      {
        "head": "Chiyu Wang",
        "relation": "author_of",
        "tail": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation"
      },
      {
        "head": "Wei Wu",
        "relation": "author_of",
        "tail": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation"
      },
      {
        "head": "Yanyong Zhang",
        "relation": "author_of",
        "tail": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation"
      },
      {
        "head": "Zisheng Wang",
        "relation": "author_of",
        "tail": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection"
      },
      {
        "head": "Junjie Chen",
        "relation": "author_of",
        "tail": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection"
      },
      {
        "head": "Chisen Wang",
        "relation": "author_of",
        "tail": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection"
      },
      {
        "head": "Cong Peng",
        "relation": "author_of",
        "tail": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection"
      },
      {
        "head": "Jianping Xuan",
        "relation": "author_of",
        "tail": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection"
      },
      {
        "head": "Tielin Shi",
        "relation": "author_of",
        "tail": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection"
      },
      {
        "head": "Ming J. Zuo",
        "relation": "author_of",
        "tail": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection"
      },
      {
        "head": "Jinghuan Zhang",
        "relation": "author_of",
        "tail": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation"
      },
      {
        "head": "Wang Chen",
        "relation": "author_of",
        "tail": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation"
      },
      {
        "head": "Jian Zhang",
        "relation": "author_of",
        "tail": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation"
      },
      {
        "head": "Manlin Zhang",
        "relation": "author_of",
        "tail": "DiffusionEngine: Diffusion model is scalable data engine for object detection"
      },
      {
        "head": "Jie Wu",
        "relation": "author_of",
        "tail": "DiffusionEngine: Diffusion model is scalable data engine for object detection"
      },
      {
        "head": "Yuxi Ren",
        "relation": "author_of",
        "tail": "DiffusionEngine: Diffusion model is scalable data engine for object detection"
      },
      {
        "head": "Jiahong Yang",
        "relation": "author_of",
        "tail": "DiffusionEngine: Diffusion model is scalable data engine for object detection"
      },
      {
        "head": "Ming Li",
        "relation": "author_of",
        "tail": "DiffusionEngine: Diffusion model is scalable data engine for object detection"
      },
      {
        "head": "Andy J. Ma",
        "relation": "author_of",
        "tail": "DiffusionEngine: Diffusion model is scalable data engine for object detection"
      },
      {
        "head": "A. Krizhevsky",
        "relation": "author_of",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "I. Sutskever",
        "relation": "author_of",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "Geoffrey E. Hinton",
        "relation": "author_of",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "Karen Simonyan",
        "relation": "author_of",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "Andrew Zisserman",
        "relation": "author_of",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "P. Cochat",
        "relation": "author_of",
        "tail": "Et al"
      },
      {
        "head": "L. Vaucoret",
        "relation": "author_of",
        "tail": "Et al"
      },
      {
        "head": "J. Sarles",
        "relation": "author_of",
        "tail": "Et al"
      },
      {
        "head": "Shaoqing Ren",
        "relation": "author_of",
        "tail": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
      },
      {
        "head": "Kaiming He",
        "relation": "author_of",
        "tail": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
      },
      {
        "head": "Ross Girshick",
        "relation": "author_of",
        "tail": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
      },
      {
        "head": "Jian Sun",
        "relation": "author_of",
        "tail": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
      },
      {
        "head": "Yuqi Cheng",
        "relation": "author_of",
        "tail": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects"
      },
      {
        "head": "Yunkang Cao",
        "relation": "author_of",
        "tail": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects"
      },
      {
        "head": "Haiming Yao",
        "relation": "author_of",
        "tail": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects"
      },
      {
        "head": "Wei Luo",
        "relation": "author_of",
        "tail": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects"
      },
      {
        "head": "Cheng Jiang",
        "relation": "author_of",
        "tail": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects"
      },
      {
        "head": "Hui Zhang",
        "relation": "author_of",
        "tail": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects"
      },
      {
        "head": "Weiming Shen",
        "relation": "author_of",
        "tail": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects"
      },
      {
        "head": "Naveen Kumar Srinivasa",
        "relation": "author_of",
        "tail": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning"
      },
      {
        "head": "Ajeet Rao Chalamala",
        "relation": "author_of",
        "tail": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning"
      },
      {
        "head": "Kumar Singh",
        "relation": "author_of",
        "tail": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning"
      },
      {
        "head": "Ieee Krishna Mohan Senior Member",
        "relation": "author_of",
        "tail": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning"
      },
      {
        "head": "K. Naveen",
        "relation": "author_of",
        "tail": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning"
      },
      {
        "head": "Srinivasa Rao",
        "relation": "author_of",
        "tail": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning"
      },
      {
        "head": "Ajeet Kumar Singh",
        "relation": "author_of",
        "tail": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning"
      },
      {
        "head": "Hongbo Jiang",
        "relation": "author_of",
        "tail": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID"
      },
      {
        "head": "Lei Ye",
        "relation": "author_of",
        "tail": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID"
      },
      {
        "head": "Jingyang Hu",
        "relation": "author_of",
        "tail": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID"
      },
      {
        "head": "Xiaotian Chen",
        "relation": "author_of",
        "tail": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID"
      },
      {
        "head": "Siyu Chen",
        "relation": "author_of",
        "tail": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID"
      },
      {
        "head": "Wei Zhang",
        "relation": "author_of",
        "tail": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID"
      },
      {
        "head": "Kehua Yang",
        "relation": "author_of",
        "tail": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID"
      },
      {
        "head": "Jingya Wang",
        "relation": "author_of",
        "tail": "Noise Perception Self-Supervised Learning for Unsupervised Person Re-Identification"
      },
      {
        "head": "Jianfeng Wen",
        "relation": "author_of",
        "tail": "Noise Perception Self-Supervised Learning for Unsupervised Person Re-Identification"
      },
      {
        "head": "Weiping Ding",
        "relation": "author_of",
        "tail": "Noise Perception Self-Supervised Learning for Unsupervised Person Re-Identification"
      },
      {
        "head": "Chunlin Yu",
        "relation": "author_of",
        "tail": "Noise Perception Self-Supervised Learning for Unsupervised Person Re-Identification"
      },
      {
        "head": "Xiatian Zhu",
        "relation": "author_of",
        "tail": "Noise Perception Self-Supervised Learning for Unsupervised Person Re-Identification"
      },
      {
        "head": "Zhiyong Wang",
        "relation": "author_of",
        "tail": "Noise Perception Self-Supervised Learning for Unsupervised Person Re-Identification"
      },
      {
        "head": "Diederik P Kingma",
        "relation": "author_of",
        "tail": "Auto-Encoding Variational Bayes"
      },
      {
        "head": "Max Welling",
        "relation": "author_of",
        "tail": "Auto-Encoding Variational Bayes"
      },
      {
        "head": "Geoffrey E. Hinton",
        "relation": "author_of",
        "tail": "Supporting Online Material for Reducing the Dimensionality of Data with Neural Networks"
      },
      {
        "head": "R. Salakhutdinov",
        "relation": "author_of",
        "tail": "Supporting Online Material for Reducing the Dimensionality of Data with Neural Networks"
      },
      {
        "head": "John C. Duchi",
        "relation": "author_of",
        "tail": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"
      },
      {
        "head": "Elad Hazan",
        "relation": "author_of",
        "tail": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"
      },
      {
        "head": "Y. Singer",
        "relation": "author_of",
        "tail": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"
      },
      {
        "head": "Alex Graves",
        "relation": "author_of",
        "tail": "Speech recognition with deep recurrent neural networks"
      },
      {
        "head": "Abdel-rahman Mohamed",
        "relation": "author_of",
        "tail": "Speech recognition with deep recurrent neural networks"
      },
      {
        "head": "Geoffrey Hinton",
        "relation": "author_of",
        "tail": "Speech recognition with deep recurrent neural networks"
      },
      {
        "head": "Nian Wang",
        "relation": "author_of",
        "tail": "Weakly Supervised Image Dehazing via Physics-Based Decomposition"
      },
      {
        "head": "Zhigao Cui",
        "relation": "author_of",
        "tail": "Weakly Supervised Image Dehazing via Physics-Based Decomposition"
      },
      {
        "head": "Yanzhao Su",
        "relation": "author_of",
        "tail": "Weakly Supervised Image Dehazing via Physics-Based Decomposition"
      },
      {
        "head": "Yunwei Lan",
        "relation": "author_of",
        "tail": "Weakly Supervised Image Dehazing via Physics-Based Decomposition"
      },
      {
        "head": "Yuanliang Xue",
        "relation": "author_of",
        "tail": "Weakly Supervised Image Dehazing via Physics-Based Decomposition"
      },
      {
        "head": "Cong Zhang",
        "relation": "author_of",
        "tail": "Weakly Supervised Image Dehazing via Physics-Based Decomposition"
      },
      {
        "head": "Aihua Li",
        "relation": "author_of",
        "tail": "Weakly Supervised Image Dehazing via Physics-Based Decomposition"
      },
      {
        "head": "Leong Kah Meng",
        "relation": "author_of",
        "tail": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer"
      },
      {
        "head": "Ho Hooi Yi",
        "relation": "author_of",
        "tail": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer"
      },
      {
        "head": "Ng Bo Wei",
        "relation": "author_of",
        "tail": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer"
      },
      {
        "head": "Lim Jia Xin",
        "relation": "author_of",
        "tail": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer"
      },
      {
        "head": "Zailan Arabee Abdul Salam",
        "relation": "author_of",
        "tail": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer"
      },
      {
        "head": "Xin Cheng",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Wangding Zeng",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Damai Dai",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Qinyu Chen",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Bingxuan Wang",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Zhenda Xie",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Kezhao Huang",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Xingkai Yu",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Zhewen Hao",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Yukun Li",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Han Zhang",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Huishuai Zhang",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Dongyan Zhao",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Wenfeng Liang",
        "relation": "author_of",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Mostafa Saberian",
        "relation": "author_of",
        "tail": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction"
      },
      {
        "head": "Vidya Samadi",
        "relation": "author_of",
        "tail": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction"
      },
      {
        "head": "Ioana Popescu",
        "relation": "author_of",
        "tail": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction"
      },
      {
        "head": "Husheng Fang",
        "relation": "author_of",
        "tail": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model"
      },
      {
        "head": "Shunlin Liang",
        "relation": "author_of",
        "tail": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model"
      },
      {
        "head": "Wenyuan Li",
        "relation": "author_of",
        "tail": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model"
      },
      {
        "head": "Yongzhe Chen",
        "relation": "author_of",
        "tail": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model"
      },
      {
        "head": "Han Ma",
        "relation": "author_of",
        "tail": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model"
      },
      {
        "head": "Jianglei Xu",
        "relation": "author_of",
        "tail": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model"
      },
      {
        "head": "Yichuan Ma",
        "relation": "author_of",
        "tail": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model"
      },
      {
        "head": "Tao He",
        "relation": "author_of",
        "tail": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model"
      },
      {
        "head": "Feng Tian",
        "relation": "author_of",
        "tail": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model"
      },
      {
        "head": "Fengjiao Zhang",
        "relation": "author_of",
        "tail": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model"
      },
      {
        "head": "Hui Liang",
        "relation": "author_of",
        "tail": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model"
      },
      {
        "head": "Christian Szegedy",
        "relation": "author_of",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "Wei Liu",
        "relation": "author_of",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "Yangqing Jia",
        "relation": "author_of",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "Pierre Sermanet",
        "relation": "author_of",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "Scott Reed",
        "relation": "author_of",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "Dragomir Anguelov",
        "relation": "author_of",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "Dumitru Erhan",
        "relation": "author_of",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "Vincent Vanhoucke",
        "relation": "author_of",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "Andrew Rabinovich",
        "relation": "author_of",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "Sergey Ioffe",
        "relation": "author_of",
        "tail": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
      },
      {
        "head": "Christian Szegedy",
        "relation": "author_of",
        "tail": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
      },
      {
        "head": "Olga Russakovsky",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Jia Deng",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Hao Su",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Jonathan Krause",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Sanjeev Satheesh",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Sean Ma",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Zhiheng Huang",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Andrej Karpathy",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Aditya Khosla",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Michael Bernstein",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Alexander C. Berg",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Li Fei-Fei",
        "relation": "author_of",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Boyang Zheng",
        "relation": "author_of",
        "tail": "Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "Nanye Ma",
        "relation": "author_of",
        "tail": "Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "Shengbang Tong",
        "relation": "author_of",
        "tail": "Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "Saining Xie",
        "relation": "author_of",
        "tail": "Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "S. Rizvi",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Daniel Levine",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Aakash Patel",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Shiyang Zhang",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Eric Wang",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Curtis Jamison Perry",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Ivan Vrkic",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Nicole Mayerli Constante",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Zirui Fu",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Sizhuang He",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "David Zhang",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Cerise Tang",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Zhuoyang Lyu",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Rayyan Y Darji",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Chang Li",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Emily Sun",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "David Jeong",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Lawrence Zhao",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Jennifer Kwan",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "David Braun",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Brian Hafler",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Hattie Chung",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "R. M. Dhodapkar",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Paul F. Jaeger",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Bryan Perozzi",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Jeffrey Ishizuka",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Shekoofeh Azizi",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "D. van Dijk",
        "relation": "author_of",
        "tail": "Scaling Large Language Models for Next-Generation Single-Cell Analysis"
      },
      {
        "head": "Zhengyu Zhao",
        "relation": "author_of",
        "tail": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights"
      },
      {
        "head": "Hanwei Zhang",
        "relation": "author_of",
        "tail": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights"
      },
      {
        "head": "Renjue Li",
        "relation": "author_of",
        "tail": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights"
      },
      {
        "head": "R. Sicre",
        "relation": "author_of",
        "tail": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights"
      },
      {
        "head": "L. Amsaleg",
        "relation": "author_of",
        "tail": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights"
      },
      {
        "head": "Michael Backes",
        "relation": "author_of",
        "tail": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights"
      },
      {
        "head": "Qi Li",
        "relation": "author_of",
        "tail": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights"
      },
      {
        "head": "Chao Shen",
        "relation": "author_of",
        "tail": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights"
      },
      {
        "head": "Şafak Kılıç",
        "relation": "author_of",
        "tail": "HybridVisionNet: An advanced hybrid deep learning framework for automated multi-class ocular disease diagnosis using fundus imaging"
      },
      {
        "head": "Yifei Ge",
        "relation": "author_of",
        "tail": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot"
      },
      {
        "head": "Zhuo Li",
        "relation": "author_of",
        "tail": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot"
      },
      {
        "head": "Xuebin Yue",
        "relation": "author_of",
        "tail": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot"
      },
      {
        "head": "Hengyi Li",
        "relation": "author_of",
        "tail": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot"
      },
      {
        "head": "Lin Meng",
        "relation": "author_of",
        "tail": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot"
      },
      {
        "head": "Ashish Vaswani",
        "relation": "author_of",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Noam Shazeer",
        "relation": "author_of",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Niki Parmar",
        "relation": "author_of",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Jakob Uszkoreit",
        "relation": "author_of",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Llion Jones",
        "relation": "author_of",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Aidan N. Gomez",
        "relation": "author_of",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Lukasz Kaiser",
        "relation": "author_of",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Illia Polosukhin",
        "relation": "author_of",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Colin Raffel",
        "relation": "author_of",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "Noam Shazeer",
        "relation": "author_of",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "Adam Roberts",
        "relation": "author_of",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "Katherine Lee",
        "relation": "author_of",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "Sharan Narang",
        "relation": "author_of",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "Michael Matena",
        "relation": "author_of",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "Yanqi Zhou",
        "relation": "author_of",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "Wei Li",
        "relation": "author_of",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "Peter J. Liu",
        "relation": "author_of",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "M. Heusel",
        "relation": "author_of",
        "tail": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"
      },
      {
        "head": "Hubert Ramsauer",
        "relation": "author_of",
        "tail": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"
      },
      {
        "head": "Thomas Unterthiner",
        "relation": "author_of",
        "tail": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"
      },
      {
        "head": "Bernhard Nessler",
        "relation": "author_of",
        "tail": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"
      },
      {
        "head": "Sepp Hochreiter",
        "relation": "author_of",
        "tail": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"
      },
      {
        "head": "Holger Caesar",
        "relation": "author_of",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "Varun Bankiti",
        "relation": "author_of",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "Alex H. Lang",
        "relation": "author_of",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "Sourabh Vora",
        "relation": "author_of",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "Venice Erin Liong",
        "relation": "author_of",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "Qiang Xu",
        "relation": "author_of",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "Anush Krishnan",
        "relation": "author_of",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "Yu Pan",
        "relation": "author_of",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "Giancarlo Baldan",
        "relation": "author_of",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "Oscar Beijbom",
        "relation": "author_of",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "Alexey Dosovitskiy",
        "relation": "author_of",
        "tail": "CARLA: An Open Urban Driving Simulator"
      },
      {
        "head": "German Ros",
        "relation": "author_of",
        "tail": "CARLA: An Open Urban Driving Simulator"
      },
      {
        "head": "Felipe Codevilla",
        "relation": "author_of",
        "tail": "CARLA: An Open Urban Driving Simulator"
      },
      {
        "head": "Antonio Lopez",
        "relation": "author_of",
        "tail": "CARLA: An Open Urban Driving Simulator"
      },
      {
        "head": "Vladlen Koltun",
        "relation": "author_of",
        "tail": "CARLA: An Open Urban Driving Simulator"
      },
      {
        "head": "Fachrina Dewi Puspitasari",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Chaoning Zhang",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Joseph Cho",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Adnan Haider",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Noor Ul Eman",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Omer Amin",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Alexis Mankowski",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Muhammad Umair",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Jingyao Zheng",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Sheng Zheng",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Lik-Hang Lee",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Caiyan Qin",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Tae-Ho Kim",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Choong Seon Hong",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Yang Yang",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Heng Tao Shen",
        "relation": "author_of",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Bohan Li",
        "relation": "author_of",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "Zhuang Ma",
        "relation": "author_of",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "Dalong Du",
        "relation": "author_of",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "Baorui Peng",
        "relation": "author_of",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "Zhujin Liang",
        "relation": "author_of",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "Zhenqiang Liu",
        "relation": "author_of",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "Chao Ma",
        "relation": "author_of",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "Yueming Jin",
        "relation": "author_of",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "Hao Zhao",
        "relation": "author_of",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "Wenjun Zeng",
        "relation": "author_of",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "Xin Jin",
        "relation": "author_of",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "Zhuoran Yang",
        "relation": "author_of",
        "tail": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask"
      },
      {
        "head": "Yanyong Zhang",
        "relation": "author_of",
        "tail": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask"
      },
      {
        "head": "Guosheng Zhao",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Yaozeng Wang",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Xiaofeng Wang",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Zheng Zhu",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Tingdong Yu",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Guan Huang",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Yongchen Zai",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Ji Jiao",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Changliang Xue",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Xiaole Wang",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Zhen Yang",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Futang Zhu",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Xingang Wang",
        "relation": "author_of",
        "tail": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving"
      },
      {
        "head": "Ahmad Rahimi",
        "relation": "author_of",
        "tail": "MAD: Motion Appearance Decoupling for efficient Driving World Models"
      },
      {
        "head": "Valentin Gerard",
        "relation": "author_of",
        "tail": "MAD: Motion Appearance Decoupling for efficient Driving World Models"
      },
      {
        "head": "Eloi Zablocki",
        "relation": "author_of",
        "tail": "MAD: Motion Appearance Decoupling for efficient Driving World Models"
      },
      {
        "head": "Matthieu Cord",
        "relation": "author_of",
        "tail": "MAD: Motion Appearance Decoupling for efficient Driving World Models"
      },
      {
        "head": "Alexandre Alahi",
        "relation": "author_of",
        "tail": "MAD: Motion Appearance Decoupling for efficient Driving World Models"
      },
      {
        "head": "W. Marsden",
        "relation": "author_of",
        "tail": "I and J"
      },
      {
        "head": "Jia Deng",
        "relation": "author_of",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "Wei Dong",
        "relation": "author_of",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "R. Socher",
        "relation": "author_of",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "Li-Jia Li",
        "relation": "author_of",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "K. Li",
        "relation": "author_of",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "Li Fei-Fei",
        "relation": "author_of",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "R. Stephenson",
        "relation": "author_of",
        "tail": "A and V"
      },
      {
        "head": "Jaskirat Singh",
        "relation": "author_of",
        "tail": "What matters for Representation Alignment: Global Information or Spatial Structure?"
      },
      {
        "head": "Xingjian Leng",
        "relation": "author_of",
        "tail": "What matters for Representation Alignment: Global Information or Spatial Structure?"
      },
      {
        "head": "Zongze Wu",
        "relation": "author_of",
        "tail": "What matters for Representation Alignment: Global Information or Spatial Structure?"
      },
      {
        "head": "Liang Zheng",
        "relation": "author_of",
        "tail": "What matters for Representation Alignment: Global Information or Spatial Structure?"
      },
      {
        "head": "Richard Zhang",
        "relation": "author_of",
        "tail": "What matters for Representation Alignment: Global Information or Spatial Structure?"
      },
      {
        "head": "Eli Shechtman",
        "relation": "author_of",
        "tail": "What matters for Representation Alignment: Global Information or Spatial Structure?"
      },
      {
        "head": "Saining Xie",
        "relation": "author_of",
        "tail": "What matters for Representation Alignment: Global Information or Spatial Structure?"
      },
      {
        "head": "Zhifeng Wang",
        "relation": "author_of",
        "tail": "SCB-DETR: Multiscale Deformable Transformers for Occlusion-Resilient Student Learning Behavior Detection in Smart Classroom"
      },
      {
        "head": "Minghui Wang",
        "relation": "author_of",
        "tail": "SCB-DETR: Multiscale Deformable Transformers for Occlusion-Resilient Student Learning Behavior Detection in Smart Classroom"
      },
      {
        "head": "Chunyan Zeng",
        "relation": "author_of",
        "tail": "SCB-DETR: Multiscale Deformable Transformers for Occlusion-Resilient Student Learning Behavior Detection in Smart Classroom"
      },
      {
        "head": "Longlong Li",
        "relation": "author_of",
        "tail": "SCB-DETR: Multiscale Deformable Transformers for Occlusion-Resilient Student Learning Behavior Detection in Smart Classroom"
      },
      {
        "head": "Ehsan Zakeri",
        "relation": "author_of",
        "tail": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination"
      },
      {
        "head": "Amanda Spilkin",
        "relation": "author_of",
        "tail": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination"
      },
      {
        "head": "Hanae Elmekki",
        "relation": "author_of",
        "tail": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination"
      },
      {
        "head": "Antonela Zanuttini",
        "relation": "author_of",
        "tail": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination"
      },
      {
        "head": "L. Kadem",
        "relation": "author_of",
        "tail": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination"
      },
      {
        "head": "Jamal Bentahar",
        "relation": "author_of",
        "tail": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination"
      },
      {
        "head": "Wen-Fang Xie",
        "relation": "author_of",
        "tail": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination"
      },
      {
        "head": "Philippe Pibarot",
        "relation": "author_of",
        "tail": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination"
      },
      {
        "head": "Ana Davila",
        "relation": "author_of",
        "tail": "Bio-Inspired Fine-Tuning for Selective Transfer Learning in Image Classification"
      },
      {
        "head": "Jacinto Colan",
        "relation": "author_of",
        "tail": "Bio-Inspired Fine-Tuning for Selective Transfer Learning in Image Classification"
      },
      {
        "head": "Yasuhisa Hasegawa",
        "relation": "author_of",
        "tail": "Bio-Inspired Fine-Tuning for Selective Transfer Learning in Image Classification"
      },
      {
        "head": "Subham Sharma",
        "relation": "author_of",
        "tail": "Hand Sign Language Detection Using Deep Learning"
      },
      {
        "head": "Sharmila Subudhi",
        "relation": "author_of",
        "tail": "Hand Sign Language Detection Using Deep Learning"
      },
      {
        "head": "Tsung-Yi Lin",
        "relation": "author_of",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Michael Maire",
        "relation": "author_of",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Serge Belongie",
        "relation": "author_of",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Lubomir Bourdev",
        "relation": "author_of",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Ross Girshick",
        "relation": "author_of",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "James Hays",
        "relation": "author_of",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Pietro Perona",
        "relation": "author_of",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Deva Ramanan",
        "relation": "author_of",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "C. Lawrence Zitnick",
        "relation": "author_of",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Piotr Dollár",
        "relation": "author_of",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Zixiao Wen",
        "relation": "author_of",
        "tail": "FANet: Frequency-Aware Attention-Based Tiny-Object Detection in Remote Sensing Images"
      },
      {
        "head": "Peifeng Li",
        "relation": "author_of",
        "tail": "FANet: Frequency-Aware Attention-Based Tiny-Object Detection in Remote Sensing Images"
      },
      {
        "head": "Yuhan Liu",
        "relation": "author_of",
        "tail": "FANet: Frequency-Aware Attention-Based Tiny-Object Detection in Remote Sensing Images"
      },
      {
        "head": "Jingming Chen",
        "relation": "author_of",
        "tail": "FANet: Frequency-Aware Attention-Based Tiny-Object Detection in Remote Sensing Images"
      },
      {
        "head": "Xiantai Xiang",
        "relation": "author_of",
        "tail": "FANet: Frequency-Aware Attention-Based Tiny-Object Detection in Remote Sensing Images"
      },
      {
        "head": "Yuan Li",
        "relation": "author_of",
        "tail": "FANet: Frequency-Aware Attention-Based Tiny-Object Detection in Remote Sensing Images"
      },
      {
        "head": "Huixian Wang",
        "relation": "author_of",
        "tail": "FANet: Frequency-Aware Attention-Based Tiny-Object Detection in Remote Sensing Images"
      },
      {
        "head": "Yongchao Zhao",
        "relation": "author_of",
        "tail": "FANet: Frequency-Aware Attention-Based Tiny-Object Detection in Remote Sensing Images"
      },
      {
        "head": "Guangyao Zhou",
        "relation": "author_of",
        "tail": "FANet: Frequency-Aware Attention-Based Tiny-Object Detection in Remote Sensing Images"
      },
      {
        "head": "Yu Gu",
        "relation": "author_of",
        "tail": "UAV-based multimodal object detection via feature enhancement and dynamic gated fusion"
      },
      {
        "head": "Weili Chen",
        "relation": "author_of",
        "tail": "UAV-based multimodal object detection via feature enhancement and dynamic gated fusion"
      },
      {
        "head": "Dongliang Peng",
        "relation": "author_of",
        "tail": "UAV-based multimodal object detection via feature enhancement and dynamic gated fusion"
      },
      {
        "head": "Xiaohui Yuan",
        "relation": "author_of",
        "tail": "An empirical analysis of deep learning methods for small object detection from satellite imagery"
      },
      {
        "head": "Aniv Chakravarty",
        "relation": "author_of",
        "tail": "An empirical analysis of deep learning methods for small object detection from satellite imagery"
      },
      {
        "head": "Elinor M. Lichtenberg",
        "relation": "author_of",
        "tail": "An empirical analysis of deep learning methods for small object detection from satellite imagery"
      },
      {
        "head": "Lichuan Gu",
        "relation": "author_of",
        "tail": "An empirical analysis of deep learning methods for small object detection from satellite imagery"
      },
      {
        "head": "Zhenchun Wei",
        "relation": "author_of",
        "tail": "An empirical analysis of deep learning methods for small object detection from satellite imagery"
      },
      {
        "head": "Tian Chen",
        "relation": "author_of",
        "tail": "An empirical analysis of deep learning methods for small object detection from satellite imagery"
      },
      {
        "head": "Yoshua Bengio",
        "relation": "author_of",
        "tail": "Representation Learning: A Review and New Perspectives"
      },
      {
        "head": "Aaron Courville",
        "relation": "author_of",
        "tail": "Representation Learning: A Review and New Perspectives"
      },
      {
        "head": "Pascal Vincent",
        "relation": "author_of",
        "tail": "Representation Learning: A Review and New Perspectives"
      },
      {
        "head": "Pascal Vincent",
        "relation": "author_of",
        "tail": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion"
      },
      {
        "head": "H. Larochelle",
        "relation": "author_of",
        "tail": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion"
      },
      {
        "head": "Isabelle Lajoie",
        "relation": "author_of",
        "tail": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion"
      },
      {
        "head": "Yoshua Bengio",
        "relation": "author_of",
        "tail": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion"
      },
      {
        "head": "Pierre-Antoine Manzagol",
        "relation": "author_of",
        "tail": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion"
      },
      {
        "head": "D. Touretzky",
        "relation": "author_of",
        "tail": "In Advances in Neural Information Processing Systems"
      },
      {
        "head": "M. C. Mozer",
        "relation": "author_of",
        "tail": "In Advances in Neural Information Processing Systems"
      },
      {
        "head": "M. E. Hasselmo",
        "relation": "author_of",
        "tail": "In Advances in Neural Information Processing Systems"
      },
      {
        "head": "RegressionChristopher",
        "relation": "author_of",
        "tail": "In Advances in Neural Information Processing Systems"
      },
      {
        "head": "I. K.",
        "relation": "author_of",
        "tail": "In Advances in Neural Information Processing Systems"
      },
      {
        "head": "WilliamsNeural",
        "relation": "author_of",
        "tail": "In Advances in Neural Information Processing Systems"
      },
      {
        "head": "GroupAston",
        "relation": "author_of",
        "tail": "In Advances in Neural Information Processing Systems"
      },
      {
        "head": "UniversityBirmingham",
        "relation": "author_of",
        "tail": "In Advances in Neural Information Processing Systems"
      },
      {
        "head": "Danilo Jimenez Rezende",
        "relation": "author_of",
        "tail": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models"
      },
      {
        "head": "S. Mohamed",
        "relation": "author_of",
        "tail": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models"
      },
      {
        "head": "Daan Wierstra",
        "relation": "author_of",
        "tail": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models"
      },
      {
        "head": "Shanchuan Lin",
        "relation": "author_of",
        "tail": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation"
      },
      {
        "head": "Anran Wang",
        "relation": "author_of",
        "tail": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation"
      },
      {
        "head": "Xiao Yang",
        "relation": "author_of",
        "tail": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation"
      },
      {
        "head": "Zhiyuan Chen",
        "relation": "author_of",
        "tail": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions"
      },
      {
        "head": "Jiajiong Cao",
        "relation": "author_of",
        "tail": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions"
      },
      {
        "head": "Zhiquan Chen",
        "relation": "author_of",
        "tail": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions"
      },
      {
        "head": "Yuming Li",
        "relation": "author_of",
        "tail": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions"
      },
      {
        "head": "Chenguang Ma",
        "relation": "author_of",
        "tail": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions"
      },
      {
        "head": "Wenzhao Zheng",
        "relation": "author_of",
        "tail": "GenAD: Generative End-to-End Autonomous Driving"
      },
      {
        "head": "Ruiqi Song",
        "relation": "author_of",
        "tail": "GenAD: Generative End-to-End Autonomous Driving"
      },
      {
        "head": "Xianda Guo",
        "relation": "author_of",
        "tail": "GenAD: Generative End-to-End Autonomous Driving"
      },
      {
        "head": "Chenming Zhang",
        "relation": "author_of",
        "tail": "GenAD: Generative End-to-End Autonomous Driving"
      },
      {
        "head": "Long Chen",
        "relation": "author_of",
        "tail": "GenAD: Generative End-to-End Autonomous Driving"
      },
      {
        "head": "Shiyin Lu",
        "relation": "author_of",
        "tail": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model"
      },
      {
        "head": "Yang Li",
        "relation": "author_of",
        "tail": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model"
      },
      {
        "head": "Qing-Guo Chen",
        "relation": "author_of",
        "tail": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model"
      },
      {
        "head": "Zhao Xu",
        "relation": "author_of",
        "tail": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model"
      },
      {
        "head": "Weihua Luo",
        "relation": "author_of",
        "tail": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model"
      },
      {
        "head": "Kaifu Zhang",
        "relation": "author_of",
        "tail": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model"
      },
      {
        "head": "Han-Jia Ye",
        "relation": "author_of",
        "tail": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model"
      },
      {
        "head": "Jie Liu",
        "relation": "author_of",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Gongye Liu",
        "relation": "author_of",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Jiajun Liang",
        "relation": "author_of",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Ziyang Yuan",
        "relation": "author_of",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Xiaokun Liu",
        "relation": "author_of",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Mingwu Zheng",
        "relation": "author_of",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Xiele Wu",
        "relation": "author_of",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Qiulin Wang",
        "relation": "author_of",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Menghan Xia",
        "relation": "author_of",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Xintao Wang",
        "relation": "author_of",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Xiaohong Liu",
        "relation": "author_of",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Fei Yang",
        "relation": "author_of",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Pengfei Wan",
        "relation": "author_of",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Di Zhang",
        "relation": "author_of",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Kun Gai",
        "relation": "author_of",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Yujiu Yang",
        "relation": "author_of",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Wanli Ouyang",
        "relation": "author_of",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Alex Graves",
        "relation": "author_of",
        "tail": "Speech Recognition with Deep Recurrent Neural Networks"
      },
      {
        "head": "Abdel-rahman Mohamed",
        "relation": "author_of",
        "tail": "Speech Recognition with Deep Recurrent Neural Networks"
      },
      {
        "head": "Geoffrey Hinton",
        "relation": "author_of",
        "tail": "Speech Recognition with Deep Recurrent Neural Networks"
      },
      {
        "head": "D. Rumelhart",
        "relation": "author_of",
        "tail": "Learning representations by back-propagating errors"
      },
      {
        "head": "Geoffrey E. Hinton",
        "relation": "author_of",
        "tail": "Learning representations by back-propagating errors"
      },
      {
        "head": "Ronald J. Williams",
        "relation": "author_of",
        "tail": "Learning representations by back-propagating errors"
      },
      {
        "head": "M. Schuster",
        "relation": "author_of",
        "tail": "Bidirectional recurrent neural networks"
      },
      {
        "head": "K. Paliwal",
        "relation": "author_of",
        "tail": "Bidirectional recurrent neural networks"
      },
      {
        "head": "Alex Graves",
        "relation": "author_of",
        "tail": "Connectionist Temporal Classiﬁcation: Labelling Unsegmented Sequence Data with Recurrent Neural Networks"
      },
      {
        "head": "Santiago Fern´andez",
        "relation": "author_of",
        "tail": "Connectionist Temporal Classiﬁcation: Labelling Unsegmented Sequence Data with Recurrent Neural Networks"
      },
      {
        "head": "Faustino J. Gomez",
        "relation": "author_of",
        "tail": "Connectionist Temporal Classiﬁcation: Labelling Unsegmented Sequence Data with Recurrent Neural Networks"
      },
      {
        "head": "J¨urgen Schmidhuber",
        "relation": "author_of",
        "tail": "Connectionist Temporal Classiﬁcation: Labelling Unsegmented Sequence Data with Recurrent Neural Networks"
      },
      {
        "head": "Alex Graves",
        "relation": "author_of",
        "tail": "2005 Special Issue: Framewise phoneme classification with bidirectional LSTM and other neural network architectures"
      },
      {
        "head": "J. Schmidhuber",
        "relation": "author_of",
        "tail": "2005 Special Issue: Framewise phoneme classification with bidirectional LSTM and other neural network architectures"
      },
      {
        "head": "Sean L. Metzger",
        "relation": "author_of",
        "tail": "A high-performance neuroprosthesis for speech decoding and avatar control"
      },
      {
        "head": "K. T. Littlejohn",
        "relation": "author_of",
        "tail": "A high-performance neuroprosthesis for speech decoding and avatar control"
      },
      {
        "head": "Alexander B. Silva",
        "relation": "author_of",
        "tail": "A high-performance neuroprosthesis for speech decoding and avatar control"
      },
      {
        "head": "D. Moses",
        "relation": "author_of",
        "tail": "A high-performance neuroprosthesis for speech decoding and avatar control"
      },
      {
        "head": "Margaret P. Seaton",
        "relation": "author_of",
        "tail": "A high-performance neuroprosthesis for speech decoding and avatar control"
      },
      {
        "head": "Ran Wang",
        "relation": "author_of",
        "tail": "A high-performance neuroprosthesis for speech decoding and avatar control"
      },
      {
        "head": "Maximilian E. Dougherty",
        "relation": "author_of",
        "tail": "A high-performance neuroprosthesis for speech decoding and avatar control"
      },
      {
        "head": "Jessie R. Liu",
        "relation": "author_of",
        "tail": "A high-performance neuroprosthesis for speech decoding and avatar control"
      },
      {
        "head": "Peter Wu",
        "relation": "author_of",
        "tail": "A high-performance neuroprosthesis for speech decoding and avatar control"
      },
      {
        "head": "M. Berger",
        "relation": "author_of",
        "tail": "A high-performance neuroprosthesis for speech decoding and avatar control"
      },
      {
        "head": "Inga Zhuravleva",
        "relation": "author_of",
        "tail": "A high-performance neuroprosthesis for speech decoding and avatar control"
      },
      {
        "head": "A. Tu-Chan",
        "relation": "author_of",
        "tail": "A high-performance neuroprosthesis for speech decoding and avatar control"
      },
      {
        "head": "K. Ganguly",
        "relation": "author_of",
        "tail": "A high-performance neuroprosthesis for speech decoding and avatar control"
      },
      {
        "head": "G. Anumanchipalli",
        "relation": "author_of",
        "tail": "A high-performance neuroprosthesis for speech decoding and avatar control"
      },
      {
        "head": "Edward F. Chang",
        "relation": "author_of",
        "tail": "A high-performance neuroprosthesis for speech decoding and avatar control"
      },
      {
        "head": "Tianming Sun",
        "relation": "author_of",
        "tail": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses"
      },
      {
        "head": "Bin Feng",
        "relation": "author_of",
        "tail": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses"
      },
      {
        "head": "Jinpeng Huo",
        "relation": "author_of",
        "tail": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses"
      },
      {
        "head": "Yu Xiao",
        "relation": "author_of",
        "tail": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses"
      },
      {
        "head": "Wengan Wang",
        "relation": "author_of",
        "tail": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses"
      },
      {
        "head": "Jin Peng",
        "relation": "author_of",
        "tail": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses"
      },
      {
        "head": "Zehua Li",
        "relation": "author_of",
        "tail": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses"
      },
      {
        "head": "Chengjie Du",
        "relation": "author_of",
        "tail": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses"
      },
      {
        "head": "Wenxian Wang",
        "relation": "author_of",
        "tail": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses"
      },
      {
        "head": "G. Zou",
        "relation": "author_of",
        "tail": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses"
      },
      {
        "head": "Lei Liu",
        "relation": "author_of",
        "tail": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses"
      },
      {
        "head": "Francis R. Willett",
        "relation": "author_of",
        "tail": "A high-performance speech neuroprosthesis"
      },
      {
        "head": "Erin M. Kunz",
        "relation": "author_of",
        "tail": "A high-performance speech neuroprosthesis"
      },
      {
        "head": "Chaofei Fan",
        "relation": "author_of",
        "tail": "A high-performance speech neuroprosthesis"
      },
      {
        "head": "Donald T. Avansino",
        "relation": "author_of",
        "tail": "A high-performance speech neuroprosthesis"
      },
      {
        "head": "G. Wilson",
        "relation": "author_of",
        "tail": "A high-performance speech neuroprosthesis"
      },
      {
        "head": "Eun Young Choi",
        "relation": "author_of",
        "tail": "A high-performance speech neuroprosthesis"
      },
      {
        "head": "Foram B. Kamdar",
        "relation": "author_of",
        "tail": "A high-performance speech neuroprosthesis"
      },
      {
        "head": "M. Glasser",
        "relation": "author_of",
        "tail": "A high-performance speech neuroprosthesis"
      },
      {
        "head": "L. Hochberg",
        "relation": "author_of",
        "tail": "A high-performance speech neuroprosthesis"
      },
      {
        "head": "S. Druckmann",
        "relation": "author_of",
        "tail": "A high-performance speech neuroprosthesis"
      },
      {
        "head": "K. Shenoy",
        "relation": "author_of",
        "tail": "A high-performance speech neuroprosthesis"
      },
      {
        "head": "J. Henderson",
        "relation": "author_of",
        "tail": "A high-performance speech neuroprosthesis"
      },
      {
        "head": "Shibhansh Dohare",
        "relation": "author_of",
        "tail": "Loss of plasticity in deep continual learning"
      },
      {
        "head": "J. F. Hernandez-Garcia",
        "relation": "author_of",
        "tail": "Loss of plasticity in deep continual learning"
      },
      {
        "head": "Qingfeng Lan",
        "relation": "author_of",
        "tail": "Loss of plasticity in deep continual learning"
      },
      {
        "head": "Parash Rahman",
        "relation": "author_of",
        "tail": "Loss of plasticity in deep continual learning"
      },
      {
        "head": "A. Mahmood",
        "relation": "author_of",
        "tail": "Loss of plasticity in deep continual learning"
      },
      {
        "head": "R. Sutton",
        "relation": "author_of",
        "tail": "Loss of plasticity in deep continual learning"
      },
      {
        "head": "S. Ambrogio",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "P. Narayanan",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "A. Okazaki",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "A. Fasoli",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "C. Mackin",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "K. Hosokawa",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "A. Nomura",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "Takeo Yasuda",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "An Chen",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "A. Friz",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "M. Ishii",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "J. Luquin",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "Y. Kohda",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "N. Saulnier",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "K. Brew",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "Samuel Choi",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "I. Ok",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "Timothy Philip",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "Victor Chan",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "M. Silvestre",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "Ishtiaq Ahsan",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "Vijay Narayanan",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "H. Tsai",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "Geoffrey W. Burr",
        "relation": "author_of",
        "tail": "An analog-AI chip for energy-efficient speech recognition and transcription"
      },
      {
        "head": "J. Shin",
        "relation": "author_of",
        "tail": "A Mathematical Theory of Communication"
      },
      {
        "head": "Sang Joon Kim",
        "relation": "author_of",
        "tail": "A Mathematical Theory of Communication"
      },
      {
        "head": "Yike Sun",
        "relation": "author_of",
        "tail": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers"
      },
      {
        "head": "Haotong Yang",
        "relation": "author_of",
        "tail": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers"
      },
      {
        "head": "Zhouchen Lin",
        "relation": "author_of",
        "tail": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers"
      },
      {
        "head": "Muhan Zhang",
        "relation": "author_of",
        "tail": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers"
      },
      {
        "head": "Ning Ding",
        "relation": "author_of",
        "tail": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling"
      },
      {
        "head": "Fangcheng Liu",
        "relation": "author_of",
        "tail": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling"
      },
      {
        "head": "Kyungrae Kim",
        "relation": "author_of",
        "tail": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling"
      },
      {
        "head": "Linji Hao",
        "relation": "author_of",
        "tail": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling"
      },
      {
        "head": "Kyeng-Hun Lee",
        "relation": "author_of",
        "tail": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling"
      },
      {
        "head": "Hyeonmok Ko",
        "relation": "author_of",
        "tail": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling"
      },
      {
        "head": "Yehui Tang",
        "relation": "author_of",
        "tail": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling"
      },
      {
        "head": "Huinan Xu",
        "relation": "author_of",
        "tail": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram"
      },
      {
        "head": "Xuyang Feng",
        "relation": "author_of",
        "tail": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram"
      },
      {
        "head": "Junhong Chen",
        "relation": "author_of",
        "tail": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram"
      },
      {
        "head": "Junchen Liu",
        "relation": "author_of",
        "tail": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram"
      },
      {
        "head": "Kaiwen Deng",
        "relation": "author_of",
        "tail": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram"
      },
      {
        "head": "Kai Ding",
        "relation": "author_of",
        "tail": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram"
      },
      {
        "head": "Shengning Long",
        "relation": "author_of",
        "tail": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram"
      },
      {
        "head": "Jiaxue Shuai",
        "relation": "author_of",
        "tail": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram"
      },
      {
        "head": "Zhaorong Li",
        "relation": "author_of",
        "tail": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram"
      },
      {
        "head": "Shiping Liu",
        "relation": "author_of",
        "tail": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram"
      },
      {
        "head": "Guirong Xue",
        "relation": "author_of",
        "tail": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram"
      },
      {
        "head": "Zhan Xiao",
        "relation": "author_of",
        "tail": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram"
      },
      {
        "head": "Albert Tseng",
        "relation": "author_of",
        "tail": "L$^3$: Large Lookup Layers"
      },
      {
        "head": "Christopher De Sa",
        "relation": "author_of",
        "tail": "L$^3$: Large Lookup Layers"
      },
      {
        "head": "Hong Liu",
        "relation": "author_of",
        "tail": "Scaling Embeddings Outperforms Scaling Experts in Language Models"
      },
      {
        "head": "Jiaqi Zhang",
        "relation": "author_of",
        "tail": "Scaling Embeddings Outperforms Scaling Experts in Language Models"
      },
      {
        "head": "Chao Wang",
        "relation": "author_of",
        "tail": "Scaling Embeddings Outperforms Scaling Experts in Language Models"
      },
      {
        "head": "Xing Hu",
        "relation": "author_of",
        "tail": "Scaling Embeddings Outperforms Scaling Experts in Language Models"
      },
      {
        "head": "Linkun Lyu",
        "relation": "author_of",
        "tail": "Scaling Embeddings Outperforms Scaling Experts in Language Models"
      },
      {
        "head": "Jiaqi Sun",
        "relation": "author_of",
        "tail": "Scaling Embeddings Outperforms Scaling Experts in Language Models"
      },
      {
        "head": "Xurui Yang",
        "relation": "author_of",
        "tail": "Scaling Embeddings Outperforms Scaling Experts in Language Models"
      },
      {
        "head": "Bo Wang",
        "relation": "author_of",
        "tail": "Scaling Embeddings Outperforms Scaling Experts in Language Models"
      },
      {
        "head": "Fengcun Li",
        "relation": "author_of",
        "tail": "Scaling Embeddings Outperforms Scaling Experts in Language Models"
      },
      {
        "head": "Yulei Qian",
        "relation": "author_of",
        "tail": "Scaling Embeddings Outperforms Scaling Experts in Language Models"
      },
      {
        "head": "Lingtong Si",
        "relation": "author_of",
        "tail": "Scaling Embeddings Outperforms Scaling Experts in Language Models"
      },
      {
        "head": "Yerui Sun",
        "relation": "author_of",
        "tail": "Scaling Embeddings Outperforms Scaling Experts in Language Models"
      },
      {
        "head": "Rumei Li",
        "relation": "author_of",
        "tail": "Scaling Embeddings Outperforms Scaling Experts in Language Models"
      },
      {
        "head": "Peng Pei",
        "relation": "author_of",
        "tail": "Scaling Embeddings Outperforms Scaling Experts in Language Models"
      },
      {
        "head": "Yuchen Xie",
        "relation": "author_of",
        "tail": "Scaling Embeddings Outperforms Scaling Experts in Language Models"
      },
      {
        "head": "Xunliang Cai",
        "relation": "author_of",
        "tail": "Scaling Embeddings Outperforms Scaling Experts in Language Models"
      },
      {
        "head": "Christian Szegedy",
        "relation": "author_of",
        "tail": "Going Deeper with Convolutions"
      },
      {
        "head": "Wei Liu",
        "relation": "author_of",
        "tail": "Going Deeper with Convolutions"
      },
      {
        "head": "Yangqing Jia",
        "relation": "author_of",
        "tail": "Going Deeper with Convolutions"
      },
      {
        "head": "Pierre Sermanet",
        "relation": "author_of",
        "tail": "Going Deeper with Convolutions"
      },
      {
        "head": "Scott Reed",
        "relation": "author_of",
        "tail": "Going Deeper with Convolutions"
      },
      {
        "head": "Dragomir Anguelov",
        "relation": "author_of",
        "tail": "Going Deeper with Convolutions"
      },
      {
        "head": "Dumitru Erhan",
        "relation": "author_of",
        "tail": "Going Deeper with Convolutions"
      },
      {
        "head": "Vincent Vanhoucke",
        "relation": "author_of",
        "tail": "Going Deeper with Convolutions"
      },
      {
        "head": "Andrew Rabinovich",
        "relation": "author_of",
        "tail": "Going Deeper with Convolutions"
      },
      {
        "head": "R. Tibshirani",
        "relation": "author_of",
        "tail": "Regression Shrinkage and Selection via the Lasso"
      },
      {
        "head": "Herve Goeau",
        "relation": "author_of",
        "tail": "LifeCLEF Plant Identification Task 2015"
      },
      {
        "head": "Pierre Bonnet",
        "relation": "author_of",
        "tail": "LifeCLEF Plant Identification Task 2015"
      },
      {
        "head": "Alexis Joly",
        "relation": "author_of",
        "tail": "LifeCLEF Plant Identification Task 2015"
      },
      {
        "head": "Safa Ben Atitallah",
        "relation": "author_of",
        "tail": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics"
      },
      {
        "head": "Maha Driss",
        "relation": "author_of",
        "tail": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics"
      },
      {
        "head": "Henda Ben Ghezela",
        "relation": "author_of",
        "tail": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics"
      },
      {
        "head": "Sareer Ul Amin",
        "relation": "author_of",
        "tail": "Enhancing pine wilt disease detection with synthetic data and external attention-based transformers"
      },
      {
        "head": "Yonghoon Jung",
        "relation": "author_of",
        "tail": "Enhancing pine wilt disease detection with synthetic data and external attention-based transformers"
      },
      {
        "head": "Muhammad Fayaz",
        "relation": "author_of",
        "tail": "Enhancing pine wilt disease detection with synthetic data and external attention-based transformers"
      },
      {
        "head": "Bumsoo Kim",
        "relation": "author_of",
        "tail": "Enhancing pine wilt disease detection with synthetic data and external attention-based transformers"
      },
      {
        "head": "Sanghyun Seo",
        "relation": "author_of",
        "tail": "Enhancing pine wilt disease detection with synthetic data and external attention-based transformers"
      },
      {
        "head": "Ayşe Aybilge Murat",
        "relation": "author_of",
        "tail": "A comprehensive review on YOLO versions for object detection"
      },
      {
        "head": "M. S. Kıran",
        "relation": "author_of",
        "tail": "A comprehensive review on YOLO versions for object detection"
      },
      {
        "head": "Hongyan Zhu",
        "relation": "author_of",
        "tail": "Harnessing large vision and language models in agriculture: a review"
      },
      {
        "head": "Shuai Qin",
        "relation": "author_of",
        "tail": "Harnessing large vision and language models in agriculture: a review"
      },
      {
        "head": "Min Su",
        "relation": "author_of",
        "tail": "Harnessing large vision and language models in agriculture: a review"
      },
      {
        "head": "Chengzhi Lin",
        "relation": "author_of",
        "tail": "Harnessing large vision and language models in agriculture: a review"
      },
      {
        "head": "Anjie Li",
        "relation": "author_of",
        "tail": "Harnessing large vision and language models in agriculture: a review"
      },
      {
        "head": "Junfeng Gao",
        "relation": "author_of",
        "tail": "Harnessing large vision and language models in agriculture: a review"
      },
      {
        "head": "Abdul Rehman Khan",
        "relation": "author_of",
        "tail": "Multi-axis vision transformer for medical image segmentation"
      },
      {
        "head": "Asifullah Khan",
        "relation": "author_of",
        "tail": "Multi-axis vision transformer for medical image segmentation"
      },
      {
        "head": "D. E. Boukhari",
        "relation": "author_of",
        "tail": "A comprehensive review of facial beauty prediction using deep learning techniques"
      },
      {
        "head": "F. Dornaika",
        "relation": "author_of",
        "tail": "A comprehensive review of facial beauty prediction using deep learning techniques"
      },
      {
        "head": "A. Chemsa",
        "relation": "author_of",
        "tail": "A comprehensive review of facial beauty prediction using deep learning techniques"
      },
      {
        "head": "Abdelmalik Taleb-Ahmed",
        "relation": "author_of",
        "tail": "A comprehensive review of facial beauty prediction using deep learning techniques"
      },
      {
        "head": "Michaela Vystrčilová",
        "relation": "author_of",
        "tail": "A systematic comparison of predictive models on the retina"
      },
      {
        "head": "Shashwat Sridhar",
        "relation": "author_of",
        "tail": "A systematic comparison of predictive models on the retina"
      },
      {
        "head": "Max F. Burg",
        "relation": "author_of",
        "tail": "A systematic comparison of predictive models on the retina"
      },
      {
        "head": "M. Khani",
        "relation": "author_of",
        "tail": "A systematic comparison of predictive models on the retina"
      },
      {
        "head": "Dimokratis Karamanlis",
        "relation": "author_of",
        "tail": "A systematic comparison of predictive models on the retina"
      },
      {
        "head": "H. Schreyer",
        "relation": "author_of",
        "tail": "A systematic comparison of predictive models on the retina"
      },
      {
        "head": "Varsha Ramakrishna",
        "relation": "author_of",
        "tail": "A systematic comparison of predictive models on the retina"
      },
      {
        "head": "Steffen Krüppel",
        "relation": "author_of",
        "tail": "A systematic comparison of predictive models on the retina"
      },
      {
        "head": "Sören J. Zapp",
        "relation": "author_of",
        "tail": "A systematic comparison of predictive models on the retina"
      },
      {
        "head": "Matthias Mietsch",
        "relation": "author_of",
        "tail": "A systematic comparison of predictive models on the retina"
      },
      {
        "head": "T. Gollisch",
        "relation": "author_of",
        "tail": "A systematic comparison of predictive models on the retina"
      },
      {
        "head": "Alexander S. Ecker",
        "relation": "author_of",
        "tail": "A systematic comparison of predictive models on the retina"
      },
      {
        "head": "D. Lowe",
        "relation": "author_of",
        "tail": "Distinctive Image Features from Scale-Invariant Keypoints"
      },
      {
        "head": "Xiang An",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Yin Xie",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Kaicheng Yang",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Wenkang Zhang",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Xiuwei Zhao",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Zheng Cheng",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Yirui Wang",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Songcen Xu",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Changrui Chen",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Didi Zhu",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Chunsheng Wu",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Huajie Tan",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Chunyuan Li",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Jing Yang",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Jie Yu",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Xiyao Wang",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Bin Qin",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Yumeng Wang",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Zizhen Yan",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Ziyong Feng",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Ziwei Liu",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Bo Li",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Jiankang Deng",
        "relation": "author_of",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Kento Kawaharazuka",
        "relation": "author_of",
        "tail": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications"
      },
      {
        "head": "Jihoon Oh",
        "relation": "author_of",
        "tail": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications"
      },
      {
        "head": "Jun Yamada",
        "relation": "author_of",
        "tail": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications"
      },
      {
        "head": "Ingmar Posner",
        "relation": "author_of",
        "tail": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications"
      },
      {
        "head": "Yuke Zhu",
        "relation": "author_of",
        "tail": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications"
      },
      {
        "head": "Lukas Muttenthaler",
        "relation": "author_of",
        "tail": "Aligning machine and human visual representations across abstraction levels"
      },
      {
        "head": "Klaus Greff",
        "relation": "author_of",
        "tail": "Aligning machine and human visual representations across abstraction levels"
      },
      {
        "head": "Frieda Born",
        "relation": "author_of",
        "tail": "Aligning machine and human visual representations across abstraction levels"
      },
      {
        "head": "Bernhard Spitzer",
        "relation": "author_of",
        "tail": "Aligning machine and human visual representations across abstraction levels"
      },
      {
        "head": "Simon Kornblith",
        "relation": "author_of",
        "tail": "Aligning machine and human visual representations across abstraction levels"
      },
      {
        "head": "M. C. Mozer",
        "relation": "author_of",
        "tail": "Aligning machine and human visual representations across abstraction levels"
      },
      {
        "head": "Klaus-Robert Muller",
        "relation": "author_of",
        "tail": "Aligning machine and human visual representations across abstraction levels"
      },
      {
        "head": "Thomas Unterthiner",
        "relation": "author_of",
        "tail": "Aligning machine and human visual representations across abstraction levels"
      },
      {
        "head": "Andrew Kyle Lampinen",
        "relation": "author_of",
        "tail": "Aligning machine and human visual representations across abstraction levels"
      },
      {
        "head": "Alexey Dosovitskiy",
        "relation": "author_of",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "Lucas Beyer",
        "relation": "author_of",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "Alexander Kolesnikov",
        "relation": "author_of",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "Dirk Weissenborn",
        "relation": "author_of",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "Xiaohua Zhai",
        "relation": "author_of",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "Thomas Unterthiner",
        "relation": "author_of",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "Mostafa Dehghani",
        "relation": "author_of",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "Matthias Minderer",
        "relation": "author_of",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "Georg Heigold",
        "relation": "author_of",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "Sylvain Gelly",
        "relation": "author_of",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "Jakob Uszkoreit",
        "relation": "author_of",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "Neil Houlsby",
        "relation": "author_of",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "Alec Radford",
        "relation": "author_of",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Jong Wook Kim",
        "relation": "author_of",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Chris Hallacy",
        "relation": "author_of",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Aditya Ramesh",
        "relation": "author_of",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Gabriel Goh",
        "relation": "author_of",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Sandhini Agarwal",
        "relation": "author_of",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Girish Sastry",
        "relation": "author_of",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Amanda Askell",
        "relation": "author_of",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Pamela Mishkin",
        "relation": "author_of",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Jack Clark",
        "relation": "author_of",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Gretchen Krueger",
        "relation": "author_of",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Ilya Sutskever",
        "relation": "author_of",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Individualized Treat",
        "relation": "author_of",
        "tail": "GENERATIVE ADVERSARIAL NETS"
      },
      {
        "head": "Jinsung Yoon",
        "relation": "author_of",
        "tail": "GENERATIVE ADVERSARIAL NETS"
      },
      {
        "head": "Zhengyang Geng",
        "relation": "author_of",
        "tail": "Improved Mean Flows: On the Challenges of Fastforward Generative Models"
      },
      {
        "head": "Yiyang Lu",
        "relation": "author_of",
        "tail": "Improved Mean Flows: On the Challenges of Fastforward Generative Models"
      },
      {
        "head": "Zongze Wu",
        "relation": "author_of",
        "tail": "Improved Mean Flows: On the Challenges of Fastforward Generative Models"
      },
      {
        "head": "Eli Shechtman",
        "relation": "author_of",
        "tail": "Improved Mean Flows: On the Challenges of Fastforward Generative Models"
      },
      {
        "head": "J. Zico Kolter",
        "relation": "author_of",
        "tail": "Improved Mean Flows: On the Challenges of Fastforward Generative Models"
      },
      {
        "head": "Kaiming He",
        "relation": "author_of",
        "tail": "Improved Mean Flows: On the Challenges of Fastforward Generative Models"
      },
      {
        "head": "Jiachen Lei",
        "relation": "author_of",
        "tail": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training"
      },
      {
        "head": "Keli Liu",
        "relation": "author_of",
        "tail": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training"
      },
      {
        "head": "Julius Berner",
        "relation": "author_of",
        "tail": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training"
      },
      {
        "head": "Haiming Yu",
        "relation": "author_of",
        "tail": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training"
      },
      {
        "head": "Hongkai Zheng",
        "relation": "author_of",
        "tail": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training"
      },
      {
        "head": "Jiahong Wu",
        "relation": "author_of",
        "tail": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training"
      },
      {
        "head": "Xiangxiang Chu",
        "relation": "author_of",
        "tail": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training"
      },
      {
        "head": "Minglei Shi",
        "relation": "author_of",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "Haolin Wang",
        "relation": "author_of",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "Borui Zhang",
        "relation": "author_of",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "Wenzhao Zheng",
        "relation": "author_of",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "Bohan Zeng",
        "relation": "author_of",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "Ziyang Yuan",
        "relation": "author_of",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "Xiaoshi Wu",
        "relation": "author_of",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "Yuanxing Zhang",
        "relation": "author_of",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "Huan Yang",
        "relation": "author_of",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "Xintao Wang",
        "relation": "author_of",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "Pengfei Wan",
        "relation": "author_of",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "Kun Gai",
        "relation": "author_of",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "Jie Zhou",
        "relation": "author_of",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "Jiwen Lu",
        "relation": "author_of",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "Yongsheng Yu",
        "relation": "author_of",
        "tail": "PixelDiT: Pixel Diffusion Transformers for Image Generation"
      },
      {
        "head": "Wei Xiong",
        "relation": "author_of",
        "tail": "PixelDiT: Pixel Diffusion Transformers for Image Generation"
      },
      {
        "head": "Weili Nie",
        "relation": "author_of",
        "tail": "PixelDiT: Pixel Diffusion Transformers for Image Generation"
      },
      {
        "head": "Yichen Sheng",
        "relation": "author_of",
        "tail": "PixelDiT: Pixel Diffusion Transformers for Image Generation"
      },
      {
        "head": "Shiqiu Liu",
        "relation": "author_of",
        "tail": "PixelDiT: Pixel Diffusion Transformers for Image Generation"
      },
      {
        "head": "Jiebo Luo",
        "relation": "author_of",
        "tail": "PixelDiT: Pixel Diffusion Transformers for Image Generation"
      },
      {
        "head": "Zhiheng Liu",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Weiming Ren",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Haozhe Liu",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Zijian Zhou",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Shoufa Chen",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Haonan Qiu",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Xiaoke Huang",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Zhaochong An",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Fanny Yang",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Aditya Patel",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Viktar Atliha",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Tony Ng",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Xiao Han",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Chuyan Zhu",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Chenyang Zhang",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Ding Liu",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Juan-Manuel Perez-Rua",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Sen He",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Jürgen Schmidhuber",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Wenhu Chen",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Ping Luo",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Wei Liu",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Tao Xiang",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Jonas Schult",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Yuren Cong",
        "relation": "author_of",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "Jacob Devlin",
        "relation": "author_of",
        "tail": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "Ming-Wei Chang",
        "relation": "author_of",
        "tail": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "Kenton Lee",
        "relation": "author_of",
        "tail": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "Kristina Toutanova",
        "relation": "author_of",
        "tail": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "Vrushali Pagire",
        "relation": "author_of",
        "tail": "A comprehensive review of object detection with traditional and deep learning methods"
      },
      {
        "head": "M. Chavali",
        "relation": "author_of",
        "tail": "A comprehensive review of object detection with traditional and deep learning methods"
      },
      {
        "head": "Ashish Kale",
        "relation": "author_of",
        "tail": "A comprehensive review of object detection with traditional and deep learning methods"
      },
      {
        "head": "Jinjie Ni",
        "relation": "author_of",
        "tail": "Diffusion Language Models are Super Data Learners"
      },
      {
        "head": "Qian Liu",
        "relation": "author_of",
        "tail": "Diffusion Language Models are Super Data Learners"
      },
      {
        "head": "Longxu Dou",
        "relation": "author_of",
        "tail": "Diffusion Language Models are Super Data Learners"
      },
      {
        "head": "Chao Du",
        "relation": "author_of",
        "tail": "Diffusion Language Models are Super Data Learners"
      },
      {
        "head": "Zili Wang",
        "relation": "author_of",
        "tail": "Diffusion Language Models are Super Data Learners"
      },
      {
        "head": "Hang Yan",
        "relation": "author_of",
        "tail": "Diffusion Language Models are Super Data Learners"
      },
      {
        "head": "Tianyu Pang",
        "relation": "author_of",
        "tail": "Diffusion Language Models are Super Data Learners"
      },
      {
        "head": "Michael Qizhe Shieh",
        "relation": "author_of",
        "tail": "Diffusion Language Models are Super Data Learners"
      },
      {
        "head": "Zirui Wu",
        "relation": "author_of",
        "tail": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas"
      },
      {
        "head": "Lin Zheng",
        "relation": "author_of",
        "tail": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas"
      },
      {
        "head": "Zhihui Xie",
        "relation": "author_of",
        "tail": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas"
      },
      {
        "head": "Jiacheng Ye",
        "relation": "author_of",
        "tail": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas"
      },
      {
        "head": "Jiahui Gao",
        "relation": "author_of",
        "tail": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas"
      },
      {
        "head": "Shansan Gong",
        "relation": "author_of",
        "tail": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas"
      },
      {
        "head": "Yansong Feng",
        "relation": "author_of",
        "tail": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas"
      },
      {
        "head": "Zhenguo Li",
        "relation": "author_of",
        "tail": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas"
      },
      {
        "head": "Wei Bi",
        "relation": "author_of",
        "tail": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas"
      },
      {
        "head": "Guorui Zhou",
        "relation": "author_of",
        "tail": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas"
      },
      {
        "head": "Lingpeng Kong",
        "relation": "author_of",
        "tail": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas"
      },
      {
        "head": "Zhicheng Cai",
        "relation": "author_of",
        "tail": "FLEX: Continuous Agent Evolution via Forward Learning from Experience"
      },
      {
        "head": "Xinyuan Guo",
        "relation": "author_of",
        "tail": "FLEX: Continuous Agent Evolution via Forward Learning from Experience"
      },
      {
        "head": "Yu Pei",
        "relation": "author_of",
        "tail": "FLEX: Continuous Agent Evolution via Forward Learning from Experience"
      },
      {
        "head": "Jiangtao Feng",
        "relation": "author_of",
        "tail": "FLEX: Continuous Agent Evolution via Forward Learning from Experience"
      },
      {
        "head": "Jinsong Su",
        "relation": "author_of",
        "tail": "FLEX: Continuous Agent Evolution via Forward Learning from Experience"
      },
      {
        "head": "Jiangjie Chen",
        "relation": "author_of",
        "tail": "FLEX: Continuous Agent Evolution via Forward Learning from Experience"
      },
      {
        "head": "Ya-Qin Zhang",
        "relation": "author_of",
        "tail": "FLEX: Continuous Agent Evolution via Forward Learning from Experience"
      },
      {
        "head": "Wei-Ying Ma",
        "relation": "author_of",
        "tail": "FLEX: Continuous Agent Evolution via Forward Learning from Experience"
      },
      {
        "head": "Mingxuan Wang",
        "relation": "author_of",
        "tail": "FLEX: Continuous Agent Evolution via Forward Learning from Experience"
      },
      {
        "head": "Hao Zhou",
        "relation": "author_of",
        "tail": "FLEX: Continuous Agent Evolution via Forward Learning from Experience"
      },
      {
        "head": "Mingyue Cheng",
        "relation": "author_of",
        "tail": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning"
      },
      {
        "head": "Jie Ouyang",
        "relation": "author_of",
        "tail": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning"
      },
      {
        "head": "Shuo Yu",
        "relation": "author_of",
        "tail": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning"
      },
      {
        "head": "Ruiran Yan",
        "relation": "author_of",
        "tail": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning"
      },
      {
        "head": "Yucong Luo",
        "relation": "author_of",
        "tail": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning"
      },
      {
        "head": "Zirui Liu",
        "relation": "author_of",
        "tail": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning"
      },
      {
        "head": "Daoyu Wang",
        "relation": "author_of",
        "tail": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning"
      },
      {
        "head": "Qi Liu",
        "relation": "author_of",
        "tail": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning"
      },
      {
        "head": "Enhong Chen",
        "relation": "author_of",
        "tail": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning"
      },
      {
        "head": "Holger Caesar",
        "relation": "author_of",
        "tail": "nuScenes: A multimodal dataset for autonomous driving"
      },
      {
        "head": "Varun Bankiti",
        "relation": "author_of",
        "tail": "nuScenes: A multimodal dataset for autonomous driving"
      },
      {
        "head": "Alex H. Lang",
        "relation": "author_of",
        "tail": "nuScenes: A multimodal dataset for autonomous driving"
      },
      {
        "head": "Sourabh Vora",
        "relation": "author_of",
        "tail": "nuScenes: A multimodal dataset for autonomous driving"
      },
      {
        "head": "Venice Erin Liong",
        "relation": "author_of",
        "tail": "nuScenes: A multimodal dataset for autonomous driving"
      },
      {
        "head": "Qiang Xu",
        "relation": "author_of",
        "tail": "nuScenes: A multimodal dataset for autonomous driving"
      },
      {
        "head": "Anush Krishnan",
        "relation": "author_of",
        "tail": "nuScenes: A multimodal dataset for autonomous driving"
      },
      {
        "head": "Yu Pan",
        "relation": "author_of",
        "tail": "nuScenes: A multimodal dataset for autonomous driving"
      },
      {
        "head": "Giancarlo Baldan",
        "relation": "author_of",
        "tail": "nuScenes: A multimodal dataset for autonomous driving"
      },
      {
        "head": "Oscar Beijbom",
        "relation": "author_of",
        "tail": "nuScenes: A multimodal dataset for autonomous driving"
      },
      {
        "head": "Navneet Dalal",
        "relation": "author_of",
        "tail": "Histograms of oriented gradients for human detection"
      },
      {
        "head": "B. Triggs",
        "relation": "author_of",
        "tail": "Histograms of oriented gradients for human detection"
      },
      {
        "head": "Yue Ma",
        "relation": "author_of",
        "tail": "Controllable Video Generation: A Survey"
      },
      {
        "head": "Kunyu Feng",
        "relation": "author_of",
        "tail": "Controllable Video Generation: A Survey"
      },
      {
        "head": "Zhongyuan Hu",
        "relation": "author_of",
        "tail": "Controllable Video Generation: A Survey"
      },
      {
        "head": "Xinyu Wang",
        "relation": "author_of",
        "tail": "Controllable Video Generation: A Survey"
      },
      {
        "head": "Yucheng Wang",
        "relation": "author_of",
        "tail": "Controllable Video Generation: A Survey"
      },
      {
        "head": "Mingzhe Zheng",
        "relation": "author_of",
        "tail": "Controllable Video Generation: A Survey"
      },
      {
        "head": "Bingyuan Wang",
        "relation": "author_of",
        "tail": "Controllable Video Generation: A Survey"
      },
      {
        "head": "Qinghe Wang",
        "relation": "author_of",
        "tail": "Controllable Video Generation: A Survey"
      },
      {
        "head": "Xuanhua He",
        "relation": "author_of",
        "tail": "Controllable Video Generation: A Survey"
      },
      {
        "head": "Hongfa Wang",
        "relation": "author_of",
        "tail": "Controllable Video Generation: A Survey"
      },
      {
        "head": "Chenyang Zhu",
        "relation": "author_of",
        "tail": "Controllable Video Generation: A Survey"
      },
      {
        "head": "Hongyu Liu",
        "relation": "author_of",
        "tail": "Controllable Video Generation: A Survey"
      },
      {
        "head": "Yingqing He",
        "relation": "author_of",
        "tail": "Controllable Video Generation: A Survey"
      },
      {
        "head": "Zeyu Wang",
        "relation": "author_of",
        "tail": "Controllable Video Generation: A Survey"
      },
      {
        "head": "Zhifeng Li",
        "relation": "author_of",
        "tail": "Controllable Video Generation: A Survey"
      },
      {
        "head": "Xiu Li",
        "relation": "author_of",
        "tail": "Controllable Video Generation: A Survey"
      },
      {
        "head": "Sirui Han",
        "relation": "author_of",
        "tail": "Controllable Video Generation: A Survey"
      },
      {
        "head": "Yike Guo",
        "relation": "author_of",
        "tail": "Controllable Video Generation: A Survey"
      },
      {
        "head": "Wei Liu",
        "relation": "author_of",
        "tail": "Controllable Video Generation: A Survey"
      },
      {
        "head": "Dan Xu",
        "relation": "author_of",
        "tail": "Controllable Video Generation: A Survey"
      },
      {
        "head": "Linfeng Zhang",
        "relation": "author_of",
        "tail": "Controllable Video Generation: A Survey"
      },
      {
        "head": "Qifeng Chen",
        "relation": "author_of",
        "tail": "Controllable Video Generation: A Survey"
      },
      {
        "head": "Lijun Chi",
        "relation": "author_of",
        "tail": "Adversarial Attacks on Autonomous Driving Systems in the Physical World: A Survey"
      },
      {
        "head": "M. Msahli",
        "relation": "author_of",
        "tail": "Adversarial Attacks on Autonomous Driving Systems in the Physical World: A Survey"
      },
      {
        "head": "Qingjie Zhang",
        "relation": "author_of",
        "tail": "Adversarial Attacks on Autonomous Driving Systems in the Physical World: A Survey"
      },
      {
        "head": "Han Qiu",
        "relation": "author_of",
        "tail": "Adversarial Attacks on Autonomous Driving Systems in the Physical World: A Survey"
      },
      {
        "head": "Tianwei Zhang",
        "relation": "author_of",
        "tail": "Adversarial Attacks on Autonomous Driving Systems in the Physical World: A Survey"
      },
      {
        "head": "Gérard Memmi",
        "relation": "author_of",
        "tail": "Adversarial Attacks on Autonomous Driving Systems in the Physical World: A Survey"
      },
      {
        "head": "Meikang Qiu",
        "relation": "author_of",
        "tail": "Adversarial Attacks on Autonomous Driving Systems in the Physical World: A Survey"
      },
      {
        "head": "NVIDIA",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": ":",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Yan Wang",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Wenjie Luo",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Junjie Bai",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Yulong Cao",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Tong Che",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Ke Chen",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Yuxiao Chen",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Jenna Diamond",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Yifan Ding",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Wenhao Ding",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Liang Feng",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Greg Heinrich",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Jack Huang",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Peter Karkus",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Boyi Li",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Pinyi Li",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Tsung-Yi Lin",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Dongran Liu",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Ming-Yu Liu",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Langechuan Liu",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Zhijian Liu",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Jason Lu",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Yunxiang Mao",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Pavlo Molchanov",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Lindsey Pavao",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Zhenghao Peng",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Mike Ranzinger",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Ed Schmerling",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Shida Shen",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Yunfei Shi",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Sarah Tariq",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Ran Tian",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Tilman Wekel",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Xinshuo Weng",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Tianjun Xiao",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Eric Yang",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Xiaodong Yang",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Yurong You",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Xiaohui Zeng",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Wenyuan Zhang",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Boris Ivanovic",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Marco Pavone",
        "relation": "author_of",
        "tail": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"
      },
      {
        "head": "Mohsen Gholami",
        "relation": "author_of",
        "tail": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes"
      },
      {
        "head": "Ahmad Rezaei",
        "relation": "author_of",
        "tail": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes"
      },
      {
        "head": "Zhou Weimin",
        "relation": "author_of",
        "tail": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes"
      },
      {
        "head": "Sitong Mao",
        "relation": "author_of",
        "tail": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes"
      },
      {
        "head": "Shunbo Zhou",
        "relation": "author_of",
        "tail": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes"
      },
      {
        "head": "Yong Zhang",
        "relation": "author_of",
        "tail": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes"
      },
      {
        "head": "Mohammad Akbari",
        "relation": "author_of",
        "tail": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes"
      },
      {
        "head": "Qing Jiang",
        "relation": "author_of",
        "tail": "Detect Anything via Next Point Prediction"
      },
      {
        "head": "Junan Huo",
        "relation": "author_of",
        "tail": "Detect Anything via Next Point Prediction"
      },
      {
        "head": "Xingyu Chen",
        "relation": "author_of",
        "tail": "Detect Anything via Next Point Prediction"
      },
      {
        "head": "Yuda Xiong",
        "relation": "author_of",
        "tail": "Detect Anything via Next Point Prediction"
      },
      {
        "head": "Zhaoyang Zeng",
        "relation": "author_of",
        "tail": "Detect Anything via Next Point Prediction"
      },
      {
        "head": "Yihao Chen",
        "relation": "author_of",
        "tail": "Detect Anything via Next Point Prediction"
      },
      {
        "head": "Tianhe Ren",
        "relation": "author_of",
        "tail": "Detect Anything via Next Point Prediction"
      },
      {
        "head": "Junzhi Yu",
        "relation": "author_of",
        "tail": "Detect Anything via Next Point Prediction"
      },
      {
        "head": "Lei Zhang",
        "relation": "author_of",
        "tail": "Detect Anything via Next Point Prediction"
      },
      {
        "head": "Volodymyr Mnih",
        "relation": "author_of",
        "tail": "Human-level control through deep reinforcement learning"
      },
      {
        "head": "K. Kavukcuoglu",
        "relation": "author_of",
        "tail": "Human-level control through deep reinforcement learning"
      },
      {
        "head": "David Silver",
        "relation": "author_of",
        "tail": "Human-level control through deep reinforcement learning"
      },
      {
        "head": "Andrei A. Rusu",
        "relation": "author_of",
        "tail": "Human-level control through deep reinforcement learning"
      },
      {
        "head": "J. Veness",
        "relation": "author_of",
        "tail": "Human-level control through deep reinforcement learning"
      },
      {
        "head": "Marc G. Bellemare",
        "relation": "author_of",
        "tail": "Human-level control through deep reinforcement learning"
      },
      {
        "head": "Alex Graves",
        "relation": "author_of",
        "tail": "Human-level control through deep reinforcement learning"
      },
      {
        "head": "Martin A. Riedmiller",
        "relation": "author_of",
        "tail": "Human-level control through deep reinforcement learning"
      },
      {
        "head": "A. Fidjeland",
        "relation": "author_of",
        "tail": "Human-level control through deep reinforcement learning"
      },
      {
        "head": "Georg Ostrovski",
        "relation": "author_of",
        "tail": "Human-level control through deep reinforcement learning"
      },
      {
        "head": "Stig Petersen",
        "relation": "author_of",
        "tail": "Human-level control through deep reinforcement learning"
      },
      {
        "head": "Charlie Beattie",
        "relation": "author_of",
        "tail": "Human-level control through deep reinforcement learning"
      },
      {
        "head": "Amir Sadik",
        "relation": "author_of",
        "tail": "Human-level control through deep reinforcement learning"
      },
      {
        "head": "Ioannis Antonoglou",
        "relation": "author_of",
        "tail": "Human-level control through deep reinforcement learning"
      },
      {
        "head": "Helen King",
        "relation": "author_of",
        "tail": "Human-level control through deep reinforcement learning"
      },
      {
        "head": "D. Kumaran",
        "relation": "author_of",
        "tail": "Human-level control through deep reinforcement learning"
      },
      {
        "head": "Daan Wierstra",
        "relation": "author_of",
        "tail": "Human-level control through deep reinforcement learning"
      },
      {
        "head": "S. Legg",
        "relation": "author_of",
        "tail": "Human-level control through deep reinforcement learning"
      },
      {
        "head": "D. Hassabis",
        "relation": "author_of",
        "tail": "Human-level control through deep reinforcement learning"
      },
      {
        "head": "Volodymyr Mnih",
        "relation": "author_of",
        "tail": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "head": "Adrià Puigdomènech Badia",
        "relation": "author_of",
        "tail": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "head": "Mehdi Mirza",
        "relation": "author_of",
        "tail": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "head": "Alex Graves",
        "relation": "author_of",
        "tail": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "head": "Timothy P. Lillicrap",
        "relation": "author_of",
        "tail": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "head": "Tim Harley",
        "relation": "author_of",
        "tail": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "head": "David Silver",
        "relation": "author_of",
        "tail": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "head": "Koray Kavukcuoglu",
        "relation": "author_of",
        "tail": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "head": "Guosheng Lin",
        "relation": "author_of",
        "tail": "RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation"
      },
      {
        "head": "Anton Milan",
        "relation": "author_of",
        "tail": "RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation"
      },
      {
        "head": "Chunhua Shen",
        "relation": "author_of",
        "tail": "RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation"
      },
      {
        "head": "Ian Reid",
        "relation": "author_of",
        "tail": "RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation"
      },
      {
        "head": "Zhijie Qiao",
        "relation": "author_of",
        "tail": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "Haowei Li",
        "relation": "author_of",
        "tail": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "Zhong Cao",
        "relation": "author_of",
        "tail": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "Henry X. Liu",
        "relation": "author_of",
        "tail": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "Yongkang Li",
        "relation": "author_of",
        "tail": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving"
      },
      {
        "head": "Kaixin Xiong",
        "relation": "author_of",
        "tail": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving"
      },
      {
        "head": "Xiangyu Guo",
        "relation": "author_of",
        "tail": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving"
      },
      {
        "head": "Fang Li",
        "relation": "author_of",
        "tail": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving"
      },
      {
        "head": "Sixu Yan",
        "relation": "author_of",
        "tail": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving"
      },
      {
        "head": "Gangwei Xu",
        "relation": "author_of",
        "tail": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving"
      },
      {
        "head": "Lijun Zhou",
        "relation": "author_of",
        "tail": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving"
      },
      {
        "head": "Long Chen",
        "relation": "author_of",
        "tail": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving"
      },
      {
        "head": "Haiyang Sun",
        "relation": "author_of",
        "tail": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving"
      },
      {
        "head": "Bing Wang",
        "relation": "author_of",
        "tail": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving"
      },
      {
        "head": "Kun Ma",
        "relation": "author_of",
        "tail": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving"
      },
      {
        "head": "Guang Chen",
        "relation": "author_of",
        "tail": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving"
      },
      {
        "head": "Hangjun Ye",
        "relation": "author_of",
        "tail": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving"
      },
      {
        "head": "Wenyu Liu",
        "relation": "author_of",
        "tail": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving"
      },
      {
        "head": "Xinggang Wang",
        "relation": "author_of",
        "tail": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving"
      },
      {
        "head": "Wei Cao",
        "relation": "author_of",
        "tail": "Pseudo-Simulation for Autonomous Driving"
      },
      {
        "head": "Marcel Hallgarten",
        "relation": "author_of",
        "tail": "Pseudo-Simulation for Autonomous Driving"
      },
      {
        "head": "Tianyu Li",
        "relation": "author_of",
        "tail": "Pseudo-Simulation for Autonomous Driving"
      },
      {
        "head": "Daniel Dauner",
        "relation": "author_of",
        "tail": "Pseudo-Simulation for Autonomous Driving"
      },
      {
        "head": "Xunjiang Gu",
        "relation": "author_of",
        "tail": "Pseudo-Simulation for Autonomous Driving"
      },
      {
        "head": "Caojun Wang",
        "relation": "author_of",
        "tail": "Pseudo-Simulation for Autonomous Driving"
      },
      {
        "head": "Yakov Miron",
        "relation": "author_of",
        "tail": "Pseudo-Simulation for Autonomous Driving"
      },
      {
        "head": "Marco Aiello",
        "relation": "author_of",
        "tail": "Pseudo-Simulation for Autonomous Driving"
      },
      {
        "head": "Hongyang Li",
        "relation": "author_of",
        "tail": "Pseudo-Simulation for Autonomous Driving"
      },
      {
        "head": "Igor Gilitschenski",
        "relation": "author_of",
        "tail": "Pseudo-Simulation for Autonomous Driving"
      },
      {
        "head": "Boris Ivanovic",
        "relation": "author_of",
        "tail": "Pseudo-Simulation for Autonomous Driving"
      },
      {
        "head": "Marco Pavone",
        "relation": "author_of",
        "tail": "Pseudo-Simulation for Autonomous Driving"
      },
      {
        "head": "Andreas Geiger",
        "relation": "author_of",
        "tail": "Pseudo-Simulation for Autonomous Driving"
      },
      {
        "head": "Kashyap Chitta",
        "relation": "author_of",
        "tail": "Pseudo-Simulation for Autonomous Driving"
      },
      {
        "head": "Zhenjie Yang",
        "relation": "author_of",
        "tail": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving"
      },
      {
        "head": "Yilin Chai",
        "relation": "author_of",
        "tail": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving"
      },
      {
        "head": "Xiaosong Jia",
        "relation": "author_of",
        "tail": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving"
      },
      {
        "head": "Qifeng Li",
        "relation": "author_of",
        "tail": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving"
      },
      {
        "head": "Yuqian Shao",
        "relation": "author_of",
        "tail": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving"
      },
      {
        "head": "Xuekai Zhu",
        "relation": "author_of",
        "tail": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving"
      },
      {
        "head": "Haisheng Su",
        "relation": "author_of",
        "tail": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving"
      },
      {
        "head": "Junchi Yan",
        "relation": "author_of",
        "tail": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving"
      },
      {
        "head": "Sicong Jiang",
        "relation": "author_of",
        "tail": "A Survey on Vision-Language-Action Models for Autonomous Driving"
      },
      {
        "head": "Zilin Huang",
        "relation": "author_of",
        "tail": "A Survey on Vision-Language-Action Models for Autonomous Driving"
      },
      {
        "head": "Kangan Qian",
        "relation": "author_of",
        "tail": "A Survey on Vision-Language-Action Models for Autonomous Driving"
      },
      {
        "head": "Ziang Luo",
        "relation": "author_of",
        "tail": "A Survey on Vision-Language-Action Models for Autonomous Driving"
      },
      {
        "head": "Tianze Zhu",
        "relation": "author_of",
        "tail": "A Survey on Vision-Language-Action Models for Autonomous Driving"
      },
      {
        "head": "Yang Zhong",
        "relation": "author_of",
        "tail": "A Survey on Vision-Language-Action Models for Autonomous Driving"
      },
      {
        "head": "Yihong Tang",
        "relation": "author_of",
        "tail": "A Survey on Vision-Language-Action Models for Autonomous Driving"
      },
      {
        "head": "Menglin Kong",
        "relation": "author_of",
        "tail": "A Survey on Vision-Language-Action Models for Autonomous Driving"
      },
      {
        "head": "Yunlong Wang",
        "relation": "author_of",
        "tail": "A Survey on Vision-Language-Action Models for Autonomous Driving"
      },
      {
        "head": "Siwen Jiao",
        "relation": "author_of",
        "tail": "A Survey on Vision-Language-Action Models for Autonomous Driving"
      },
      {
        "head": "Hao Ye",
        "relation": "author_of",
        "tail": "A Survey on Vision-Language-Action Models for Autonomous Driving"
      },
      {
        "head": "Zihao Sheng",
        "relation": "author_of",
        "tail": "A Survey on Vision-Language-Action Models for Autonomous Driving"
      },
      {
        "head": "Xin Zhao",
        "relation": "author_of",
        "tail": "A Survey on Vision-Language-Action Models for Autonomous Driving"
      },
      {
        "head": "Tuopu Wen",
        "relation": "author_of",
        "tail": "A Survey on Vision-Language-Action Models for Autonomous Driving"
      },
      {
        "head": "Zheng Fu",
        "relation": "author_of",
        "tail": "A Survey on Vision-Language-Action Models for Autonomous Driving"
      },
      {
        "head": "Sikai Chen",
        "relation": "author_of",
        "tail": "A Survey on Vision-Language-Action Models for Autonomous Driving"
      },
      {
        "head": "Kun Jiang",
        "relation": "author_of",
        "tail": "A Survey on Vision-Language-Action Models for Autonomous Driving"
      },
      {
        "head": "Diange Yang",
        "relation": "author_of",
        "tail": "A Survey on Vision-Language-Action Models for Autonomous Driving"
      },
      {
        "head": "Seongjin Choi",
        "relation": "author_of",
        "tail": "A Survey on Vision-Language-Action Models for Autonomous Driving"
      },
      {
        "head": "Lijun Sun",
        "relation": "author_of",
        "tail": "A Survey on Vision-Language-Action Models for Autonomous Driving"
      },
      {
        "head": "M. Page",
        "relation": "author_of",
        "tail": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis"
      },
      {
        "head": "J. McKenzie",
        "relation": "author_of",
        "tail": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis"
      },
      {
        "head": "P. Bossuyt",
        "relation": "author_of",
        "tail": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis"
      },
      {
        "head": "I. Boutron",
        "relation": "author_of",
        "tail": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis"
      },
      {
        "head": "T. Hoffmann",
        "relation": "author_of",
        "tail": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis"
      },
      {
        "head": "C. Mulrow",
        "relation": "author_of",
        "tail": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis"
      },
      {
        "head": "Larissa Shamseer",
        "relation": "author_of",
        "tail": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis"
      },
      {
        "head": "J. Tetzlaff",
        "relation": "author_of",
        "tail": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis"
      },
      {
        "head": "E. Akl",
        "relation": "author_of",
        "tail": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis"
      },
      {
        "head": "S. Brennan",
        "relation": "author_of",
        "tail": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis"
      },
      {
        "head": "R. Chou",
        "relation": "author_of",
        "tail": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis"
      },
      {
        "head": "Julie May Glanville",
        "relation": "author_of",
        "tail": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis"
      },
      {
        "head": "J. Grimshaw",
        "relation": "author_of",
        "tail": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis"
      },
      {
        "head": "A. Hrõbjartsson",
        "relation": "author_of",
        "tail": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis"
      },
      {
        "head": "M. Lalu",
        "relation": "author_of",
        "tail": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis"
      },
      {
        "head": "Tianjing Li",
        "relation": "author_of",
        "tail": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis"
      },
      {
        "head": "E. Loder",
        "relation": "author_of",
        "tail": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis"
      },
      {
        "head": "E. Mayo-Wilson",
        "relation": "author_of",
        "tail": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis"
      },
      {
        "head": "Steve McDonald",
        "relation": "author_of",
        "tail": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis"
      },
      {
        "head": "L. McGuinness",
        "relation": "author_of",
        "tail": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis"
      },
      {
        "head": "L. Stewart",
        "relation": "author_of",
        "tail": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis"
      },
      {
        "head": "James Thomas",
        "relation": "author_of",
        "tail": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis"
      },
      {
        "head": "A. Tricco",
        "relation": "author_of",
        "tail": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis"
      },
      {
        "head": "V. Welch",
        "relation": "author_of",
        "tail": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis"
      },
      {
        "head": "P. Whiting",
        "relation": "author_of",
        "tail": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis"
      },
      {
        "head": "D. Moher",
        "relation": "author_of",
        "tail": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis"
      },
      {
        "head": "Zheng Zhu",
        "relation": "author_of",
        "tail": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond"
      },
      {
        "head": "Xiaofeng Wang",
        "relation": "author_of",
        "tail": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond"
      },
      {
        "head": "Wangbo Zhao",
        "relation": "author_of",
        "tail": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond"
      },
      {
        "head": "Chen Min",
        "relation": "author_of",
        "tail": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond"
      },
      {
        "head": "Bohan Li",
        "relation": "author_of",
        "tail": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond"
      },
      {
        "head": "Nianchen Deng",
        "relation": "author_of",
        "tail": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond"
      },
      {
        "head": "Min Dou",
        "relation": "author_of",
        "tail": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond"
      },
      {
        "head": "Yuqi Wang",
        "relation": "author_of",
        "tail": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond"
      },
      {
        "head": "Botian Shi",
        "relation": "author_of",
        "tail": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond"
      },
      {
        "head": "Kai Wang",
        "relation": "author_of",
        "tail": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond"
      },
      {
        "head": "Chi Zhang",
        "relation": "author_of",
        "tail": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond"
      },
      {
        "head": "Yang You",
        "relation": "author_of",
        "tail": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond"
      },
      {
        "head": "Zhaoxiang Zhang",
        "relation": "author_of",
        "tail": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond"
      },
      {
        "head": "Dawei Zhao",
        "relation": "author_of",
        "tail": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond"
      },
      {
        "head": "Liang Xiao",
        "relation": "author_of",
        "tail": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond"
      },
      {
        "head": "Jian Zhao",
        "relation": "author_of",
        "tail": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond"
      },
      {
        "head": "Jiwen Lu",
        "relation": "author_of",
        "tail": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond"
      },
      {
        "head": "Guan Huang",
        "relation": "author_of",
        "tail": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond"
      },
      {
        "head": "Jingtao Ding",
        "relation": "author_of",
        "tail": "Understanding World or Predicting Future? A Comprehensive Survey of World Models"
      },
      {
        "head": "Yunke Zhang",
        "relation": "author_of",
        "tail": "Understanding World or Predicting Future? A Comprehensive Survey of World Models"
      },
      {
        "head": "Yu Shang",
        "relation": "author_of",
        "tail": "Understanding World or Predicting Future? A Comprehensive Survey of World Models"
      },
      {
        "head": "Jie Feng",
        "relation": "author_of",
        "tail": "Understanding World or Predicting Future? A Comprehensive Survey of World Models"
      },
      {
        "head": "Yuheng Zhang",
        "relation": "author_of",
        "tail": "Understanding World or Predicting Future? A Comprehensive Survey of World Models"
      },
      {
        "head": "Zefang Zong",
        "relation": "author_of",
        "tail": "Understanding World or Predicting Future? A Comprehensive Survey of World Models"
      },
      {
        "head": "Yuan Yuan",
        "relation": "author_of",
        "tail": "Understanding World or Predicting Future? A Comprehensive Survey of World Models"
      },
      {
        "head": "Hongyuan Su",
        "relation": "author_of",
        "tail": "Understanding World or Predicting Future? A Comprehensive Survey of World Models"
      },
      {
        "head": "Nian Li",
        "relation": "author_of",
        "tail": "Understanding World or Predicting Future? A Comprehensive Survey of World Models"
      },
      {
        "head": "Jinghua Piao",
        "relation": "author_of",
        "tail": "Understanding World or Predicting Future? A Comprehensive Survey of World Models"
      },
      {
        "head": "Yucheng Deng",
        "relation": "author_of",
        "tail": "Understanding World or Predicting Future? A Comprehensive Survey of World Models"
      },
      {
        "head": "Nicholas Sukiennik",
        "relation": "author_of",
        "tail": "Understanding World or Predicting Future? A Comprehensive Survey of World Models"
      },
      {
        "head": "Chen Gao",
        "relation": "author_of",
        "tail": "Understanding World or Predicting Future? A Comprehensive Survey of World Models"
      },
      {
        "head": "Fengli Xu",
        "relation": "author_of",
        "tail": "Understanding World or Predicting Future? A Comprehensive Survey of World Models"
      },
      {
        "head": "Yong Li",
        "relation": "author_of",
        "tail": "Understanding World or Predicting Future? A Comprehensive Survey of World Models"
      },
      {
        "head": "Yuan Yuan",
        "relation": "author_of",
        "tail": "A Survey of Multimodal Learning: Methods, Applications, and Future"
      },
      {
        "head": "Zhaojian Li",
        "relation": "author_of",
        "tail": "A Survey of Multimodal Learning: Methods, Applications, and Future"
      },
      {
        "head": "Bin Zhao",
        "relation": "author_of",
        "tail": "A Survey of Multimodal Learning: Methods, Applications, and Future"
      },
      {
        "head": "Xuannan Liu",
        "relation": "author_of",
        "tail": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey"
      },
      {
        "head": "Xing Cui",
        "relation": "author_of",
        "tail": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey"
      },
      {
        "head": "Peipei Li",
        "relation": "author_of",
        "tail": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey"
      },
      {
        "head": "Zekun Li",
        "relation": "author_of",
        "tail": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey"
      },
      {
        "head": "Huaibo Huang",
        "relation": "author_of",
        "tail": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey"
      },
      {
        "head": "Shuhan Xia",
        "relation": "author_of",
        "tail": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey"
      },
      {
        "head": "Miaoxuan Zhang",
        "relation": "author_of",
        "tail": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey"
      },
      {
        "head": "Yueying Zou",
        "relation": "author_of",
        "tail": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey"
      },
      {
        "head": "Ran He",
        "relation": "author_of",
        "tail": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey"
      },
      {
        "head": "Galina Ilieva",
        "relation": "author_of",
        "tail": "Effects of Generative AI in Tourism Industry"
      },
      {
        "head": "Tania Yankova",
        "relation": "author_of",
        "tail": "Effects of Generative AI in Tourism Industry"
      },
      {
        "head": "Stanislava Klisarova-Belcheva",
        "relation": "author_of",
        "tail": "Effects of Generative AI in Tourism Industry"
      },
      {
        "head": "Jie Hu",
        "relation": "author_of",
        "tail": "Squeeze-and-Excitation Networks"
      },
      {
        "head": "Li Shen",
        "relation": "author_of",
        "tail": "Squeeze-and-Excitation Networks"
      },
      {
        "head": "Samuel Albanie",
        "relation": "author_of",
        "tail": "Squeeze-and-Excitation Networks"
      },
      {
        "head": "Gang Sun",
        "relation": "author_of",
        "tail": "Squeeze-and-Excitation Networks"
      },
      {
        "head": "Enhua Wu",
        "relation": "author_of",
        "tail": "Squeeze-and-Excitation Networks"
      },
      {
        "head": "I. Loshchilov",
        "relation": "author_of",
        "tail": "Decoupled Weight Decay Regularization"
      },
      {
        "head": "F. Hutter",
        "relation": "author_of",
        "tail": "Decoupled Weight Decay Regularization"
      },
      {
        "head": "Mingxing Tan",
        "relation": "author_of",
        "tail": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"
      },
      {
        "head": "Quoc V. Le",
        "relation": "author_of",
        "tail": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"
      },
      {
        "head": "Tianqi Liu",
        "relation": "author_of",
        "tail": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control"
      },
      {
        "head": "Zhaoxi Chen",
        "relation": "author_of",
        "tail": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control"
      },
      {
        "head": "Zihao Huang",
        "relation": "author_of",
        "tail": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control"
      },
      {
        "head": "Shaocong Xu",
        "relation": "author_of",
        "tail": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control"
      },
      {
        "head": "Saining Zhang",
        "relation": "author_of",
        "tail": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control"
      },
      {
        "head": "Chongjie Ye",
        "relation": "author_of",
        "tail": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control"
      },
      {
        "head": "Bohan Li",
        "relation": "author_of",
        "tail": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control"
      },
      {
        "head": "Zhiguo Cao",
        "relation": "author_of",
        "tail": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control"
      },
      {
        "head": "Wei Li",
        "relation": "author_of",
        "tail": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control"
      },
      {
        "head": "Hao Zhao",
        "relation": "author_of",
        "tail": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control"
      },
      {
        "head": "Ziwei Liu",
        "relation": "author_of",
        "tail": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control"
      },
      {
        "head": "Tianze Xia",
        "relation": "author_of",
        "tail": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World"
      },
      {
        "head": "Yongkang Li",
        "relation": "author_of",
        "tail": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World"
      },
      {
        "head": "Lijun Zhou",
        "relation": "author_of",
        "tail": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World"
      },
      {
        "head": "Jingfeng Yao",
        "relation": "author_of",
        "tail": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World"
      },
      {
        "head": "Kaixin Xiong",
        "relation": "author_of",
        "tail": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World"
      },
      {
        "head": "Haiyang Sun",
        "relation": "author_of",
        "tail": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World"
      },
      {
        "head": "Bing Wang",
        "relation": "author_of",
        "tail": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World"
      },
      {
        "head": "Kun Ma",
        "relation": "author_of",
        "tail": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World"
      },
      {
        "head": "Guang Chen",
        "relation": "author_of",
        "tail": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World"
      },
      {
        "head": "Hangjun Ye",
        "relation": "author_of",
        "tail": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World"
      },
      {
        "head": "Wenyu Liu",
        "relation": "author_of",
        "tail": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World"
      },
      {
        "head": "Xinggang Wang",
        "relation": "author_of",
        "tail": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World"
      },
      {
        "head": "Sicheng Zuo",
        "relation": "author_of",
        "tail": "DVGT: Driving Visual Geometry Transformer"
      },
      {
        "head": "Zixun Xie",
        "relation": "author_of",
        "tail": "DVGT: Driving Visual Geometry Transformer"
      },
      {
        "head": "Wenzhao Zheng",
        "relation": "author_of",
        "tail": "DVGT: Driving Visual Geometry Transformer"
      },
      {
        "head": "Shaoqing Xu",
        "relation": "author_of",
        "tail": "DVGT: Driving Visual Geometry Transformer"
      },
      {
        "head": "Fang Li",
        "relation": "author_of",
        "tail": "DVGT: Driving Visual Geometry Transformer"
      },
      {
        "head": "Shengyin Jiang",
        "relation": "author_of",
        "tail": "DVGT: Driving Visual Geometry Transformer"
      },
      {
        "head": "Long Chen",
        "relation": "author_of",
        "tail": "DVGT: Driving Visual Geometry Transformer"
      },
      {
        "head": "Zhi-Xin Yang",
        "relation": "author_of",
        "tail": "DVGT: Driving Visual Geometry Transformer"
      },
      {
        "head": "Jiwen Lu",
        "relation": "author_of",
        "tail": "DVGT: Driving Visual Geometry Transformer"
      },
      {
        "head": "Lvmin Zhang",
        "relation": "author_of",
        "tail": "Adding Conditional Control to Text-to-Image Diffusion Models"
      },
      {
        "head": "Anyi Rao",
        "relation": "author_of",
        "tail": "Adding Conditional Control to Text-to-Image Diffusion Models"
      },
      {
        "head": "Maneesh Agrawala",
        "relation": "author_of",
        "tail": "Adding Conditional Control to Text-to-Image Diffusion Models"
      },
      {
        "head": "Hyung Won Chung",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Le Hou",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Shayne Longpre",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Barret Zoph",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Yi Tay",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "William Fedus",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Yunxuan Li",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Xuezhi Wang",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Mostafa Dehghani",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Siddhartha Brahma",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Albert Webson",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Shixiang Shane Gu",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Zhuyun Dai",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Mirac Suzgun",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Xinyun Chen",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Aakanksha Chowdhery",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Alex Castro-Ros",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Marie Pellat",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Kevin Robinson",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Dasha Valter",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Sharan Narang",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Gaurav Mishra",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Adams Yu",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Vincent Zhao",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Yanping Huang",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Andrew Dai",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Hongkun Yu",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Slav Petrov",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Ed H. Chi",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Jeff Dean",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Jacob Devlin",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Adam Roberts",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Denny Zhou",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Quoc V. Le",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Jason Wei",
        "relation": "author_of",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Robin Rombach",
        "relation": "author_of",
        "tail": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "head": "Andreas Blattmann",
        "relation": "author_of",
        "tail": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "head": "Dominik Lorenz",
        "relation": "author_of",
        "tail": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "head": "Patrick Esser",
        "relation": "author_of",
        "tail": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "head": "Björn Ommer",
        "relation": "author_of",
        "tail": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "head": "Romain Lopez",
        "relation": "author_of",
        "tail": "AUTO-ENCODING VARIATIONAL BAYES"
      },
      {
        "head": "Pierre Boyeau",
        "relation": "author_of",
        "tail": "AUTO-ENCODING VARIATIONAL BAYES"
      },
      {
        "head": "N. Yosef",
        "relation": "author_of",
        "tail": "AUTO-ENCODING VARIATIONAL BAYES"
      },
      {
        "head": "Michael I. Jordan",
        "relation": "author_of",
        "tail": "AUTO-ENCODING VARIATIONAL BAYES"
      },
      {
        "head": "J. Regier",
        "relation": "author_of",
        "tail": "AUTO-ENCODING VARIATIONAL BAYES"
      },
      {
        "head": "Richard Zhang",
        "relation": "author_of",
        "tail": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric"
      },
      {
        "head": "Phillip Isola",
        "relation": "author_of",
        "tail": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric"
      },
      {
        "head": "Alexei A. Efros",
        "relation": "author_of",
        "tail": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric"
      },
      {
        "head": "Eli Shechtman",
        "relation": "author_of",
        "tail": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric"
      },
      {
        "head": "Oliver Wang",
        "relation": "author_of",
        "tail": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric"
      },
      {
        "head": "Pei Sun",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Henrik Kretzschmar",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Xerxes Dotiwalla",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Aurelien Chouard",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Vijaysai Patnaik",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Paul Tsui",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "James Guo",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Yin Zhou",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Yuning Chai",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Benjamin Caine",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Vijay Vasudevan",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Wei Han",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Jiquan Ngiam",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Hang Zhao",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Aleksei Timofeev",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Scott Ettinger",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Maxim Krivokon",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Amy Gao",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Aditya Joshi",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Sheng Zhao",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Shuyang Cheng",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Yu Zhang",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Jonathon Shlens",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Zhifeng Chen",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Dragomir Anguelov",
        "relation": "author_of",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Gilad Cohen",
        "relation": "author_of",
        "tail": "Generative Adversarial Networks"
      },
      {
        "head": "Raja Giryes",
        "relation": "author_of",
        "tail": "Generative Adversarial Networks"
      },
      {
        "head": "Zekai Zhang",
        "relation": "author_of",
        "tail": "Generalization of Diffusion Models Arises with a Balanced Representation Space"
      },
      {
        "head": "Xiao Li",
        "relation": "author_of",
        "tail": "Generalization of Diffusion Models Arises with a Balanced Representation Space"
      },
      {
        "head": "Xiang Li",
        "relation": "author_of",
        "tail": "Generalization of Diffusion Models Arises with a Balanced Representation Space"
      },
      {
        "head": "Lianghe Shi",
        "relation": "author_of",
        "tail": "Generalization of Diffusion Models Arises with a Balanced Representation Space"
      },
      {
        "head": "Meng Wu",
        "relation": "author_of",
        "tail": "Generalization of Diffusion Models Arises with a Balanced Representation Space"
      },
      {
        "head": "Molei Tao",
        "relation": "author_of",
        "tail": "Generalization of Diffusion Models Arises with a Balanced Representation Space"
      },
      {
        "head": "Qing Qu",
        "relation": "author_of",
        "tail": "Generalization of Diffusion Models Arises with a Balanced Representation Space"
      },
      {
        "head": "Donglin Yang",
        "relation": "author_of",
        "tail": "Stable Velocity: A Variance Perspective on Flow Matching"
      },
      {
        "head": "Yongxing Zhang",
        "relation": "author_of",
        "tail": "Stable Velocity: A Variance Perspective on Flow Matching"
      },
      {
        "head": "Xin Yu",
        "relation": "author_of",
        "tail": "Stable Velocity: A Variance Perspective on Flow Matching"
      },
      {
        "head": "Liang Hou",
        "relation": "author_of",
        "tail": "Stable Velocity: A Variance Perspective on Flow Matching"
      },
      {
        "head": "Xin Tao",
        "relation": "author_of",
        "tail": "Stable Velocity: A Variance Perspective on Flow Matching"
      },
      {
        "head": "Pengfei Wan",
        "relation": "author_of",
        "tail": "Stable Velocity: A Variance Perspective on Flow Matching"
      },
      {
        "head": "Xiaojuan Qi",
        "relation": "author_of",
        "tail": "Stable Velocity: A Variance Perspective on Flow Matching"
      },
      {
        "head": "Renjie Liao",
        "relation": "author_of",
        "tail": "Stable Velocity: A Variance Perspective on Flow Matching"
      },
      {
        "head": "Ramón Calvo-González",
        "relation": "author_of",
        "tail": "Laminating Representation Autoencoders for Efficient Diffusion"
      },
      {
        "head": "François Fleuret",
        "relation": "author_of",
        "tail": "Laminating Representation Autoencoders for Efficient Diffusion"
      },
      {
        "head": "Yao Teng",
        "relation": "author_of",
        "tail": "Adaptive 1D Video Diffusion Autoencoder"
      },
      {
        "head": "Minxuan Lin",
        "relation": "author_of",
        "tail": "Adaptive 1D Video Diffusion Autoencoder"
      },
      {
        "head": "Xian Liu",
        "relation": "author_of",
        "tail": "Adaptive 1D Video Diffusion Autoencoder"
      },
      {
        "head": "Shuai Wang",
        "relation": "author_of",
        "tail": "Adaptive 1D Video Diffusion Autoencoder"
      },
      {
        "head": "Xiao Yang",
        "relation": "author_of",
        "tail": "Adaptive 1D Video Diffusion Autoencoder"
      },
      {
        "head": "Xihui Liu",
        "relation": "author_of",
        "tail": "Adaptive 1D Video Diffusion Autoencoder"
      },
      {
        "head": "Nicolas Sereyjol-Garros",
        "relation": "author_of",
        "tail": "Test-Time Conditioning with Representation-Aligned Visual Features"
      },
      {
        "head": "Ellington Kirby",
        "relation": "author_of",
        "tail": "Test-Time Conditioning with Representation-Aligned Visual Features"
      },
      {
        "head": "Victor Letzelter",
        "relation": "author_of",
        "tail": "Test-Time Conditioning with Representation-Aligned Visual Features"
      },
      {
        "head": "Victor Besnier",
        "relation": "author_of",
        "tail": "Test-Time Conditioning with Representation-Aligned Visual Features"
      },
      {
        "head": "Nermin Samet",
        "relation": "author_of",
        "tail": "Test-Time Conditioning with Representation-Aligned Visual Features"
      },
      {
        "head": "Ana Davila",
        "relation": "author_of",
        "tail": "Bio-inspired fine-tuning for selective transfer learning in image classification"
      },
      {
        "head": "Jacinto Colan",
        "relation": "author_of",
        "tail": "Bio-inspired fine-tuning for selective transfer learning in image classification"
      },
      {
        "head": "Yasuhisa Hasegawa",
        "relation": "author_of",
        "tail": "Bio-inspired fine-tuning for selective transfer learning in image classification"
      },
      {
        "head": "Gao Huang",
        "relation": "author_of",
        "tail": "Densely Connected Convolutional Networks"
      },
      {
        "head": "Zhuang Liu",
        "relation": "author_of",
        "tail": "Densely Connected Convolutional Networks"
      },
      {
        "head": "Laurens van der Maaten",
        "relation": "author_of",
        "tail": "Densely Connected Convolutional Networks"
      },
      {
        "head": "Kilian Q. Weinberger",
        "relation": "author_of",
        "tail": "Densely Connected Convolutional Networks"
      },
      {
        "head": "Zhongyu Zeng",
        "relation": "author_of",
        "tail": "An extratropical cyclone center location method on satellite images based on transfer learning"
      },
      {
        "head": "Xuan Peng",
        "relation": "author_of",
        "tail": "An extratropical cyclone center location method on satellite images based on transfer learning"
      },
      {
        "head": "J. B. Awotunde",
        "relation": "author_of",
        "tail": "An Advanced Convolutional Neural Network Architecture Utilizing Transfer Learning for Melanoma Detection"
      },
      {
        "head": "Korede Israel Adeyanju",
        "relation": "author_of",
        "tail": "An Advanced Convolutional Neural Network Architecture Utilizing Transfer Learning for Melanoma Detection"
      },
      {
        "head": "Kehinde Elisha Akerele",
        "relation": "author_of",
        "tail": "An Advanced Convolutional Neural Network Architecture Utilizing Transfer Learning for Melanoma Detection"
      },
      {
        "head": "Oluwatobi Akinlade",
        "relation": "author_of",
        "tail": "An Advanced Convolutional Neural Network Architecture Utilizing Transfer Learning for Melanoma Detection"
      },
      {
        "head": "S. Folorunso",
        "relation": "author_of",
        "tail": "An Advanced Convolutional Neural Network Architecture Utilizing Transfer Learning for Melanoma Detection"
      },
      {
        "head": "S. Ajagbe",
        "relation": "author_of",
        "tail": "An Advanced Convolutional Neural Network Architecture Utilizing Transfer Learning for Melanoma Detection"
      },
      {
        "head": "Ana Davila",
        "relation": "author_of",
        "tail": "Adaptive Transfer Learning for Surgical Tool Presence Detection in Laparoscopic Videos Through Gradual Freezing Fine‐Tuning"
      },
      {
        "head": "Jacinto Colan",
        "relation": "author_of",
        "tail": "Adaptive Transfer Learning for Surgical Tool Presence Detection in Laparoscopic Videos Through Gradual Freezing Fine‐Tuning"
      },
      {
        "head": "Yasuhisa Hasegawa",
        "relation": "author_of",
        "tail": "Adaptive Transfer Learning for Surgical Tool Presence Detection in Laparoscopic Videos Through Gradual Freezing Fine‐Tuning"
      },
      {
        "head": "Subham Sharma",
        "relation": "author_of",
        "tail": "VGG Induced Deep Hand Sign Language Detection"
      },
      {
        "head": "Sharmila Subudhi",
        "relation": "author_of",
        "tail": "VGG Induced Deep Hand Sign Language Detection"
      },
      {
        "head": "Camillo Lugaresi",
        "relation": "author_of",
        "tail": "MediaPipe: A Framework for Building Perception Pipelines"
      },
      {
        "head": "Jiuqiang Tang",
        "relation": "author_of",
        "tail": "MediaPipe: A Framework for Building Perception Pipelines"
      },
      {
        "head": "Hadon Nash",
        "relation": "author_of",
        "tail": "MediaPipe: A Framework for Building Perception Pipelines"
      },
      {
        "head": "Chris McClanahan",
        "relation": "author_of",
        "tail": "MediaPipe: A Framework for Building Perception Pipelines"
      },
      {
        "head": "Esha Uboweja",
        "relation": "author_of",
        "tail": "MediaPipe: A Framework for Building Perception Pipelines"
      },
      {
        "head": "Michael Hays",
        "relation": "author_of",
        "tail": "MediaPipe: A Framework for Building Perception Pipelines"
      },
      {
        "head": "Fan Zhang",
        "relation": "author_of",
        "tail": "MediaPipe: A Framework for Building Perception Pipelines"
      },
      {
        "head": "Chuo-Ling Chang",
        "relation": "author_of",
        "tail": "MediaPipe: A Framework for Building Perception Pipelines"
      },
      {
        "head": "Ming Guang Yong",
        "relation": "author_of",
        "tail": "MediaPipe: A Framework for Building Perception Pipelines"
      },
      {
        "head": "Juhyun Lee",
        "relation": "author_of",
        "tail": "MediaPipe: A Framework for Building Perception Pipelines"
      },
      {
        "head": "Wan-Teh Chang",
        "relation": "author_of",
        "tail": "MediaPipe: A Framework for Building Perception Pipelines"
      },
      {
        "head": "Wei Hua",
        "relation": "author_of",
        "tail": "MediaPipe: A Framework for Building Perception Pipelines"
      },
      {
        "head": "Manfred Georg",
        "relation": "author_of",
        "tail": "MediaPipe: A Framework for Building Perception Pipelines"
      },
      {
        "head": "Matthias Grundmann",
        "relation": "author_of",
        "tail": "MediaPipe: A Framework for Building Perception Pipelines"
      },
      {
        "head": "Cem Keskin",
        "relation": "author_of",
        "tail": "Hand Pose Estimation and Hand Shape Classification Using Multi-layered Randomized Decision Forests"
      },
      {
        "head": "Mustafa Furkan Kıraç",
        "relation": "author_of",
        "tail": "Hand Pose Estimation and Hand Shape Classification Using Multi-layered Randomized Decision Forests"
      },
      {
        "head": "Yunus Emre Kara",
        "relation": "author_of",
        "tail": "Hand Pose Estimation and Hand Shape Classification Using Multi-layered Randomized Decision Forests"
      },
      {
        "head": "L. Akarun",
        "relation": "author_of",
        "tail": "Hand Pose Estimation and Hand Shape Classification Using Multi-layered Randomized Decision Forests"
      },
      {
        "head": "S. P. Priyal",
        "relation": "author_of",
        "tail": "A robust static hand gesture recognition system using geometry based normalizations and Krawtchouk moments"
      },
      {
        "head": "P. Bora",
        "relation": "author_of",
        "tail": "A robust static hand gesture recognition system using geometry based normalizations and Krawtchouk moments"
      },
      {
        "head": "Octavian Dudas",
        "relation": "author_of",
        "tail": "Hand signal classification system for sign language communication in Virtual Reality"
      },
      {
        "head": "C. Nandra",
        "relation": "author_of",
        "tail": "Hand signal classification system for sign language communication in Virtual Reality"
      },
      {
        "head": "C. Mocan",
        "relation": "author_of",
        "tail": "Hand signal classification system for sign language communication in Virtual Reality"
      },
      {
        "head": "D. Gorgan",
        "relation": "author_of",
        "tail": "Hand signal classification system for sign language communication in Virtual Reality"
      },
      {
        "head": "Avinash Dhiran",
        "relation": "author_of",
        "tail": "Real-Time Static Hand Sign Recognition System using MediaPipe and Fully Connected Neural Network"
      },
      {
        "head": "Anurag Kumbhare",
        "relation": "author_of",
        "tail": "Real-Time Static Hand Sign Recognition System using MediaPipe and Fully Connected Neural Network"
      },
      {
        "head": "Achal Patil",
        "relation": "author_of",
        "tail": "Real-Time Static Hand Sign Recognition System using MediaPipe and Fully Connected Neural Network"
      },
      {
        "head": "Mrugank Vichare",
        "relation": "author_of",
        "tail": "Real-Time Static Hand Sign Recognition System using MediaPipe and Fully Connected Neural Network"
      },
      {
        "head": "Dhananjay Patel",
        "relation": "author_of",
        "tail": "Real-Time Static Hand Sign Recognition System using MediaPipe and Fully Connected Neural Network"
      },
      {
        "head": "Saransh Mishra",
        "relation": "author_of",
        "tail": "Real-Time Gesture Recognition to Aid Communication in Children with Motor Impairments"
      },
      {
        "head": "Pavan Nair",
        "relation": "author_of",
        "tail": "Real-Time Gesture Recognition to Aid Communication in Children with Motor Impairments"
      },
      {
        "head": "Pushpalatha M",
        "relation": "author_of",
        "tail": "Real-Time Gesture Recognition to Aid Communication in Children with Motor Impairments"
      },
      {
        "head": "Poornima S",
        "relation": "author_of",
        "tail": "Real-Time Gesture Recognition to Aid Communication in Children with Motor Impairments"
      },
      {
        "head": "A. Krizhevsky",
        "relation": "author_of",
        "tail": "Learning Multiple Layers of Features from Tiny Images"
      },
      {
        "head": "Ross Girshick",
        "relation": "author_of",
        "tail": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation"
      },
      {
        "head": "Jeff Donahue",
        "relation": "author_of",
        "tail": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation"
      },
      {
        "head": "Trevor Darrell",
        "relation": "author_of",
        "tail": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation"
      },
      {
        "head": "Jitendra Malik",
        "relation": "author_of",
        "tail": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation"
      },
      {
        "head": "Shansong Liu",
        "relation": "author_of",
        "tail": "MuMu-LLaMA: Multi-modal music understanding and generation via large language models"
      },
      {
        "head": "Atin Sakkeer Hussain",
        "relation": "author_of",
        "tail": "MuMu-LLaMA: Multi-modal music understanding and generation via large language models"
      },
      {
        "head": "Qilong Wu",
        "relation": "author_of",
        "tail": "MuMu-LLaMA: Multi-modal music understanding and generation via large language models"
      },
      {
        "head": "Chenshuo Sun",
        "relation": "author_of",
        "tail": "MuMu-LLaMA: Multi-modal music understanding and generation via large language models"
      },
      {
        "head": "Ying Shan",
        "relation": "author_of",
        "tail": "MuMu-LLaMA: Multi-modal music understanding and generation via large language models"
      },
      {
        "head": "Jiahang Tu",
        "relation": "author_of",
        "tail": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy"
      },
      {
        "head": "Ye Li",
        "relation": "author_of",
        "tail": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy"
      },
      {
        "head": "Yiming Wu",
        "relation": "author_of",
        "tail": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy"
      },
      {
        "head": "Hanbin Zhao",
        "relation": "author_of",
        "tail": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy"
      },
      {
        "head": "Chao Zhang",
        "relation": "author_of",
        "tail": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy"
      },
      {
        "head": "Hui Qian",
        "relation": "author_of",
        "tail": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy"
      },
      {
        "head": "Haotian Lv",
        "relation": "author_of",
        "tail": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism"
      },
      {
        "head": "Yuhui Zhang",
        "relation": "author_of",
        "tail": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism"
      },
      {
        "head": "Jiangbo Dai",
        "relation": "author_of",
        "tail": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism"
      },
      {
        "head": "Hanli Wu",
        "relation": "author_of",
        "tail": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism"
      },
      {
        "head": "Jiaji Wang",
        "relation": "author_of",
        "tail": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism"
      },
      {
        "head": "Dawei Wang",
        "relation": "author_of",
        "tail": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism"
      },
      {
        "head": "Mingxin Li",
        "relation": "author_of",
        "tail": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking"
      },
      {
        "head": "Yanzhao Zhang",
        "relation": "author_of",
        "tail": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking"
      },
      {
        "head": "Dingkun Long",
        "relation": "author_of",
        "tail": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking"
      },
      {
        "head": "Keqin Chen",
        "relation": "author_of",
        "tail": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking"
      },
      {
        "head": "Sibo Song",
        "relation": "author_of",
        "tail": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking"
      },
      {
        "head": "Shuai Bai",
        "relation": "author_of",
        "tail": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking"
      },
      {
        "head": "Zhibo Yang",
        "relation": "author_of",
        "tail": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking"
      },
      {
        "head": "Pengjun Xie",
        "relation": "author_of",
        "tail": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking"
      },
      {
        "head": "An Yang",
        "relation": "author_of",
        "tail": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking"
      },
      {
        "head": "Dayiheng Liu",
        "relation": "author_of",
        "tail": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking"
      },
      {
        "head": "Jingren Zhou",
        "relation": "author_of",
        "tail": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking"
      },
      {
        "head": "Junyang Lin",
        "relation": "author_of",
        "tail": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking"
      },
      {
        "head": "Mingyue Chen",
        "relation": "author_of",
        "tail": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness"
      },
      {
        "head": "Xin Liao",
        "relation": "author_of",
        "tail": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness"
      },
      {
        "head": "Han Fang",
        "relation": "author_of",
        "tail": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness"
      },
      {
        "head": "Jinlin Guo",
        "relation": "author_of",
        "tail": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness"
      },
      {
        "head": "Yanxiang Chen",
        "relation": "author_of",
        "tail": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness"
      },
      {
        "head": "Xiaoshuai Wu",
        "relation": "author_of",
        "tail": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness"
      },
      {
        "head": "A. Dempster",
        "relation": "author_of",
        "tail": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
      },
      {
        "head": "N. Laird",
        "relation": "author_of",
        "tail": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
      },
      {
        "head": "D. Rubin",
        "relation": "author_of",
        "tail": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
      },
      {
        "head": "L. Maaten",
        "relation": "author_of",
        "tail": "Visualizing Data using t-SNE"
      },
      {
        "head": "Geoffrey E. Hinton",
        "relation": "author_of",
        "tail": "Visualizing Data using t-SNE"
      },
      {
        "head": "Jacy Reese Anthis",
        "relation": "author_of",
        "tail": "LLM Social Simulations Are a Promising Research Method"
      },
      {
        "head": "Ryan Liu",
        "relation": "author_of",
        "tail": "LLM Social Simulations Are a Promising Research Method"
      },
      {
        "head": "Sean M. Richardson",
        "relation": "author_of",
        "tail": "LLM Social Simulations Are a Promising Research Method"
      },
      {
        "head": "Austin C. Kozlowski",
        "relation": "author_of",
        "tail": "LLM Social Simulations Are a Promising Research Method"
      },
      {
        "head": "Bernard Koch",
        "relation": "author_of",
        "tail": "LLM Social Simulations Are a Promising Research Method"
      },
      {
        "head": "James Evans",
        "relation": "author_of",
        "tail": "LLM Social Simulations Are a Promising Research Method"
      },
      {
        "head": "Erik Brynjolfsson",
        "relation": "author_of",
        "tail": "LLM Social Simulations Are a Promising Research Method"
      },
      {
        "head": "Michael Bernstein",
        "relation": "author_of",
        "tail": "LLM Social Simulations Are a Promising Research Method"
      },
      {
        "head": "Zhiwen Xiao",
        "relation": "author_of",
        "tail": "Federated Contrastive Learning With Feature-Based Distillation for Human Activity Recognition"
      },
      {
        "head": "Huagang Tong",
        "relation": "author_of",
        "tail": "Federated Contrastive Learning With Feature-Based Distillation for Human Activity Recognition"
      },
      {
        "head": "Runqian Wang",
        "relation": "author_of",
        "tail": "Diffuse and Disperse: Image Generation with Representation Regularization"
      },
      {
        "head": "Kaiming He",
        "relation": "author_of",
        "tail": "Diffuse and Disperse: Image Generation with Representation Regularization"
      },
      {
        "head": "Ibomoiye Domor Mienye",
        "relation": "author_of",
        "tail": "Deep Autoencoder Neural Networks: A Comprehensive Review and New Perspectives"
      },
      {
        "head": "Theo G. Swart",
        "relation": "author_of",
        "tail": "Deep Autoencoder Neural Networks: A Comprehensive Review and New Perspectives"
      },
      {
        "head": "Jusheng Zhang",
        "relation": "author_of",
        "tail": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems"
      },
      {
        "head": "Zimeng Huang",
        "relation": "author_of",
        "tail": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems"
      },
      {
        "head": "Yijia Fan",
        "relation": "author_of",
        "tail": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems"
      },
      {
        "head": "Ningyuan Liu",
        "relation": "author_of",
        "tail": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems"
      },
      {
        "head": "Mingyan Li",
        "relation": "author_of",
        "tail": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems"
      },
      {
        "head": "Zhuojie Yang",
        "relation": "author_of",
        "tail": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems"
      },
      {
        "head": "Jiawei Yao",
        "relation": "author_of",
        "tail": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems"
      },
      {
        "head": "Jian Wang",
        "relation": "author_of",
        "tail": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems"
      },
      {
        "head": "Keze Wang",
        "relation": "author_of",
        "tail": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems"
      },
      {
        "head": "Olaf Ronneberger",
        "relation": "author_of",
        "tail": "U-Net: Convolutional Networks for Biomedical Image Segmentation"
      },
      {
        "head": "Philipp Fischer",
        "relation": "author_of",
        "tail": "U-Net: Convolutional Networks for Biomedical Image Segmentation"
      },
      {
        "head": "Thomas Brox",
        "relation": "author_of",
        "tail": "U-Net: Convolutional Networks for Biomedical Image Segmentation"
      },
      {
        "head": "Tianwei Yin",
        "relation": "author_of",
        "tail": "Improved Distribution Matching Distillation for Fast Image Synthesis"
      },
      {
        "head": "Michaël Gharbi",
        "relation": "author_of",
        "tail": "Improved Distribution Matching Distillation for Fast Image Synthesis"
      },
      {
        "head": "Taesung Park",
        "relation": "author_of",
        "tail": "Improved Distribution Matching Distillation for Fast Image Synthesis"
      },
      {
        "head": "Richard Zhang",
        "relation": "author_of",
        "tail": "Improved Distribution Matching Distillation for Fast Image Synthesis"
      },
      {
        "head": "Eli Shechtman",
        "relation": "author_of",
        "tail": "Improved Distribution Matching Distillation for Fast Image Synthesis"
      },
      {
        "head": "Fredo Durand",
        "relation": "author_of",
        "tail": "Improved Distribution Matching Distillation for Fast Image Synthesis"
      },
      {
        "head": "William T. Freeman",
        "relation": "author_of",
        "tail": "Improved Distribution Matching Distillation for Fast Image Synthesis"
      },
      {
        "head": "Axel Sauer",
        "relation": "author_of",
        "tail": "Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation"
      },
      {
        "head": "Frederic Boesel",
        "relation": "author_of",
        "tail": "Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation"
      },
      {
        "head": "Tim Dockhorn",
        "relation": "author_of",
        "tail": "Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation"
      },
      {
        "head": "Andreas Blattmann",
        "relation": "author_of",
        "tail": "Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation"
      },
      {
        "head": "Patrick Esser",
        "relation": "author_of",
        "tail": "Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation"
      },
      {
        "head": "Robin Rombach",
        "relation": "author_of",
        "tail": "Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation"
      },
      {
        "head": "Takuya Akiba",
        "relation": "author_of",
        "tail": "Evolutionary optimization of model merging recipes"
      },
      {
        "head": "Makoto Shing",
        "relation": "author_of",
        "tail": "Evolutionary optimization of model merging recipes"
      },
      {
        "head": "Yujin Tang",
        "relation": "author_of",
        "tail": "Evolutionary optimization of model merging recipes"
      },
      {
        "head": "Qi Sun",
        "relation": "author_of",
        "tail": "Evolutionary optimization of model merging recipes"
      },
      {
        "head": "David Ha",
        "relation": "author_of",
        "tail": "Evolutionary optimization of model merging recipes"
      },
      {
        "head": "Tianwei Yin",
        "relation": "author_of",
        "tail": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
      },
      {
        "head": "Qiang Zhang",
        "relation": "author_of",
        "tail": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
      },
      {
        "head": "Richard Zhang",
        "relation": "author_of",
        "tail": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
      },
      {
        "head": "William T. Freeman",
        "relation": "author_of",
        "tail": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
      },
      {
        "head": "Fredo Durand",
        "relation": "author_of",
        "tail": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
      },
      {
        "head": "Eli Shechtman",
        "relation": "author_of",
        "tail": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
      },
      {
        "head": "Xun Huang",
        "relation": "author_of",
        "tail": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
      },
      {
        "head": "Zinan Guo",
        "relation": "author_of",
        "tail": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment"
      },
      {
        "head": "Yanze Wu",
        "relation": "author_of",
        "tail": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment"
      },
      {
        "head": "Zhuowei Chen",
        "relation": "author_of",
        "tail": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment"
      },
      {
        "head": "Lang Chen",
        "relation": "author_of",
        "tail": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment"
      },
      {
        "head": "Peng Zhang",
        "relation": "author_of",
        "tail": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment"
      },
      {
        "head": "Qian He",
        "relation": "author_of",
        "tail": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment"
      },
      {
        "head": "Jonathan Ho",
        "relation": "author_of",
        "tail": "Denoising Diffusion Probabilistic Models"
      },
      {
        "head": "Ajay Jain",
        "relation": "author_of",
        "tail": "Denoising Diffusion Probabilistic Models"
      },
      {
        "head": "Pieter Abbeel",
        "relation": "author_of",
        "tail": "Denoising Diffusion Probabilistic Models"
      },
      {
        "head": "Weijie Kong",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Qi Tian",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Zijian Zhang",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Rox Min",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Zuozhuo Dai",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Jin Zhou",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Jiangfeng Xiong",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Xin Li",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Bo Wu",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Jianwei Zhang",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Kathrina Wu",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Qin Lin",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Junkun Yuan",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Yanxin Long",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Aladdin Wang",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Andong Wang",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Changlin Li",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Duojun Huang",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Fang Yang",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Hao Tan",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Hongmei Wang",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Jacob Song",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Jiawang Bai",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Jianbing Wu",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Jinbao Xue",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Joey Wang",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Kai Wang",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Mengyang Liu",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Pengyu Li",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Shuai Li",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Weiyan Wang",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Wenqing Yu",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Xinchi Deng",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Yang Li",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Yi Chen",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Yutao Cui",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Yuanbo Peng",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Zhentao Yu",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Zhiyu He",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Zhiyong Xu",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Zixiang Zhou",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Zunnan Xu",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Yangyu Tao",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Qinglin Lu",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Songtao Liu",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Dax Zhou",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Hongfa Wang",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Yong Yang",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Di Wang",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Yuhong Liu",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Jie Jiang",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Caesar Zhong",
        "relation": "author_of",
        "tail": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "head": "Jianwen Jiang",
        "relation": "author_of",
        "tail": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency"
      },
      {
        "head": "Chao Liang",
        "relation": "author_of",
        "tail": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency"
      },
      {
        "head": "Jiaqi Yang",
        "relation": "author_of",
        "tail": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency"
      },
      {
        "head": "Gaojie Lin",
        "relation": "author_of",
        "tail": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency"
      },
      {
        "head": "Tianyun Zhong",
        "relation": "author_of",
        "tail": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency"
      },
      {
        "head": "Yanbo Zheng",
        "relation": "author_of",
        "tail": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency"
      },
      {
        "head": "Gaojie Lin",
        "relation": "author_of",
        "tail": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models"
      },
      {
        "head": "Jianwen Jiang",
        "relation": "author_of",
        "tail": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models"
      },
      {
        "head": "Jiaqi Yang",
        "relation": "author_of",
        "tail": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models"
      },
      {
        "head": "Zerong Zheng",
        "relation": "author_of",
        "tail": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models"
      },
      {
        "head": "Chao Liang",
        "relation": "author_of",
        "tail": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models"
      },
      {
        "head": "Jiahao Cui",
        "relation": "author_of",
        "tail": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation"
      },
      {
        "head": "Hui Li",
        "relation": "author_of",
        "tail": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation"
      },
      {
        "head": "Yao Yao",
        "relation": "author_of",
        "tail": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation"
      },
      {
        "head": "Hao Zhu",
        "relation": "author_of",
        "tail": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation"
      },
      {
        "head": "Hanlin Shang",
        "relation": "author_of",
        "tail": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation"
      },
      {
        "head": "Kaihui Cheng",
        "relation": "author_of",
        "tail": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation"
      },
      {
        "head": "Hang Zhou",
        "relation": "author_of",
        "tail": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation"
      },
      {
        "head": "Siyu Zhu",
        "relation": "author_of",
        "tail": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation"
      },
      {
        "head": "Jingdong Wang",
        "relation": "author_of",
        "tail": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation"
      },
      {
        "head": "Rang Meng",
        "relation": "author_of",
        "tail": "EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation"
      },
      {
        "head": "Xingyu Zhang",
        "relation": "author_of",
        "tail": "EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation"
      },
      {
        "head": "Yuming Li",
        "relation": "author_of",
        "tail": "EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation"
      },
      {
        "head": "Chenguang Ma",
        "relation": "author_of",
        "tail": "EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation"
      },
      {
        "head": "Tsung-Yi Lin",
        "relation": "author_of",
        "tail": "Feature Pyramid Networks for Object Detection"
      },
      {
        "head": "Piotr Dollár",
        "relation": "author_of",
        "tail": "Feature Pyramid Networks for Object Detection"
      },
      {
        "head": "Ross Girshick",
        "relation": "author_of",
        "tail": "Feature Pyramid Networks for Object Detection"
      },
      {
        "head": "Kaiming He",
        "relation": "author_of",
        "tail": "Feature Pyramid Networks for Object Detection"
      },
      {
        "head": "Bharath Hariharan",
        "relation": "author_of",
        "tail": "Feature Pyramid Networks for Object Detection"
      },
      {
        "head": "Serge Belongie",
        "relation": "author_of",
        "tail": "Feature Pyramid Networks for Object Detection"
      },
      {
        "head": "Kyunghyun Cho",
        "relation": "author_of",
        "tail": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"
      },
      {
        "head": "Bart van Merrienboer",
        "relation": "author_of",
        "tail": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"
      },
      {
        "head": "Caglar Gulcehre",
        "relation": "author_of",
        "tail": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"
      },
      {
        "head": "Dzmitry Bahdanau",
        "relation": "author_of",
        "tail": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"
      },
      {
        "head": "Fethi Bougares",
        "relation": "author_of",
        "tail": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"
      },
      {
        "head": "Holger Schwenk",
        "relation": "author_of",
        "tail": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"
      },
      {
        "head": "Yoshua Bengio",
        "relation": "author_of",
        "tail": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"
      },
      {
        "head": "Erfei Cui",
        "relation": "author_of",
        "tail": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving"
      },
      {
        "head": "Wenhai Wang",
        "relation": "author_of",
        "tail": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving"
      },
      {
        "head": "Zhiqi Li",
        "relation": "author_of",
        "tail": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving"
      },
      {
        "head": "Jiangwei Xie",
        "relation": "author_of",
        "tail": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving"
      },
      {
        "head": "Haoming Zou",
        "relation": "author_of",
        "tail": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving"
      },
      {
        "head": "Hanming Deng",
        "relation": "author_of",
        "tail": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving"
      },
      {
        "head": "Gen Luo",
        "relation": "author_of",
        "tail": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving"
      },
      {
        "head": "Lewei Lu",
        "relation": "author_of",
        "tail": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving"
      },
      {
        "head": "Xizhou Zhu",
        "relation": "author_of",
        "tail": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving"
      },
      {
        "head": "Jifeng Dai",
        "relation": "author_of",
        "tail": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving"
      },
      {
        "head": "Shenyuan Gao",
        "relation": "author_of",
        "tail": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability"
      },
      {
        "head": "Jiazhi Yang",
        "relation": "author_of",
        "tail": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability"
      },
      {
        "head": "Li Chen",
        "relation": "author_of",
        "tail": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability"
      },
      {
        "head": "Kashyap Chitta",
        "relation": "author_of",
        "tail": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability"
      },
      {
        "head": "Yihang Qiu",
        "relation": "author_of",
        "tail": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability"
      },
      {
        "head": "Andreas Geiger",
        "relation": "author_of",
        "tail": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability"
      },
      {
        "head": "Jun Zhang",
        "relation": "author_of",
        "tail": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability"
      },
      {
        "head": "Hongyang Li",
        "relation": "author_of",
        "tail": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability"
      },
      {
        "head": "Bencheng Liao",
        "relation": "author_of",
        "tail": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving"
      },
      {
        "head": "Shaoyu Chen",
        "relation": "author_of",
        "tail": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving"
      },
      {
        "head": "Haoran Yin",
        "relation": "author_of",
        "tail": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving"
      },
      {
        "head": "Bo Jiang",
        "relation": "author_of",
        "tail": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving"
      },
      {
        "head": "Cheng Wang",
        "relation": "author_of",
        "tail": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving"
      },
      {
        "head": "Sixu Yan",
        "relation": "author_of",
        "tail": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving"
      },
      {
        "head": "Xinbang Zhang",
        "relation": "author_of",
        "tail": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving"
      },
      {
        "head": "Xiangyu Li",
        "relation": "author_of",
        "tail": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving"
      },
      {
        "head": "Ying Zhang",
        "relation": "author_of",
        "tail": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving"
      },
      {
        "head": "Qian Zhang",
        "relation": "author_of",
        "tail": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving"
      },
      {
        "head": "Xinggang Wang",
        "relation": "author_of",
        "tail": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving"
      },
      {
        "head": "Bingyi Kang",
        "relation": "author_of",
        "tail": "How Far is Video Generation from World Model: A Physical Law Perspective"
      },
      {
        "head": "Yang Yue",
        "relation": "author_of",
        "tail": "How Far is Video Generation from World Model: A Physical Law Perspective"
      },
      {
        "head": "Rui Lu",
        "relation": "author_of",
        "tail": "How Far is Video Generation from World Model: A Physical Law Perspective"
      },
      {
        "head": "Zhijie Lin",
        "relation": "author_of",
        "tail": "How Far is Video Generation from World Model: A Physical Law Perspective"
      },
      {
        "head": "Yang Zhao",
        "relation": "author_of",
        "tail": "How Far is Video Generation from World Model: A Physical Law Perspective"
      },
      {
        "head": "Kaixin Wang",
        "relation": "author_of",
        "tail": "How Far is Video Generation from World Model: A Physical Law Perspective"
      },
      {
        "head": "Gao Huang",
        "relation": "author_of",
        "tail": "How Far is Video Generation from World Model: A Physical Law Perspective"
      },
      {
        "head": "Jiashi Feng",
        "relation": "author_of",
        "tail": "How Far is Video Generation from World Model: A Physical Law Perspective"
      },
      {
        "head": "Jyh-Jing Hwang",
        "relation": "author_of",
        "tail": "EMMA: End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "Runsheng Xu",
        "relation": "author_of",
        "tail": "EMMA: End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "Hubert Lin",
        "relation": "author_of",
        "tail": "EMMA: End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "Wei-Chih Hung",
        "relation": "author_of",
        "tail": "EMMA: End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "Jingwei Ji",
        "relation": "author_of",
        "tail": "EMMA: End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "Kristy Choi",
        "relation": "author_of",
        "tail": "EMMA: End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "Di Huang",
        "relation": "author_of",
        "tail": "EMMA: End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "Tong He",
        "relation": "author_of",
        "tail": "EMMA: End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "Paul Covington",
        "relation": "author_of",
        "tail": "EMMA: End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "Benjamin Sapp",
        "relation": "author_of",
        "tail": "EMMA: End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "Yin Zhou",
        "relation": "author_of",
        "tail": "EMMA: End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "James Guo",
        "relation": "author_of",
        "tail": "EMMA: End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "Dragomir Anguelov",
        "relation": "author_of",
        "tail": "EMMA: End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "Mingxing Tan",
        "relation": "author_of",
        "tail": "EMMA: End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "Tom B. Brown",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Benjamin Mann",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Nick Ryder",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Melanie Subbiah",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Jared Kaplan",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Prafulla Dhariwal",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Arvind Neelakantan",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Pranav Shyam",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Girish Sastry",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Amanda Askell",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Sandhini Agarwal",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Ariel Herbert-Voss",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Gretchen Krueger",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Tom Henighan",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Rewon Child",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Aditya Ramesh",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Daniel M. Ziegler",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Jeffrey Wu",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Clemens Winter",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Christopher Hesse",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Mark Chen",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Eric Sigler",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Mateusz Litwin",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Scott Gray",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Benjamin Chess",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Jack Clark",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Christopher Berner",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Sam McCandlish",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Alec Radford",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Ilya Sutskever",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Dario Amodei",
        "relation": "author_of",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Zhe Chen",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Weiyun Wang",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Yue Cao",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Yangzhou Liu",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Zhangwei Gao",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Erfei Cui",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Jinguo Zhu",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Shenglong Ye",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Hao Tian",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Zhaoyang Liu",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Lixin Gu",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Xuehui Wang",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Qingyun Li",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Yiming Ren",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Zixuan Chen",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Jiapeng Luo",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Jiahao Wang",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Tan Jiang",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Bo Wang",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Conghui He",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Botian Shi",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Xingcheng Zhang",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Han Lv",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Yi Wang",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Wenqi Shao",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Pei Chu",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Zhongying Tu",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Tong He",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Zhiyong Wu",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Huipeng Deng",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Jiaye Ge",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Kai Chen",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Kaipeng Zhang",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Limin Wang",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Min Dou",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Lewei Lu",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Xizhou Zhu",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Tong Lu",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Dahua Lin",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Yu Qiao",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Jifeng Dai",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Wenhai Wang",
        "relation": "author_of",
        "tail": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      },
      {
        "head": "Jinguo Zhu",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Weiyun Wang",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Zhe Chen",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Zhaoyang Liu",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Shenglong Ye",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Lixin Gu",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Hao Tian",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Yuchen Duan",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Weijie Su",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Jie Shao",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Zhangwei Gao",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Erfei Cui",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Xuehui Wang",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Yue Cao",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Yangzhou Liu",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Xingguang Wei",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Hongjie Zhang",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Haomin Wang",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Weiye Xu",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Hao Li",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Jiahao Wang",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Nianchen Deng",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Songze Li",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Yinan He",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Tan Jiang",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Jiapeng Luo",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Yi Wang",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Conghui He",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Botian Shi",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Xingcheng Zhang",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Wenqi Shao",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Junjun He",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Yingtong Xiong",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Wenwen Qu",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Peng Sun",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Penglong Jiao",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Han Lv",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Lijun Wu",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Kaipeng Zhang",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Huipeng Deng",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Jiaye Ge",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Kai Chen",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Limin Wang",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Min Dou",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Lewei Lu",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Xizhou Zhu",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Tong Lu",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Dahua Lin",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Yu Qiao",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Jifeng Dai",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Wenhai Wang",
        "relation": "author_of",
        "tail": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "head": "Haodong Duan",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Xinyu Fang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Junming Yang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Xiangyu Zhao",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Yuxuan Qiao",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Mo Li",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Amit Agarwal",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Zhe Chen",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Lin Chen",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Yuan Liu",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Yubo Ma",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Hailong Sun",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Yifan Zhang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Shiyin Lu",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Tack Hwa Wong",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Weiyun Wang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Peiheng Zhou",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Xiaozhe Li",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Chaoyou Fu",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Junbo Cui",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Jixuan Chen",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Enxin Song",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Song Mao",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Shengyuan Ding",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Tianhao Liang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Zicheng Zhang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Xiaoyi Dong",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Yuhang Zang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Pan Zhang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Jiaqi Wang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Dahua Lin",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Kai Chen",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Guowei Xu",
        "relation": "author_of",
        "tail": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step"
      },
      {
        "head": "Peng Jin",
        "relation": "author_of",
        "tail": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step"
      },
      {
        "head": "Ziang Wu",
        "relation": "author_of",
        "tail": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step"
      },
      {
        "head": "Hao Li",
        "relation": "author_of",
        "tail": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step"
      },
      {
        "head": "Yibing Song",
        "relation": "author_of",
        "tail": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step"
      },
      {
        "head": "Lichao Sun",
        "relation": "author_of",
        "tail": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step"
      },
      {
        "head": "Li Yuan",
        "relation": "author_of",
        "tail": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step"
      },
      {
        "head": "Weiyun Wang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Zhangwei Gao",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Lixin Gu",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Hengjun Pu",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Long Cui",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Xingguang Wei",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Zhaoyang Liu",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Linglin Jing",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Shenglong Ye",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Jie Shao",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Zhaokai Wang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Zhe Chen",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Hongjie Zhang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Ganlin Yang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Haomin Wang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Qi Wei",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Jinhui Yin",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Wenhao Li",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Erfei Cui",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Guanzhou Chen",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Zichen Ding",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Changyao Tian",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Zhenyu Wu",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Jingjing Xie",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Zehao Li",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Bowen Yang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Yuchen Duan",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Xuehui Wang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Zhi Hou",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Haoran Hao",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Tianyi Zhang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Songze Li",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Xiangyu Zhao",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Haodong Duan",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Nianchen Deng",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Bin Fu",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Yinan He",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Yi Wang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Conghui He",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Botian Shi",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Junjun He",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Yingtong Xiong",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Han Lv",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Lijun Wu",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Wenqi Shao",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Kaipeng Zhang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Huipeng Deng",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Biqing Qi",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Jiaye Ge",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Qipeng Guo",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Wenwei Zhang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Songyang Zhang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Maosong Cao",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Junyao Lin",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Kexian Tang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Jianfei Gao",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Haian Huang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Yuzhe Gu",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Chengqi Lyu",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Huanze Tang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Rui Wang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Haijun Lv",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Wanli Ouyang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Limin Wang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Min Dou",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Xizhou Zhu",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Tong Lu",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Dahua Lin",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Jifeng Dai",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Weijie Su",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Bowen Zhou",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Kai Chen",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Yu Qiao",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Wenhai Wang",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "Gen Luo",
        "relation": "author_of",
        "tail": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "head": "John Schulman",
        "relation": "author_of",
        "tail": "Proximal Policy Optimization Algorithms"
      },
      {
        "head": "Filip Wolski",
        "relation": "author_of",
        "tail": "Proximal Policy Optimization Algorithms"
      },
      {
        "head": "Prafulla Dhariwal",
        "relation": "author_of",
        "tail": "Proximal Policy Optimization Algorithms"
      },
      {
        "head": "Alec Radford",
        "relation": "author_of",
        "tail": "Proximal Policy Optimization Algorithms"
      },
      {
        "head": "Oleg Klimov",
        "relation": "author_of",
        "tail": "Proximal Policy Optimization Algorithms"
      },
      {
        "head": "Jie Liu",
        "relation": "author_of",
        "tail": "Flow-GRPO: Training Flow Matching Models via Online RL"
      },
      {
        "head": "Gongye Liu",
        "relation": "author_of",
        "tail": "Flow-GRPO: Training Flow Matching Models via Online RL"
      },
      {
        "head": "Jiajun Liang",
        "relation": "author_of",
        "tail": "Flow-GRPO: Training Flow Matching Models via Online RL"
      },
      {
        "head": "Yangguang Li",
        "relation": "author_of",
        "tail": "Flow-GRPO: Training Flow Matching Models via Online RL"
      },
      {
        "head": "Jiaheng Liu",
        "relation": "author_of",
        "tail": "Flow-GRPO: Training Flow Matching Models via Online RL"
      },
      {
        "head": "Xintao Wang",
        "relation": "author_of",
        "tail": "Flow-GRPO: Training Flow Matching Models via Online RL"
      },
      {
        "head": "Pengfei Wan",
        "relation": "author_of",
        "tail": "Flow-GRPO: Training Flow Matching Models via Online RL"
      },
      {
        "head": "Di Zhang",
        "relation": "author_of",
        "tail": "Flow-GRPO: Training Flow Matching Models via Online RL"
      },
      {
        "head": "Wanli Ouyang",
        "relation": "author_of",
        "tail": "Flow-GRPO: Training Flow Matching Models via Online RL"
      },
      {
        "head": "Zeyue Xue",
        "relation": "author_of",
        "tail": "DanceGRPO: Unleashing GRPO on Visual Generation"
      },
      {
        "head": "Jie Wu",
        "relation": "author_of",
        "tail": "DanceGRPO: Unleashing GRPO on Visual Generation"
      },
      {
        "head": "Yu Gao",
        "relation": "author_of",
        "tail": "DanceGRPO: Unleashing GRPO on Visual Generation"
      },
      {
        "head": "Fangyuan Kong",
        "relation": "author_of",
        "tail": "DanceGRPO: Unleashing GRPO on Visual Generation"
      },
      {
        "head": "Lingting Zhu",
        "relation": "author_of",
        "tail": "DanceGRPO: Unleashing GRPO on Visual Generation"
      },
      {
        "head": "Mengzhao Chen",
        "relation": "author_of",
        "tail": "DanceGRPO: Unleashing GRPO on Visual Generation"
      },
      {
        "head": "Zhiheng Liu",
        "relation": "author_of",
        "tail": "DanceGRPO: Unleashing GRPO on Visual Generation"
      },
      {
        "head": "Wei Liu",
        "relation": "author_of",
        "tail": "DanceGRPO: Unleashing GRPO on Visual Generation"
      },
      {
        "head": "Qiushan Guo",
        "relation": "author_of",
        "tail": "DanceGRPO: Unleashing GRPO on Visual Generation"
      },
      {
        "head": "Weilin Huang",
        "relation": "author_of",
        "tail": "DanceGRPO: Unleashing GRPO on Visual Generation"
      },
      {
        "head": "Ping Luo",
        "relation": "author_of",
        "tail": "DanceGRPO: Unleashing GRPO on Visual Generation"
      },
      {
        "head": "Yu Gao",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Haoyuan Guo",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Tuyen Hoang",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Weilin Huang",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Lu Jiang",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Fangyuan Kong",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Huixia Li",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Jiashi Li",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Liang Li",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Xiaojie Li",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Xunsong Li",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Yifu Li",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Shanchuan Lin",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Zhijie Lin",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Jiawei Liu",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Shu Liu",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Xiaonan Nie",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Zhiwu Qing",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Yuxi Ren",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Li Sun",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Zhi Tian",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Rui Wang",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Sen Wang",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Guoqiang Wei",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Guohong Wu",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Jie Wu",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Ruiqi Xia",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Fei Xiao",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Xuefeng Xiao",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Jiangqiao Yan",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Ceyuan Yang",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Jianchao Yang",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Runkai Yang",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Tao Yang",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Yihang Yang",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Zilyu Ye",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Xuejiao Zeng",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Yan Zeng",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Heng Zhang",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Yang Zhao",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Xiaozheng Zheng",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Peihao Zhu",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Jiaxin Zou",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Feilong Zuo",
        "relation": "author_of",
        "tail": "Seedance 1.0: Exploring the Boundaries of Video Generation Models"
      },
      {
        "head": "Guibin Chen",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Dixuan Lin",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Jiangping Yang",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Chunze Lin",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Junchen Zhu",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Mingyuan Fan",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Hao Zhang",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Sheng Chen",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Zheng Chen",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Chengcheng Ma",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Weiming Xiong",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Wei Wang",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Nuo Pang",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Kang Kang",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Zhiheng Xu",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Yuzhe Jin",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Yupeng Liang",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Yubing Song",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Peng Zhao",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Boyuan Xu",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Di Qiu",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Debang Li",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Zhengcong Fei",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Yang Li",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Yahui Zhou",
        "relation": "author_of",
        "tail": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "head": "Yibin Wang",
        "relation": "author_of",
        "tail": "Unified Reward Model for Multimodal Understanding and Generation"
      },
      {
        "head": "Yuhang Zang",
        "relation": "author_of",
        "tail": "Unified Reward Model for Multimodal Understanding and Generation"
      },
      {
        "head": "Hao Li",
        "relation": "author_of",
        "tail": "Unified Reward Model for Multimodal Understanding and Generation"
      },
      {
        "head": "Cheng Jin",
        "relation": "author_of",
        "tail": "Unified Reward Model for Multimodal Understanding and Generation"
      },
      {
        "head": "Jiaqi Wang",
        "relation": "author_of",
        "tail": "Unified Reward Model for Multimodal Understanding and Generation"
      },
      {
        "head": "OpenAI",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Josh Achiam",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Steven Adler",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Sandhini Agarwal",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Lama Ahmad",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Ilge Akkaya",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Florencia Leoni Aleman",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Diogo Almeida",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Janko Altenschmidt",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Sam Altman",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Shyamal Anadkat",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Red Avila",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Igor Babuschkin",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Suchir Balaji",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Valerie Balcom",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Paul Baltescu",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Haiming Bao",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Mohammad Bavarian",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jeff Belgum",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Irwan Bello",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jake Berdine",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Gabriel Bernadett-Shapiro",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Christopher Berner",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Lenny Bogdonoff",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Oleg Boiko",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Madelaine Boyd",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Anna-Luisa Brakman",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Greg Brockman",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Tim Brooks",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Miles Brundage",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Kevin Button",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Trevor Cai",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Rosie Campbell",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Andrew Cann",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Brittany Carey",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Chelsea Carlson",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Rory Carmichael",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Brooke Chan",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Che Chang",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Fotis Chantzis",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Derek Chen",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Sully Chen",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Ruby Chen",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jason Chen",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Mark Chen",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Ben Chess",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Chester Cho",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Casey Chu",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Hyung Won Chung",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Dave Cummings",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jeremiah Currier",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Yunxing Dai",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Cory Decareaux",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Thomas Degry",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Noah Deutsch",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Damien Deville",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Arka Dhar",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "David Dohan",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Steve Dowling",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Sheila Dunning",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Adrien Ecoffet",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Atty Eleti",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Tyna Eloundou",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "David Farhi",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Liam Fedus",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Niko Felix",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Simón Posada Fishman",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Juston Forte",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Isabella Fulford",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Leo Gao",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Elie Georges",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Christian Gibson",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Vik Goel",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Tarun Gogineni",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Gabriel Goh",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Rapha Gontijo-Lopes",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jonathan Gordon",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Morgan Grafstein",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Scott Gray",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Ryan Greene",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Joshua Gross",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Shixiang Shane Gu",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Yufei Guo",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Chris Hallacy",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jesse Han",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jeff Harris",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Yuchen He",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Mike Heaton",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Johannes Heidecke",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Chris Hesse",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Alan Hickey",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Wade Hickey",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Peter Hoeschele",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Brandon Houghton",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Kenny Hsu",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Shengli Hu",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Xin Hu",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Joost Huizinga",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Shantanu Jain",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Shawn Jain",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Joanne Jang",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Angela Jiang",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Roger Jiang",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Haozhun Jin",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Denny Jin",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Shino Jomoto",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Billie Jonn",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Heewoo Jun",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Tomer Kaftan",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Łukasz Kaiser",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Ali Kamali",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Ingmar Kanitscheider",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Nitish Shirish Keskar",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Tabarak Khan",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Logan Kilpatrick",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jong Wook Kim",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Christina Kim",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Yongjik Kim",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jan Hendrik Kirchner",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jamie Kiros",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Matt Knight",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Daniel Kokotajlo",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Łukasz Kondraciuk",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Andrew Kondrich",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Aris Konstantinidis",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Kyle Kosic",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Gretchen Krueger",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Vishal Kuo",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Michael Lampe",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Ikai Lan",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Teddy Lee",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jan Leike",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jade Leung",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Daniel Levy",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Chak Ming Li",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Rachel Lim",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Molly Lin",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Stephanie Lin",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Mateusz Litwin",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Theresa Lopez",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Ryan Lowe",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Patricia Lue",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Anna Makanju",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Kim Malfacini",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Sam Manning",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Todor Markov",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Yaniv Markovski",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Bianca Martin",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Katie Mayer",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Andrew Mayne",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Bob McGrew",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Scott Mayer McKinney",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Christine McLeavey",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Paul McMillan",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jake McNeil",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "David Medina",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Aalok Mehta",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jacob Menick",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Luke Metz",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Andrey Mishchenko",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Pamela Mishkin",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Vinnie Monaco",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Evan Morikawa",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Daniel Mossing",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Tong Mu",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Mira Murati",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Oleg Murk",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "David Mély",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Ashvin Nair",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Reiichiro Nakano",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Rajeev Nayak",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Arvind Neelakantan",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Richard Ngo",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Hyeonwoo Noh",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Long Ouyang",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Cullen O'Keefe",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jakub Pachocki",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Alex Paino",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Joe Palermo",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Ashley Pantuliano",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Giambattista Parascandolo",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Joel Parish",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Emy Parparita",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Alex Passos",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Mikhail Pavlov",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Andrew Peng",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Adam Perelman",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Filipe de Avila Belbute Peres",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Michael Petrov",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Henrique Ponde de Oliveira Pinto",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Michael",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Pokorny",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Michelle Pokrass",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Vitchyr H. Pong",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Tolly Powell",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Alethea Power",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Boris Power",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Elizabeth Proehl",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Raul Puri",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Alec Radford",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jack Rae",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Aditya Ramesh",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Cameron Raymond",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Francis Real",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Kendra Rimbach",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Carl Ross",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Bob Rotsted",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Henri Roussez",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Nick Ryder",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Mario Saltarelli",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Ted Sanders",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Shibani Santurkar",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Girish Sastry",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Heather Schmidt",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "David Schnurr",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "John Schulman",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Daniel Selsam",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Kyla Sheppard",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Toki Sherbakov",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jessica Shieh",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Sarah Shoker",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Pranav Shyam",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Szymon Sidor",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Eric Sigler",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Maddie Simens",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jordan Sitkin",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Katarina Slama",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Ian Sohl",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Benjamin Sokolowsky",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Yang Song",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Natalie Staudacher",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Felipe Petroski Such",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Natalie Summers",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Ilya Sutskever",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jie Tang",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Nikolas Tezak",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Madeleine B. Thompson",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Phil Tillet",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Amin Tootoonchian",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Elizabeth Tseng",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Preston Tuggle",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Nick Turley",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jerry Tworek",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Juan Felipe Cerón Uribe",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Andrea Vallone",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Arun Vijayvergiya",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Chelsea Voss",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Carroll Wainwright",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Justin Jay Wang",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Alvin Wang",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Ben Wang",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jonathan Ward",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jason Wei",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "CJ Weinmann",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Akila Welihinda",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Peter Welinder",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jiayi Weng",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Lilian Weng",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Matt Wiethoff",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Dave Willner",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Clemens Winter",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Samuel Wolrich",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Hannah Wong",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Lauren Workman",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Sherwin Wu",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Jeff Wu",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Michael Wu",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Kai Xiao",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Tao Xu",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Sarah Yoo",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Kevin Yu",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Qiming Yuan",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Wojciech Zaremba",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Rowan Zellers",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Chong Zhang",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Marvin Zhang",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Shengjia Zhao",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Tianhao Zheng",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Juntang Zhuang",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "William Zhuk",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Barret Zoph",
        "relation": "author_of",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "Hugo Touvron",
        "relation": "author_of",
        "tail": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "head": "Thibaut Lavril",
        "relation": "author_of",
        "tail": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "head": "Gautier Izacard",
        "relation": "author_of",
        "tail": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "head": "Xavier Martinet",
        "relation": "author_of",
        "tail": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "head": "Marie-Anne Lachaux",
        "relation": "author_of",
        "tail": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "head": "Timothée Lacroix",
        "relation": "author_of",
        "tail": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "head": "Baptiste Rozière",
        "relation": "author_of",
        "tail": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "head": "Naman Goyal",
        "relation": "author_of",
        "tail": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "head": "Eric Hambro",
        "relation": "author_of",
        "tail": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "head": "Faisal Azhar",
        "relation": "author_of",
        "tail": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "head": "Aurelien Rodriguez",
        "relation": "author_of",
        "tail": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "head": "Armand Joulin",
        "relation": "author_of",
        "tail": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "head": "Edouard Grave",
        "relation": "author_of",
        "tail": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "head": "Guillaume Lample",
        "relation": "author_of",
        "tail": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "head": "Jason Wei",
        "relation": "author_of",
        "tail": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Xuezhi Wang",
        "relation": "author_of",
        "tail": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Dale Schuurmans",
        "relation": "author_of",
        "tail": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Maarten Bosma",
        "relation": "author_of",
        "tail": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Brian Ichter",
        "relation": "author_of",
        "tail": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Fei Xia",
        "relation": "author_of",
        "tail": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Ed Chi",
        "relation": "author_of",
        "tail": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Quoc Le",
        "relation": "author_of",
        "tail": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Denny Zhou",
        "relation": "author_of",
        "tail": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Karl Cobbe",
        "relation": "author_of",
        "tail": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "head": "Vineet Kosaraju",
        "relation": "author_of",
        "tail": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "head": "Mohammad Bavarian",
        "relation": "author_of",
        "tail": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "head": "Mark Chen",
        "relation": "author_of",
        "tail": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "head": "Heewoo Jun",
        "relation": "author_of",
        "tail": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "head": "Lukasz Kaiser",
        "relation": "author_of",
        "tail": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "head": "Matthias Plappert",
        "relation": "author_of",
        "tail": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "head": "Jerry Tworek",
        "relation": "author_of",
        "tail": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "head": "Jacob Hilton",
        "relation": "author_of",
        "tail": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "head": "Reiichiro Nakano",
        "relation": "author_of",
        "tail": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "head": "Christopher Hesse",
        "relation": "author_of",
        "tail": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "head": "John Schulman",
        "relation": "author_of",
        "tail": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "head": "K. Luger",
        "relation": "author_of",
        "tail": "Crystal structure of the nucleosome core particle at 2.8 Å resolution"
      },
      {
        "head": "A. Mäder",
        "relation": "author_of",
        "tail": "Crystal structure of the nucleosome core particle at 2.8 Å resolution"
      },
      {
        "head": "R. K. Richmond",
        "relation": "author_of",
        "tail": "Crystal structure of the nucleosome core particle at 2.8 Å resolution"
      },
      {
        "head": "D. Sargent",
        "relation": "author_of",
        "tail": "Crystal structure of the nucleosome core particle at 2.8 Å resolution"
      },
      {
        "head": "T. Richmond",
        "relation": "author_of",
        "tail": "Crystal structure of the nucleosome core particle at 2.8 Å resolution"
      },
      {
        "head": "Albert Gu",
        "relation": "author_of",
        "tail": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"
      },
      {
        "head": "Tri Dao",
        "relation": "author_of",
        "tail": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"
      },
      {
        "head": "Stephen M. Mount",
        "relation": "author_of",
        "tail": "A catalogue of splice junction sequences."
      },
      {
        "head": "F. Crick",
        "relation": "author_of",
        "tail": "Origin of the Genetic Code"
      },
      {
        "head": "Ilya Loshchilov",
        "relation": "author_of",
        "tail": "SGDR: Stochastic Gradient Descent with Warm Restarts"
      },
      {
        "head": "Frank Hutter",
        "relation": "author_of",
        "tail": "SGDR: Stochastic Gradient Descent with Warm Restarts"
      },
      {
        "head": "Noam Shazeer",
        "relation": "author_of",
        "tail": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"
      },
      {
        "head": "Azalia Mirhoseini",
        "relation": "author_of",
        "tail": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"
      },
      {
        "head": "Krzysztof Maziarz",
        "relation": "author_of",
        "tail": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"
      },
      {
        "head": "Andy Davis",
        "relation": "author_of",
        "tail": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"
      },
      {
        "head": "Quoc Le",
        "relation": "author_of",
        "tail": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"
      },
      {
        "head": "Geoffrey Hinton",
        "relation": "author_of",
        "tail": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"
      },
      {
        "head": "Jeff Dean",
        "relation": "author_of",
        "tail": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"
      },
      {
        "head": "J. Ziv",
        "relation": "author_of",
        "tail": "Compression of individual sequences via variable-rate coding"
      },
      {
        "head": "A. Lempel",
        "relation": "author_of",
        "tail": "Compression of individual sequences via variable-rate coding"
      },
      {
        "head": "Stephen Merity",
        "relation": "author_of",
        "tail": "Pointer Sentinel Mixture Models"
      },
      {
        "head": "Caiming Xiong",
        "relation": "author_of",
        "tail": "Pointer Sentinel Mixture Models"
      },
      {
        "head": "James Bradbury",
        "relation": "author_of",
        "tail": "Pointer Sentinel Mixture Models"
      },
      {
        "head": "Richard Socher",
        "relation": "author_of",
        "tail": "Pointer Sentinel Mixture Models"
      },
      {
        "head": "Dan Hendrycks",
        "relation": "author_of",
        "tail": "Measuring Massive Multitask Language Understanding"
      },
      {
        "head": "Collin Burns",
        "relation": "author_of",
        "tail": "Measuring Massive Multitask Language Understanding"
      },
      {
        "head": "Steven Basart",
        "relation": "author_of",
        "tail": "Measuring Massive Multitask Language Understanding"
      },
      {
        "head": "Andy Zou",
        "relation": "author_of",
        "tail": "Measuring Massive Multitask Language Understanding"
      },
      {
        "head": "Mantas Mazeika",
        "relation": "author_of",
        "tail": "Measuring Massive Multitask Language Understanding"
      },
      {
        "head": "Dawn Song",
        "relation": "author_of",
        "tail": "Measuring Massive Multitask Language Understanding"
      },
      {
        "head": "Jacob Steinhardt",
        "relation": "author_of",
        "tail": "Measuring Massive Multitask Language Understanding"
      },
      {
        "head": "Hunter Lightman",
        "relation": "author_of",
        "tail": "Let's Verify Step by Step"
      },
      {
        "head": "Vineet Kosaraju",
        "relation": "author_of",
        "tail": "Let's Verify Step by Step"
      },
      {
        "head": "Yura Burda",
        "relation": "author_of",
        "tail": "Let's Verify Step by Step"
      },
      {
        "head": "Harri Edwards",
        "relation": "author_of",
        "tail": "Let's Verify Step by Step"
      },
      {
        "head": "Bowen Baker",
        "relation": "author_of",
        "tail": "Let's Verify Step by Step"
      },
      {
        "head": "Teddy Lee",
        "relation": "author_of",
        "tail": "Let's Verify Step by Step"
      },
      {
        "head": "Jan Leike",
        "relation": "author_of",
        "tail": "Let's Verify Step by Step"
      },
      {
        "head": "John Schulman",
        "relation": "author_of",
        "tail": "Let's Verify Step by Step"
      },
      {
        "head": "Ilya Sutskever",
        "relation": "author_of",
        "tail": "Let's Verify Step by Step"
      },
      {
        "head": "Karl Cobbe",
        "relation": "author_of",
        "tail": "Let's Verify Step by Step"
      },
      {
        "head": "David Rein",
        "relation": "author_of",
        "tail": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark"
      },
      {
        "head": "Betty Li Hou",
        "relation": "author_of",
        "tail": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark"
      },
      {
        "head": "Asa Cooper Stickland",
        "relation": "author_of",
        "tail": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark"
      },
      {
        "head": "Jackson Petty",
        "relation": "author_of",
        "tail": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark"
      },
      {
        "head": "Richard Yuanzhe Pang",
        "relation": "author_of",
        "tail": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark"
      },
      {
        "head": "Julien Dirani",
        "relation": "author_of",
        "tail": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark"
      },
      {
        "head": "Julian Michael",
        "relation": "author_of",
        "tail": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark"
      },
      {
        "head": "Samuel R. Bowman",
        "relation": "author_of",
        "tail": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark"
      },
      {
        "head": "Dmitry Lepikhin",
        "relation": "author_of",
        "tail": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"
      },
      {
        "head": "HyoukJoong Lee",
        "relation": "author_of",
        "tail": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"
      },
      {
        "head": "Yuanzhong Xu",
        "relation": "author_of",
        "tail": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"
      },
      {
        "head": "Dehao Chen",
        "relation": "author_of",
        "tail": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"
      },
      {
        "head": "Orhan Firat",
        "relation": "author_of",
        "tail": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"
      },
      {
        "head": "Yanping Huang",
        "relation": "author_of",
        "tail": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"
      },
      {
        "head": "Maxim Krikun",
        "relation": "author_of",
        "tail": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"
      },
      {
        "head": "Noam Shazeer",
        "relation": "author_of",
        "tail": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"
      },
      {
        "head": "Zhifeng Chen",
        "relation": "author_of",
        "tail": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"
      },
      {
        "head": "Radford M. Neal",
        "relation": "author_of",
        "tail": "Pattern Recognition and Machine Learning"
      },
      {
        "head": "L. Breiman",
        "relation": "author_of",
        "tail": "Bagging Predictors"
      },
      {
        "head": "Guan Wang",
        "relation": "author_of",
        "tail": "Automatic Image-Based Plant Disease Severity Estimation Using Deep Learning"
      },
      {
        "head": "Yu Sun",
        "relation": "author_of",
        "tail": "Automatic Image-Based Plant Disease Severity Estimation Using Deep Learning"
      },
      {
        "head": "Jianxin Wang",
        "relation": "author_of",
        "tail": "Automatic Image-Based Plant Disease Severity Estimation Using Deep Learning"
      },
      {
        "head": "Mostafa Mehdipour-Ghazi",
        "relation": "author_of",
        "tail": "Plant identification using deep neural networks via optimization of transfer learning parameters"
      },
      {
        "head": "B. Yanikoglu",
        "relation": "author_of",
        "tail": "Plant identification using deep neural networks via optimization of transfer learning parameters"
      },
      {
        "head": "E. Aptoula",
        "relation": "author_of",
        "tail": "Plant identification using deep neural networks via optimization of transfer learning parameters"
      },
      {
        "head": "Sue Han Lee",
        "relation": "author_of",
        "tail": "New perspectives on plant disease characterization based on deep learning"
      },
      {
        "head": "H. Goëau",
        "relation": "author_of",
        "tail": "New perspectives on plant disease characterization based on deep learning"
      },
      {
        "head": "P. Bonnet",
        "relation": "author_of",
        "tail": "New perspectives on plant disease characterization based on deep learning"
      },
      {
        "head": "A. Joly",
        "relation": "author_of",
        "tail": "New perspectives on plant disease characterization based on deep learning"
      },
      {
        "head": "Yu Sun",
        "relation": "author_of",
        "tail": "Deep Learning for Plant Identification in Natural Environment"
      },
      {
        "head": "Yuan Liu",
        "relation": "author_of",
        "tail": "Deep Learning for Plant Identification in Natural Environment"
      },
      {
        "head": "Guan Wang",
        "relation": "author_of",
        "tail": "Deep Learning for Plant Identification in Natural Environment"
      },
      {
        "head": "Haiyan Zhang",
        "relation": "author_of",
        "tail": "Deep Learning for Plant Identification in Natural Environment"
      },
      {
        "head": "Jose Carranza-Rojas",
        "relation": "author_of",
        "tail": "Going deeper in the automated identification of Herbarium specimens"
      },
      {
        "head": "Hervé Goeau",
        "relation": "author_of",
        "tail": "Going deeper in the automated identification of Herbarium specimens"
      },
      {
        "head": "P. Bonnet",
        "relation": "author_of",
        "tail": "Going deeper in the automated identification of Herbarium specimens"
      },
      {
        "head": "Erick Mata-Montero",
        "relation": "author_of",
        "tail": "Going deeper in the automated identification of Herbarium specimens"
      },
      {
        "head": "A. Joly",
        "relation": "author_of",
        "tail": "Going deeper in the automated identification of Herbarium specimens"
      },
      {
        "head": "Mark Sandler",
        "relation": "author_of",
        "tail": "MobileNetV2: Inverted Residuals and Linear Bottlenecks"
      },
      {
        "head": "Andrew Howard",
        "relation": "author_of",
        "tail": "MobileNetV2: Inverted Residuals and Linear Bottlenecks"
      },
      {
        "head": "Menglong Zhu",
        "relation": "author_of",
        "tail": "MobileNetV2: Inverted Residuals and Linear Bottlenecks"
      },
      {
        "head": "Andrey Zhmoginov",
        "relation": "author_of",
        "tail": "MobileNetV2: Inverted Residuals and Linear Bottlenecks"
      },
      {
        "head": "Liang-Chieh Chen",
        "relation": "author_of",
        "tail": "MobileNetV2: Inverted Residuals and Linear Bottlenecks"
      },
      {
        "head": "H. Brendan McMahan",
        "relation": "author_of",
        "tail": "Communication-Efficient Learning of Deep Networks from Decentralized Data"
      },
      {
        "head": "Eider Moore",
        "relation": "author_of",
        "tail": "Communication-Efficient Learning of Deep Networks from Decentralized Data"
      },
      {
        "head": "Daniel Ramage",
        "relation": "author_of",
        "tail": "Communication-Efficient Learning of Deep Networks from Decentralized Data"
      },
      {
        "head": "Seth Hampson",
        "relation": "author_of",
        "tail": "Communication-Efficient Learning of Deep Networks from Decentralized Data"
      },
      {
        "head": "Blaise Agüera y Arcas",
        "relation": "author_of",
        "tail": "Communication-Efficient Learning of Deep Networks from Decentralized Data"
      },
      {
        "head": "Fuzhen Zhuang",
        "relation": "author_of",
        "tail": "A Comprehensive Survey on Transfer Learning"
      },
      {
        "head": "Zhiyuan Qi",
        "relation": "author_of",
        "tail": "A Comprehensive Survey on Transfer Learning"
      },
      {
        "head": "Keyu Duan",
        "relation": "author_of",
        "tail": "A Comprehensive Survey on Transfer Learning"
      },
      {
        "head": "Dongbo Xi",
        "relation": "author_of",
        "tail": "A Comprehensive Survey on Transfer Learning"
      },
      {
        "head": "Yongchun Zhu",
        "relation": "author_of",
        "tail": "A Comprehensive Survey on Transfer Learning"
      },
      {
        "head": "Hengshu Zhu",
        "relation": "author_of",
        "tail": "A Comprehensive Survey on Transfer Learning"
      },
      {
        "head": "Hui Xiong",
        "relation": "author_of",
        "tail": "A Comprehensive Survey on Transfer Learning"
      },
      {
        "head": "Qing He",
        "relation": "author_of",
        "tail": "A Comprehensive Survey on Transfer Learning"
      },
      {
        "head": "A. Khan",
        "relation": "author_of",
        "tail": "Privacy Preserved and Decentralized Smartphone Recommendation System"
      },
      {
        "head": "Maha Driss",
        "relation": "author_of",
        "tail": "Privacy Preserved and Decentralized Smartphone Recommendation System"
      },
      {
        "head": "Wadii Boulila",
        "relation": "author_of",
        "tail": "Privacy Preserved and Decentralized Smartphone Recommendation System"
      },
      {
        "head": "G. A. Sampedro",
        "relation": "author_of",
        "tail": "Privacy Preserved and Decentralized Smartphone Recommendation System"
      },
      {
        "head": "Sidra Abbas",
        "relation": "author_of",
        "tail": "Privacy Preserved and Decentralized Smartphone Recommendation System"
      },
      {
        "head": "Chitapong Wechtaisong",
        "relation": "author_of",
        "tail": "Privacy Preserved and Decentralized Smartphone Recommendation System"
      },
      {
        "head": "Tesfahunegn Minwuyelet Mengistu",
        "relation": "author_of",
        "tail": "A Survey on Heterogeneity Taxonomy, Security and Privacy Preservation in the Integration of IoT, Wireless Sensor Networks and Federated Learning"
      },
      {
        "head": "Taewoon Kim",
        "relation": "author_of",
        "tail": "A Survey on Heterogeneity Taxonomy, Security and Privacy Preservation in the Integration of IoT, Wireless Sensor Networks and Federated Learning"
      },
      {
        "head": "Jenn-Wei Lin",
        "relation": "author_of",
        "tail": "A Survey on Heterogeneity Taxonomy, Security and Privacy Preservation in the Integration of IoT, Wireless Sensor Networks and Federated Learning"
      },
      {
        "head": "A. Alamer",
        "relation": "author_of",
        "tail": "A privacy-preserving federated learning with a secure collaborative for malware detection models using Internet of Things resources"
      },
      {
        "head": "Manel Khazri Khlifi",
        "relation": "author_of",
        "tail": "Graph-based deep learning techniques for remote sensing applications: Techniques, taxonomy, and applications - A comprehensive review"
      },
      {
        "head": "Wadii Boulila",
        "relation": "author_of",
        "tail": "Graph-based deep learning techniques for remote sensing applications: Techniques, taxonomy, and applications - A comprehensive review"
      },
      {
        "head": "I. Farah",
        "relation": "author_of",
        "tail": "Graph-based deep learning techniques for remote sensing applications: Techniques, taxonomy, and applications - A comprehensive review"
      },
      {
        "head": "Anwesha Mukherjee",
        "relation": "author_of",
        "tail": "Federated Learning Architectures: A Performance Evaluation With Crop Yield Prediction Application"
      },
      {
        "head": "Rajkumar Buyya",
        "relation": "author_of",
        "tail": "Federated Learning Architectures: A Performance Evaluation With Crop Yield Prediction Application"
      },
      {
        "head": "Alexander Kirillov",
        "relation": "author_of",
        "tail": "Segment Anything"
      },
      {
        "head": "Eric Mintun",
        "relation": "author_of",
        "tail": "Segment Anything"
      },
      {
        "head": "Nikhila Ravi",
        "relation": "author_of",
        "tail": "Segment Anything"
      },
      {
        "head": "Hanzi Mao",
        "relation": "author_of",
        "tail": "Segment Anything"
      },
      {
        "head": "Chloe Rolland",
        "relation": "author_of",
        "tail": "Segment Anything"
      },
      {
        "head": "Laura Gustafson",
        "relation": "author_of",
        "tail": "Segment Anything"
      },
      {
        "head": "Tete Xiao",
        "relation": "author_of",
        "tail": "Segment Anything"
      },
      {
        "head": "Spencer Whitehead",
        "relation": "author_of",
        "tail": "Segment Anything"
      },
      {
        "head": "Alexander C. Berg",
        "relation": "author_of",
        "tail": "Segment Anything"
      },
      {
        "head": "Wan-Yen Lo",
        "relation": "author_of",
        "tail": "Segment Anything"
      },
      {
        "head": "Piotr Dollár",
        "relation": "author_of",
        "tail": "Segment Anything"
      },
      {
        "head": "Ross Girshick",
        "relation": "author_of",
        "tail": "Segment Anything"
      },
      {
        "head": "Haotian Liu",
        "relation": "author_of",
        "tail": "Visual Instruction Tuning"
      },
      {
        "head": "Chunyuan Li",
        "relation": "author_of",
        "tail": "Visual Instruction Tuning"
      },
      {
        "head": "Qingyang Wu",
        "relation": "author_of",
        "tail": "Visual Instruction Tuning"
      },
      {
        "head": "Yong Jae Lee",
        "relation": "author_of",
        "tail": "Visual Instruction Tuning"
      },
      {
        "head": "Haotian Liu",
        "relation": "author_of",
        "tail": "Improved Baselines with Visual Instruction Tuning"
      },
      {
        "head": "Chunyuan Li",
        "relation": "author_of",
        "tail": "Improved Baselines with Visual Instruction Tuning"
      },
      {
        "head": "Yuheng Li",
        "relation": "author_of",
        "tail": "Improved Baselines with Visual Instruction Tuning"
      },
      {
        "head": "Yong Jae Lee",
        "relation": "author_of",
        "tail": "Improved Baselines with Visual Instruction Tuning"
      },
      {
        "head": "Zhiyuan You",
        "relation": "author_of",
        "tail": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset"
      },
      {
        "head": "Jinjin Gu",
        "relation": "author_of",
        "tail": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset"
      },
      {
        "head": "Xin Cai",
        "relation": "author_of",
        "tail": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset"
      },
      {
        "head": "Zheyuan Li",
        "relation": "author_of",
        "tail": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset"
      },
      {
        "head": "Kaiwen Zhu",
        "relation": "author_of",
        "tail": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset"
      },
      {
        "head": "Chao Dong",
        "relation": "author_of",
        "tail": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset"
      },
      {
        "head": "Tianfan Xue",
        "relation": "author_of",
        "tail": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset"
      },
      {
        "head": "Xintong Zhang",
        "relation": "author_of",
        "tail": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs"
      },
      {
        "head": "Zhi Gao",
        "relation": "author_of",
        "tail": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs"
      },
      {
        "head": "Bofei Zhang",
        "relation": "author_of",
        "tail": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs"
      },
      {
        "head": "Pengxiang Li",
        "relation": "author_of",
        "tail": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs"
      },
      {
        "head": "Xiaowen Zhang",
        "relation": "author_of",
        "tail": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs"
      },
      {
        "head": "Yang Liu",
        "relation": "author_of",
        "tail": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs"
      },
      {
        "head": "Tao Yuan",
        "relation": "author_of",
        "tail": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs"
      },
      {
        "head": "Yuwei Wu",
        "relation": "author_of",
        "tail": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs"
      },
      {
        "head": "Yunde Jia",
        "relation": "author_of",
        "tail": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs"
      },
      {
        "head": "Song-Chun Zhu",
        "relation": "author_of",
        "tail": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs"
      },
      {
        "head": "Qing Li",
        "relation": "author_of",
        "tail": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs"
      },
      {
        "head": "Zhangquan Chen",
        "relation": "author_of",
        "tail": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views"
      },
      {
        "head": "Manyuan Zhang",
        "relation": "author_of",
        "tail": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views"
      },
      {
        "head": "Xinlei Yu",
        "relation": "author_of",
        "tail": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views"
      },
      {
        "head": "Xufang Luo",
        "relation": "author_of",
        "tail": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views"
      },
      {
        "head": "Mingze Sun",
        "relation": "author_of",
        "tail": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views"
      },
      {
        "head": "Zihao Pan",
        "relation": "author_of",
        "tail": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views"
      },
      {
        "head": "Yan Feng",
        "relation": "author_of",
        "tail": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views"
      },
      {
        "head": "Peng Pei",
        "relation": "author_of",
        "tail": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views"
      },
      {
        "head": "Xunliang Cai",
        "relation": "author_of",
        "tail": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views"
      },
      {
        "head": "Ruqi Huang",
        "relation": "author_of",
        "tail": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views"
      },
      {
        "head": "Tiancheng Gu",
        "relation": "author_of",
        "tail": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning"
      },
      {
        "head": "Kaicheng Yang",
        "relation": "author_of",
        "tail": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning"
      },
      {
        "head": "Kaichen Zhang",
        "relation": "author_of",
        "tail": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning"
      },
      {
        "head": "Xiang An",
        "relation": "author_of",
        "tail": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning"
      },
      {
        "head": "Ziyong Feng",
        "relation": "author_of",
        "tail": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning"
      },
      {
        "head": "Yueyi Zhang",
        "relation": "author_of",
        "tail": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning"
      },
      {
        "head": "Weidong Cai",
        "relation": "author_of",
        "tail": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning"
      },
      {
        "head": "Jiankang Deng",
        "relation": "author_of",
        "tail": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning"
      },
      {
        "head": "Lidong Bing",
        "relation": "author_of",
        "tail": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning"
      },
      {
        "head": "Kaichen Zhang",
        "relation": "author_of",
        "tail": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe"
      },
      {
        "head": "Keming Wu",
        "relation": "author_of",
        "tail": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe"
      },
      {
        "head": "Zuhao Yang",
        "relation": "author_of",
        "tail": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe"
      },
      {
        "head": "Bo Li",
        "relation": "author_of",
        "tail": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe"
      },
      {
        "head": "Kairui Hu",
        "relation": "author_of",
        "tail": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe"
      },
      {
        "head": "Bin Wang",
        "relation": "author_of",
        "tail": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe"
      },
      {
        "head": "Ziwei Liu",
        "relation": "author_of",
        "tail": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe"
      },
      {
        "head": "Xingxuan Li",
        "relation": "author_of",
        "tail": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe"
      },
      {
        "head": "Lidong Bing",
        "relation": "author_of",
        "tail": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe"
      },
      {
        "head": "Ranjan Sapkota",
        "relation": "author_of",
        "tail": "Vision-Language-Action (VLA) Models: Concepts, Progress, Applications and Challenges"
      },
      {
        "head": "Yang Cao",
        "relation": "author_of",
        "tail": "Vision-Language-Action (VLA) Models: Concepts, Progress, Applications and Challenges"
      },
      {
        "head": "Konstantinos I. Roumeliotis",
        "relation": "author_of",
        "tail": "Vision-Language-Action (VLA) Models: Concepts, Progress, Applications and Challenges"
      },
      {
        "head": "Manoj Karkee",
        "relation": "author_of",
        "tail": "Vision-Language-Action (VLA) Models: Concepts, Progress, Applications and Challenges"
      },
      {
        "head": "Kohei Sendai",
        "relation": "author_of",
        "tail": "Leave No Observation Behind: Real-time Correction for VLA Action Chunks"
      },
      {
        "head": "Maxime Alvarez",
        "relation": "author_of",
        "tail": "Leave No Observation Behind: Real-time Correction for VLA Action Chunks"
      },
      {
        "head": "Tatsuya Matsushima",
        "relation": "author_of",
        "tail": "Leave No Observation Behind: Real-time Correction for VLA Action Chunks"
      },
      {
        "head": "Yutaka Matsuo",
        "relation": "author_of",
        "tail": "Leave No Observation Behind: Real-time Correction for VLA Action Chunks"
      },
      {
        "head": "Yusuke Iwasawa",
        "relation": "author_of",
        "tail": "Leave No Observation Behind: Real-time Correction for VLA Action Chunks"
      },
      {
        "head": "Shuhan Tan",
        "relation": "author_of",
        "tail": "Latent Chain-of-Thought World Modeling for End-to-End Driving"
      },
      {
        "head": "Kashyap Chitta",
        "relation": "author_of",
        "tail": "Latent Chain-of-Thought World Modeling for End-to-End Driving"
      },
      {
        "head": "Yuxiao Chen",
        "relation": "author_of",
        "tail": "Latent Chain-of-Thought World Modeling for End-to-End Driving"
      },
      {
        "head": "Ran Tian",
        "relation": "author_of",
        "tail": "Latent Chain-of-Thought World Modeling for End-to-End Driving"
      },
      {
        "head": "Yurong You",
        "relation": "author_of",
        "tail": "Latent Chain-of-Thought World Modeling for End-to-End Driving"
      },
      {
        "head": "Yan Wang",
        "relation": "author_of",
        "tail": "Latent Chain-of-Thought World Modeling for End-to-End Driving"
      },
      {
        "head": "Wenjie Luo",
        "relation": "author_of",
        "tail": "Latent Chain-of-Thought World Modeling for End-to-End Driving"
      },
      {
        "head": "Yulong Cao",
        "relation": "author_of",
        "tail": "Latent Chain-of-Thought World Modeling for End-to-End Driving"
      },
      {
        "head": "Philipp Krahenbuhl",
        "relation": "author_of",
        "tail": "Latent Chain-of-Thought World Modeling for End-to-End Driving"
      },
      {
        "head": "Marco Pavone",
        "relation": "author_of",
        "tail": "Latent Chain-of-Thought World Modeling for End-to-End Driving"
      },
      {
        "head": "Boris Ivanovic",
        "relation": "author_of",
        "tail": "Latent Chain-of-Thought World Modeling for End-to-End Driving"
      },
      {
        "head": "Zheng Xiong",
        "relation": "author_of",
        "tail": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks"
      },
      {
        "head": "Kang Li",
        "relation": "author_of",
        "tail": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks"
      },
      {
        "head": "Zilin Wang",
        "relation": "author_of",
        "tail": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks"
      },
      {
        "head": "Matthew Jackson",
        "relation": "author_of",
        "tail": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks"
      },
      {
        "head": "Jakob Foerster",
        "relation": "author_of",
        "tail": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks"
      },
      {
        "head": "Shimon Whiteson",
        "relation": "author_of",
        "tail": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks"
      },
      {
        "head": "Yifan Ye",
        "relation": "author_of",
        "tail": "Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models"
      },
      {
        "head": "Jiaqi Ma",
        "relation": "author_of",
        "tail": "Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models"
      },
      {
        "head": "Jun Cen",
        "relation": "author_of",
        "tail": "Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models"
      },
      {
        "head": "Zhihe Lu",
        "relation": "author_of",
        "tail": "Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models"
      },
      {
        "head": "Zhaohu Xing",
        "relation": "author_of",
        "tail": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation"
      },
      {
        "head": "Tian Ye",
        "relation": "author_of",
        "tail": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation"
      },
      {
        "head": "Yijun Yang",
        "relation": "author_of",
        "tail": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation"
      },
      {
        "head": "D. Cai",
        "relation": "author_of",
        "tail": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation"
      },
      {
        "head": "Baowen Gai",
        "relation": "author_of",
        "tail": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation"
      },
      {
        "head": "Xiao-Jian Wu",
        "relation": "author_of",
        "tail": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation"
      },
      {
        "head": "Feng Gao",
        "relation": "author_of",
        "tail": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation"
      },
      {
        "head": "Lei Zhu",
        "relation": "author_of",
        "tail": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation"
      },
      {
        "head": "Hai Liu",
        "relation": "author_of",
        "tail": "TransSIL: A Silhouette Cue-Aware Image Classification Framework for Bird Ecological Monitoring Systems"
      },
      {
        "head": "Yu Song",
        "relation": "author_of",
        "tail": "TransSIL: A Silhouette Cue-Aware Image Classification Framework for Bird Ecological Monitoring Systems"
      },
      {
        "head": "Tingting Liu",
        "relation": "author_of",
        "tail": "TransSIL: A Silhouette Cue-Aware Image Classification Framework for Bird Ecological Monitoring Systems"
      },
      {
        "head": "Lin Chen",
        "relation": "author_of",
        "tail": "TransSIL: A Silhouette Cue-Aware Image Classification Framework for Bird Ecological Monitoring Systems"
      },
      {
        "head": "Zhaoli Zhang",
        "relation": "author_of",
        "tail": "TransSIL: A Silhouette Cue-Aware Image Classification Framework for Bird Ecological Monitoring Systems"
      },
      {
        "head": "Xiaolan Yang",
        "relation": "author_of",
        "tail": "TransSIL: A Silhouette Cue-Aware Image Classification Framework for Bird Ecological Monitoring Systems"
      },
      {
        "head": "Neal N. Xiong",
        "relation": "author_of",
        "tail": "TransSIL: A Silhouette Cue-Aware Image Classification Framework for Bird Ecological Monitoring Systems"
      },
      {
        "head": "Honghu Chu",
        "relation": "author_of",
        "tail": "CBRFormer: rendering technology-based transformer for refinement segmentation of bridge crack images"
      },
      {
        "head": "Jiahao Gai",
        "relation": "author_of",
        "tail": "CBRFormer: rendering technology-based transformer for refinement segmentation of bridge crack images"
      },
      {
        "head": "Weiwei Chen",
        "relation": "author_of",
        "tail": "CBRFormer: rendering technology-based transformer for refinement segmentation of bridge crack images"
      },
      {
        "head": "Jun Ma",
        "relation": "author_of",
        "tail": "CBRFormer: rendering technology-based transformer for refinement segmentation of bridge crack images"
      },
      {
        "head": "Yinjun Jia",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Bowen Gao",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Jiaxin Tan",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Jiqing Zheng",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Xin Hong",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Wenyu Zhu",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Haichuan Tan",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Yuan Xiao",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Liping Tan",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Hongyi Cai",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Yanwen Huang",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Zhiheng Deng",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Xiangwei Wu",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Yue Jin",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Yafei Yuan",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Jiekang Tian",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Wei He",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Weiying Ma",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Ya-Qin Zhang",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Lei Liu",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Chuangye Yan",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Wei Zhang",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Yanyan Lan",
        "relation": "author_of",
        "tail": "Deep contrastive learning enables genome-wide virtual screening."
      },
      {
        "head": "Guodong Fan",
        "relation": "author_of",
        "tail": "LLaVA-based semantic feature modulation diffusion model for underwater image enhancement"
      },
      {
        "head": "Shengning Zhou",
        "relation": "author_of",
        "tail": "LLaVA-based semantic feature modulation diffusion model for underwater image enhancement"
      },
      {
        "head": "Zhen Hua",
        "relation": "author_of",
        "tail": "LLaVA-based semantic feature modulation diffusion model for underwater image enhancement"
      },
      {
        "head": "Jinjiang Li",
        "relation": "author_of",
        "tail": "LLaVA-based semantic feature modulation diffusion model for underwater image enhancement"
      },
      {
        "head": "Jingchun Zhou",
        "relation": "author_of",
        "tail": "LLaVA-based semantic feature modulation diffusion model for underwater image enhancement"
      },
      {
        "head": "Jiahua Dong",
        "relation": "author_of",
        "tail": "3DGS-Drag: Dragging Gaussians for Intuitive Point-Based 3D Editing"
      },
      {
        "head": "Yu-Xiong Wang",
        "relation": "author_of",
        "tail": "3DGS-Drag: Dragging Gaussians for Intuitive Point-Based 3D Editing"
      },
      {
        "head": "Hasi Hays",
        "relation": "author_of",
        "tail": "Attention mechanisms in neural networks"
      },
      {
        "head": "Yiyang Lu",
        "relation": "author_of",
        "tail": "Bidirectional Normalizing Flow: From Data to Noise and Back"
      },
      {
        "head": "Qiao Sun",
        "relation": "author_of",
        "tail": "Bidirectional Normalizing Flow: From Data to Noise and Back"
      },
      {
        "head": "Xianbang Wang",
        "relation": "author_of",
        "tail": "Bidirectional Normalizing Flow: From Data to Noise and Back"
      },
      {
        "head": "Zhicheng Jiang",
        "relation": "author_of",
        "tail": "Bidirectional Normalizing Flow: From Data to Noise and Back"
      },
      {
        "head": "Hanhong Zhao",
        "relation": "author_of",
        "tail": "Bidirectional Normalizing Flow: From Data to Noise and Back"
      },
      {
        "head": "Kaiming He",
        "relation": "author_of",
        "tail": "Bidirectional Normalizing Flow: From Data to Noise and Back"
      },
      {
        "head": "Yiyang Lu",
        "relation": "author_of",
        "tail": "One-step Latent-free Image Generation with Pixel Mean Flows"
      },
      {
        "head": "Susie Lu",
        "relation": "author_of",
        "tail": "One-step Latent-free Image Generation with Pixel Mean Flows"
      },
      {
        "head": "Qiao Sun",
        "relation": "author_of",
        "tail": "One-step Latent-free Image Generation with Pixel Mean Flows"
      },
      {
        "head": "Hanhong Zhao",
        "relation": "author_of",
        "tail": "One-step Latent-free Image Generation with Pixel Mean Flows"
      },
      {
        "head": "Zhicheng Jiang",
        "relation": "author_of",
        "tail": "One-step Latent-free Image Generation with Pixel Mean Flows"
      },
      {
        "head": "Xianbang Wang",
        "relation": "author_of",
        "tail": "One-step Latent-free Image Generation with Pixel Mean Flows"
      },
      {
        "head": "Tianhong Li",
        "relation": "author_of",
        "tail": "One-step Latent-free Image Generation with Pixel Mean Flows"
      },
      {
        "head": "Zhengyang Geng",
        "relation": "author_of",
        "tail": "One-step Latent-free Image Generation with Pixel Mean Flows"
      },
      {
        "head": "Kaiming He",
        "relation": "author_of",
        "tail": "One-step Latent-free Image Generation with Pixel Mean Flows"
      },
      {
        "head": "Peter Potaptchik",
        "relation": "author_of",
        "tail": "Meta Flow Maps enable scalable reward alignment"
      },
      {
        "head": "Adhi Saravanan",
        "relation": "author_of",
        "tail": "Meta Flow Maps enable scalable reward alignment"
      },
      {
        "head": "Abbas Mammadov",
        "relation": "author_of",
        "tail": "Meta Flow Maps enable scalable reward alignment"
      },
      {
        "head": "Alvaro Prat",
        "relation": "author_of",
        "tail": "Meta Flow Maps enable scalable reward alignment"
      },
      {
        "head": "Michael S. Albergo",
        "relation": "author_of",
        "tail": "Meta Flow Maps enable scalable reward alignment"
      },
      {
        "head": "Yee Whye Teh",
        "relation": "author_of",
        "tail": "Meta Flow Maps enable scalable reward alignment"
      },
      {
        "head": "Yinan Huang",
        "relation": "author_of",
        "tail": "Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective"
      },
      {
        "head": "Hans Hao-Hsun Hsu",
        "relation": "author_of",
        "tail": "Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective"
      },
      {
        "head": "Junran Wang",
        "relation": "author_of",
        "tail": "Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective"
      },
      {
        "head": "Bo Dai",
        "relation": "author_of",
        "tail": "Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective"
      },
      {
        "head": "Pan Li",
        "relation": "author_of",
        "tail": "Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective"
      },
      {
        "head": "Mingyang Deng",
        "relation": "author_of",
        "tail": "Generative Modeling via Drifting"
      },
      {
        "head": "He Li",
        "relation": "author_of",
        "tail": "Generative Modeling via Drifting"
      },
      {
        "head": "Tianhong Li",
        "relation": "author_of",
        "tail": "Generative Modeling via Drifting"
      },
      {
        "head": "Yilun Du",
        "relation": "author_of",
        "tail": "Generative Modeling via Drifting"
      },
      {
        "head": "Kaiming He",
        "relation": "author_of",
        "tail": "Generative Modeling via Drifting"
      },
      {
        "head": "Ting Chen",
        "relation": "author_of",
        "tail": "A Simple Framework for Contrastive Learning of Visual Representations"
      },
      {
        "head": "Simon Kornblith",
        "relation": "author_of",
        "tail": "A Simple Framework for Contrastive Learning of Visual Representations"
      },
      {
        "head": "Mohammad Norouzi",
        "relation": "author_of",
        "tail": "A Simple Framework for Contrastive Learning of Visual Representations"
      },
      {
        "head": "Geoffrey Hinton",
        "relation": "author_of",
        "tail": "A Simple Framework for Contrastive Learning of Visual Representations"
      },
      {
        "head": "Chubin Chen",
        "relation": "author_of",
        "tail": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning"
      },
      {
        "head": "Sujie Hu",
        "relation": "author_of",
        "tail": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning"
      },
      {
        "head": "Jiashu Zhu",
        "relation": "author_of",
        "tail": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning"
      },
      {
        "head": "Meiqi Wu",
        "relation": "author_of",
        "tail": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning"
      },
      {
        "head": "Jintao Chen",
        "relation": "author_of",
        "tail": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning"
      },
      {
        "head": "Yanxun Li",
        "relation": "author_of",
        "tail": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning"
      },
      {
        "head": "Nisha Huang",
        "relation": "author_of",
        "tail": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning"
      },
      {
        "head": "Chengyu Fang",
        "relation": "author_of",
        "tail": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning"
      },
      {
        "head": "Jiahong Wu",
        "relation": "author_of",
        "tail": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning"
      },
      {
        "head": "Xiangxiang Chu",
        "relation": "author_of",
        "tail": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning"
      },
      {
        "head": "Xiu Li",
        "relation": "author_of",
        "tail": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning"
      },
      {
        "head": "Meiqi Wu",
        "relation": "author_of",
        "tail": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints"
      },
      {
        "head": "Jiashu Zhu",
        "relation": "author_of",
        "tail": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints"
      },
      {
        "head": "Xiaokun Feng",
        "relation": "author_of",
        "tail": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints"
      },
      {
        "head": "Chubin Chen",
        "relation": "author_of",
        "tail": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints"
      },
      {
        "head": "Chen Zhu",
        "relation": "author_of",
        "tail": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints"
      },
      {
        "head": "Bingze Song",
        "relation": "author_of",
        "tail": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints"
      },
      {
        "head": "Fangyuan Mao",
        "relation": "author_of",
        "tail": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints"
      },
      {
        "head": "Jiahong Wu",
        "relation": "author_of",
        "tail": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints"
      },
      {
        "head": "Xiangxiang Chu",
        "relation": "author_of",
        "tail": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints"
      },
      {
        "head": "Kaiqi Huang",
        "relation": "author_of",
        "tail": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints"
      },
      {
        "head": "Shengbang Tong",
        "relation": "author_of",
        "tail": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "Boyang Zheng",
        "relation": "author_of",
        "tail": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "Ziteng Wang",
        "relation": "author_of",
        "tail": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "Bingda Tang",
        "relation": "author_of",
        "tail": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "Nanye Ma",
        "relation": "author_of",
        "tail": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "Ellis Brown",
        "relation": "author_of",
        "tail": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "Jihan Yang",
        "relation": "author_of",
        "tail": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "Rob Fergus",
        "relation": "author_of",
        "tail": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "Yann LeCun",
        "relation": "author_of",
        "tail": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "Saining Xie",
        "relation": "author_of",
        "tail": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "Jingtong Yue",
        "relation": "author_of",
        "tail": "Simulating the Visual World with Artificial Intelligence: A Roadmap"
      },
      {
        "head": "Ziqi Huang",
        "relation": "author_of",
        "tail": "Simulating the Visual World with Artificial Intelligence: A Roadmap"
      },
      {
        "head": "Zhaoxi Chen",
        "relation": "author_of",
        "tail": "Simulating the Visual World with Artificial Intelligence: A Roadmap"
      },
      {
        "head": "Xintao Wang",
        "relation": "author_of",
        "tail": "Simulating the Visual World with Artificial Intelligence: A Roadmap"
      },
      {
        "head": "Pengfei Wan",
        "relation": "author_of",
        "tail": "Simulating the Visual World with Artificial Intelligence: A Roadmap"
      },
      {
        "head": "Ziwei Liu",
        "relation": "author_of",
        "tail": "Simulating the Visual World with Artificial Intelligence: A Roadmap"
      },
      {
        "head": "Bohan Zeng",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Kaixin Zhu",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Daili Hua",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Bozhou Li",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Chengzhuo Tong",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Yuran Wang",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Xinyi Huang",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Yifan Dai",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Zixiang Zhang",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Yifan Yang",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Zhou Liu",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Hao Liang",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Xiaochen Ma",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Ruichuan An",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Tianyi Bai",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Hongcheng Gao",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Junbo Niu",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Yang Shi",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Xinlong Chen",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Yue Ding",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Minglei Shi",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Kai Zeng",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Yiwen Tang",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Yuanxing Zhang",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Pengfei Wan",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Xintao Wang",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Wentao Zhang",
        "relation": "author_of",
        "tail": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"
      },
      {
        "head": "Qingyu Shi",
        "relation": "author_of",
        "tail": "RecTok: Reconstruction Distillation along Rectified Flow"
      },
      {
        "head": "Size Wu",
        "relation": "author_of",
        "tail": "RecTok: Reconstruction Distillation along Rectified Flow"
      },
      {
        "head": "Jinbin Bai",
        "relation": "author_of",
        "tail": "RecTok: Reconstruction Distillation along Rectified Flow"
      },
      {
        "head": "Kaidong Yu",
        "relation": "author_of",
        "tail": "RecTok: Reconstruction Distillation along Rectified Flow"
      },
      {
        "head": "Yujing Wang",
        "relation": "author_of",
        "tail": "RecTok: Reconstruction Distillation along Rectified Flow"
      },
      {
        "head": "Yunhai Tong",
        "relation": "author_of",
        "tail": "RecTok: Reconstruction Distillation along Rectified Flow"
      },
      {
        "head": "Xiangtai Li",
        "relation": "author_of",
        "tail": "RecTok: Reconstruction Distillation along Rectified Flow"
      },
      {
        "head": "Xuelong Li",
        "relation": "author_of",
        "tail": "RecTok: Reconstruction Distillation along Rectified Flow"
      },
      {
        "head": "Guanfang Dong",
        "relation": "author_of",
        "tail": "RePack then Refine: Efficient Diffusion Transformer with Vision Foundation Model"
      },
      {
        "head": "Luke Schultz",
        "relation": "author_of",
        "tail": "RePack then Refine: Efficient Diffusion Transformer with Vision Foundation Model"
      },
      {
        "head": "Negar Hassanpour",
        "relation": "author_of",
        "tail": "RePack then Refine: Efficient Diffusion Transformer with Vision Foundation Model"
      },
      {
        "head": "Chao Gao",
        "relation": "author_of",
        "tail": "RePack then Refine: Efficient Diffusion Transformer with Vision Foundation Model"
      },
      {
        "head": "Prafulla Dhariwal",
        "relation": "author_of",
        "tail": "Diffusion Models Beat GANs on Image Synthesis"
      },
      {
        "head": "Alex Nichol",
        "relation": "author_of",
        "tail": "Diffusion Models Beat GANs on Image Synthesis"
      },
      {
        "head": "Maxime Oquab",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Timothée Darcet",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Théo Moutakanni",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Huy Vo",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Marc Szafraniec",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Vasil Khalidov",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Pierre Fernandez",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Daniel Haziza",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Francisco Massa",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Alaaeldin El-Nouby",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Mahmoud Assran",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Nicolas Ballas",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Wojciech Galuba",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Russell Howes",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Po-Yao Huang",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Shang-Wen Li",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Ishan Misra",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Michael Rabbat",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Vasu Sharma",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Gabriel Synnaeve",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Hu Xu",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Hervé Jegou",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Julien Mairal",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Patrick Labatut",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Armand Joulin",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Piotr Bojanowski",
        "relation": "author_of",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "Zehong Ma",
        "relation": "author_of",
        "tail": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss"
      },
      {
        "head": "Ruihan Xu",
        "relation": "author_of",
        "tail": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss"
      },
      {
        "head": "Shiliang Zhang",
        "relation": "author_of",
        "tail": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss"
      },
      {
        "head": "Shanshan Zhao",
        "relation": "author_of",
        "tail": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities"
      },
      {
        "head": "Xinjie Zhang",
        "relation": "author_of",
        "tail": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities"
      },
      {
        "head": "Jintao Guo",
        "relation": "author_of",
        "tail": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities"
      },
      {
        "head": "Jiakui Hu",
        "relation": "author_of",
        "tail": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities"
      },
      {
        "head": "Lunhao Duan",
        "relation": "author_of",
        "tail": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities"
      },
      {
        "head": "Minghao Fu",
        "relation": "author_of",
        "tail": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities"
      },
      {
        "head": "Yong Xien Chng",
        "relation": "author_of",
        "tail": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities"
      },
      {
        "head": "Guo-Hua Wang",
        "relation": "author_of",
        "tail": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities"
      },
      {
        "head": "Qing-Guo Chen",
        "relation": "author_of",
        "tail": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities"
      },
      {
        "head": "Zhao Xu",
        "relation": "author_of",
        "tail": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities"
      },
      {
        "head": "Weihua Luo",
        "relation": "author_of",
        "tail": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities"
      },
      {
        "head": "Kaifu Zhang",
        "relation": "author_of",
        "tail": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities"
      },
      {
        "head": "Jana Zeller",
        "relation": "author_of",
        "tail": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery"
      },
      {
        "head": "Thaddäus Wiedemer",
        "relation": "author_of",
        "tail": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery"
      },
      {
        "head": "Fanfei Li",
        "relation": "author_of",
        "tail": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery"
      },
      {
        "head": "Thomas Klein",
        "relation": "author_of",
        "tail": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery"
      },
      {
        "head": "Prasanna Mayilvahanan",
        "relation": "author_of",
        "tail": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery"
      },
      {
        "head": "Matthias Bethge",
        "relation": "author_of",
        "tail": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery"
      },
      {
        "head": "Felix Wichmann",
        "relation": "author_of",
        "tail": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery"
      },
      {
        "head": "Ryan Cotterell",
        "relation": "author_of",
        "tail": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery"
      },
      {
        "head": "Wieland Brendel",
        "relation": "author_of",
        "tail": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery"
      },
      {
        "head": "Letian Zhang",
        "relation": "author_of",
        "tail": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation"
      },
      {
        "head": "Sucheng Ren",
        "relation": "author_of",
        "tail": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation"
      },
      {
        "head": "Yanqing Liu",
        "relation": "author_of",
        "tail": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation"
      },
      {
        "head": "Xianhang Li",
        "relation": "author_of",
        "tail": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation"
      },
      {
        "head": "Zeyu Wang",
        "relation": "author_of",
        "tail": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation"
      },
      {
        "head": "Yuyin Zhou",
        "relation": "author_of",
        "tail": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation"
      },
      {
        "head": "Huaxiu Yao",
        "relation": "author_of",
        "tail": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation"
      },
      {
        "head": "Zeyu Zheng",
        "relation": "author_of",
        "tail": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation"
      },
      {
        "head": "Weili Nie",
        "relation": "author_of",
        "tail": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation"
      },
      {
        "head": "Guilin Liu",
        "relation": "author_of",
        "tail": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation"
      },
      {
        "head": "Zhiding Yu",
        "relation": "author_of",
        "tail": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation"
      },
      {
        "head": "Cihang Xie",
        "relation": "author_of",
        "tail": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation"
      },
      {
        "head": "Tomas Mikolov",
        "relation": "author_of",
        "tail": "Distributed Representations of Words and Phrases and their Compositionality"
      },
      {
        "head": "Ilya Sutskever",
        "relation": "author_of",
        "tail": "Distributed Representations of Words and Phrases and their Compositionality"
      },
      {
        "head": "Kai Chen",
        "relation": "author_of",
        "tail": "Distributed Representations of Words and Phrases and their Compositionality"
      },
      {
        "head": "Greg Corrado",
        "relation": "author_of",
        "tail": "Distributed Representations of Words and Phrases and their Compositionality"
      },
      {
        "head": "Jeffrey Dean",
        "relation": "author_of",
        "tail": "Distributed Representations of Words and Phrases and their Compositionality"
      },
      {
        "head": "Jeffrey Pennington",
        "relation": "author_of",
        "tail": "GloVe: Global Vectors for Word Representation"
      },
      {
        "head": "R. Socher",
        "relation": "author_of",
        "tail": "GloVe: Global Vectors for Word Representation"
      },
      {
        "head": "Christopher D. Manning",
        "relation": "author_of",
        "tail": "GloVe: Global Vectors for Word Representation"
      },
      {
        "head": "Matthew E. Peters",
        "relation": "author_of",
        "tail": "Deep Contextualized Word Representations"
      },
      {
        "head": "Mark Neumann",
        "relation": "author_of",
        "tail": "Deep Contextualized Word Representations"
      },
      {
        "head": "Mohit Iyyer",
        "relation": "author_of",
        "tail": "Deep Contextualized Word Representations"
      },
      {
        "head": "Matt Gardner",
        "relation": "author_of",
        "tail": "Deep Contextualized Word Representations"
      },
      {
        "head": "Christopher Clark",
        "relation": "author_of",
        "tail": "Deep Contextualized Word Representations"
      },
      {
        "head": "Kenton Lee",
        "relation": "author_of",
        "tail": "Deep Contextualized Word Representations"
      },
      {
        "head": "Luke Zettlemoyer",
        "relation": "author_of",
        "tail": "Deep Contextualized Word Representations"
      },
      {
        "head": "Quentin Fournier",
        "relation": "author_of",
        "tail": "Protein Language Models: Is Scaling Necessary?"
      },
      {
        "head": "Robert M. Vernon",
        "relation": "author_of",
        "tail": "Protein Language Models: Is Scaling Necessary?"
      },
      {
        "head": "Almer M. van der Sloot",
        "relation": "author_of",
        "tail": "Protein Language Models: Is Scaling Necessary?"
      },
      {
        "head": "Benjamin Schulz",
        "relation": "author_of",
        "tail": "Protein Language Models: Is Scaling Necessary?"
      },
      {
        "head": "Sarath Chandar",
        "relation": "author_of",
        "tail": "Protein Language Models: Is Scaling Necessary?"
      },
      {
        "head": "C. Langmead",
        "relation": "author_of",
        "tail": "Protein Language Models: Is Scaling Necessary?"
      },
      {
        "head": "Scott Friedman",
        "relation": "author_of",
        "tail": "Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis"
      },
      {
        "head": "Sonja Schmer-Galunder",
        "relation": "author_of",
        "tail": "Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis"
      },
      {
        "head": "Anthony Chen",
        "relation": "author_of",
        "tail": "Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis"
      },
      {
        "head": "Jeffrey Rye",
        "relation": "author_of",
        "tail": "Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis"
      },
      {
        "head": "Alexander C. Li",
        "relation": "author_of",
        "tail": "Generative Classifiers Avoid Shortcut Solutions"
      },
      {
        "head": "Ananya Kumar",
        "relation": "author_of",
        "tail": "Generative Classifiers Avoid Shortcut Solutions"
      },
      {
        "head": "Deepak Pathak",
        "relation": "author_of",
        "tail": "Generative Classifiers Avoid Shortcut Solutions"
      },
      {
        "head": "Y. Sun",
        "relation": "author_of",
        "tail": "Edge Large AI Model Agent-Empowered Cognitive Multimodal Semantic Communication"
      },
      {
        "head": "Yinqiu Liu",
        "relation": "author_of",
        "tail": "Edge Large AI Model Agent-Empowered Cognitive Multimodal Semantic Communication"
      },
      {
        "head": "Shaoyong Guo",
        "relation": "author_of",
        "tail": "Edge Large AI Model Agent-Empowered Cognitive Multimodal Semantic Communication"
      },
      {
        "head": "Xuesong Qiu",
        "relation": "author_of",
        "tail": "Edge Large AI Model Agent-Empowered Cognitive Multimodal Semantic Communication"
      },
      {
        "head": "Jiewei Chen",
        "relation": "author_of",
        "tail": "Edge Large AI Model Agent-Empowered Cognitive Multimodal Semantic Communication"
      },
      {
        "head": "Jiakai Hao",
        "relation": "author_of",
        "tail": "Edge Large AI Model Agent-Empowered Cognitive Multimodal Semantic Communication"
      },
      {
        "head": "Dusist Niyato",
        "relation": "author_of",
        "tail": "Edge Large AI Model Agent-Empowered Cognitive Multimodal Semantic Communication"
      },
      {
        "head": "Alec Radford",
        "relation": "author_of",
        "tail": "Language Models are Unsupervised Multitask Learners"
      },
      {
        "head": "Jeff Wu",
        "relation": "author_of",
        "tail": "Language Models are Unsupervised Multitask Learners"
      },
      {
        "head": "R. Child",
        "relation": "author_of",
        "tail": "Language Models are Unsupervised Multitask Learners"
      },
      {
        "head": "D. Luan",
        "relation": "author_of",
        "tail": "Language Models are Unsupervised Multitask Learners"
      },
      {
        "head": "Dario Amodei",
        "relation": "author_of",
        "tail": "Language Models are Unsupervised Multitask Learners"
      },
      {
        "head": "I. Sutskever",
        "relation": "author_of",
        "tail": "Language Models are Unsupervised Multitask Learners"
      },
      {
        "head": "Aaron Grattafiori",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Abhimanyu Dubey",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Abhinav Jauhri",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Abhinav Pandey",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Abhishek Kadian",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ahmad Al-Dahle",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Aiesha Letman",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Akhil Mathur",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Alan Schelten",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Alex Vaughan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Amy Yang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Angela Fan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Anirudh Goyal",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Anthony Hartshorn",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Aobo Yang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Archi Mitra",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Archie Sravankumar",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Artem Korenev",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Arthur Hinsvark",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Arun Rao",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Aston Zhang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Aurelien Rodriguez",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Austen Gregerson",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ava Spataru",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Baptiste Roziere",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Bethany Biron",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Binh Tang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Bobbie Chern",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Charlotte Caucheteux",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Chaya Nayak",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Chloe Bi",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Chris Marra",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Chris McConnell",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Christian Keller",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Christophe Touret",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Chunyang Wu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Corinne Wong",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Cristian Canton Ferrer",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Cyrus Nikolaidis",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Damien Allonsius",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Daniel Song",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Danielle Pintz",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Danny Livshits",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Danny Wyatt",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "David Esiobu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Dhruv Choudhary",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Dhruv Mahajan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Diego Garcia-Olano",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Diego Perino",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Dieuwke Hupkes",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Egor Lakomkin",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ehab AlBadawy",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Elina Lobanova",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Emily Dinan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Eric Michael Smith",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Filip Radenovic",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Francisco Guzmán",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Frank Zhang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Gabriel Synnaeve",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Gabrielle Lee",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Georgia Lewis Anderson",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Govind Thattai",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Graeme Nail",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Gregoire Mialon",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Guan Pang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Guillem Cucurell",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Hailey Nguyen",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Hannah Korevaar",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Hu Xu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Hugo Touvron",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Iliyan Zarov",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Imanol Arrieta Ibarra",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Isabel Kloumann",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ishan Misra",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ivan Evtimov",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jack Zhang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jade Copet",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jaewon Lee",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jan Geffert",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jana Vranes",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jason Park",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jay Mahadeokar",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jeet Shah",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jelmer van der Linde",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jennifer Billock",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jenny Hong",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jenya Lee",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jeremy Fu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jianfeng Chi",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jianyu Huang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jiawen Liu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jie Wang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jiecao Yu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Joanna Bitton",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Joe Spisak",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jongsoo Park",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Joseph Rocca",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Joshua Johnstun",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Joshua Saxe",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Junteng Jia",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kalyan Vasuden Alwala",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Karthik Prasad",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kartikeya Upasani",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kate Plawiak",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ke Li",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kenneth Heafield",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kevin Stone",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Khalid El-Arini",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Krithika Iyer",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kshitiz Malik",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kuenley Chiu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kunal Bhalla",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kushal Lakhotia",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Lauren Rantala-Yeary",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Laurens van der Maaten",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Lawrence Chen",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Liang Tan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Liz Jenkins",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Louis Martin",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Lovish Madaan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Lubo Malo",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Lukas Blecher",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Lukas Landzaat",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Luke de Oliveira",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Madeline Muzzi",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Mahesh Pasupuleti",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Mannat Singh",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Manohar Paluri",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Marcin Kardas",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Maria Tsimpoukelli",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Mathew Oldham",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Mathieu Rita",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Maya Pavlova",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Melanie Kambadur",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Mike Lewis",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Min Si",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Mitesh Kumar Singh",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Mona Hassan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Naman Goyal",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Narjes Torabi",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Nikolay Bashlykov",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Nikolay Bogoychev",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Niladri Chatterji",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ning Zhang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Olivier Duchenne",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Onur Çelebi",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Patrick Alrassy",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Pengchuan Zhang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Pengwei Li",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Petar Vasic",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Peter Weng",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Prajjwal Bhargava",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Pratik Dubal",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Praveen Krishnan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Punit Singh Koura",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Puxin Xu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Qing He",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Qingxiao Dong",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ragavan Srinivasan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Raj Ganapathy",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ramon Calderer",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ricardo Silveira Cabral",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Robert Stojnic",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Roberta Raileanu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Rohan Maheswari",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Rohit Girdhar",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Rohit Patel",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Romain Sauvestre",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ronnie Polidoro",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Roshan Sumbaly",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ross Taylor",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ruan Silva",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Rui Hou",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Rui Wang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Saghar Hosseini",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sahana Chennabasappa",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sanjay Singh",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sean Bell",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Seohyun Sonia Kim",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sergey Edunov",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Shaoliang Nie",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sharan Narang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sharath Raparthy",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sheng Shen",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Shengye Wan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Shruti Bhosale",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Shun Zhang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Simon Vandenhende",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Soumya Batra",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Spencer Whitman",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sten Sootla",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Stephane Collot",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Suchin Gururangan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sydney Borodinsky",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Tamar Herman",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Tara Fowler",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Tarek Sheasha",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Thomas Georgiou",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Thomas Scialom",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Tobias Speckbacher",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Todor Mihaylov",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Tong Xiao",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ujjwal Karn",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Vedanuj Goswami",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Vibhor Gupta",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Vignesh Ramanathan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Viktor Kerkez",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Vincent Gonguet",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Virginie Do",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Vish Vogeti",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Vítor Albiero",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Vladan Petrovic",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Weiwei Chu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Wenhan Xiong",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Wenyin Fu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Whitney Meers",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Xavier Martinet",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Xiaodong Wang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Xiaofang Wang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Xiaoqing Ellen Tan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Xide Xia",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Xinfeng Xie",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Xuchao Jia",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Xuewei Wang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yaelle Goldschlag",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yashesh Gaur",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yasmine Babaei",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yi Wen",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yiwen Song",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yuchen Zhang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yue Li",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yuning Mao",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Zacharie Delpierre Coudert",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Zheng Yan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Zhengxing Chen",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Zoe Papakipos",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Aaditya Singh",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Aayushi Srivastava",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Abha Jain",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Adam Kelsey",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Adam Shajnfeld",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Adithya Gangidi",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Adolfo Victoria",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ahuva Goldstand",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ajay Menon",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ajay Sharma",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Alex Boesenberg",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Alexei Baevski",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Allie Feinstein",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Amanda Kallet",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Amit Sangani",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Amos Teo",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Anam Yunus",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Andrei Lupu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Andres Alvarado",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Andrew Caples",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Andrew Gu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Andrew Ho",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Andrew Poulton",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Andrew Ryan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ankit Ramchandani",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Annie Dong",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Annie Franco",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Anuj Goyal",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Aparajita Saraf",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Arkabandhu Chowdhury",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ashley Gabriel",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ashwin Bharambe",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Assaf Eisenman",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Azadeh Yazdan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Beau James",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ben Maurer",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Benjamin Leonhardi",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Bernie Huang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Beth Loyd",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Beto De Paola",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Bhargavi Paranjape",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Bing Liu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Bo Wu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Boyu Ni",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Braden Hancock",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Bram Wasti",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Brandon Spence",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Brani Stojkovic",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Brian Gamido",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Britt Montalvo",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Carl Parker",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Carly Burton",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Catalina Mejia",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ce Liu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Changhan Wang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Changkyu Kim",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Chao Zhou",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Chester Hu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ching-Hsiang Chu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Chris Cai",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Chris Tindal",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Christoph Feichtenhofer",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Cynthia Gao",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Damon Civin",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Dana Beaty",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Daniel Kreymer",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Daniel Li",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "David Adkins",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "David Xu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Davide Testuggine",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Delia David",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Devi Parikh",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Diana Liskovich",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Didem Foss",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Dingkang Wang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Duc Le",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Dustin Holland",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Edward Dowling",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Eissa Jamil",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Elaine Montgomery",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Eleonora Presani",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Emily Hahn",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Emily Wood",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Eric-Tuan Le",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Erik Brinkman",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Esteban Arcaute",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Evan Dunbar",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Evan Smothers",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Fei Sun",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Felix Kreuk",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Feng Tian",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Filippos Kokkinos",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Firat Ozgenel",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Francesco Caggioni",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Frank Kanayet",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Frank Seide",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Gabriela Medina Florez",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Gabriella Schwarz",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Gada Badeer",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Georgia Swee",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Gil Halpern",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Grant Herman",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Grigory Sizov",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Guangyi",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Zhang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Guna Lakshminarayanan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Hakan Inan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Hamid Shojanazeri",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Han Zou",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Hannah Wang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Hanwen Zha",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Haroun Habeeb",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Harrison Rudolph",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Helen Suk",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Henry Aspegren",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Hunter Goldman",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Hongyuan Zhan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ibrahim Damlaj",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Igor Molybog",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Igor Tufanov",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ilias Leontiadis",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Irina-Elena Veliche",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Itai Gat",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jake Weissman",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "James Geboski",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "James Kohli",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Janice Lam",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Japhet Asher",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jean-Baptiste Gaya",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jeff Marcus",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jeff Tang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jennifer Chan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jenny Zhen",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jeremy Reizenstein",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jeremy Teboul",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jessica Zhong",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jian Jin",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jingyi Yang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Joe Cummings",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jon Carvill",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jon Shepard",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jonathan McPhie",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Jonathan Torres",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Josh Ginsburg",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Junjie Wang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kai Wu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kam Hou U",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Karan Saxena",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kartikay Khandelwal",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Katayoun Zand",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kathy Matosich",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kaushik Veeraraghavan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kelly Michelena",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Keqian Li",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kiran Jagadeesh",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kun Huang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kunal Chawla",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Kyle Huang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Lailin Chen",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Lakshya Garg",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Lavender A",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Leandro Silva",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Lee Bell",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Lei Zhang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Liangpeng Guo",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Licheng Yu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Liron Moshkovich",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Luca Wehrstedt",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Madian Khabsa",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Manav Avalani",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Manish Bhatt",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Martynas Mankus",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Matan Hasson",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Matthew Lennie",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Matthias Reso",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Maxim Groshev",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Maxim Naumov",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Maya Lathi",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Meghan Keneally",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Miao Liu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Michael L. Seltzer",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Michal Valko",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Michelle Restrepo",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Mihir Patel",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Mik Vyatskov",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Mikayel Samvelyan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Mike Clark",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Mike Macey",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Mike Wang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Miquel Jubert Hermoso",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Mo Metanat",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Mohammad Rastegari",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Munish Bansal",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Nandhini Santhanam",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Natascha Parks",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Natasha White",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Navyata Bawa",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Nayan Singhal",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Nick Egebo",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Nicolas Usunier",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Nikhil Mehta",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Nikolay Pavlovich Laptev",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ning Dong",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Norman Cheng",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Oleg Chernoguz",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Olivia Hart",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Omkar Salpekar",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ozlem Kalinli",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Parkin Kent",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Parth Parekh",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Paul Saab",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Pavan Balaji",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Pedro Rittner",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Philip Bontrager",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Pierre Roux",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Piotr Dollar",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Polina Zvyagina",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Prashant Ratanchandani",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Pritish Yuvraj",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Qian Liang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Rachad Alao",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Rachel Rodriguez",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Rafi Ayub",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Raghotham Murthy",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Raghu Nayani",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Rahul Mitra",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Rangaprabhu Parthasarathy",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Raymond Li",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Rebekkah Hogan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Robin Battey",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Rocky Wang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Russ Howes",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ruty Rinott",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sachin Mehta",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sachin Siby",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sai Jayesh Bondu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Samyak Datta",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sara Chugh",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sara Hunt",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sargun Dhillon",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sasha Sidorov",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Satadru Pan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Saurabh Mahajan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Saurabh Verma",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Seiji Yamamoto",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sharadh Ramaswamy",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Shaun Lindsay",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Shaun Lindsay",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sheng Feng",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Shenghao Lin",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Shengxin Cindy Zha",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Shishir Patil",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Shiva Shankar",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Shuqiang Zhang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Shuqiang Zhang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sinong Wang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sneha Agarwal",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Soji Sajuyigbe",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Soumith Chintala",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Stephanie Max",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Stephen Chen",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Steve Kehoe",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Steve Satterfield",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sudarshan Govindaprasad",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sumit Gupta",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Summer Deng",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sungmin Cho",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sunny Virk",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Suraj Subramanian",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sy Choudhury",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Sydney Goldman",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Tal Remez",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Tamar Glaser",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Tamara Best",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Thilo Koehler",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Thomas Robinson",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Tianhe Li",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Tianjun Zhang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Tim Matthews",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Timothy Chou",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Tzook Shaked",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Varun Vontimitta",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Victoria Ajayi",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Victoria Montanez",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Vijai Mohan",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Vinay Satish Kumar",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Vishal Mangla",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Vlad Ionescu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Vlad Poenaru",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Vlad Tiberiu Mihailescu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Vladimir Ivanov",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Wei Li",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Wenchen Wang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Wenwen Jiang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Wes Bouaziz",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Will Constable",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Xiaocheng Tang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Xiaojian Wu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Xiaolan Wang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Xilun Wu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Xinbo Gao",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yaniv Kleinman",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yanjun Chen",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ye Hu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ye Jia",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ye Qi",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yenda Li",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yilin Zhang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Ying Zhang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yossi Adi",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Youngjin Nam",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yu",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Wang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yu Zhao",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yuchen Hao",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yundi Qian",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yunlu Li",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Yuzi He",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Zach Rait",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Zachary DeVito",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Zef Rosnbrick",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Zhaoduo Wen",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Zhenyu Yang",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Zhiwei Zhao",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Zhiyu Ma",
        "relation": "author_of",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Tianyi Li",
        "relation": "author_of",
        "tail": "A Survey on Diffusion Language Models"
      },
      {
        "head": "Mingda Chen",
        "relation": "author_of",
        "tail": "A Survey on Diffusion Language Models"
      },
      {
        "head": "Bowei Guo",
        "relation": "author_of",
        "tail": "A Survey on Diffusion Language Models"
      },
      {
        "head": "Zhiqiang Shen",
        "relation": "author_of",
        "tail": "A Survey on Diffusion Language Models"
      },
      {
        "head": "Jinjie Ni",
        "relation": "author_of",
        "tail": "Training Optimal Large Diffusion Language Models"
      },
      {
        "head": "Qian Liu",
        "relation": "author_of",
        "tail": "Training Optimal Large Diffusion Language Models"
      },
      {
        "head": "Chao Du",
        "relation": "author_of",
        "tail": "Training Optimal Large Diffusion Language Models"
      },
      {
        "head": "Longxu Dou",
        "relation": "author_of",
        "tail": "Training Optimal Large Diffusion Language Models"
      },
      {
        "head": "Hang Yan",
        "relation": "author_of",
        "tail": "Training Optimal Large Diffusion Language Models"
      },
      {
        "head": "Zili Wang",
        "relation": "author_of",
        "tail": "Training Optimal Large Diffusion Language Models"
      },
      {
        "head": "Tianyu Pang",
        "relation": "author_of",
        "tail": "Training Optimal Large Diffusion Language Models"
      },
      {
        "head": "Michael Qizhe Shieh",
        "relation": "author_of",
        "tail": "Training Optimal Large Diffusion Language Models"
      },
      {
        "head": "Siyan Zhao",
        "relation": "author_of",
        "tail": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models"
      },
      {
        "head": "Mengchen Liu",
        "relation": "author_of",
        "tail": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models"
      },
      {
        "head": "Jing Huang",
        "relation": "author_of",
        "tail": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models"
      },
      {
        "head": "Miao Liu",
        "relation": "author_of",
        "tail": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models"
      },
      {
        "head": "Chenyu Wang",
        "relation": "author_of",
        "tail": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models"
      },
      {
        "head": "Bo Liu",
        "relation": "author_of",
        "tail": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models"
      },
      {
        "head": "Yuandong Tian",
        "relation": "author_of",
        "tail": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models"
      },
      {
        "head": "Guan Pang",
        "relation": "author_of",
        "tail": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models"
      },
      {
        "head": "Sean Bell",
        "relation": "author_of",
        "tail": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models"
      },
      {
        "head": "Aditya Grover",
        "relation": "author_of",
        "tail": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models"
      },
      {
        "head": "Feiyu Chen",
        "relation": "author_of",
        "tail": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models"
      },
      {
        "head": "Marianne Arriola",
        "relation": "author_of",
        "tail": "Encoder-Decoder Diffusion Language Models for Efficient Training and Inference"
      },
      {
        "head": "Yair Schiff",
        "relation": "author_of",
        "tail": "Encoder-Decoder Diffusion Language Models for Efficient Training and Inference"
      },
      {
        "head": "Hao Phung",
        "relation": "author_of",
        "tail": "Encoder-Decoder Diffusion Language Models for Efficient Training and Inference"
      },
      {
        "head": "Aaron Gokaslan",
        "relation": "author_of",
        "tail": "Encoder-Decoder Diffusion Language Models for Efficient Training and Inference"
      },
      {
        "head": "Volodymyr Kuleshov",
        "relation": "author_of",
        "tail": "Encoder-Decoder Diffusion Language Models for Efficient Training and Inference"
      },
      {
        "head": "Chenghao Fan",
        "relation": "author_of",
        "tail": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model"
      },
      {
        "head": "Wen Heng",
        "relation": "author_of",
        "tail": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model"
      },
      {
        "head": "Bo Li",
        "relation": "author_of",
        "tail": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model"
      },
      {
        "head": "Sichen Liu",
        "relation": "author_of",
        "tail": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model"
      },
      {
        "head": "Yuxuan Song",
        "relation": "author_of",
        "tail": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model"
      },
      {
        "head": "Jing Su",
        "relation": "author_of",
        "tail": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model"
      },
      {
        "head": "Xiaoye Qu",
        "relation": "author_of",
        "tail": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model"
      },
      {
        "head": "Kai Shen",
        "relation": "author_of",
        "tail": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model"
      },
      {
        "head": "Wei Wei",
        "relation": "author_of",
        "tail": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model"
      },
      {
        "head": "Zhilin Yang",
        "relation": "author_of",
        "tail": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
      },
      {
        "head": "Zihang Dai",
        "relation": "author_of",
        "tail": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
      },
      {
        "head": "Yiming Yang",
        "relation": "author_of",
        "tail": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
      },
      {
        "head": "Jaime Carbonell",
        "relation": "author_of",
        "tail": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
      },
      {
        "head": "Ruslan Salakhutdinov",
        "relation": "author_of",
        "tail": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
      },
      {
        "head": "Quoc V. Le",
        "relation": "author_of",
        "tail": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
      },
      {
        "head": "N. Cambridge",
        "relation": "author_of",
        "tail": "Paper"
      },
      {
        "head": "Jinze Bai",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Shuai Bai",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Yunfei Chu",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Zeyu Cui",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Kai Dang",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Xiaodong Deng",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Yang Fan",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Wenbin Ge",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Yu Han",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Fei Huang",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Binyuan Hui",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Luo Ji",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Mei Li",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Junyang Lin",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Runji Lin",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Dayiheng Liu",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Gao Liu",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Chengqiang Lu",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Keming Lu",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Jianxin Ma",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Rui Men",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Xingzhang Ren",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Xuancheng Ren",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Chuanqi Tan",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Sinan Tan",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Jianhong Tu",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Peng Wang",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Shijie Wang",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Wei Wang",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Shengguang Wu",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Benfeng Xu",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Jin Xu",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "An Yang",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Hao Yang",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Jian Yang",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Shusheng Yang",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Yang Yao",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Bowen Yu",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Hongyi Yuan",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Zheng Yuan",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Jianwei Zhang",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Xingxuan Zhang",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Yichang Zhang",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Zhenru Zhang",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Chang Zhou",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Jingren Zhou",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Xiaohuan Zhou",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Tianhang Zhu",
        "relation": "author_of",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "Baptiste Rozière",
        "relation": "author_of",
        "tail": "Code Llama: Open Foundation Models for Code"
      },
      {
        "head": "Jonas Gehring",
        "relation": "author_of",
        "tail": "Code Llama: Open Foundation Models for Code"
      },
      {
        "head": "Fabian Gloeckle",
        "relation": "author_of",
        "tail": "Code Llama: Open Foundation Models for Code"
      },
      {
        "head": "Sten Sootla",
        "relation": "author_of",
        "tail": "Code Llama: Open Foundation Models for Code"
      },
      {
        "head": "Itai Gat",
        "relation": "author_of",
        "tail": "Code Llama: Open Foundation Models for Code"
      },
      {
        "head": "Xiaoqing Ellen Tan",
        "relation": "author_of",
        "tail": "Code Llama: Open Foundation Models for Code"
      },
      {
        "head": "Yossi Adi",
        "relation": "author_of",
        "tail": "Code Llama: Open Foundation Models for Code"
      },
      {
        "head": "Jingyu Liu",
        "relation": "author_of",
        "tail": "Code Llama: Open Foundation Models for Code"
      },
      {
        "head": "Romain Sauvestre",
        "relation": "author_of",
        "tail": "Code Llama: Open Foundation Models for Code"
      },
      {
        "head": "Tal Remez",
        "relation": "author_of",
        "tail": "Code Llama: Open Foundation Models for Code"
      },
      {
        "head": "Jérémy Rapin",
        "relation": "author_of",
        "tail": "Code Llama: Open Foundation Models for Code"
      },
      {
        "head": "Artyom Kozhevnikov",
        "relation": "author_of",
        "tail": "Code Llama: Open Foundation Models for Code"
      },
      {
        "head": "Ivan Evtimov",
        "relation": "author_of",
        "tail": "Code Llama: Open Foundation Models for Code"
      },
      {
        "head": "Joanna Bitton",
        "relation": "author_of",
        "tail": "Code Llama: Open Foundation Models for Code"
      },
      {
        "head": "Manish Bhatt",
        "relation": "author_of",
        "tail": "Code Llama: Open Foundation Models for Code"
      },
      {
        "head": "Cristian Canton Ferrer",
        "relation": "author_of",
        "tail": "Code Llama: Open Foundation Models for Code"
      },
      {
        "head": "Aaron Grattafiori",
        "relation": "author_of",
        "tail": "Code Llama: Open Foundation Models for Code"
      },
      {
        "head": "Wenhan Xiong",
        "relation": "author_of",
        "tail": "Code Llama: Open Foundation Models for Code"
      },
      {
        "head": "Alexandre Défossez",
        "relation": "author_of",
        "tail": "Code Llama: Open Foundation Models for Code"
      },
      {
        "head": "Jade Copet",
        "relation": "author_of",
        "tail": "Code Llama: Open Foundation Models for Code"
      },
      {
        "head": "Faisal Azhar",
        "relation": "author_of",
        "tail": "Code Llama: Open Foundation Models for Code"
      },
      {
        "head": "Hugo Touvron",
        "relation": "author_of",
        "tail": "Code Llama: Open Foundation Models for Code"
      },
      {
        "head": "Louis Martin",
        "relation": "author_of",
        "tail": "Code Llama: Open Foundation Models for Code"
      },
      {
        "head": "Nicolas Usunier",
        "relation": "author_of",
        "tail": "Code Llama: Open Foundation Models for Code"
      },
      {
        "head": "Thomas Scialom",
        "relation": "author_of",
        "tail": "Code Llama: Open Foundation Models for Code"
      },
      {
        "head": "Gabriel Synnaeve",
        "relation": "author_of",
        "tail": "Code Llama: Open Foundation Models for Code"
      },
      {
        "head": "Yicun Yang",
        "relation": "author_of",
        "tail": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way"
      },
      {
        "head": "Cong Wang",
        "relation": "author_of",
        "tail": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way"
      },
      {
        "head": "Shaobo Wang",
        "relation": "author_of",
        "tail": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way"
      },
      {
        "head": "Zichen Wen",
        "relation": "author_of",
        "tail": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way"
      },
      {
        "head": "Biqing Qi",
        "relation": "author_of",
        "tail": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way"
      },
      {
        "head": "Hanlin Xu",
        "relation": "author_of",
        "tail": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way"
      },
      {
        "head": "Linfeng Zhang",
        "relation": "author_of",
        "tail": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way"
      },
      {
        "head": "Itai Gat",
        "relation": "author_of",
        "tail": "Set Block Decoding is a Language Model Inference Accelerator"
      },
      {
        "head": "Heli Ben-Hamu",
        "relation": "author_of",
        "tail": "Set Block Decoding is a Language Model Inference Accelerator"
      },
      {
        "head": "Marton Havasi",
        "relation": "author_of",
        "tail": "Set Block Decoding is a Language Model Inference Accelerator"
      },
      {
        "head": "Daniel Haziza",
        "relation": "author_of",
        "tail": "Set Block Decoding is a Language Model Inference Accelerator"
      },
      {
        "head": "Jeremy Reizenstein",
        "relation": "author_of",
        "tail": "Set Block Decoding is a Language Model Inference Accelerator"
      },
      {
        "head": "Gabriel Synnaeve",
        "relation": "author_of",
        "tail": "Set Block Decoding is a Language Model Inference Accelerator"
      },
      {
        "head": "David Lopez-Paz",
        "relation": "author_of",
        "tail": "Set Block Decoding is a Language Model Inference Accelerator"
      },
      {
        "head": "Brian Karrer",
        "relation": "author_of",
        "tail": "Set Block Decoding is a Language Model Inference Accelerator"
      },
      {
        "head": "Yaron Lipman",
        "relation": "author_of",
        "tail": "Set Block Decoding is a Language Model Inference Accelerator"
      },
      {
        "head": "John Nguyen",
        "relation": "author_of",
        "tail": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows"
      },
      {
        "head": "Marton Havasi",
        "relation": "author_of",
        "tail": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows"
      },
      {
        "head": "Tariq Berrada",
        "relation": "author_of",
        "tail": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows"
      },
      {
        "head": "Luke Zettlemoyer",
        "relation": "author_of",
        "tail": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows"
      },
      {
        "head": "Ricky T. Q. Chen",
        "relation": "author_of",
        "tail": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows"
      },
      {
        "head": "Niels Mündler",
        "relation": "author_of",
        "tail": "Constrained Decoding of Diffusion LLMs with Context-Free Grammars"
      },
      {
        "head": "Jasper Dekoninck",
        "relation": "author_of",
        "tail": "Constrained Decoding of Diffusion LLMs with Context-Free Grammars"
      },
      {
        "head": "Martin Vechev",
        "relation": "author_of",
        "tail": "Constrained Decoding of Diffusion LLMs with Context-Free Grammars"
      },
      {
        "head": "Julianna Piskorz",
        "relation": "author_of",
        "tail": "Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models"
      },
      {
        "head": "Cristina Pinneri",
        "relation": "author_of",
        "tail": "Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models"
      },
      {
        "head": "Alvaro Correia",
        "relation": "author_of",
        "tail": "Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models"
      },
      {
        "head": "Motasem Alfarra",
        "relation": "author_of",
        "tail": "Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models"
      },
      {
        "head": "Risheek Garrepalli",
        "relation": "author_of",
        "tail": "Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models"
      },
      {
        "head": "Christos Louizos",
        "relation": "author_of",
        "tail": "Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models"
      },
      {
        "head": "Chang Yang",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Chuang Zhou",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Yilin Xiao",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Su Dong",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Luyao Zhuang",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Yujing Zhang",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Zhu Wang",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Zijin Hong",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Zheng Yuan",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Zhishang Xiang",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Shengyuan Chen",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Huachi Zhou",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Qinggang Zhang",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Ninghao Liu",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Jinsong Su",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Xinrun Wang",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Yi Chang",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Xiao Huang",
        "relation": "author_of",
        "tail": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications"
      },
      {
        "head": "Hao Lu",
        "relation": "author_of",
        "tail": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search"
      },
      {
        "head": "Haoyuan Huang",
        "relation": "author_of",
        "tail": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search"
      },
      {
        "head": "Yulin Zhou",
        "relation": "author_of",
        "tail": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search"
      },
      {
        "head": "Chen Li",
        "relation": "author_of",
        "tail": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search"
      },
      {
        "head": "Ningxin Zhu",
        "relation": "author_of",
        "tail": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search"
      },
      {
        "head": "Yu Cheng",
        "relation": "author_of",
        "tail": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking"
      },
      {
        "head": "Jiuan Zhou",
        "relation": "author_of",
        "tail": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking"
      },
      {
        "head": "Yongkang Hu",
        "relation": "author_of",
        "tail": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking"
      },
      {
        "head": "Yihang Chen",
        "relation": "author_of",
        "tail": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking"
      },
      {
        "head": "Huichi Zhou",
        "relation": "author_of",
        "tail": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking"
      },
      {
        "head": "Mingang Chen",
        "relation": "author_of",
        "tail": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking"
      },
      {
        "head": "Zhizhong Zhang",
        "relation": "author_of",
        "tail": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking"
      },
      {
        "head": "Kun Shao",
        "relation": "author_of",
        "tail": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking"
      },
      {
        "head": "Yuan Xie",
        "relation": "author_of",
        "tail": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking"
      },
      {
        "head": "Zhaoxia Yin",
        "relation": "author_of",
        "tail": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking"
      },
      {
        "head": "Qirui Mi",
        "relation": "author_of",
        "tail": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents"
      },
      {
        "head": "Zhijian Ma",
        "relation": "author_of",
        "tail": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents"
      },
      {
        "head": "Mengyue Yang",
        "relation": "author_of",
        "tail": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents"
      },
      {
        "head": "Haoxuan Li",
        "relation": "author_of",
        "tail": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents"
      },
      {
        "head": "Yisen Wang",
        "relation": "author_of",
        "tail": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents"
      },
      {
        "head": "Haifeng Zhang",
        "relation": "author_of",
        "tail": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents"
      },
      {
        "head": "Jun Wang",
        "relation": "author_of",
        "tail": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents"
      },
      {
        "head": "Mingyue Cheng",
        "relation": "author_of",
        "tail": "Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting"
      },
      {
        "head": "Xiaoyu Tao",
        "relation": "author_of",
        "tail": "Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting"
      },
      {
        "head": "Qi Liu",
        "relation": "author_of",
        "tail": "Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting"
      },
      {
        "head": "Ze Guo",
        "relation": "author_of",
        "tail": "Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting"
      },
      {
        "head": "Enhong Chen",
        "relation": "author_of",
        "tail": "Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting"
      },
      {
        "head": "Yue Ma",
        "relation": "author_of",
        "tail": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation"
      },
      {
        "head": "Zexuan Yan",
        "relation": "author_of",
        "tail": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation"
      },
      {
        "head": "Hongyu Liu",
        "relation": "author_of",
        "tail": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation"
      },
      {
        "head": "Hongfa Wang",
        "relation": "author_of",
        "tail": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation"
      },
      {
        "head": "Heng Pan",
        "relation": "author_of",
        "tail": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation"
      },
      {
        "head": "Yingqing He",
        "relation": "author_of",
        "tail": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation"
      },
      {
        "head": "Junkun Yuan",
        "relation": "author_of",
        "tail": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation"
      },
      {
        "head": "Ailing Zeng",
        "relation": "author_of",
        "tail": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation"
      },
      {
        "head": "Chengfei Cai",
        "relation": "author_of",
        "tail": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation"
      },
      {
        "head": "Heung-Yeung Shum",
        "relation": "author_of",
        "tail": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation"
      },
      {
        "head": "Zhifeng Li",
        "relation": "author_of",
        "tail": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation"
      },
      {
        "head": "Wei Liu",
        "relation": "author_of",
        "tail": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation"
      },
      {
        "head": "Linfeng Zhang",
        "relation": "author_of",
        "tail": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation"
      },
      {
        "head": "Qifeng Chen",
        "relation": "author_of",
        "tail": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation"
      },
      {
        "head": "Kunyu Feng",
        "relation": "author_of",
        "tail": "Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis"
      },
      {
        "head": "Yue Ma",
        "relation": "author_of",
        "tail": "Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis"
      },
      {
        "head": "Xinhua Zhang",
        "relation": "author_of",
        "tail": "Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis"
      },
      {
        "head": "Boshi Liu",
        "relation": "author_of",
        "tail": "Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis"
      },
      {
        "head": "Yikuang Yuluo",
        "relation": "author_of",
        "tail": "Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis"
      },
      {
        "head": "Yinhan Zhang",
        "relation": "author_of",
        "tail": "Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis"
      },
      {
        "head": "Runtao Liu",
        "relation": "author_of",
        "tail": "Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis"
      },
      {
        "head": "Hongyu Liu",
        "relation": "author_of",
        "tail": "Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis"
      },
      {
        "head": "Zhiyuan Qin",
        "relation": "author_of",
        "tail": "Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis"
      },
      {
        "head": "Shanhui Mo",
        "relation": "author_of",
        "tail": "Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis"
      },
      {
        "head": "Qifeng Chen",
        "relation": "author_of",
        "tail": "Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis"
      },
      {
        "head": "Zeyu Wang",
        "relation": "author_of",
        "tail": "Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis"
      },
      {
        "head": "Xinyao Liao",
        "relation": "author_of",
        "tail": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing"
      },
      {
        "head": "Xianfang Zeng",
        "relation": "author_of",
        "tail": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing"
      },
      {
        "head": "Ziye Song",
        "relation": "author_of",
        "tail": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing"
      },
      {
        "head": "Zhoujie Fu",
        "relation": "author_of",
        "tail": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing"
      },
      {
        "head": "Gang Yu",
        "relation": "author_of",
        "tail": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing"
      },
      {
        "head": "Guosheng Lin",
        "relation": "author_of",
        "tail": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing"
      },
      {
        "head": "Zeyu Zhu",
        "relation": "author_of",
        "tail": "Paper2Video: Automatic Video Generation from Scientific Papers"
      },
      {
        "head": "Kevin Qinghong Lin",
        "relation": "author_of",
        "tail": "Paper2Video: Automatic Video Generation from Scientific Papers"
      },
      {
        "head": "Mike Zheng Shou",
        "relation": "author_of",
        "tail": "Paper2Video: Automatic Video Generation from Scientific Papers"
      },
      {
        "head": "Yiyang Chen",
        "relation": "author_of",
        "tail": "ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment"
      },
      {
        "head": "Xuanhua He",
        "relation": "author_of",
        "tail": "ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment"
      },
      {
        "head": "Xiujun Ma",
        "relation": "author_of",
        "tail": "ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment"
      },
      {
        "head": "Yue Ma",
        "relation": "author_of",
        "tail": "ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment"
      },
      {
        "head": "DeepSeek-AI",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Daya Guo",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Dejian Yang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Haowei Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Junxiao Song",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Peiyi Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Qihao Zhu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Runxin Xu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Ruoyu Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Shirong Ma",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xiao Bi",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xiaokang Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xingkai Yu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yu Wu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Z. F. Wu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zhibin Gou",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zhihong Shao",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zhuoshu Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Ziyi Gao",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Aixin Liu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Bing Xue",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Bingxuan Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Bochao Wu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Bei Feng",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Chengda Lu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Chenggang Zhao",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Chengqi Deng",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Chenyu Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Chong Ruan",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Damai Dai",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Deli Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Dongjie Ji",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Erhang Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Fangyun Lin",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Fucong Dai",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Fuli Luo",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Guangbo Hao",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Guanting Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Guowei Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "H. Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Han Bao",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Hanwei Xu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Haocheng Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Honghui Ding",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Huajian Xin",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Huazuo Gao",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Hui Qu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Hui Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Jianzhong Guo",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Jiashi Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Jiawei Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Jingchang Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Jingyang Yuan",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Junjie Qiu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Junlong Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "J. L. Cai",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Jiaqi Ni",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Jian Liang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Jin Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Kai Dong",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Kai Hu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Kaige Gao",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Kang Guan",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Kexin Huang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Kuai Yu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Lean Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Lecong Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Liang Zhao",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Litong Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Liyue Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Lei Xu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Leyi Xia",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Mingchuan Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Minghua Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Minghui Tang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Meng Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Miaojun Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Mingming Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Ning Tian",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Panpan Huang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Peng Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Qiancheng Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Qinyu Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Qiushi Du",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Ruiqi Ge",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Ruisong Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Ruizhe Pan",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Runji Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "R. J. Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "R. L. Jin",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Ruyi Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Shanghao Lu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Shangyan Zhou",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Shanhuang Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Shengfeng Ye",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Shiyu Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Shuiping Yu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Shunfeng Zhou",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Shuting Pan",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "S. S. Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Shuang Zhou",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Shaoqing Wu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Shengfeng Ye",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Tao Yun",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Tian Pei",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Tianyu Sun",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "T. Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Wangding Zeng",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Wanjia Zhao",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Wen Liu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Wenfeng Liang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Wenjun Gao",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Wenqin Yu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Wentao Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "W. L. Xiao",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Wei An",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xiaodong Liu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xiaohan Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xiaokang Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xiaotao Nie",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xin Cheng",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xin Liu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xin Xie",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xingchao Liu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xinyu Yang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xinyuan Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xuecheng Su",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xuheng Lin",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "X. Q. Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xiangyue Jin",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xiaojin Shen",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xiaosha Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xiaowen Sun",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xiaoxiang Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xinnan Song",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xinyi Zhou",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xianzu Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Xinxia Shan",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Y. K. Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Y. Q. Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Y. X. Wei",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yang Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yanhong Xu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yao Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yao Zhao",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yaofeng Sun",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yaohui Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yi Yu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yichao Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yifan Shi",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yiliang Xiong",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Ying He",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yishi Piao",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yisong Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yixuan Tan",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yiyang Ma",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yiyuan Liu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yongqiang Guo",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yuan Ou",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yuduan Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yue Gong",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yuheng Zou",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yujia He",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yunfan Xiong",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yuxiang Luo",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yuxiang You",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yuxuan Liu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yuyang Zhou",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Y. X. Zhu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yanhong Xu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yanping Huang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yaohui Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yi Zheng",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yuchen Zhu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yunxian Ma",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Ying Tang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yukun Zha",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Yuting Yan",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Z. Z. Ren",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zehui Ren",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zhangli Sha",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zhe Fu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zhean Xu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zhenda Xie",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zhengyan Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zhewen Hao",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zhicheng Ma",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zhigang Yan",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zhiyu Wu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zihui Gu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zijia Zhu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zijun Liu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zilin Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Ziwei Xie",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Ziyang Song",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zizheng Pan",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zhen Huang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zhipeng Xu",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zhongyu Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Zhen Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Marah Abdin",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Jyoti Aneja",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Hany Awadalla",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Ahmed Awadallah",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Ammar Ahmad Awan",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Nguyen Bach",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Amit Bahree",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Arash Bakhtiari",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Jianmin Bao",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Harkirat Behl",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Alon Benhaim",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Misha Bilenko",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Johan Bjorck",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Sébastien Bubeck",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Martin Cai",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Qin Cai",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Vishrav Chaudhary",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Dong Chen",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Dongdong Chen",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Weizhu Chen",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Yen-Chun Chen",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Yi-Ling Chen",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Hao Cheng",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Parul Chopra",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Xiyang Dai",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Matthew Dixon",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Ronen Eldan",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Victor Fragoso",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Jianfeng Gao",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Mei Gao",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Min Gao",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Amit Garg",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Allie Del Giorno",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Abhishek Goswami",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Suriya Gunasekar",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Emman Haider",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Junheng Hao",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Russell J. Hewett",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Wenxiang Hu",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Jamie Huynh",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Dan Iter",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Sam Ade Jacobs",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Mojan Javaheripi",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Xin Jin",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Nikos Karampatziakis",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Piero Kauffmann",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Mahoud Khademi",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Dongwoo Kim",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Young Jin Kim",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Lev Kurilenko",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "James R. Lee",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Yin Tat Lee",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Yuanzhi Li",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Yunsheng Li",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Chen Liang",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Lars Liden",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Xihui Lin",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Zeqi Lin",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Ce Liu",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Liyuan Liu",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Mengchen Liu",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Weishung Liu",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Xiaodong Liu",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Chong Luo",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Piyush Madan",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Ali Mahmoudzadeh",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "David Majercak",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Matt Mazzola",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Caio César Teodoro Mendes",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Arindam Mitra",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Hardik Modi",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Anh Nguyen",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Brandon Norick",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Barun Patra",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Daniel Perez-Becker",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Thomas Portet",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Reid Pryzant",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Heyang Qin",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Marko Radmilac",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Liliang Ren",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Gustavo de Rosa",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Corby Rosset",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Sambudha Roy",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Olatunji Ruwase",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Olli Saarikivi",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Amin Saied",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Adil Salim",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Michael Santacroce",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Shital Shah",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Ning Shang",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Hiteshi Sharma",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Yelong Shen",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Swadheen Shukla",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Xia Song",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Masahiro Tanaka",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Andrea Tupini",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Praneetha Vaddamanu",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Chunyu Wang",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Guanhua Wang",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Lijuan Wang",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Shuohang Wang",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Xin Wang",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Yu Wang",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Rachel Ward",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Wen Wen",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Philipp Witte",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Haiping Wu",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Xiaoxia Wu",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Michael Wyatt",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Bin Xiao",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Can Xu",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Jiahang Xu",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Weijian Xu",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Jilong Xue",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Sonali Yadav",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Fan Yang",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Jianwei Yang",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Yifan Yang",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Ziyi Yang",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Donghan Yu",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Lu Yuan",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Chenruidong Zhang",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Cyril Zhang",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Jianwen Zhang",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Li Lyna Zhang",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Yi Zhang",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Yue Zhang",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Yunan Zhang",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Xiren Zhou",
        "relation": "author_of",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Bo Li",
        "relation": "author_of",
        "tail": "LLaVA-OneVision: Easy Visual Task Transfer"
      },
      {
        "head": "Yuanhan Zhang",
        "relation": "author_of",
        "tail": "LLaVA-OneVision: Easy Visual Task Transfer"
      },
      {
        "head": "Dong Guo",
        "relation": "author_of",
        "tail": "LLaVA-OneVision: Easy Visual Task Transfer"
      },
      {
        "head": "Renrui Zhang",
        "relation": "author_of",
        "tail": "LLaVA-OneVision: Easy Visual Task Transfer"
      },
      {
        "head": "Feng Li",
        "relation": "author_of",
        "tail": "LLaVA-OneVision: Easy Visual Task Transfer"
      },
      {
        "head": "Hao Zhang",
        "relation": "author_of",
        "tail": "LLaVA-OneVision: Easy Visual Task Transfer"
      },
      {
        "head": "Kaichen Zhang",
        "relation": "author_of",
        "tail": "LLaVA-OneVision: Easy Visual Task Transfer"
      },
      {
        "head": "Peiyuan Zhang",
        "relation": "author_of",
        "tail": "LLaVA-OneVision: Easy Visual Task Transfer"
      },
      {
        "head": "Yanwei Li",
        "relation": "author_of",
        "tail": "LLaVA-OneVision: Easy Visual Task Transfer"
      },
      {
        "head": "Ziwei Liu",
        "relation": "author_of",
        "tail": "LLaVA-OneVision: Easy Visual Task Transfer"
      },
      {
        "head": "Chunyuan Li",
        "relation": "author_of",
        "tail": "LLaVA-OneVision: Easy Visual Task Transfer"
      },
      {
        "head": "Weichen Liu",
        "relation": "author_of",
        "tail": "Spatial Reasoning in Multimodal Large Language Models: A Survey of Tasks, Benchmarks and Methods"
      },
      {
        "head": "Qiyao Xue",
        "relation": "author_of",
        "tail": "Spatial Reasoning in Multimodal Large Language Models: A Survey of Tasks, Benchmarks and Methods"
      },
      {
        "head": "Haoming Wang",
        "relation": "author_of",
        "tail": "Spatial Reasoning in Multimodal Large Language Models: A Survey of Tasks, Benchmarks and Methods"
      },
      {
        "head": "Xiangyu Yin",
        "relation": "author_of",
        "tail": "Spatial Reasoning in Multimodal Large Language Models: A Survey of Tasks, Benchmarks and Methods"
      },
      {
        "head": "Boyuan Yang",
        "relation": "author_of",
        "tail": "Spatial Reasoning in Multimodal Large Language Models: A Survey of Tasks, Benchmarks and Methods"
      },
      {
        "head": "Wei Gao",
        "relation": "author_of",
        "tail": "Spatial Reasoning in Multimodal Large Language Models: A Survey of Tasks, Benchmarks and Methods"
      },
      {
        "head": "Yuxi Xiao",
        "relation": "author_of",
        "tail": "SpatialTree: How Spatial Abilities Branch Out in MLLMs"
      },
      {
        "head": "Longfei Li",
        "relation": "author_of",
        "tail": "SpatialTree: How Spatial Abilities Branch Out in MLLMs"
      },
      {
        "head": "Shen Yan",
        "relation": "author_of",
        "tail": "SpatialTree: How Spatial Abilities Branch Out in MLLMs"
      },
      {
        "head": "Xinhang Liu",
        "relation": "author_of",
        "tail": "SpatialTree: How Spatial Abilities Branch Out in MLLMs"
      },
      {
        "head": "Sida Peng",
        "relation": "author_of",
        "tail": "SpatialTree: How Spatial Abilities Branch Out in MLLMs"
      },
      {
        "head": "Yunchao Wei",
        "relation": "author_of",
        "tail": "SpatialTree: How Spatial Abilities Branch Out in MLLMs"
      },
      {
        "head": "Xiaowei Zhou",
        "relation": "author_of",
        "tail": "SpatialTree: How Spatial Abilities Branch Out in MLLMs"
      },
      {
        "head": "Bingyi Kang",
        "relation": "author_of",
        "tail": "SpatialTree: How Spatial Abilities Branch Out in MLLMs"
      },
      {
        "head": "Mingrui Wu",
        "relation": "author_of",
        "tail": "From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs"
      },
      {
        "head": "Zhaozhi Wang",
        "relation": "author_of",
        "tail": "From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs"
      },
      {
        "head": "Fangjinhua Wang",
        "relation": "author_of",
        "tail": "From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs"
      },
      {
        "head": "Jiaolong Yang",
        "relation": "author_of",
        "tail": "From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs"
      },
      {
        "head": "Marc Pollefeys",
        "relation": "author_of",
        "tail": "From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs"
      },
      {
        "head": "Tong Zhang",
        "relation": "author_of",
        "tail": "From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs"
      },
      {
        "head": "Meng Cao",
        "relation": "author_of",
        "tail": "SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery"
      },
      {
        "head": "Xingyu Li",
        "relation": "author_of",
        "tail": "SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery"
      },
      {
        "head": "Xue Liu",
        "relation": "author_of",
        "tail": "SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery"
      },
      {
        "head": "Ian Reid",
        "relation": "author_of",
        "tail": "SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery"
      },
      {
        "head": "Xiaodan Liang",
        "relation": "author_of",
        "tail": "SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery"
      },
      {
        "head": "Joseph Redmon",
        "relation": "author_of",
        "tail": "You Only Look Once: Unified, Real-Time Object Detection"
      },
      {
        "head": "Santosh Divvala",
        "relation": "author_of",
        "tail": "You Only Look Once: Unified, Real-Time Object Detection"
      },
      {
        "head": "Ross Girshick",
        "relation": "author_of",
        "tail": "You Only Look Once: Unified, Real-Time Object Detection"
      },
      {
        "head": "Ali Farhadi",
        "relation": "author_of",
        "tail": "You Only Look Once: Unified, Real-Time Object Detection"
      },
      {
        "head": "Zhichao Sun",
        "relation": "author_of",
        "tail": "RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes"
      },
      {
        "head": "Yepeng Liu",
        "relation": "author_of",
        "tail": "RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes"
      },
      {
        "head": "Zhiling Su",
        "relation": "author_of",
        "tail": "RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes"
      },
      {
        "head": "Huachao Zhu",
        "relation": "author_of",
        "tail": "RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes"
      },
      {
        "head": "Yuliang Gu",
        "relation": "author_of",
        "tail": "RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes"
      },
      {
        "head": "Yuda Zou",
        "relation": "author_of",
        "tail": "RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes"
      },
      {
        "head": "Zelong Liu",
        "relation": "author_of",
        "tail": "RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes"
      },
      {
        "head": "Gui-Song Xia",
        "relation": "author_of",
        "tail": "RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes"
      },
      {
        "head": "Bo Du",
        "relation": "author_of",
        "tail": "RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes"
      },
      {
        "head": "Yongchao Xu",
        "relation": "author_of",
        "tail": "RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes"
      },
      {
        "head": "Shenghao Fu",
        "relation": "author_of",
        "tail": "WeDetect: Fast Open-Vocabulary Object Detection as Retrieval"
      },
      {
        "head": "Yukun Su",
        "relation": "author_of",
        "tail": "WeDetect: Fast Open-Vocabulary Object Detection as Retrieval"
      },
      {
        "head": "Fengyun Rao",
        "relation": "author_of",
        "tail": "WeDetect: Fast Open-Vocabulary Object Detection as Retrieval"
      },
      {
        "head": "Jing Lyu",
        "relation": "author_of",
        "tail": "WeDetect: Fast Open-Vocabulary Object Detection as Retrieval"
      },
      {
        "head": "Xiaohua Xie",
        "relation": "author_of",
        "tail": "WeDetect: Fast Open-Vocabulary Object Detection as Retrieval"
      },
      {
        "head": "Wei-Shi Zheng",
        "relation": "author_of",
        "tail": "WeDetect: Fast Open-Vocabulary Object Detection as Retrieval"
      },
      {
        "head": "Jiwan Chung",
        "relation": "author_of",
        "tail": "v1: Learning to Point Visual Tokens for Multimodal Grounded Reasoning"
      },
      {
        "head": "Junhyeok Kim",
        "relation": "author_of",
        "tail": "v1: Learning to Point Visual Tokens for Multimodal Grounded Reasoning"
      },
      {
        "head": "Siyeol Kim",
        "relation": "author_of",
        "tail": "v1: Learning to Point Visual Tokens for Multimodal Grounded Reasoning"
      },
      {
        "head": "Jaeyoung Lee",
        "relation": "author_of",
        "tail": "v1: Learning to Point Visual Tokens for Multimodal Grounded Reasoning"
      },
      {
        "head": "Min Soo Kim",
        "relation": "author_of",
        "tail": "v1: Learning to Point Visual Tokens for Multimodal Grounded Reasoning"
      },
      {
        "head": "Youngjae Yu",
        "relation": "author_of",
        "tail": "v1: Learning to Point Visual Tokens for Multimodal Grounded Reasoning"
      },
      {
        "head": "Jiazhe Wei",
        "relation": "author_of",
        "tail": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design"
      },
      {
        "head": "Ken Li",
        "relation": "author_of",
        "tail": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design"
      },
      {
        "head": "Tianyu Lao",
        "relation": "author_of",
        "tail": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design"
      },
      {
        "head": "Haofan Wang",
        "relation": "author_of",
        "tail": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design"
      },
      {
        "head": "Liang Wang",
        "relation": "author_of",
        "tail": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design"
      },
      {
        "head": "Caifeng Shan",
        "relation": "author_of",
        "tail": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design"
      },
      {
        "head": "Chenyang Si",
        "relation": "author_of",
        "tail": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design"
      },
      {
        "head": "Jianhua Han",
        "relation": "author_of",
        "tail": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving"
      },
      {
        "head": "Meng Tian",
        "relation": "author_of",
        "tail": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving"
      },
      {
        "head": "Jiangtong Zhu",
        "relation": "author_of",
        "tail": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving"
      },
      {
        "head": "Fan He",
        "relation": "author_of",
        "tail": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving"
      },
      {
        "head": "Huixin Zhang",
        "relation": "author_of",
        "tail": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving"
      },
      {
        "head": "Sitong Guo",
        "relation": "author_of",
        "tail": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving"
      },
      {
        "head": "Dechang Zhu",
        "relation": "author_of",
        "tail": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving"
      },
      {
        "head": "Hao Tang",
        "relation": "author_of",
        "tail": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving"
      },
      {
        "head": "Pei Xu",
        "relation": "author_of",
        "tail": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving"
      },
      {
        "head": "Yuze Guo",
        "relation": "author_of",
        "tail": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving"
      },
      {
        "head": "Minzhe Niu",
        "relation": "author_of",
        "tail": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving"
      },
      {
        "head": "Haojie Zhu",
        "relation": "author_of",
        "tail": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving"
      },
      {
        "head": "Qichao Dong",
        "relation": "author_of",
        "tail": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving"
      },
      {
        "head": "Xuechao Yan",
        "relation": "author_of",
        "tail": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving"
      },
      {
        "head": "Siyuan Dong",
        "relation": "author_of",
        "tail": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving"
      },
      {
        "head": "Lu Hou",
        "relation": "author_of",
        "tail": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving"
      },
      {
        "head": "Qingqiu Huang",
        "relation": "author_of",
        "tail": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving"
      },
      {
        "head": "Xiaosong Jia",
        "relation": "author_of",
        "tail": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving"
      },
      {
        "head": "Hang Xu",
        "relation": "author_of",
        "tail": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving"
      },
      {
        "head": "R. S. Sutton",
        "relation": "author_of",
        "tail": "Reinforcement Learning: An Introduction"
      },
      {
        "head": "A. Barto",
        "relation": "author_of",
        "tail": "Reinforcement Learning: An Introduction"
      },
      {
        "head": "Timothy P. Lillicrap",
        "relation": "author_of",
        "tail": "Continuous control with deep reinforcement learning"
      },
      {
        "head": "Jonathan J. Hunt",
        "relation": "author_of",
        "tail": "Continuous control with deep reinforcement learning"
      },
      {
        "head": "Alexander Pritzel",
        "relation": "author_of",
        "tail": "Continuous control with deep reinforcement learning"
      },
      {
        "head": "Nicolas Heess",
        "relation": "author_of",
        "tail": "Continuous control with deep reinforcement learning"
      },
      {
        "head": "Tom Erez",
        "relation": "author_of",
        "tail": "Continuous control with deep reinforcement learning"
      },
      {
        "head": "Yuval Tassa",
        "relation": "author_of",
        "tail": "Continuous control with deep reinforcement learning"
      },
      {
        "head": "David Silver",
        "relation": "author_of",
        "tail": "Continuous control with deep reinforcement learning"
      },
      {
        "head": "Daan Wierstra",
        "relation": "author_of",
        "tail": "Continuous control with deep reinforcement learning"
      },
      {
        "head": "Shenzhi Wang",
        "relation": "author_of",
        "tail": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning"
      },
      {
        "head": "Le Yu",
        "relation": "author_of",
        "tail": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning"
      },
      {
        "head": "Chang Gao",
        "relation": "author_of",
        "tail": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning"
      },
      {
        "head": "Chujie Zheng",
        "relation": "author_of",
        "tail": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning"
      },
      {
        "head": "Shixuan Liu",
        "relation": "author_of",
        "tail": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning"
      },
      {
        "head": "Rui Lu",
        "relation": "author_of",
        "tail": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning"
      },
      {
        "head": "Kai Dang",
        "relation": "author_of",
        "tail": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning"
      },
      {
        "head": "Xionghui Chen",
        "relation": "author_of",
        "tail": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning"
      },
      {
        "head": "Jianxin Yang",
        "relation": "author_of",
        "tail": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning"
      },
      {
        "head": "Zhenru Zhang",
        "relation": "author_of",
        "tail": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning"
      },
      {
        "head": "Yuqiong Liu",
        "relation": "author_of",
        "tail": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning"
      },
      {
        "head": "An Yang",
        "relation": "author_of",
        "tail": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning"
      },
      {
        "head": "Andrew Zhao",
        "relation": "author_of",
        "tail": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning"
      },
      {
        "head": "Yang Yue",
        "relation": "author_of",
        "tail": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning"
      },
      {
        "head": "Shiji Song",
        "relation": "author_of",
        "tail": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning"
      },
      {
        "head": "Bowen Yu",
        "relation": "author_of",
        "tail": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning"
      },
      {
        "head": "Gao Huang",
        "relation": "author_of",
        "tail": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning"
      },
      {
        "head": "Junyang Lin",
        "relation": "author_of",
        "tail": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning"
      },
      {
        "head": "Ganqu Cui",
        "relation": "author_of",
        "tail": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models"
      },
      {
        "head": "Yuchen Zhang",
        "relation": "author_of",
        "tail": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models"
      },
      {
        "head": "Jiacheng Chen",
        "relation": "author_of",
        "tail": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models"
      },
      {
        "head": "Lifan Yuan",
        "relation": "author_of",
        "tail": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models"
      },
      {
        "head": "Zhi Wang",
        "relation": "author_of",
        "tail": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models"
      },
      {
        "head": "Yuxin Zuo",
        "relation": "author_of",
        "tail": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models"
      },
      {
        "head": "Haozhan Li",
        "relation": "author_of",
        "tail": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models"
      },
      {
        "head": "Yuchen Fan",
        "relation": "author_of",
        "tail": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models"
      },
      {
        "head": "Huayu Chen",
        "relation": "author_of",
        "tail": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models"
      },
      {
        "head": "Weize Chen",
        "relation": "author_of",
        "tail": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models"
      },
      {
        "head": "Zhiyuan Liu",
        "relation": "author_of",
        "tail": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models"
      },
      {
        "head": "Hao Peng",
        "relation": "author_of",
        "tail": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models"
      },
      {
        "head": "Lei Bai",
        "relation": "author_of",
        "tail": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models"
      },
      {
        "head": "Wanli Ouyang",
        "relation": "author_of",
        "tail": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models"
      },
      {
        "head": "Yu Cheng",
        "relation": "author_of",
        "tail": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models"
      },
      {
        "head": "Bowen Zhou",
        "relation": "author_of",
        "tail": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models"
      },
      {
        "head": "Ning Ding",
        "relation": "author_of",
        "tail": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models"
      },
      {
        "head": "Jianhao Yan",
        "relation": "author_of",
        "tail": "Learning to Reason under Off-Policy Guidance"
      },
      {
        "head": "Yafu Li",
        "relation": "author_of",
        "tail": "Learning to Reason under Off-Policy Guidance"
      },
      {
        "head": "Zican Hu",
        "relation": "author_of",
        "tail": "Learning to Reason under Off-Policy Guidance"
      },
      {
        "head": "Zhi Wang",
        "relation": "author_of",
        "tail": "Learning to Reason under Off-Policy Guidance"
      },
      {
        "head": "Ganqu Cui",
        "relation": "author_of",
        "tail": "Learning to Reason under Off-Policy Guidance"
      },
      {
        "head": "Xiaoye Qu",
        "relation": "author_of",
        "tail": "Learning to Reason under Off-Policy Guidance"
      },
      {
        "head": "Yu Cheng",
        "relation": "author_of",
        "tail": "Learning to Reason under Off-Policy Guidance"
      },
      {
        "head": "Yue Zhang",
        "relation": "author_of",
        "tail": "Learning to Reason under Off-Policy Guidance"
      },
      {
        "head": "Komal Kumar",
        "relation": "author_of",
        "tail": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models"
      },
      {
        "head": "Tajamul Ashraf",
        "relation": "author_of",
        "tail": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models"
      },
      {
        "head": "Omkar Thawakar",
        "relation": "author_of",
        "tail": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models"
      },
      {
        "head": "Rao Muhammad Anwer",
        "relation": "author_of",
        "tail": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models"
      },
      {
        "head": "Hisham Cholakkal",
        "relation": "author_of",
        "tail": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models"
      },
      {
        "head": "Mubarak Shah",
        "relation": "author_of",
        "tail": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models"
      },
      {
        "head": "Ming-Hsuan Yang",
        "relation": "author_of",
        "tail": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models"
      },
      {
        "head": "Phillip H. S. Torr",
        "relation": "author_of",
        "tail": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models"
      },
      {
        "head": "Fahad Shahbaz Khan",
        "relation": "author_of",
        "tail": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models"
      },
      {
        "head": "Salman Khan",
        "relation": "author_of",
        "tail": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models"
      },
      {
        "head": "Nicolas Le Roux",
        "relation": "author_of",
        "tail": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement learning for LLMs"
      },
      {
        "head": "Marc G. Bellemare",
        "relation": "author_of",
        "tail": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement learning for LLMs"
      },
      {
        "head": "Jonathan Lebensold",
        "relation": "author_of",
        "tail": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement learning for LLMs"
      },
      {
        "head": "Arnaud Bergeron",
        "relation": "author_of",
        "tail": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement learning for LLMs"
      },
      {
        "head": "Joshua Greaves",
        "relation": "author_of",
        "tail": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement learning for LLMs"
      },
      {
        "head": "Alex Fréchette",
        "relation": "author_of",
        "tail": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement learning for LLMs"
      },
      {
        "head": "Carolyne Pelletier",
        "relation": "author_of",
        "tail": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement learning for LLMs"
      },
      {
        "head": "Eric Thibodeau-Laufer",
        "relation": "author_of",
        "tail": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement learning for LLMs"
      },
      {
        "head": "Sándor Toth",
        "relation": "author_of",
        "tail": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement learning for LLMs"
      },
      {
        "head": "Sam Work",
        "relation": "author_of",
        "tail": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement learning for LLMs"
      },
      {
        "head": "Guosheng Lin",
        "relation": "author_of",
        "tail": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation"
      },
      {
        "head": "Anton Milan",
        "relation": "author_of",
        "tail": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation"
      },
      {
        "head": "Chunhua Shen",
        "relation": "author_of",
        "tail": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation"
      },
      {
        "head": "Ian Reid",
        "relation": "author_of",
        "tail": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation"
      },
      {
        "head": "Evan Shelhamer",
        "relation": "author_of",
        "tail": "Fully convolutional networks for semantic segmentation"
      },
      {
        "head": "Jonathan Long",
        "relation": "author_of",
        "tail": "Fully convolutional networks for semantic segmentation"
      },
      {
        "head": "Trevor Darrell",
        "relation": "author_of",
        "tail": "Fully convolutional networks for semantic segmentation"
      },
      {
        "head": "Florinel-Alin Croitoru",
        "relation": "author_of",
        "tail": "Diffusion Models in Vision: A Survey"
      },
      {
        "head": "Vlad Hondru",
        "relation": "author_of",
        "tail": "Diffusion Models in Vision: A Survey"
      },
      {
        "head": "Radu Tudor Ionescu",
        "relation": "author_of",
        "tail": "Diffusion Models in Vision: A Survey"
      },
      {
        "head": "Mubarak Shah",
        "relation": "author_of",
        "tail": "Diffusion Models in Vision: A Survey"
      },
      {
        "head": "Meng-Hao Guo",
        "relation": "author_of",
        "tail": "SegNeXt: Rethinking Convolutional Attention Design for Semantic Segmentation"
      },
      {
        "head": "Cheng-Ze Lu",
        "relation": "author_of",
        "tail": "SegNeXt: Rethinking Convolutional Attention Design for Semantic Segmentation"
      },
      {
        "head": "Qibin Hou",
        "relation": "author_of",
        "tail": "SegNeXt: Rethinking Convolutional Attention Design for Semantic Segmentation"
      },
      {
        "head": "Zhengning Liu",
        "relation": "author_of",
        "tail": "SegNeXt: Rethinking Convolutional Attention Design for Semantic Segmentation"
      },
      {
        "head": "Ming-Ming Cheng",
        "relation": "author_of",
        "tail": "SegNeXt: Rethinking Convolutional Attention Design for Semantic Segmentation"
      },
      {
        "head": "Shi-Min Hu",
        "relation": "author_of",
        "tail": "SegNeXt: Rethinking Convolutional Attention Design for Semantic Segmentation"
      },
      {
        "head": "Jiaming Zhang",
        "relation": "author_of",
        "tail": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation With Transformers"
      },
      {
        "head": "Huayao Liu",
        "relation": "author_of",
        "tail": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation With Transformers"
      },
      {
        "head": "Kailun Yang",
        "relation": "author_of",
        "tail": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation With Transformers"
      },
      {
        "head": "Xinxin Hu",
        "relation": "author_of",
        "tail": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation With Transformers"
      },
      {
        "head": "Ruiping Liu",
        "relation": "author_of",
        "tail": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation With Transformers"
      },
      {
        "head": "Rainer Stiefelhagen",
        "relation": "author_of",
        "tail": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation With Transformers"
      },
      {
        "head": "Lei Ke",
        "relation": "author_of",
        "tail": "Segment Anything in High Quality"
      },
      {
        "head": "Mingqiao Ye",
        "relation": "author_of",
        "tail": "Segment Anything in High Quality"
      },
      {
        "head": "Martin Danelljan",
        "relation": "author_of",
        "tail": "Segment Anything in High Quality"
      },
      {
        "head": "Yifan Liu",
        "relation": "author_of",
        "tail": "Segment Anything in High Quality"
      },
      {
        "head": "Yu-Wing Tai",
        "relation": "author_of",
        "tail": "Segment Anything in High Quality"
      },
      {
        "head": "Chi-Keung Tang",
        "relation": "author_of",
        "tail": "Segment Anything in High Quality"
      },
      {
        "head": "Fisher Yu",
        "relation": "author_of",
        "tail": "Segment Anything in High Quality"
      },
      {
        "head": "Jiacong Xu",
        "relation": "author_of",
        "tail": "PIDNet: A Real-time Semantic Segmentation Network Inspired by PID Controllers"
      },
      {
        "head": "Zixiang Xiong",
        "relation": "author_of",
        "tail": "PIDNet: A Real-time Semantic Segmentation Network Inspired by PID Controllers"
      },
      {
        "head": "Shankar P. Bhattacharyya",
        "relation": "author_of",
        "tail": "PIDNet: A Real-time Semantic Segmentation Network Inspired by PID Controllers"
      },
      {
        "head": "An Yang",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Baosong Yang",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Binyuan Hui",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Bo Zheng",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Bowen Yu",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Chang Zhou",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Chengpeng Li",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Chengyuan Li",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Dayiheng Liu",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Fei Huang",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Guanting Dong",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Haoran Wei",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Huan Lin",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Jialong Tang",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Jialin Wang",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Jian Yang",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Jianhong Tu",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Jianwei Zhang",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Jianxin Ma",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Jianxin Yang",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Jin Xu",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Jingren Zhou",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Jinze Bai",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Jinzheng He",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Junyang Lin",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Kai Dang",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Keming Lu",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Keqin Chen",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Kexin Yang",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Mei Li",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Mingfeng Xue",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Na Ni",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Pei Zhang",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Peng Wang",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Ru Peng",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Rui Men",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Ruize Gao",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Runji Lin",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Shijie Wang",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Shuai Bai",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Sinan Tan",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Tianhang Zhu",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Tianhao Li",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Tianyu Liu",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Wenbin Ge",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Xiaodong Deng",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Xiaohuan Zhou",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Xingzhang Ren",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Xinyu Zhang",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Xipin Wei",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Xuancheng Ren",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Xuejing Liu",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Yang Fan",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Yang Yao",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Yichang Zhang",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Yu Wan",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Yunfei Chu",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Yuqiong Liu",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Zeyu Cui",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Zhenru Zhang",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Zhifang Guo",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Zhihao Fan",
        "relation": "author_of",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "Shuang Zeng",
        "relation": "author_of",
        "tail": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving"
      },
      {
        "head": "Xinyuan Chang",
        "relation": "author_of",
        "tail": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving"
      },
      {
        "head": "Mengwei Xie",
        "relation": "author_of",
        "tail": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving"
      },
      {
        "head": "Xinran Liu",
        "relation": "author_of",
        "tail": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving"
      },
      {
        "head": "Yifan Bai",
        "relation": "author_of",
        "tail": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving"
      },
      {
        "head": "Zheng Pan",
        "relation": "author_of",
        "tail": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving"
      },
      {
        "head": "Mu Xu",
        "relation": "author_of",
        "tail": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving"
      },
      {
        "head": "Xing Wei",
        "relation": "author_of",
        "tail": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving"
      },
      {
        "head": "Ning Guo",
        "relation": "author_of",
        "tail": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving"
      },
      {
        "head": "Zewei Zhou",
        "relation": "author_of",
        "tail": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning"
      },
      {
        "head": "Tianhui Cai",
        "relation": "author_of",
        "tail": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning"
      },
      {
        "head": "Seth Z. Zhao",
        "relation": "author_of",
        "tail": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning"
      },
      {
        "head": "Yun Zhang",
        "relation": "author_of",
        "tail": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning"
      },
      {
        "head": "Zhiyu Huang",
        "relation": "author_of",
        "tail": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning"
      },
      {
        "head": "Bolei Zhou",
        "relation": "author_of",
        "tail": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning"
      },
      {
        "head": "Jiaqi Ma",
        "relation": "author_of",
        "tail": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning"
      },
      {
        "head": "Haohan Chi",
        "relation": "author_of",
        "tail": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models"
      },
      {
        "head": "Huan-ang Gao",
        "relation": "author_of",
        "tail": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models"
      },
      {
        "head": "Ziming Liu",
        "relation": "author_of",
        "tail": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models"
      },
      {
        "head": "Jianing Liu",
        "relation": "author_of",
        "tail": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models"
      },
      {
        "head": "Chenyu Liu",
        "relation": "author_of",
        "tail": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models"
      },
      {
        "head": "Jinwei Li",
        "relation": "author_of",
        "tail": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models"
      },
      {
        "head": "Kaisen Yang",
        "relation": "author_of",
        "tail": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models"
      },
      {
        "head": "Yangcheng Yu",
        "relation": "author_of",
        "tail": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models"
      },
      {
        "head": "Zeda Wang",
        "relation": "author_of",
        "tail": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models"
      },
      {
        "head": "Wenyi Li",
        "relation": "author_of",
        "tail": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models"
      },
      {
        "head": "Leichen Wang",
        "relation": "author_of",
        "tail": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models"
      },
      {
        "head": "Xingtao Hu",
        "relation": "author_of",
        "tail": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models"
      },
      {
        "head": "Hao Sun",
        "relation": "author_of",
        "tail": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models"
      },
      {
        "head": "Hang Zhao",
        "relation": "author_of",
        "tail": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models"
      },
      {
        "head": "Hao Zhao",
        "relation": "author_of",
        "tail": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models"
      },
      {
        "head": "Kangan Qian",
        "relation": "author_of",
        "tail": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving"
      },
      {
        "head": "Sicong Jiang",
        "relation": "author_of",
        "tail": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving"
      },
      {
        "head": "Yang Zhong",
        "relation": "author_of",
        "tail": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving"
      },
      {
        "head": "Ziang Luo",
        "relation": "author_of",
        "tail": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving"
      },
      {
        "head": "Zilin Huang",
        "relation": "author_of",
        "tail": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving"
      },
      {
        "head": "Tianze Zhu",
        "relation": "author_of",
        "tail": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving"
      },
      {
        "head": "Kun Jiang",
        "relation": "author_of",
        "tail": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving"
      },
      {
        "head": "Mengmeng Yang",
        "relation": "author_of",
        "tail": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving"
      },
      {
        "head": "Zheng Fu",
        "relation": "author_of",
        "tail": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving"
      },
      {
        "head": "Jinyu Miao",
        "relation": "author_of",
        "tail": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving"
      },
      {
        "head": "Yining Shi",
        "relation": "author_of",
        "tail": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving"
      },
      {
        "head": "He Zhe Lim",
        "relation": "author_of",
        "tail": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving"
      },
      {
        "head": "Li Liu",
        "relation": "author_of",
        "tail": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving"
      },
      {
        "head": "Tianbao Zhou",
        "relation": "author_of",
        "tail": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving"
      },
      {
        "head": "Huang Yu",
        "relation": "author_of",
        "tail": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving"
      },
      {
        "head": "Yifei Hu",
        "relation": "author_of",
        "tail": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving"
      },
      {
        "head": "Guang Li",
        "relation": "author_of",
        "tail": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving"
      },
      {
        "head": "Guang Chen",
        "relation": "author_of",
        "tail": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving"
      },
      {
        "head": "Hao Ye",
        "relation": "author_of",
        "tail": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving"
      },
      {
        "head": "Lijun Sun",
        "relation": "author_of",
        "tail": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving"
      },
      {
        "head": "Diange Yang",
        "relation": "author_of",
        "tail": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving"
      },
      {
        "head": "Ronald J. Williams",
        "relation": "author_of",
        "tail": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
      },
      {
        "head": "Yue Li",
        "relation": "author_of",
        "tail": "Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning"
      },
      {
        "head": "Meng Tian",
        "relation": "author_of",
        "tail": "Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning"
      },
      {
        "head": "Dechang Zhu",
        "relation": "author_of",
        "tail": "Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning"
      },
      {
        "head": "Jiangtong Zhu",
        "relation": "author_of",
        "tail": "Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning"
      },
      {
        "head": "Zhenyu Lin",
        "relation": "author_of",
        "tail": "Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning"
      },
      {
        "head": "Zhiwei Xiong",
        "relation": "author_of",
        "tail": "Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning"
      },
      {
        "head": "Xinhai Zhao",
        "relation": "author_of",
        "tail": "Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning"
      },
      {
        "head": "Anqing Jiang",
        "relation": "author_of",
        "tail": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model"
      },
      {
        "head": "Yu Gao",
        "relation": "author_of",
        "tail": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model"
      },
      {
        "head": "Yiru Wang",
        "relation": "author_of",
        "tail": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model"
      },
      {
        "head": "Zhigang Sun",
        "relation": "author_of",
        "tail": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model"
      },
      {
        "head": "Shuo Wang",
        "relation": "author_of",
        "tail": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model"
      },
      {
        "head": "Yuwen Heng",
        "relation": "author_of",
        "tail": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model"
      },
      {
        "head": "Hao Sun",
        "relation": "author_of",
        "tail": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model"
      },
      {
        "head": "Shichen Tang",
        "relation": "author_of",
        "tail": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model"
      },
      {
        "head": "Lijuan Zhu",
        "relation": "author_of",
        "tail": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model"
      },
      {
        "head": "Jinhao Chai",
        "relation": "author_of",
        "tail": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model"
      },
      {
        "head": "Jijun Wang",
        "relation": "author_of",
        "tail": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model"
      },
      {
        "head": "Zichong Gu",
        "relation": "author_of",
        "tail": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model"
      },
      {
        "head": "Hao Jiang",
        "relation": "author_of",
        "tail": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model"
      },
      {
        "head": "Li Sun",
        "relation": "author_of",
        "tail": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model"
      },
      {
        "head": "Yingyan Li",
        "relation": "author_of",
        "tail": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving"
      },
      {
        "head": "Shuyao Shang",
        "relation": "author_of",
        "tail": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving"
      },
      {
        "head": "Weisong Liu",
        "relation": "author_of",
        "tail": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving"
      },
      {
        "head": "Bing Zhan",
        "relation": "author_of",
        "tail": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving"
      },
      {
        "head": "Haochen Wang",
        "relation": "author_of",
        "tail": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving"
      },
      {
        "head": "Yuqi Wang",
        "relation": "author_of",
        "tail": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving"
      },
      {
        "head": "Yuntao Chen",
        "relation": "author_of",
        "tail": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving"
      },
      {
        "head": "Xiaoman Wang",
        "relation": "author_of",
        "tail": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving"
      },
      {
        "head": "Yasong An",
        "relation": "author_of",
        "tail": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving"
      },
      {
        "head": "Chufeng Tang",
        "relation": "author_of",
        "tail": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving"
      },
      {
        "head": "Lu Hou",
        "relation": "author_of",
        "tail": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving"
      },
      {
        "head": "Lue Fan",
        "relation": "author_of",
        "tail": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving"
      },
      {
        "head": "Zhaoxiang Zhang",
        "relation": "author_of",
        "tail": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving"
      },
      {
        "head": "Bernhard Kerbl",
        "relation": "author_of",
        "tail": "3D Gaussian Splatting for Real-Time Radiance Field Rendering"
      },
      {
        "head": "Georgios Kopanas",
        "relation": "author_of",
        "tail": "3D Gaussian Splatting for Real-Time Radiance Field Rendering"
      },
      {
        "head": "Thomas Leimkühler",
        "relation": "author_of",
        "tail": "3D Gaussian Splatting for Real-Time Radiance Field Rendering"
      },
      {
        "head": "George Drettakis",
        "relation": "author_of",
        "tail": "3D Gaussian Splatting for Real-Time Radiance Field Rendering"
      },
      {
        "head": "Johannes L. Schönberger",
        "relation": "author_of",
        "tail": "Structure-from-Motion Revisited"
      },
      {
        "head": "Jan-Michael Frahm",
        "relation": "author_of",
        "tail": "Structure-from-Motion Revisited"
      },
      {
        "head": "Zhenxin Li",
        "relation": "author_of",
        "tail": "Generalized Trajectory Scoring for End-to-end Multimodal Planning"
      },
      {
        "head": "Wenhao Yao",
        "relation": "author_of",
        "tail": "Generalized Trajectory Scoring for End-to-end Multimodal Planning"
      },
      {
        "head": "Zi Wang",
        "relation": "author_of",
        "tail": "Generalized Trajectory Scoring for End-to-end Multimodal Planning"
      },
      {
        "head": "Xinglong Sun",
        "relation": "author_of",
        "tail": "Generalized Trajectory Scoring for End-to-end Multimodal Planning"
      },
      {
        "head": "Joshua Chen",
        "relation": "author_of",
        "tail": "Generalized Trajectory Scoring for End-to-end Multimodal Planning"
      },
      {
        "head": "Nadine Chang",
        "relation": "author_of",
        "tail": "Generalized Trajectory Scoring for End-to-end Multimodal Planning"
      },
      {
        "head": "Maying Shen",
        "relation": "author_of",
        "tail": "Generalized Trajectory Scoring for End-to-end Multimodal Planning"
      },
      {
        "head": "Zuxuan Wu",
        "relation": "author_of",
        "tail": "Generalized Trajectory Scoring for End-to-end Multimodal Planning"
      },
      {
        "head": "Shiyi Lan",
        "relation": "author_of",
        "tail": "Generalized Trajectory Scoring for End-to-end Multimodal Planning"
      },
      {
        "head": "Jose M. Alvarez",
        "relation": "author_of",
        "tail": "Generalized Trajectory Scoring for End-to-end Multimodal Planning"
      },
      {
        "head": "Lan Feng",
        "relation": "author_of",
        "tail": "RAP: 3D Rasterization Augmented End-to-End Planning"
      },
      {
        "head": "Yang Gao",
        "relation": "author_of",
        "tail": "RAP: 3D Rasterization Augmented End-to-End Planning"
      },
      {
        "head": "Eloi Zablocki",
        "relation": "author_of",
        "tail": "RAP: 3D Rasterization Augmented End-to-End Planning"
      },
      {
        "head": "Quanyi Li",
        "relation": "author_of",
        "tail": "RAP: 3D Rasterization Augmented End-to-End Planning"
      },
      {
        "head": "Wuyang Li",
        "relation": "author_of",
        "tail": "RAP: 3D Rasterization Augmented End-to-End Planning"
      },
      {
        "head": "Sichao Liu",
        "relation": "author_of",
        "tail": "RAP: 3D Rasterization Augmented End-to-End Planning"
      },
      {
        "head": "Matthieu Cord",
        "relation": "author_of",
        "tail": "RAP: 3D Rasterization Augmented End-to-End Planning"
      },
      {
        "head": "Alexandre Alahi",
        "relation": "author_of",
        "tail": "RAP: 3D Rasterization Augmented End-to-End Planning"
      },
      {
        "head": "Maciej K. Wozniak",
        "relation": "author_of",
        "tail": "PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving"
      },
      {
        "head": "Lianhang Liu",
        "relation": "author_of",
        "tail": "PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving"
      },
      {
        "head": "Yixi Cai",
        "relation": "author_of",
        "tail": "PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving"
      },
      {
        "head": "Patric Jensfelt",
        "relation": "author_of",
        "tail": "PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving"
      },
      {
        "head": "Junnan Li",
        "relation": "author_of",
        "tail": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"
      },
      {
        "head": "Dongxu Li",
        "relation": "author_of",
        "tail": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"
      },
      {
        "head": "Silvio Savarese",
        "relation": "author_of",
        "tail": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"
      },
      {
        "head": "Steven Hoi",
        "relation": "author_of",
        "tail": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"
      },
      {
        "head": "Yaron Lipman",
        "relation": "author_of",
        "tail": "Flow Matching for Generative Modeling"
      },
      {
        "head": "Ricky T. Q. Chen",
        "relation": "author_of",
        "tail": "Flow Matching for Generative Modeling"
      },
      {
        "head": "Heli Ben-Hamu",
        "relation": "author_of",
        "tail": "Flow Matching for Generative Modeling"
      },
      {
        "head": "Maximilian Nickel",
        "relation": "author_of",
        "tail": "Flow Matching for Generative Modeling"
      },
      {
        "head": "Matt Le",
        "relation": "author_of",
        "tail": "Flow Matching for Generative Modeling"
      },
      {
        "head": "Cheng Chi",
        "relation": "author_of",
        "tail": "Diffusion policy: Visuomotor policy learning via action diffusion"
      },
      {
        "head": "Zhenjia Xu",
        "relation": "author_of",
        "tail": "Diffusion policy: Visuomotor policy learning via action diffusion"
      },
      {
        "head": "Siyuan Feng",
        "relation": "author_of",
        "tail": "Diffusion policy: Visuomotor policy learning via action diffusion"
      },
      {
        "head": "Eric Cousineau",
        "relation": "author_of",
        "tail": "Diffusion policy: Visuomotor policy learning via action diffusion"
      },
      {
        "head": "Yilun Du",
        "relation": "author_of",
        "tail": "Diffusion policy: Visuomotor policy learning via action diffusion"
      },
      {
        "head": "Benjamin Burchfiel",
        "relation": "author_of",
        "tail": "Diffusion policy: Visuomotor policy learning via action diffusion"
      },
      {
        "head": "Russ Tedrake",
        "relation": "author_of",
        "tail": "Diffusion policy: Visuomotor policy learning via action diffusion"
      },
      {
        "head": "Shuran Song",
        "relation": "author_of",
        "tail": "Diffusion policy: Visuomotor policy learning via action diffusion"
      },
      {
        "head": "Fanlong Zeng",
        "relation": "author_of",
        "tail": "Large Language Models for Robotics: A Survey"
      },
      {
        "head": "Wensheng Gan",
        "relation": "author_of",
        "tail": "Large Language Models for Robotics: A Survey"
      },
      {
        "head": "Zezheng Huai",
        "relation": "author_of",
        "tail": "Large Language Models for Robotics: A Survey"
      },
      {
        "head": "Lichao Sun",
        "relation": "author_of",
        "tail": "Large Language Models for Robotics: A Survey"
      },
      {
        "head": "Hechang Chen",
        "relation": "author_of",
        "tail": "Large Language Models for Robotics: A Survey"
      },
      {
        "head": "Yongheng Wang",
        "relation": "author_of",
        "tail": "Large Language Models for Robotics: A Survey"
      },
      {
        "head": "Ning Liu",
        "relation": "author_of",
        "tail": "Large Language Models for Robotics: A Survey"
      },
      {
        "head": "Philip S. Yu",
        "relation": "author_of",
        "tail": "Large Language Models for Robotics: A Survey"
      },
      {
        "head": "Zhenjie Yang",
        "relation": "author_of",
        "tail": "Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)"
      },
      {
        "head": "Xiaosong Jia",
        "relation": "author_of",
        "tail": "Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)"
      },
      {
        "head": "Qifeng Li",
        "relation": "author_of",
        "tail": "Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)"
      },
      {
        "head": "Xue Yang",
        "relation": "author_of",
        "tail": "Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)"
      },
      {
        "head": "Maoqing Yao",
        "relation": "author_of",
        "tail": "Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)"
      },
      {
        "head": "Junchi Yan",
        "relation": "author_of",
        "tail": "Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)"
      },
      {
        "head": "Dapeng Zhang",
        "relation": "author_of",
        "tail": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey"
      },
      {
        "head": "Jing Sun",
        "relation": "author_of",
        "tail": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey"
      },
      {
        "head": "Chenghui Hu",
        "relation": "author_of",
        "tail": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey"
      },
      {
        "head": "Xiaoyan Wu",
        "relation": "author_of",
        "tail": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey"
      },
      {
        "head": "Zhenlong Yuan",
        "relation": "author_of",
        "tail": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey"
      },
      {
        "head": "Rui Zhou",
        "relation": "author_of",
        "tail": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey"
      },
      {
        "head": "Fei Shen",
        "relation": "author_of",
        "tail": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey"
      },
      {
        "head": "Qingguo Zhou",
        "relation": "author_of",
        "tail": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey"
      },
      {
        "head": "Jiazhi Yang",
        "relation": "author_of",
        "tail": "ReSim: Reliable World Simulation for Autonomous Driving"
      },
      {
        "head": "Kashyap Chitta",
        "relation": "author_of",
        "tail": "ReSim: Reliable World Simulation for Autonomous Driving"
      },
      {
        "head": "Shenyuan Gao",
        "relation": "author_of",
        "tail": "ReSim: Reliable World Simulation for Autonomous Driving"
      },
      {
        "head": "Long Chen",
        "relation": "author_of",
        "tail": "ReSim: Reliable World Simulation for Autonomous Driving"
      },
      {
        "head": "Yuqian Shao",
        "relation": "author_of",
        "tail": "ReSim: Reliable World Simulation for Autonomous Driving"
      },
      {
        "head": "Xiaosong Jia",
        "relation": "author_of",
        "tail": "ReSim: Reliable World Simulation for Autonomous Driving"
      },
      {
        "head": "Hongyang Li",
        "relation": "author_of",
        "tail": "ReSim: Reliable World Simulation for Autonomous Driving"
      },
      {
        "head": "Andreas Geiger",
        "relation": "author_of",
        "tail": "ReSim: Reliable World Simulation for Autonomous Driving"
      },
      {
        "head": "Xiangyu Yue",
        "relation": "author_of",
        "tail": "ReSim: Reliable World Simulation for Autonomous Driving"
      },
      {
        "head": "Li Chen",
        "relation": "author_of",
        "tail": "ReSim: Reliable World Simulation for Autonomous Driving"
      },
      {
        "head": "Hugo Touvron",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Louis Martin",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Kevin Stone",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Peter Albert",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Amjad Almahairi",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Yasmine Babaei",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Nikolay Bashlykov",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Soumya Batra",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Prajjwal Bhargava",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Shruti Bhosale",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Dan Bikel",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Lukas Blecher",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Cristian Canton Ferrer",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Moya Chen",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Guillem Cucurull",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "David Esiobu",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Jude Fernandes",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Jeremy Fu",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Wenyin Fu",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Brian Fuller",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Cynthia Gao",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Vedanuj Goswami",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Naman Goyal",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Anthony Hartshorn",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Saghar Hosseini",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Rui Hou",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Hakan Inan",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Marcin Kardas",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Viktor Kerkez",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Madian Khabsa",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Isabel Kloumann",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Artem Korenev",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Punit Singh Koura",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Marie-Anne Lachaux",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Thibaut Lavril",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Jenya Lee",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Diana Liskovich",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Yinghai Lu",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Yuning Mao",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Xavier Martinet",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Todor Mihaylov",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Pushkar Mishra",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Igor Molybog",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Yixin Nie",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Andrew Poulton",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Jeremy Reizenstein",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Rashi Rungta",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Kalyan Saladi",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Alan Schelten",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Ruan Silva",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Eric Michael Smith",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Ranjan Subramanian",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Xiaoqing Ellen Tan",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Binh Tang",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Ross Taylor",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Adina Williams",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Jian Xiang Kuan",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Puxin Xu",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Zheng Yan",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Iliyan Zarov",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Yuchen Zhang",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Angela Fan",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Melanie Kambadur",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Sharan Narang",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Aurelien Rodriguez",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Robert Stojnic",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Sergey Edunov",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Thomas Scialom",
        "relation": "author_of",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "Weifan Guan",
        "relation": "author_of",
        "tail": "Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey"
      },
      {
        "head": "Qinghao Hu",
        "relation": "author_of",
        "tail": "Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey"
      },
      {
        "head": "Aosheng Li",
        "relation": "author_of",
        "tail": "Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey"
      },
      {
        "head": "Jian Cheng",
        "relation": "author_of",
        "tail": "Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey"
      },
      {
        "head": "Yuechen Luo",
        "relation": "author_of",
        "tail": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving"
      },
      {
        "head": "Fang Li",
        "relation": "author_of",
        "tail": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving"
      },
      {
        "head": "Shaoqing Xu",
        "relation": "author_of",
        "tail": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving"
      },
      {
        "head": "Zhiyi Lai",
        "relation": "author_of",
        "tail": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving"
      },
      {
        "head": "Lei Yang",
        "relation": "author_of",
        "tail": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving"
      },
      {
        "head": "Qimao Chen",
        "relation": "author_of",
        "tail": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving"
      },
      {
        "head": "Ziang Luo",
        "relation": "author_of",
        "tail": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving"
      },
      {
        "head": "Zixun Xie",
        "relation": "author_of",
        "tail": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving"
      },
      {
        "head": "Shengyin Jiang",
        "relation": "author_of",
        "tail": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving"
      },
      {
        "head": "Jiaxin Liu",
        "relation": "author_of",
        "tail": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving"
      },
      {
        "head": "Long Chen",
        "relation": "author_of",
        "tail": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving"
      },
      {
        "head": "Bing Wang",
        "relation": "author_of",
        "tail": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving"
      },
      {
        "head": "Zhi-xin Yang",
        "relation": "author_of",
        "tail": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving"
      },
      {
        "head": "Ruiyang Hao",
        "relation": "author_of",
        "tail": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition"
      },
      {
        "head": "Haibao Yu",
        "relation": "author_of",
        "tail": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition"
      },
      {
        "head": "Jiaru Zhong",
        "relation": "author_of",
        "tail": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition"
      },
      {
        "head": "Chuanye Wang",
        "relation": "author_of",
        "tail": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition"
      },
      {
        "head": "Jiahao Wang",
        "relation": "author_of",
        "tail": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition"
      },
      {
        "head": "Yiming Kan",
        "relation": "author_of",
        "tail": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition"
      },
      {
        "head": "Wenxian Yang",
        "relation": "author_of",
        "tail": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition"
      },
      {
        "head": "Siqi Fan",
        "relation": "author_of",
        "tail": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition"
      },
      {
        "head": "Huilin Yin",
        "relation": "author_of",
        "tail": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition"
      },
      {
        "head": "Jianing Qiu",
        "relation": "author_of",
        "tail": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition"
      },
      {
        "head": "Yao Mu",
        "relation": "author_of",
        "tail": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition"
      },
      {
        "head": "Jiankai Sun",
        "relation": "author_of",
        "tail": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition"
      },
      {
        "head": "Li Chen",
        "relation": "author_of",
        "tail": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition"
      },
      {
        "head": "Walter Zimmer",
        "relation": "author_of",
        "tail": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition"
      },
      {
        "head": "Dandan Zhang",
        "relation": "author_of",
        "tail": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition"
      },
      {
        "head": "Shanghang Zhang",
        "relation": "author_of",
        "tail": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition"
      },
      {
        "head": "Mac Schwager",
        "relation": "author_of",
        "tail": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition"
      },
      {
        "head": "Ping Luo",
        "relation": "author_of",
        "tail": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition"
      },
      {
        "head": "Zaiqing Nie",
        "relation": "author_of",
        "tail": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition"
      },
      {
        "head": "Yang Liu",
        "relation": "author_of",
        "tail": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI"
      },
      {
        "head": "Weixing Chen",
        "relation": "author_of",
        "tail": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI"
      },
      {
        "head": "Yongjie Bai",
        "relation": "author_of",
        "tail": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI"
      },
      {
        "head": "Xiaodan Liang",
        "relation": "author_of",
        "tail": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI"
      },
      {
        "head": "Guanbin Li",
        "relation": "author_of",
        "tail": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI"
      },
      {
        "head": "Wen Gao",
        "relation": "author_of",
        "tail": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI"
      },
      {
        "head": "Liang Lin",
        "relation": "author_of",
        "tail": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI"
      },
      {
        "head": "Yueen Ma",
        "relation": "author_of",
        "tail": "A Survey on Vision-Language-Action Models for Embodied AI"
      },
      {
        "head": "Zixing Song",
        "relation": "author_of",
        "tail": "A Survey on Vision-Language-Action Models for Embodied AI"
      },
      {
        "head": "Yuzheng Zhuang",
        "relation": "author_of",
        "tail": "A Survey on Vision-Language-Action Models for Embodied AI"
      },
      {
        "head": "Jianye Hao",
        "relation": "author_of",
        "tail": "A Survey on Vision-Language-Action Models for Embodied AI"
      },
      {
        "head": "Irwin King",
        "relation": "author_of",
        "tail": "A Survey on Vision-Language-Action Models for Embodied AI"
      },
      {
        "head": "Guosheng Zhao",
        "relation": "author_of",
        "tail": "DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation"
      },
      {
        "head": "Chaojun Ni",
        "relation": "author_of",
        "tail": "DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation"
      },
      {
        "head": "Xiaofeng Wang",
        "relation": "author_of",
        "tail": "DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation"
      },
      {
        "head": "Zheng Zhu",
        "relation": "author_of",
        "tail": "DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation"
      },
      {
        "head": "Xueyang Zhang",
        "relation": "author_of",
        "tail": "DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation"
      },
      {
        "head": "Yida Wang",
        "relation": "author_of",
        "tail": "DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation"
      },
      {
        "head": "Guan Huang",
        "relation": "author_of",
        "tail": "DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation"
      },
      {
        "head": "Xinze Chen",
        "relation": "author_of",
        "tail": "DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation"
      },
      {
        "head": "Boyuan Wang",
        "relation": "author_of",
        "tail": "DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation"
      },
      {
        "head": "Youyi Zhang",
        "relation": "author_of",
        "tail": "DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation"
      },
      {
        "head": "Wenjun Mei",
        "relation": "author_of",
        "tail": "DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation"
      },
      {
        "head": "Xingang Wang",
        "relation": "author_of",
        "tail": "DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation"
      },
      {
        "head": "Fanqing Meng",
        "relation": "author_of",
        "tail": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation"
      },
      {
        "head": "Jiaqi Liao",
        "relation": "author_of",
        "tail": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation"
      },
      {
        "head": "Xinyu Tan",
        "relation": "author_of",
        "tail": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation"
      },
      {
        "head": "Wenqi Shao",
        "relation": "author_of",
        "tail": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation"
      },
      {
        "head": "Quanfeng Lu",
        "relation": "author_of",
        "tail": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation"
      },
      {
        "head": "Kaipeng Zhang",
        "relation": "author_of",
        "tail": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation"
      },
      {
        "head": "Yu Cheng",
        "relation": "author_of",
        "tail": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation"
      },
      {
        "head": "Dianqi Li",
        "relation": "author_of",
        "tail": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation"
      },
      {
        "head": "Yu Qiao",
        "relation": "author_of",
        "tail": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation"
      },
      {
        "head": "Ping Luo",
        "relation": "author_of",
        "tail": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation"
      },
      {
        "head": "Yu Shang",
        "relation": "author_of",
        "tail": "AgentSquare: Automatic LLM Agent Search in Modular Design Space"
      },
      {
        "head": "Yu Li",
        "relation": "author_of",
        "tail": "AgentSquare: Automatic LLM Agent Search in Modular Design Space"
      },
      {
        "head": "Keyu Zhao",
        "relation": "author_of",
        "tail": "AgentSquare: Automatic LLM Agent Search in Modular Design Space"
      },
      {
        "head": "Likai Ma",
        "relation": "author_of",
        "tail": "AgentSquare: Automatic LLM Agent Search in Modular Design Space"
      },
      {
        "head": "Jiahe Liu",
        "relation": "author_of",
        "tail": "AgentSquare: Automatic LLM Agent Search in Modular Design Space"
      },
      {
        "head": "Fengli Xu",
        "relation": "author_of",
        "tail": "AgentSquare: Automatic LLM Agent Search in Modular Design Space"
      },
      {
        "head": "Yong Li",
        "relation": "author_of",
        "tail": "AgentSquare: Automatic LLM Agent Search in Modular Design Space"
      },
      {
        "head": "Jie Feng",
        "relation": "author_of",
        "tail": "CityGPT: Empowering Urban Spatial Cognition of Large Language Models"
      },
      {
        "head": "Tianhui Liu",
        "relation": "author_of",
        "tail": "CityGPT: Empowering Urban Spatial Cognition of Large Language Models"
      },
      {
        "head": "Yuwei Du",
        "relation": "author_of",
        "tail": "CityGPT: Empowering Urban Spatial Cognition of Large Language Models"
      },
      {
        "head": "Siqi Guo",
        "relation": "author_of",
        "tail": "CityGPT: Empowering Urban Spatial Cognition of Large Language Models"
      },
      {
        "head": "Yuming Lin",
        "relation": "author_of",
        "tail": "CityGPT: Empowering Urban Spatial Cognition of Large Language Models"
      },
      {
        "head": "Yong Li",
        "relation": "author_of",
        "tail": "CityGPT: Empowering Urban Spatial Cognition of Large Language Models"
      },
      {
        "head": "Kaiwen Zhang",
        "relation": "author_of",
        "tail": "Epona: Autoregressive Diffusion World Model for Autonomous Driving"
      },
      {
        "head": "Zhenyu Tang",
        "relation": "author_of",
        "tail": "Epona: Autoregressive Diffusion World Model for Autonomous Driving"
      },
      {
        "head": "Xiaotao Hu",
        "relation": "author_of",
        "tail": "Epona: Autoregressive Diffusion World Model for Autonomous Driving"
      },
      {
        "head": "Xingang Pan",
        "relation": "author_of",
        "tail": "Epona: Autoregressive Diffusion World Model for Autonomous Driving"
      },
      {
        "head": "Xiaoyang Guo",
        "relation": "author_of",
        "tail": "Epona: Autoregressive Diffusion World Model for Autonomous Driving"
      },
      {
        "head": "Yuan Liu",
        "relation": "author_of",
        "tail": "Epona: Autoregressive Diffusion World Model for Autonomous Driving"
      },
      {
        "head": "Jingwei Huang",
        "relation": "author_of",
        "tail": "Epona: Autoregressive Diffusion World Model for Autonomous Driving"
      },
      {
        "head": "Li Yuan",
        "relation": "author_of",
        "tail": "Epona: Autoregressive Diffusion World Model for Autonomous Driving"
      },
      {
        "head": "Qian Zhang",
        "relation": "author_of",
        "tail": "Epona: Autoregressive Diffusion World Model for Autonomous Driving"
      },
      {
        "head": "Xiao-Xiao Long",
        "relation": "author_of",
        "tail": "Epona: Autoregressive Diffusion World Model for Autonomous Driving"
      },
      {
        "head": "Xun Cao",
        "relation": "author_of",
        "tail": "Epona: Autoregressive Diffusion World Model for Autonomous Driving"
      },
      {
        "head": "Wei Yin",
        "relation": "author_of",
        "tail": "Epona: Autoregressive Diffusion World Model for Autonomous Driving"
      },
      {
        "head": "Yinhan Liu",
        "relation": "author_of",
        "tail": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
      },
      {
        "head": "Myle Ott",
        "relation": "author_of",
        "tail": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
      },
      {
        "head": "Naman Goyal",
        "relation": "author_of",
        "tail": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
      },
      {
        "head": "Jingfei Du",
        "relation": "author_of",
        "tail": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
      },
      {
        "head": "Mandar Joshi",
        "relation": "author_of",
        "tail": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
      },
      {
        "head": "Danqi Chen",
        "relation": "author_of",
        "tail": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
      },
      {
        "head": "Omer Levy",
        "relation": "author_of",
        "tail": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
      },
      {
        "head": "Mike Lewis",
        "relation": "author_of",
        "tail": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
      },
      {
        "head": "Luke Zettlemoyer",
        "relation": "author_of",
        "tail": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
      },
      {
        "head": "Veselin Stoyanov",
        "relation": "author_of",
        "tail": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
      },
      {
        "head": "Yongliang Wu",
        "relation": "author_of",
        "tail": "Unlearning Concepts in Diffusion Model via Concept Domain Correction and Concept Preserving Gradient"
      },
      {
        "head": "Shiji Zhou",
        "relation": "author_of",
        "tail": "Unlearning Concepts in Diffusion Model via Concept Domain Correction and Concept Preserving Gradient"
      },
      {
        "head": "Mingzhuo Yang",
        "relation": "author_of",
        "tail": "Unlearning Concepts in Diffusion Model via Concept Domain Correction and Concept Preserving Gradient"
      },
      {
        "head": "Lianzhe Wang",
        "relation": "author_of",
        "tail": "Unlearning Concepts in Diffusion Model via Concept Domain Correction and Concept Preserving Gradient"
      },
      {
        "head": "Heng Chang",
        "relation": "author_of",
        "tail": "Unlearning Concepts in Diffusion Model via Concept Domain Correction and Concept Preserving Gradient"
      },
      {
        "head": "Wenbo Zhu",
        "relation": "author_of",
        "tail": "Unlearning Concepts in Diffusion Model via Concept Domain Correction and Concept Preserving Gradient"
      },
      {
        "head": "Xinting Hu",
        "relation": "author_of",
        "tail": "Unlearning Concepts in Diffusion Model via Concept Domain Correction and Concept Preserving Gradient"
      },
      {
        "head": "Xiao Zhou",
        "relation": "author_of",
        "tail": "Unlearning Concepts in Diffusion Model via Concept Domain Correction and Concept Preserving Gradient"
      },
      {
        "head": "Xu Yang",
        "relation": "author_of",
        "tail": "Unlearning Concepts in Diffusion Model via Concept Domain Correction and Concept Preserving Gradient"
      },
      {
        "head": "Xingjun Ma",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Yifeng Gao",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Yixu Wang",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Ruofan Wang",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Xin Wang",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Ye Sun",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Yifan Ding",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Hengyuan Xu",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Yunhao Chen",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Yunhan Zhao",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Hanxun Huang",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Yige Li",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Jiaming Zhang",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Xiang Zheng",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Yang Bai",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Henghui Ding",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Zuxuan Wu",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Xipeng Qiu",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Jingfeng Zhang",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Yiming Li",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Jun Sun",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Cong Wang",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Jindong Gu",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Baoyuan Wu",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Siheng Chen",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Tianwei Zhang",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Yang Liu",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Min Gong",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Tongliang Liu",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Shirui Pan",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Cihang Xie",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Tianyu Pang",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Yinpeng Dong",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Ruoxi Jia",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Yang Zhang",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Shi-jie Ma",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Xiangyu Zhang",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Neil Gong",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Chaowei Xiao",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Sarah Erfani",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Bo Li",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Masashi Sugiyama",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Dacheng Tao",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "James Bailey",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Yu-Gang Jiang",
        "relation": "author_of",
        "tail": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "head": "Zicheng Zhang",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Junying Wang",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Farong Wen",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Yijin Guo",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Xiangyu Zhao",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Xinyu Fang",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Shengyuan Ding",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Ziheng Jia",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Jiahao Xiao",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Ye Shen",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Yushuo Zheng",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Xiaorong Zhu",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Yalun Wu",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Ziheng Jiao",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Wei Sun",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Zijian Chen",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Kaiwei Zhang",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Kang Fu",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Yuqin Cao",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Ming Hu",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Yue Zhou",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Xuemei Zhou",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Juntai Cao",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Wei Zhou",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Jinyu Cao",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Ronghui Li",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Donghao Zhou",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Yuan Tian",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Xiangyang Zhu",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Chun-yuan Li",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Haoning Wu",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Xiaohong Liu",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Junjun He",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Yu Zhou",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Hui Liu",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Lin Zhang",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Zesheng Wang",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Huiyu Duan",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Yingjie Zhou",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Xiongkuo Min",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Qi Jia",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Dongzhan Zhou",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Wenlong Zhang",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Jiezhang Cao",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Xue Yang",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Junzhi Yu",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Songyang Zhang",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Haodong Duan",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Guangtao Zhai",
        "relation": "author_of",
        "tail": "Large multimodal models evaluation: a survey"
      },
      {
        "head": "Yangyang Guo",
        "relation": "author_of",
        "tail": "The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense"
      },
      {
        "head": "Fangkai Jiao",
        "relation": "author_of",
        "tail": "The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense"
      },
      {
        "head": "Liqiang Nie",
        "relation": "author_of",
        "tail": "The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense"
      },
      {
        "head": "Mohan Kankanhalli",
        "relation": "author_of",
        "tail": "The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense"
      },
      {
        "head": "Xuannan Liu",
        "relation": "author_of",
        "tail": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs"
      },
      {
        "head": "Zekun Li",
        "relation": "author_of",
        "tail": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs"
      },
      {
        "head": "Zheqi He",
        "relation": "author_of",
        "tail": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs"
      },
      {
        "head": "Peipei Li",
        "relation": "author_of",
        "tail": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs"
      },
      {
        "head": "Shuhan Xia",
        "relation": "author_of",
        "tail": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs"
      },
      {
        "head": "Xing Cui",
        "relation": "author_of",
        "tail": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs"
      },
      {
        "head": "Huaibo Huang",
        "relation": "author_of",
        "tail": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs"
      },
      {
        "head": "Xi Yang",
        "relation": "author_of",
        "tail": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs"
      },
      {
        "head": "Ran He",
        "relation": "author_of",
        "tail": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs"
      },
      {
        "head": "Jinmiao Zhao",
        "relation": "author_of",
        "tail": "Gradient-Guided Learning Network for Infrared Small Target Detection"
      },
      {
        "head": "Chuang Yu",
        "relation": "author_of",
        "tail": "Gradient-Guided Learning Network for Infrared Small Target Detection"
      },
      {
        "head": "Zelin Shi",
        "relation": "author_of",
        "tail": "Gradient-Guided Learning Network for Infrared Small Target Detection"
      },
      {
        "head": "Yunpeng Liu",
        "relation": "author_of",
        "tail": "Gradient-Guided Learning Network for Infrared Small Target Detection"
      },
      {
        "head": "Yingdi Zhang",
        "relation": "author_of",
        "tail": "Gradient-Guided Learning Network for Infrared Small Target Detection"
      },
      {
        "head": "Waleed Khalid",
        "relation": "author_of",
        "tail": "A Retrieval-Augmented Generation Approach to Extracting Algorithmic Logic from Neural Networks"
      },
      {
        "head": "Dmitry Ignatov",
        "relation": "author_of",
        "tail": "A Retrieval-Augmented Generation Approach to Extracting Algorithmic Logic from Neural Networks"
      },
      {
        "head": "Radu Timofte",
        "relation": "author_of",
        "tail": "A Retrieval-Augmented Generation Approach to Extracting Algorithmic Logic from Neural Networks"
      },
      {
        "head": "Yu Tian",
        "relation": "author_of",
        "tail": "CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation"
      },
      {
        "head": "Zhongheng Yang",
        "relation": "author_of",
        "tail": "CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation"
      },
      {
        "head": "Chenshi Liu",
        "relation": "author_of",
        "tail": "CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation"
      },
      {
        "head": "Yiyun Su",
        "relation": "author_of",
        "tail": "CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation"
      },
      {
        "head": "Ziwei Hong",
        "relation": "author_of",
        "tail": "CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation"
      },
      {
        "head": "Zexi Gong",
        "relation": "author_of",
        "tail": "CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation"
      },
      {
        "head": "Jingyuan Xu",
        "relation": "author_of",
        "tail": "CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation"
      },
      {
        "head": "Yang Lu",
        "relation": "author_of",
        "tail": "IMobileTransformer: A fusion-based lightweight model for rice disease identification"
      },
      {
        "head": "Haoyang Zhou",
        "relation": "author_of",
        "tail": "IMobileTransformer: A fusion-based lightweight model for rice disease identification"
      },
      {
        "head": "Peng Wang",
        "relation": "author_of",
        "tail": "IMobileTransformer: A fusion-based lightweight model for rice disease identification"
      },
      {
        "head": "Erzhi Wang",
        "relation": "author_of",
        "tail": "IMobileTransformer: A fusion-based lightweight model for rice disease identification"
      },
      {
        "head": "Gongfa Li",
        "relation": "author_of",
        "tail": "IMobileTransformer: A fusion-based lightweight model for rice disease identification"
      },
      {
        "head": "Tongjian Yu",
        "relation": "author_of",
        "tail": "IMobileTransformer: A fusion-based lightweight model for rice disease identification"
      },
      {
        "head": "Jinhui Yi",
        "relation": "author_of",
        "tail": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images"
      },
      {
        "head": "Gina Lopez",
        "relation": "author_of",
        "tail": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images"
      },
      {
        "head": "S. Hadir",
        "relation": "author_of",
        "tail": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images"
      },
      {
        "head": "Jan Weyler",
        "relation": "author_of",
        "tail": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images"
      },
      {
        "head": "Lasse Klingbeil",
        "relation": "author_of",
        "tail": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images"
      },
      {
        "head": "Marion Deichmann",
        "relation": "author_of",
        "tail": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images"
      },
      {
        "head": "Juergen Gall",
        "relation": "author_of",
        "tail": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images"
      },
      {
        "head": "S. J. Seidel",
        "relation": "author_of",
        "tail": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images"
      },
      {
        "head": "Isaac Robinson",
        "relation": "author_of",
        "tail": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers"
      },
      {
        "head": "Peter Robicheaux",
        "relation": "author_of",
        "tail": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers"
      },
      {
        "head": "Matvei Popov",
        "relation": "author_of",
        "tail": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers"
      },
      {
        "head": "Deva Ramanan",
        "relation": "author_of",
        "tail": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers"
      },
      {
        "head": "Neehar Peri",
        "relation": "author_of",
        "tail": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers"
      },
      {
        "head": "Zhou Wang",
        "relation": "author_of",
        "tail": "Image quality assessment: from error visibility to structural similarity"
      },
      {
        "head": "A. Bovik",
        "relation": "author_of",
        "tail": "Image quality assessment: from error visibility to structural similarity"
      },
      {
        "head": "H. Sheikh",
        "relation": "author_of",
        "tail": "Image quality assessment: from error visibility to structural similarity"
      },
      {
        "head": "Eero P. Simoncelli",
        "relation": "author_of",
        "tail": "Image quality assessment: from error visibility to structural similarity"
      },
      {
        "head": "Wei Cao",
        "relation": "author_of",
        "tail": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction"
      },
      {
        "head": "Hao Zhang",
        "relation": "author_of",
        "tail": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction"
      },
      {
        "head": "Fengrui Tian",
        "relation": "author_of",
        "tail": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction"
      },
      {
        "head": "Yulun Wu",
        "relation": "author_of",
        "tail": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction"
      },
      {
        "head": "Yingying Li",
        "relation": "author_of",
        "tail": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction"
      },
      {
        "head": "Shenlong Wang",
        "relation": "author_of",
        "tail": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction"
      },
      {
        "head": "Ning Yu",
        "relation": "author_of",
        "tail": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction"
      },
      {
        "head": "Yaoyao Liu",
        "relation": "author_of",
        "tail": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction"
      },
      {
        "head": "Yuxue Yang",
        "relation": "author_of",
        "tail": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos"
      },
      {
        "head": "Lue Fan",
        "relation": "author_of",
        "tail": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos"
      },
      {
        "head": "Ziqi Shi",
        "relation": "author_of",
        "tail": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos"
      },
      {
        "head": "Junran Peng",
        "relation": "author_of",
        "tail": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos"
      },
      {
        "head": "Feng Wang",
        "relation": "author_of",
        "tail": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos"
      },
      {
        "head": "Zhaoxiang Zhang",
        "relation": "author_of",
        "tail": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos"
      },
      {
        "head": "William Peebles",
        "relation": "author_of",
        "tail": "Scalable Diffusion Models with Transformers"
      },
      {
        "head": "Saining Xie",
        "relation": "author_of",
        "tail": "Scalable Diffusion Models with Transformers"
      },
      {
        "head": "Jianlin Su",
        "relation": "author_of",
        "tail": "RoFormer: Enhanced Transformer with Rotary Position Embedding"
      },
      {
        "head": "Yu Lu",
        "relation": "author_of",
        "tail": "RoFormer: Enhanced Transformer with Rotary Position Embedding"
      },
      {
        "head": "Shengfeng Pan",
        "relation": "author_of",
        "tail": "RoFormer: Enhanced Transformer with Rotary Position Embedding"
      },
      {
        "head": "Ahmed Murtadha",
        "relation": "author_of",
        "tail": "RoFormer: Enhanced Transformer with Rotary Position Embedding"
      },
      {
        "head": "Bo Wen",
        "relation": "author_of",
        "tail": "RoFormer: Enhanced Transformer with Rotary Position Embedding"
      },
      {
        "head": "Yunfeng Liu",
        "relation": "author_of",
        "tail": "RoFormer: Enhanced Transformer with Rotary Position Embedding"
      },
      {
        "head": "Sifan Tu",
        "relation": "author_of",
        "tail": "The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey"
      },
      {
        "head": "Xin Zhou",
        "relation": "author_of",
        "tail": "The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey"
      },
      {
        "head": "Dingkang Liang",
        "relation": "author_of",
        "tail": "The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey"
      },
      {
        "head": "Xingyu Jiang",
        "relation": "author_of",
        "tail": "The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey"
      },
      {
        "head": "Yumeng Zhang",
        "relation": "author_of",
        "tail": "The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey"
      },
      {
        "head": "Xiaofan Li",
        "relation": "author_of",
        "tail": "The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey"
      },
      {
        "head": "Xiang Bai",
        "relation": "author_of",
        "tail": "The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey"
      },
      {
        "head": "Andreas Geiger",
        "relation": "author_of",
        "tail": "Vision meets robotics: The KITTI dataset"
      },
      {
        "head": "Philip Lenz",
        "relation": "author_of",
        "tail": "Vision meets robotics: The KITTI dataset"
      },
      {
        "head": "C. Stiller",
        "relation": "author_of",
        "tail": "Vision meets robotics: The KITTI dataset"
      },
      {
        "head": "R. Urtasun",
        "relation": "author_of",
        "tail": "Vision meets robotics: The KITTI dataset"
      },
      {
        "head": "S. Umeyama",
        "relation": "author_of",
        "tail": "Least-Squares Estimation of Transformation Parameters Between Two Point Patterns"
      },
      {
        "head": "Run Wang",
        "relation": "author_of",
        "tail": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations"
      },
      {
        "head": "Chaoyi Zhou",
        "relation": "author_of",
        "tail": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations"
      },
      {
        "head": "Amir Salarpour",
        "relation": "author_of",
        "tail": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations"
      },
      {
        "head": "Xi Liu",
        "relation": "author_of",
        "tail": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations"
      },
      {
        "head": "Zhi-Qi Cheng",
        "relation": "author_of",
        "tail": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations"
      },
      {
        "head": "Feng Luo",
        "relation": "author_of",
        "tail": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations"
      },
      {
        "head": "Mert D. Pesé",
        "relation": "author_of",
        "tail": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations"
      },
      {
        "head": "Siyu Huang",
        "relation": "author_of",
        "tail": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations"
      },
      {
        "head": "Xingbang Hao",
        "relation": "author_of",
        "tail": "Deep Learning"
      },
      {
        "head": "Guigang Zhang",
        "relation": "author_of",
        "tail": "Deep Learning"
      },
      {
        "head": "Shang Ma",
        "relation": "author_of",
        "tail": "Deep Learning"
      },
      {
        "head": "Nikhil Keetha",
        "relation": "author_of",
        "tail": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction"
      },
      {
        "head": "Norman Müller",
        "relation": "author_of",
        "tail": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction"
      },
      {
        "head": "Johannes Schönberger",
        "relation": "author_of",
        "tail": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction"
      },
      {
        "head": "Lorenzo Porzi",
        "relation": "author_of",
        "tail": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction"
      },
      {
        "head": "Yuchen Zhang",
        "relation": "author_of",
        "tail": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction"
      },
      {
        "head": "Tobias Fischer",
        "relation": "author_of",
        "tail": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction"
      },
      {
        "head": "Arno Knapitsch",
        "relation": "author_of",
        "tail": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction"
      },
      {
        "head": "Duncan Zauss",
        "relation": "author_of",
        "tail": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction"
      },
      {
        "head": "Ethan Weber",
        "relation": "author_of",
        "tail": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction"
      },
      {
        "head": "Nelson Antunes",
        "relation": "author_of",
        "tail": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction"
      },
      {
        "head": "Jonathon Luiten",
        "relation": "author_of",
        "tail": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction"
      },
      {
        "head": "Manuel Lopez-Antequera",
        "relation": "author_of",
        "tail": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction"
      },
      {
        "head": "Samuel Rota Bulò",
        "relation": "author_of",
        "tail": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction"
      },
      {
        "head": "Christian Richardt",
        "relation": "author_of",
        "tail": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction"
      },
      {
        "head": "Deva Ramanan",
        "relation": "author_of",
        "tail": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction"
      },
      {
        "head": "Sebastian Scherer",
        "relation": "author_of",
        "tail": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction"
      },
      {
        "head": "Peter Kontschieder",
        "relation": "author_of",
        "tail": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction"
      },
      {
        "head": "Team Seedream",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": ":",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Yunpeng Chen",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Yu Gao",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Lixue Gong",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Meng Guo",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Qiushan Guo",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Zhiyao Guo",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Xiaoxia Hou",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Weilin Huang",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Yixuan Huang",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Xiaowen Jian",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Huafeng Kuang",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Zhichao Lai",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Fanshi Li",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Liang Li",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Xiaochen Lian",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Chao Liao",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Liyang Liu",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Wei Liu",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Yanzuo Lu",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Zhengxiong Luo",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Tongtong Ou",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Guang Shi",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Yichun Shi",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Shiqi Sun",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Yu Tian",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Zhi Tian",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Peng Wang",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Rui Wang",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Xun Wang",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Ye Wang",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Guofeng Wu",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Jie Wu",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Wenxu Wu",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Yonghui Wu",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Xin Xia",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Xuefeng Xiao",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Shuang Xu",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Xin Yan",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Ceyuan Yang",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Jianchao Yang",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Zhonghua Zhai",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Chenlin Zhang",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Heng Zhang",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Qi Zhang",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Xinyu Zhang",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Yuwei Zhang",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Shijia Zhao",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Wenliang Zhao",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Wenjia Zhu",
        "relation": "author_of",
        "tail": "Seedream 4.0: Toward Next-generation Multimodal Image Generation"
      },
      {
        "head": "Junyan Ye",
        "relation": "author_of",
        "tail": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation"
      },
      {
        "head": "Dongzhi Jiang",
        "relation": "author_of",
        "tail": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation"
      },
      {
        "head": "Zihao Wang",
        "relation": "author_of",
        "tail": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation"
      },
      {
        "head": "Leqi Zhu",
        "relation": "author_of",
        "tail": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation"
      },
      {
        "head": "Zhenghao Hu",
        "relation": "author_of",
        "tail": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation"
      },
      {
        "head": "Zilong Huang",
        "relation": "author_of",
        "tail": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation"
      },
      {
        "head": "Jun He",
        "relation": "author_of",
        "tail": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation"
      },
      {
        "head": "Zhiyuan Yan",
        "relation": "author_of",
        "tail": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation"
      },
      {
        "head": "Jinghua Yu",
        "relation": "author_of",
        "tail": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation"
      },
      {
        "head": "Hongsheng Li",
        "relation": "author_of",
        "tail": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation"
      },
      {
        "head": "Conghui He",
        "relation": "author_of",
        "tail": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation"
      },
      {
        "head": "Weijia Li",
        "relation": "author_of",
        "tail": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation"
      },
      {
        "head": "Yi Xin",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Qi Qin",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Siqi Luo",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Kaiwen Zhu",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Juncheng Yan",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Yan Tai",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Jiayi Lei",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Yuewen Cao",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Keqi Wang",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Yibin Wang",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Jinbin Bai",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Qian Yu",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Dengyang Jiang",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Yuandong Pu",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Haoxing Chen",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Le Zhuo",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Junjun He",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Gen Luo",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Tianbin Li",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Ming Hu",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Jin Ye",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Shenglong Ye",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Bo Zhang",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Chang Xu",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Wenhai Wang",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Hongsheng Li",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Guangtao Zhai",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Tianfan Xue",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Bin Fu",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Xiaohong Liu",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Yu Qiao",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "Yihao Liu",
        "relation": "author_of",
        "tail": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"
      },
      {
        "head": "NextStep Team",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Chunrui Han",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Guopeng Li",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Jingwei Wu",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Quan Sun",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Yan Cai",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Yuang Peng",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Zheng Ge",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Deyu Zhou",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Haomiao Tang",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Hongyu Zhou",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Kenkun Liu",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Ailin Huang",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Bin Wang",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Changxin Miao",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Deshan Sun",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "En Yu",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Fukun Yin",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Gang Yu",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Hao Nie",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Haoran Lv",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Hanpeng Hu",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Jia Wang",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Jian Zhou",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Jianjian Sun",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Kaijun Tan",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Kang An",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Kangheng Lin",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Liang Zhao",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Mei Chen",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Peng Xing",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Rui Wang",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Shiyu Liu",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Shutao Xia",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Tianhao You",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Wei Ji",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Xianfang Zeng",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Xin Han",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Xuelin Zhang",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Yana Wei",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Yanming Xu",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Yimin Jiang",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Yingming Wang",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Yu Zhou",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Yucheng Han",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Ziyang Meng",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Binxing Jiao",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Daxin Jiang",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Xiangyu Zhang",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Yibo Zhu",
        "relation": "author_of",
        "tail": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "head": "Long Ouyang",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Jeff Wu",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Xu Jiang",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Diogo Almeida",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Carroll L. Wainwright",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Pamela Mishkin",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Chong Zhang",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Sandhini Agarwal",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Katarina Slama",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Alex Ray",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "John Schulman",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Jacob Hilton",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Fraser Kelton",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Luke Miller",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Maddie Simens",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Amanda Askell",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Peter Welinder",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Paul Christiano",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Jan Leike",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Ryan Lowe",
        "relation": "author_of",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "Bowen Jin",
        "relation": "author_of",
        "tail": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning"
      },
      {
        "head": "Hansi Zeng",
        "relation": "author_of",
        "tail": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning"
      },
      {
        "head": "Zhenrui Yue",
        "relation": "author_of",
        "tail": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning"
      },
      {
        "head": "Jinsung Yoon",
        "relation": "author_of",
        "tail": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning"
      },
      {
        "head": "Sercan Arik",
        "relation": "author_of",
        "tail": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning"
      },
      {
        "head": "Dong Wang",
        "relation": "author_of",
        "tail": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning"
      },
      {
        "head": "Hamed Zamani",
        "relation": "author_of",
        "tail": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning"
      },
      {
        "head": "Jiawei Han",
        "relation": "author_of",
        "tail": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning"
      },
      {
        "head": "Karan Singhal",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Tao Tu",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Juraj Gottweis",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "R. Sayres",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Ellery Wulczyn",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Mohamed Amin",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Le Hou",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Kevin Clark",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Stephen R. Pfohl",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Heather Cole-Lewis",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Darlene Neal",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Q. Rashid",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Mike Schaekermann",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Amy Wang",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Dev Dash",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Jonathan H. Chen",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Nigam H. Shah",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Sami Lachgar",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "P. Mansfield",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Sushant Prakash",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Bradley Green",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Ewa Dominowska",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Blaise Agüera y Arcas",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Nenad Tomašev",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Yun Liu",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Renee Wong",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Christopher Semturs",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "S. Mahdavi",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Joelle K. Barral",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Dale R. Webster",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "G. Corrado",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Yossi Matias",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Shekoofeh Azizi",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "A. Karthikesalingam",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Vivek Natarajan",
        "relation": "author_of",
        "tail": "Toward expert-level medical question answering with large language models"
      },
      {
        "head": "Tianzhe Chu",
        "relation": "author_of",
        "tail": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training"
      },
      {
        "head": "Yuexiang Zhai",
        "relation": "author_of",
        "tail": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training"
      },
      {
        "head": "Jihan Yang",
        "relation": "author_of",
        "tail": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training"
      },
      {
        "head": "Shengbang Tong",
        "relation": "author_of",
        "tail": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training"
      },
      {
        "head": "Saining Xie",
        "relation": "author_of",
        "tail": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training"
      },
      {
        "head": "Dale Schuurmans",
        "relation": "author_of",
        "tail": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training"
      },
      {
        "head": "Quoc V. Le",
        "relation": "author_of",
        "tail": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training"
      },
      {
        "head": "Sergey Levine",
        "relation": "author_of",
        "tail": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training"
      },
      {
        "head": "Yi Ma",
        "relation": "author_of",
        "tail": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training"
      },
      {
        "head": "Peijia Lin",
        "relation": "author_of",
        "tail": "Equivariant Diffusion for Crystal Structure Prediction"
      },
      {
        "head": "Pin Chen",
        "relation": "author_of",
        "tail": "Equivariant Diffusion for Crystal Structure Prediction"
      },
      {
        "head": "Rui Jiao",
        "relation": "author_of",
        "tail": "Equivariant Diffusion for Crystal Structure Prediction"
      },
      {
        "head": "Qing Mo",
        "relation": "author_of",
        "tail": "Equivariant Diffusion for Crystal Structure Prediction"
      },
      {
        "head": "Jianhuan Cen",
        "relation": "author_of",
        "tail": "Equivariant Diffusion for Crystal Structure Prediction"
      },
      {
        "head": "Wenbing Huang",
        "relation": "author_of",
        "tail": "Equivariant Diffusion for Crystal Structure Prediction"
      },
      {
        "head": "Yang Liu",
        "relation": "author_of",
        "tail": "Equivariant Diffusion for Crystal Structure Prediction"
      },
      {
        "head": "Dan Huang",
        "relation": "author_of",
        "tail": "Equivariant Diffusion for Crystal Structure Prediction"
      },
      {
        "head": "Yutong Lu",
        "relation": "author_of",
        "tail": "Equivariant Diffusion for Crystal Structure Prediction"
      },
      {
        "head": "Wenqiang Sun",
        "relation": "author_of",
        "tail": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling"
      },
      {
        "head": "Haiyu Zhang",
        "relation": "author_of",
        "tail": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling"
      },
      {
        "head": "Haoyuan Wang",
        "relation": "author_of",
        "tail": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling"
      },
      {
        "head": "Junta Wu",
        "relation": "author_of",
        "tail": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling"
      },
      {
        "head": "Zehan Wang",
        "relation": "author_of",
        "tail": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling"
      },
      {
        "head": "Zhenwei Wang",
        "relation": "author_of",
        "tail": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling"
      },
      {
        "head": "Yunhong Wang",
        "relation": "author_of",
        "tail": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling"
      },
      {
        "head": "Jun Zhang",
        "relation": "author_of",
        "tail": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling"
      },
      {
        "head": "Tengfei Wang",
        "relation": "author_of",
        "tail": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling"
      },
      {
        "head": "Chunchao Guo",
        "relation": "author_of",
        "tail": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling"
      },
      {
        "head": "Jianfeng Xiang",
        "relation": "author_of",
        "tail": "Native and Compact Structured Latents for 3D Generation"
      },
      {
        "head": "Xiaoxue Chen",
        "relation": "author_of",
        "tail": "Native and Compact Structured Latents for 3D Generation"
      },
      {
        "head": "Sicheng Xu",
        "relation": "author_of",
        "tail": "Native and Compact Structured Latents for 3D Generation"
      },
      {
        "head": "Ruicheng Wang",
        "relation": "author_of",
        "tail": "Native and Compact Structured Latents for 3D Generation"
      },
      {
        "head": "Zelong Lv",
        "relation": "author_of",
        "tail": "Native and Compact Structured Latents for 3D Generation"
      },
      {
        "head": "Yu Deng",
        "relation": "author_of",
        "tail": "Native and Compact Structured Latents for 3D Generation"
      },
      {
        "head": "Hongyuan Zhu",
        "relation": "author_of",
        "tail": "Native and Compact Structured Latents for 3D Generation"
      },
      {
        "head": "Yue Dong",
        "relation": "author_of",
        "tail": "Native and Compact Structured Latents for 3D Generation"
      },
      {
        "head": "Hao Zhao",
        "relation": "author_of",
        "tail": "Native and Compact Structured Latents for 3D Generation"
      },
      {
        "head": "Nicholas Jing Yuan",
        "relation": "author_of",
        "tail": "Native and Compact Structured Latents for 3D Generation"
      },
      {
        "head": "Jiaolong Yang",
        "relation": "author_of",
        "tail": "Native and Compact Structured Latents for 3D Generation"
      },
      {
        "head": "Basile Terver",
        "relation": "author_of",
        "tail": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?"
      },
      {
        "head": "Tsung-Yen Yang",
        "relation": "author_of",
        "tail": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?"
      },
      {
        "head": "Jean Ponce",
        "relation": "author_of",
        "tail": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?"
      },
      {
        "head": "Adrien Bardes",
        "relation": "author_of",
        "tail": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?"
      },
      {
        "head": "Yann LeCun",
        "relation": "author_of",
        "tail": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?"
      },
      {
        "head": "Guangyi Zhang",
        "relation": "author_of",
        "tail": "Progressive Learned Image Transmission for Semantic Communication Using Hierarchical VAE"
      },
      {
        "head": "Hanlei Li",
        "relation": "author_of",
        "tail": "Progressive Learned Image Transmission for Semantic Communication Using Hierarchical VAE"
      },
      {
        "head": "Yunlong Cai",
        "relation": "author_of",
        "tail": "Progressive Learned Image Transmission for Semantic Communication Using Hierarchical VAE"
      },
      {
        "head": "Qiyu Hu",
        "relation": "author_of",
        "tail": "Progressive Learned Image Transmission for Semantic Communication Using Hierarchical VAE"
      },
      {
        "head": "Guanding Yu",
        "relation": "author_of",
        "tail": "Progressive Learned Image Transmission for Semantic Communication Using Hierarchical VAE"
      },
      {
        "head": "Zhijing Qin",
        "relation": "author_of",
        "tail": "Progressive Learned Image Transmission for Semantic Communication Using Hierarchical VAE"
      },
      {
        "head": "Wenjun Lin",
        "relation": "author_of",
        "tail": "STORM: Search-Guided Generative World Models for Robotic Manipulation"
      },
      {
        "head": "Jensen Zhang",
        "relation": "author_of",
        "tail": "STORM: Search-Guided Generative World Models for Robotic Manipulation"
      },
      {
        "head": "Kaitong Cai",
        "relation": "author_of",
        "tail": "STORM: Search-Guided Generative World Models for Robotic Manipulation"
      },
      {
        "head": "Keze Wang",
        "relation": "author_of",
        "tail": "STORM: Search-Guided Generative World Models for Robotic Manipulation"
      },
      {
        "head": "Qianqian Wang",
        "relation": "author_of",
        "tail": "Continuous 3D Perception Model with Persistent State"
      },
      {
        "head": "Yifei Zhang",
        "relation": "author_of",
        "tail": "Continuous 3D Perception Model with Persistent State"
      },
      {
        "head": "Aleksander Holynski",
        "relation": "author_of",
        "tail": "Continuous 3D Perception Model with Persistent State"
      },
      {
        "head": "Alexei A. Efros",
        "relation": "author_of",
        "tail": "Continuous 3D Perception Model with Persistent State"
      },
      {
        "head": "Angjoo Kanazawa",
        "relation": "author_of",
        "tail": "Continuous 3D Perception Model with Persistent State"
      },
      {
        "head": "Xuanchi Ren",
        "relation": "author_of",
        "tail": "Gen3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control"
      },
      {
        "head": "Tianchang Shen",
        "relation": "author_of",
        "tail": "Gen3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control"
      },
      {
        "head": "Jiahui Huang",
        "relation": "author_of",
        "tail": "Gen3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control"
      },
      {
        "head": "Huan Ling",
        "relation": "author_of",
        "tail": "Gen3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control"
      },
      {
        "head": "Yifan Lu",
        "relation": "author_of",
        "tail": "Gen3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control"
      },
      {
        "head": "Merlin Nimier-David",
        "relation": "author_of",
        "tail": "Gen3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control"
      },
      {
        "head": "Thomas Müller",
        "relation": "author_of",
        "tail": "Gen3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control"
      },
      {
        "head": "Alexander Keller",
        "relation": "author_of",
        "tail": "Gen3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control"
      },
      {
        "head": "Sanja Fidler",
        "relation": "author_of",
        "tail": "Gen3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control"
      },
      {
        "head": "Jun Gao",
        "relation": "author_of",
        "tail": "Gen3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control"
      },
      {
        "head": "Shangzhan Zhang",
        "relation": "author_of",
        "tail": "FLARE: Feed-Forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views"
      },
      {
        "head": "Jianyuan Wang",
        "relation": "author_of",
        "tail": "FLARE: Feed-Forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views"
      },
      {
        "head": "Yinghao Xu",
        "relation": "author_of",
        "tail": "FLARE: Feed-Forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views"
      },
      {
        "head": "Nan Xue",
        "relation": "author_of",
        "tail": "FLARE: Feed-Forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views"
      },
      {
        "head": "Christian Rupprecht",
        "relation": "author_of",
        "tail": "FLARE: Feed-Forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views"
      },
      {
        "head": "Xiaowei Zhou",
        "relation": "author_of",
        "tail": "FLARE: Feed-Forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views"
      },
      {
        "head": "Yujun Shen",
        "relation": "author_of",
        "tail": "FLARE: Feed-Forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views"
      },
      {
        "head": "Gordon Wetzstein",
        "relation": "author_of",
        "tail": "FLARE: Feed-Forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views"
      },
      {
        "head": "Luigi Piccinelli",
        "relation": "author_of",
        "tail": "UniDepthV2: Universal Monocular Metric Depth Estimation Made Simpler"
      },
      {
        "head": "Christos Sakaridis",
        "relation": "author_of",
        "tail": "UniDepthV2: Universal Monocular Metric Depth Estimation Made Simpler"
      },
      {
        "head": "Yung-Hsu Yang",
        "relation": "author_of",
        "tail": "UniDepthV2: Universal Monocular Metric Depth Estimation Made Simpler"
      },
      {
        "head": "Mattia Segu",
        "relation": "author_of",
        "tail": "UniDepthV2: Universal Monocular Metric Depth Estimation Made Simpler"
      },
      {
        "head": "Siyuan Li",
        "relation": "author_of",
        "tail": "UniDepthV2: Universal Monocular Metric Depth Estimation Made Simpler"
      },
      {
        "head": "Wim Abbeloos",
        "relation": "author_of",
        "tail": "UniDepthV2: Universal Monocular Metric Depth Estimation Made Simpler"
      },
      {
        "head": "Luc Van Gool",
        "relation": "author_of",
        "tail": "UniDepthV2: Universal Monocular Metric Depth Estimation Made Simpler"
      },
      {
        "head": "Shuo Xing",
        "relation": "author_of",
        "tail": "OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving"
      },
      {
        "head": "Chengyuan Qian",
        "relation": "author_of",
        "tail": "OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving"
      },
      {
        "head": "Yuping Wang",
        "relation": "author_of",
        "tail": "OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving"
      },
      {
        "head": "Hongyuan Hua",
        "relation": "author_of",
        "tail": "OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving"
      },
      {
        "head": "Kexin Tian",
        "relation": "author_of",
        "tail": "OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving"
      },
      {
        "head": "Yang Zhou",
        "relation": "author_of",
        "tail": "OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving"
      },
      {
        "head": "Zhengzhong Tu",
        "relation": "author_of",
        "tail": "OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving"
      },
      {
        "head": "Ana Davila",
        "relation": "author_of",
        "tail": "Adaptive transfer learning for surgical tool presence detection in laparoscopic videos through gradual freezing fine-tuning"
      },
      {
        "head": "Jacinto Colan",
        "relation": "author_of",
        "tail": "Adaptive transfer learning for surgical tool presence detection in laparoscopic videos through gradual freezing fine-tuning"
      },
      {
        "head": "Yasuhisa Hasegawa",
        "relation": "author_of",
        "tail": "Adaptive transfer learning for surgical tool presence detection in laparoscopic videos through gradual freezing fine-tuning"
      },
      {
        "head": "Ross Girshick",
        "relation": "author_of",
        "tail": "Rich feature hierarchies for accurate object detection and semantic segmentation"
      },
      {
        "head": "Jeff Donahue",
        "relation": "author_of",
        "tail": "Rich feature hierarchies for accurate object detection and semantic segmentation"
      },
      {
        "head": "Trevor Darrell",
        "relation": "author_of",
        "tail": "Rich feature hierarchies for accurate object detection and semantic segmentation"
      },
      {
        "head": "Jitendra Malik",
        "relation": "author_of",
        "tail": "Rich feature hierarchies for accurate object detection and semantic segmentation"
      },
      {
        "head": "Haotian Lv",
        "relation": "author_of",
        "tail": "Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism"
      },
      {
        "head": "Yuhui Zhang",
        "relation": "author_of",
        "tail": "Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism"
      },
      {
        "head": "Jiangbo Dai",
        "relation": "author_of",
        "tail": "Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism"
      },
      {
        "head": "Hanli Wu",
        "relation": "author_of",
        "tail": "Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism"
      },
      {
        "head": "Jiaji Wang",
        "relation": "author_of",
        "tail": "Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism"
      },
      {
        "head": "Dawei Wang",
        "relation": "author_of",
        "tail": "Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism"
      },
      {
        "head": "Takuya Akiba",
        "relation": "author_of",
        "tail": "Evolutionary Optimization of Model Merging Recipes"
      },
      {
        "head": "Makoto Shing",
        "relation": "author_of",
        "tail": "Evolutionary Optimization of Model Merging Recipes"
      },
      {
        "head": "Yujin Tang",
        "relation": "author_of",
        "tail": "Evolutionary Optimization of Model Merging Recipes"
      },
      {
        "head": "Qi Sun",
        "relation": "author_of",
        "tail": "Evolutionary Optimization of Model Merging Recipes"
      },
      {
        "head": "David Ha",
        "relation": "author_of",
        "tail": "Evolutionary Optimization of Model Merging Recipes"
      },
      {
        "head": "Kyunghyun Cho",
        "relation": "author_of",
        "tail": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation"
      },
      {
        "head": "Bart van Merrienboer",
        "relation": "author_of",
        "tail": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation"
      },
      {
        "head": "Caglar Gulcehre",
        "relation": "author_of",
        "tail": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation"
      },
      {
        "head": "Dzmitry Bahdanau",
        "relation": "author_of",
        "tail": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation"
      },
      {
        "head": "Fethi Bougares",
        "relation": "author_of",
        "tail": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation"
      },
      {
        "head": "Holger Schwenk",
        "relation": "author_of",
        "tail": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation"
      },
      {
        "head": "Yoshua Bengio",
        "relation": "author_of",
        "tail": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation"
      },
      {
        "head": "Erfei Cui",
        "relation": "author_of",
        "tail": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving"
      },
      {
        "head": "Wenhai Wang",
        "relation": "author_of",
        "tail": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving"
      },
      {
        "head": "Zhiqi Li",
        "relation": "author_of",
        "tail": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving"
      },
      {
        "head": "Jiangwei Xie",
        "relation": "author_of",
        "tail": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving"
      },
      {
        "head": "Haoming Zou",
        "relation": "author_of",
        "tail": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving"
      },
      {
        "head": "Hanming Deng",
        "relation": "author_of",
        "tail": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving"
      },
      {
        "head": "Gen Luo",
        "relation": "author_of",
        "tail": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving"
      },
      {
        "head": "Lewei Lu",
        "relation": "author_of",
        "tail": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving"
      },
      {
        "head": "Xizhou Zhu",
        "relation": "author_of",
        "tail": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving"
      },
      {
        "head": "Jifeng Dai",
        "relation": "author_of",
        "tail": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving"
      },
      {
        "head": "Haodong Duan",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Xinyu Fang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Junming Yang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Xiangyu Zhao",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Yuxuan Qiao",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Mo Li",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Amit Agarwal",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Zhe Chen",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Lin Chen",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Yuan Liu",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Yubo Ma",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Hailong Sun",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Yifan Zhang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Shiyin Lu",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Tack Hwa Wong",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Weiyun Wang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Peiheng Zhou",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Xiaozhe Li",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Chaoyou Fu",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Junbo Cui",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Jixuan Chen",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Enxin Song",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Song Mao",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Shengyuan Ding",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Tianhao Liang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Zicheng Zhang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Xiaoyi Dong",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Yuhang Zang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Pan Zhang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Jiaqi Wang",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Dahua Lin",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Kai Chen",
        "relation": "author_of",
        "tail": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
      },
      {
        "head": "Jason Wei",
        "relation": "author_of",
        "tail": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Xuezhi Wang",
        "relation": "author_of",
        "tail": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Dale Schuurmans",
        "relation": "author_of",
        "tail": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Maarten Bosma",
        "relation": "author_of",
        "tail": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Brian Ichter",
        "relation": "author_of",
        "tail": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Fei Xia",
        "relation": "author_of",
        "tail": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Ed Chi",
        "relation": "author_of",
        "tail": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Quoc Le",
        "relation": "author_of",
        "tail": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Denny Zhou",
        "relation": "author_of",
        "tail": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "Anwesha Mukherjee",
        "relation": "author_of",
        "tail": "Federated Learning Architectures: A Performance Evaluation with Crop Yield Prediction Application"
      },
      {
        "head": "Rajkumar Buyya",
        "relation": "author_of",
        "tail": "Federated Learning Architectures: A Performance Evaluation with Crop Yield Prediction Application"
      },
      {
        "head": "Zhiyuan You",
        "relation": "author_of",
        "tail": "Enhancing Descriptive Image Quality Assessment with A Large-scale Multi-modal Dataset"
      },
      {
        "head": "Jinjin Gu",
        "relation": "author_of",
        "tail": "Enhancing Descriptive Image Quality Assessment with A Large-scale Multi-modal Dataset"
      },
      {
        "head": "Xin Cai",
        "relation": "author_of",
        "tail": "Enhancing Descriptive Image Quality Assessment with A Large-scale Multi-modal Dataset"
      },
      {
        "head": "Zheyuan Li",
        "relation": "author_of",
        "tail": "Enhancing Descriptive Image Quality Assessment with A Large-scale Multi-modal Dataset"
      },
      {
        "head": "Kaiwen Zhu",
        "relation": "author_of",
        "tail": "Enhancing Descriptive Image Quality Assessment with A Large-scale Multi-modal Dataset"
      },
      {
        "head": "Chao Dong",
        "relation": "author_of",
        "tail": "Enhancing Descriptive Image Quality Assessment with A Large-scale Multi-modal Dataset"
      },
      {
        "head": "Tianfan Xue",
        "relation": "author_of",
        "tail": "Enhancing Descriptive Image Quality Assessment with A Large-scale Multi-modal Dataset"
      },
      {
        "head": "Matthew E. Peters",
        "relation": "author_of",
        "tail": "Deep contextualized word representations"
      },
      {
        "head": "Mark Neumann",
        "relation": "author_of",
        "tail": "Deep contextualized word representations"
      },
      {
        "head": "Mohit Iyyer",
        "relation": "author_of",
        "tail": "Deep contextualized word representations"
      },
      {
        "head": "Matt Gardner",
        "relation": "author_of",
        "tail": "Deep contextualized word representations"
      },
      {
        "head": "Christopher Clark",
        "relation": "author_of",
        "tail": "Deep contextualized word representations"
      },
      {
        "head": "Kenton Lee",
        "relation": "author_of",
        "tail": "Deep contextualized word representations"
      },
      {
        "head": "Luke Zettlemoyer",
        "relation": "author_of",
        "tail": "Deep contextualized word representations"
      },
      {
        "head": "DeepSeek-AI",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Daya Guo",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Dejian Yang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Haowei Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Junxiao Song",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Peiyi Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Qihao Zhu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Runxin Xu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Ruoyu Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Shirong Ma",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xiao Bi",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xiaokang Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xingkai Yu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yu Wu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Z. F. Wu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zhibin Gou",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zhihong Shao",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zhuoshu Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Ziyi Gao",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Aixin Liu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Bing Xue",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Bingxuan Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Bochao Wu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Bei Feng",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Chengda Lu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Chenggang Zhao",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Chengqi Deng",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Chenyu Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Chong Ruan",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Damai Dai",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Deli Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Dongjie Ji",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Erhang Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Fangyun Lin",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Fucong Dai",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Fuli Luo",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Guangbo Hao",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Guanting Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Guowei Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "H. Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Han Bao",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Hanwei Xu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Haocheng Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Honghui Ding",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Huajian Xin",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Huazuo Gao",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Hui Qu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Hui Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Jianzhong Guo",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Jiashi Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Jiawei Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Jingchang Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Jingyang Yuan",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Junjie Qiu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Junlong Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "J. L. Cai",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Jiaqi Ni",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Jian Liang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Jin Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Kai Dong",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Kai Hu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Kaige Gao",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Kang Guan",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Kexin Huang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Kuai Yu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Lean Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Lecong Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Liang Zhao",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Litong Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Liyue Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Lei Xu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Leyi Xia",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Mingchuan Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Minghua Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Minghui Tang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Meng Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Miaojun Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Mingming Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Ning Tian",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Panpan Huang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Peng Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Qiancheng Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Qinyu Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Qiushi Du",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Ruiqi Ge",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Ruisong Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Ruizhe Pan",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Runji Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "R. J. Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "R. L. Jin",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Ruyi Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Shanghao Lu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Shangyan Zhou",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Shanhuang Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Shengfeng Ye",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Shiyu Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Shuiping Yu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Shunfeng Zhou",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Shuting Pan",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "S. S. Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Shuang Zhou",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Shaoqing Wu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Shengfeng Ye",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Tao Yun",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Tian Pei",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Tianyu Sun",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "T. Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Wangding Zeng",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Wanjia Zhao",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Wen Liu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Wenfeng Liang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Wenjun Gao",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Wenqin Yu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Wentao Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "W. L. Xiao",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Wei An",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xiaodong Liu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xiaohan Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xiaokang Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xiaotao Nie",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xin Cheng",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xin Liu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xin Xie",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xingchao Liu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xinyu Yang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xinyuan Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xuecheng Su",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xuheng Lin",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "X. Q. Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xiangyue Jin",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xiaojin Shen",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xiaosha Chen",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xiaowen Sun",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xiaoxiang Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xinnan Song",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xinyi Zhou",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xianzu Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Xinxia Shan",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Y. K. Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Y. Q. Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Y. X. Wei",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yang Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yanhong Xu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yao Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yao Zhao",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yaofeng Sun",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yaohui Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yi Yu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yichao Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yifan Shi",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yiliang Xiong",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Ying He",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yishi Piao",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yisong Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yixuan Tan",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yiyang Ma",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yiyuan Liu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yongqiang Guo",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yuan Ou",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yuduan Wang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yue Gong",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yuheng Zou",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yujia He",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yunfan Xiong",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yuxiang Luo",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yuxiang You",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yuxuan Liu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yuyang Zhou",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Y. X. Zhu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yanhong Xu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yanping Huang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yaohui Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yi Zheng",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yuchen Zhu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yunxian Ma",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Ying Tang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yukun Zha",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Yuting Yan",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Z. Z. Ren",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zehui Ren",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zhangli Sha",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zhe Fu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zhean Xu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zhenda Xie",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zhengyan Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zhewen Hao",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zhicheng Ma",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zhigang Yan",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zhiyu Wu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zihui Gu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zijia Zhu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zijun Liu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zilin Li",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Ziwei Xie",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Ziyang Song",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zizheng Pan",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zhen Huang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zhipeng Xu",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zhongyu Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Zhen Zhang",
        "relation": "author_of",
        "tail": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "head": "Jonathan Long",
        "relation": "author_of",
        "tail": "Fully Convolutional Networks for Semantic Segmentation"
      },
      {
        "head": "Evan Shelhamer",
        "relation": "author_of",
        "tail": "Fully Convolutional Networks for Semantic Segmentation"
      },
      {
        "head": "Trevor Darrell",
        "relation": "author_of",
        "tail": "Fully Convolutional Networks for Semantic Segmentation"
      },
      {
        "head": "Jiaming Zhang",
        "relation": "author_of",
        "tail": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with Transformers"
      },
      {
        "head": "Huayao Liu",
        "relation": "author_of",
        "tail": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with Transformers"
      },
      {
        "head": "Kailun Yang",
        "relation": "author_of",
        "tail": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with Transformers"
      },
      {
        "head": "Xinxin Hu",
        "relation": "author_of",
        "tail": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with Transformers"
      },
      {
        "head": "Ruiping Liu",
        "relation": "author_of",
        "tail": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with Transformers"
      },
      {
        "head": "Rainer Stiefelhagen",
        "relation": "author_of",
        "tail": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with Transformers"
      },
      {
        "head": "Cheng Chi",
        "relation": "author_of",
        "tail": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion"
      },
      {
        "head": "Zhenjia Xu",
        "relation": "author_of",
        "tail": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion"
      },
      {
        "head": "Siyuan Feng",
        "relation": "author_of",
        "tail": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion"
      },
      {
        "head": "Eric Cousineau",
        "relation": "author_of",
        "tail": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion"
      },
      {
        "head": "Yilun Du",
        "relation": "author_of",
        "tail": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion"
      },
      {
        "head": "Benjamin Burchfiel",
        "relation": "author_of",
        "tail": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion"
      },
      {
        "head": "Russ Tedrake",
        "relation": "author_of",
        "tail": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion"
      },
      {
        "head": "Shuran Song",
        "relation": "author_of",
        "tail": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion"
      },
      {
        "head": "Yang Liu",
        "relation": "author_of",
        "tail": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI"
      },
      {
        "head": "Weixing Chen",
        "relation": "author_of",
        "tail": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI"
      },
      {
        "head": "Yongjie Bai",
        "relation": "author_of",
        "tail": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI"
      },
      {
        "head": "Xiaodan Liang",
        "relation": "author_of",
        "tail": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI"
      },
      {
        "head": "Guanbin Li",
        "relation": "author_of",
        "tail": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI"
      },
      {
        "head": "Wen Gao",
        "relation": "author_of",
        "tail": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI"
      },
      {
        "head": "Liang Lin",
        "relation": "author_of",
        "tail": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI"
      },
      {
        "head": "Xuanchi Ren",
        "relation": "author_of",
        "tail": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control"
      },
      {
        "head": "Tianchang Shen",
        "relation": "author_of",
        "tail": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control"
      },
      {
        "head": "Jiahui Huang",
        "relation": "author_of",
        "tail": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control"
      },
      {
        "head": "Huan Ling",
        "relation": "author_of",
        "tail": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control"
      },
      {
        "head": "Yifan Lu",
        "relation": "author_of",
        "tail": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control"
      },
      {
        "head": "Merlin Nimier-David",
        "relation": "author_of",
        "tail": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control"
      },
      {
        "head": "Thomas Müller",
        "relation": "author_of",
        "tail": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control"
      },
      {
        "head": "Alexander Keller",
        "relation": "author_of",
        "tail": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control"
      },
      {
        "head": "Sanja Fidler",
        "relation": "author_of",
        "tail": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control"
      },
      {
        "head": "Jun Gao",
        "relation": "author_of",
        "tail": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control"
      },
      {
        "head": "Shangzhan Zhang",
        "relation": "author_of",
        "tail": "FLARE: Feed-forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views"
      },
      {
        "head": "Jianyuan Wang",
        "relation": "author_of",
        "tail": "FLARE: Feed-forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views"
      },
      {
        "head": "Yinghao Xu",
        "relation": "author_of",
        "tail": "FLARE: Feed-forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views"
      },
      {
        "head": "Nan Xue",
        "relation": "author_of",
        "tail": "FLARE: Feed-forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views"
      },
      {
        "head": "Christian Rupprecht",
        "relation": "author_of",
        "tail": "FLARE: Feed-forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views"
      },
      {
        "head": "Xiaowei Zhou",
        "relation": "author_of",
        "tail": "FLARE: Feed-forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views"
      },
      {
        "head": "Yujun Shen",
        "relation": "author_of",
        "tail": "FLARE: Feed-forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views"
      },
      {
        "head": "Gordon Wetzstein",
        "relation": "author_of",
        "tail": "FLARE: Feed-forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views"
      },
      {
        "head": "Attention Is All You Need",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Attention Is All You Need",
        "relation": "cites",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "Attention Is All You Need",
        "relation": "cites",
        "tail": "Long Short-Term Memory"
      },
      {
        "head": "Attention Is All You Need",
        "relation": "cites",
        "tail": "Dropout: a simple way to prevent neural networks from overfitting"
      },
      {
        "head": "Attention Is All You Need",
        "relation": "cites",
        "tail": "Rethinking the Inception Architecture for Computer Vision"
      },
      {
        "head": "A comprehensive review of recommender systems: Transitioning from theory to practice",
        "relation": "cites",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
        "relation": "cites",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection",
        "relation": "cites",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation",
        "relation": "cites",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "DiffusionEngine: Diffusion model is scalable data engine for object detection",
        "relation": "cites",
        "tail": "Attention Is All You Need"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "cites",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "cites",
        "tail": "Long Short-Term Memory"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "cites",
        "tail": "Et al"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "cites",
        "tail": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
      },
      {
        "head": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "DiffusionEngine: Diffusion model is scalable data engine for object detection",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Noise Perception Self-Supervised Learning for Unsupervised Person Re-Identification",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Adam: A Method for Stochastic Optimization",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "Adam: A Method for Stochastic Optimization",
        "relation": "cites",
        "tail": "Auto-Encoding Variational Bayes"
      },
      {
        "head": "Adam: A Method for Stochastic Optimization",
        "relation": "cites",
        "tail": "Supporting Online Material for Reducing the Dimensionality of Data with Neural Networks"
      },
      {
        "head": "Adam: A Method for Stochastic Optimization",
        "relation": "cites",
        "tail": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"
      },
      {
        "head": "Adam: A Method for Stochastic Optimization",
        "relation": "cites",
        "tail": "Speech recognition with deep recurrent neural networks"
      },
      {
        "head": "Weakly Supervised Image Dehazing via Physics-Based Decomposition",
        "relation": "cites",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer",
        "relation": "cites",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
        "relation": "cites",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction",
        "relation": "cites",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model",
        "relation": "cites",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "Rethinking the Inception Architecture for Computer Vision",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "Rethinking the Inception Architecture for Computer Vision",
        "relation": "cites",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "Rethinking the Inception Architecture for Computer Vision",
        "relation": "cites",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "Rethinking the Inception Architecture for Computer Vision",
        "relation": "cites",
        "tail": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
      },
      {
        "head": "Rethinking the Inception Architecture for Computer Vision",
        "relation": "cites",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Diffusion Transformers with Representation Autoencoders",
        "relation": "cites",
        "tail": "Rethinking the Inception Architecture for Computer Vision"
      },
      {
        "head": "Scaling Large Language Models for Next-Generation Single-Cell Analysis",
        "relation": "cites",
        "tail": "Rethinking the Inception Architecture for Computer Vision"
      },
      {
        "head": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights",
        "relation": "cites",
        "tail": "Rethinking the Inception Architecture for Computer Vision"
      },
      {
        "head": "HybridVisionNet: An advanced hybrid deep learning framework for automated multi-class ocular disease diagnosis using fundus imaging",
        "relation": "cites",
        "tail": "Rethinking the Inception Architecture for Computer Vision"
      },
      {
        "head": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot",
        "relation": "cites",
        "tail": "Rethinking the Inception Architecture for Computer Vision"
      },
      {
        "head": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
        "relation": "cites",
        "tail": "Attention is All you Need"
      },
      {
        "head": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
        "relation": "cites",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
        "relation": "cites",
        "tail": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"
      },
      {
        "head": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
        "relation": "cites",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
        "relation": "cites",
        "tail": "CARLA: An Open Urban Driving Simulator"
      },
      {
        "head": "Sora as a World Model? A Complete Survey on Text-to-Video Generation",
        "relation": "cites",
        "tail": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation"
      },
      {
        "head": "OmniNWM: Omniscient Driving Navigation World Models",
        "relation": "cites",
        "tail": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation"
      },
      {
        "head": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask",
        "relation": "cites",
        "tail": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation"
      },
      {
        "head": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving",
        "relation": "cites",
        "tail": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation"
      },
      {
        "head": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
        "relation": "cites",
        "tail": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation"
      },
      {
        "head": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
        "relation": "cites",
        "tail": "I and J"
      },
      {
        "head": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
        "relation": "cites",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
        "relation": "cites",
        "tail": "A and V"
      },
      {
        "head": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
        "relation": "cites",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "What matters for Representation Alignment: Global Information or Spatial Structure?",
        "relation": "cites",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "SCB-DETR: Multiscale Deformable Transformers for Occlusion-Resilient Student Learning Behavior Detection in Smart Classroom",
        "relation": "cites",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination",
        "relation": "cites",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "Bio-Inspired Fine-Tuning for Selective Transfer Learning in Image Classification",
        "relation": "cites",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "Hand Sign Language Detection Using Deep Learning",
        "relation": "cites",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
        "relation": "cites",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
        "relation": "cites",
        "tail": "and as an in"
      },
      {
        "head": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
        "relation": "cites",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects",
        "relation": "cites",
        "tail": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
      },
      {
        "head": "FANet: Frequency-Aware Attention-Based Tiny-Object Detection in Remote Sensing Images",
        "relation": "cites",
        "tail": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
      },
      {
        "head": "UAV-based multimodal object detection via feature enhancement and dynamic gated fusion",
        "relation": "cites",
        "tail": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
      },
      {
        "head": "An empirical analysis of deep learning methods for small object detection from satellite imagery",
        "relation": "cites",
        "tail": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
      },
      {
        "head": "DiffusionEngine: Diffusion model is scalable data engine for object detection",
        "relation": "cites",
        "tail": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
      },
      {
        "head": "Auto-Encoding Variational Bayes",
        "relation": "cites",
        "tail": "Representation Learning: A Review and New Perspectives"
      },
      {
        "head": "Auto-Encoding Variational Bayes",
        "relation": "cites",
        "tail": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"
      },
      {
        "head": "Auto-Encoding Variational Bayes",
        "relation": "cites",
        "tail": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion"
      },
      {
        "head": "Auto-Encoding Variational Bayes",
        "relation": "cites",
        "tail": "In Advances in Neural Information Processing Systems"
      },
      {
        "head": "Auto-Encoding Variational Bayes",
        "relation": "cites",
        "tail": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models"
      },
      {
        "head": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation",
        "relation": "cites",
        "tail": "Auto-Encoding Variational Bayes"
      },
      {
        "head": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions",
        "relation": "cites",
        "tail": "Auto-Encoding Variational Bayes"
      },
      {
        "head": "GenAD: Generative End-to-End Autonomous Driving",
        "relation": "cites",
        "tail": "Auto-Encoding Variational Bayes"
      },
      {
        "head": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model",
        "relation": "cites",
        "tail": "Auto-Encoding Variational Bayes"
      },
      {
        "head": "Improving Video Generation with Human Feedback",
        "relation": "cites",
        "tail": "Auto-Encoding Variational Bayes"
      },
      {
        "head": "Speech Recognition with Deep Recurrent Neural Networks",
        "relation": "cites",
        "tail": "Long Short-Term Memory"
      },
      {
        "head": "Speech Recognition with Deep Recurrent Neural Networks",
        "relation": "cites",
        "tail": "Learning representations by back-propagating errors"
      },
      {
        "head": "Speech Recognition with Deep Recurrent Neural Networks",
        "relation": "cites",
        "tail": "Bidirectional recurrent neural networks"
      },
      {
        "head": "Speech Recognition with Deep Recurrent Neural Networks",
        "relation": "cites",
        "tail": "Connectionist Temporal Classiﬁcation: Labelling Unsegmented Sequence Data with Recurrent Neural Networks"
      },
      {
        "head": "Speech Recognition with Deep Recurrent Neural Networks",
        "relation": "cites",
        "tail": "2005 Special Issue: Framewise phoneme classification with bidirectional LSTM and other neural network architectures"
      },
      {
        "head": "A high-performance neuroprosthesis for speech decoding and avatar control",
        "relation": "cites",
        "tail": "Speech Recognition with Deep Recurrent Neural Networks"
      },
      {
        "head": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses",
        "relation": "cites",
        "tail": "Speech Recognition with Deep Recurrent Neural Networks"
      },
      {
        "head": "A high-performance speech neuroprosthesis",
        "relation": "cites",
        "tail": "Speech Recognition with Deep Recurrent Neural Networks"
      },
      {
        "head": "Loss of plasticity in deep continual learning",
        "relation": "cites",
        "tail": "Speech Recognition with Deep Recurrent Neural Networks"
      },
      {
        "head": "An analog-AI chip for energy-efficient speech recognition and transcription",
        "relation": "cites",
        "tail": "Speech Recognition with Deep Recurrent Neural Networks"
      },
      {
        "head": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
        "relation": "cites",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
        "relation": "cites",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
        "relation": "cites",
        "tail": "A Mathematical Theory of Communication"
      },
      {
        "head": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
        "relation": "cites",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers",
        "relation": "cites",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling",
        "relation": "cites",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram",
        "relation": "cites",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "L$^3$: Large Lookup Layers",
        "relation": "cites",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Scaling Embeddings Outperforms Scaling Experts in Language Models",
        "relation": "cites",
        "tail": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
      },
      {
        "head": "Going Deeper with Convolutions",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "Going Deeper with Convolutions",
        "relation": "cites",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "Going Deeper with Convolutions",
        "relation": "cites",
        "tail": "Gradient-based learning applied to document recognition"
      },
      {
        "head": "Going Deeper with Convolutions",
        "relation": "cites",
        "tail": "Regression Shrinkage and Selection via the Lasso"
      },
      {
        "head": "Going Deeper with Convolutions",
        "relation": "cites",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "LifeCLEF Plant Identification Task 2015",
        "relation": "cites",
        "tail": "Going Deeper with Convolutions"
      },
      {
        "head": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics",
        "relation": "cites",
        "tail": "Going Deeper with Convolutions"
      },
      {
        "head": "Enhancing pine wilt disease detection with synthetic data and external attention-based transformers",
        "relation": "cites",
        "tail": "Going Deeper with Convolutions"
      },
      {
        "head": "A comprehensive review on YOLO versions for object detection",
        "relation": "cites",
        "tail": "Going Deeper with Convolutions"
      },
      {
        "head": "Harnessing large vision and language models in agriculture: a review",
        "relation": "cites",
        "tail": "Going Deeper with Convolutions"
      },
      {
        "head": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
        "relation": "cites",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
        "relation": "cites",
        "tail": "Gradient-based learning applied to document recognition"
      },
      {
        "head": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
        "relation": "cites",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "A comprehensive review on YOLO versions for object detection",
        "relation": "cites",
        "tail": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
      },
      {
        "head": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer",
        "relation": "cites",
        "tail": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
      },
      {
        "head": "Multi-axis vision transformer for medical image segmentation",
        "relation": "cites",
        "tail": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
      },
      {
        "head": "A comprehensive review of facial beauty prediction using deep learning techniques",
        "relation": "cites",
        "tail": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
      },
      {
        "head": "A systematic comparison of predictive models on the retina",
        "relation": "cites",
        "tail": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
      },
      {
        "head": "ImageNet Large Scale Visual Recognition Challenge",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "ImageNet Large Scale Visual Recognition Challenge",
        "relation": "cites",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "ImageNet Large Scale Visual Recognition Challenge",
        "relation": "cites",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "ImageNet Large Scale Visual Recognition Challenge",
        "relation": "cites",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "ImageNet Large Scale Visual Recognition Challenge",
        "relation": "cites",
        "tail": "Distinctive Image Features from Scale-Invariant Keypoints"
      },
      {
        "head": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training",
        "relation": "cites",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Diffusion Transformers with Representation Autoencoders",
        "relation": "cites",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications",
        "relation": "cites",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "A comprehensive review on YOLO versions for object detection",
        "relation": "cites",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Aligning machine and human visual representations across abstraction levels",
        "relation": "cites",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Diffusion Transformers with Representation Autoencoders",
        "relation": "cites",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "Diffusion Transformers with Representation Autoencoders",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Diffusion Transformers with Representation Autoencoders",
        "relation": "cites",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Diffusion Transformers with Representation Autoencoders",
        "relation": "cites",
        "tail": "GENERATIVE ADVERSARIAL NETS"
      },
      {
        "head": "Diffusion Transformers with Representation Autoencoders",
        "relation": "cites",
        "tail": "Rethinking the Inception Architecture for Computer Vision"
      },
      {
        "head": "Improved Mean Flows: On the Challenges of Fastforward Generative Models",
        "relation": "cites",
        "tail": "Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training",
        "relation": "cites",
        "tail": "Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
        "relation": "cites",
        "tail": "Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "PixelDiT: Pixel Diffusion Transformers for Image Generation",
        "relation": "cites",
        "tail": "Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models",
        "relation": "cites",
        "tail": "Diffusion Transformers with Representation Autoencoders"
      },
      {
        "head": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "relation": "cites",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "relation": "cites",
        "tail": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "relation": "cites",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "relation": "cites",
        "tail": "Dropout: a simple way to prevent neural networks from overfitting"
      },
      {
        "head": "A comprehensive review of object detection with traditional and deep learning methods",
        "relation": "cites",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "Diffusion Language Models are Super Data Learners",
        "relation": "cites",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas",
        "relation": "cites",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "FLEX: Continuous Agent Evolution via Forward Learning from Experience",
        "relation": "cites",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning",
        "relation": "cites",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "nuScenes: A multimodal dataset for autonomous driving",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "nuScenes: A multimodal dataset for autonomous driving",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "nuScenes: A multimodal dataset for autonomous driving",
        "relation": "cites",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "nuScenes: A multimodal dataset for autonomous driving",
        "relation": "cites",
        "tail": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
      },
      {
        "head": "nuScenes: A multimodal dataset for autonomous driving",
        "relation": "cites",
        "tail": "Histograms of oriented gradients for human detection"
      },
      {
        "head": "Controllable Video Generation: A Survey",
        "relation": "cites",
        "tail": "nuScenes: A multimodal dataset for autonomous driving"
      },
      {
        "head": "Adversarial Attacks on Autonomous Driving Systems in the Physical World: A Survey",
        "relation": "cites",
        "tail": "nuScenes: A multimodal dataset for autonomous driving"
      },
      {
        "head": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail",
        "relation": "cites",
        "tail": "nuScenes: A multimodal dataset for autonomous driving"
      },
      {
        "head": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes",
        "relation": "cites",
        "tail": "nuScenes: A multimodal dataset for autonomous driving"
      },
      {
        "head": "Detect Anything via Next Point Prediction",
        "relation": "cites",
        "tail": "nuScenes: A multimodal dataset for autonomous driving"
      },
      {
        "head": "CARLA: An Open Urban Driving Simulator",
        "relation": "cites",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "CARLA: An Open Urban Driving Simulator",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "CARLA: An Open Urban Driving Simulator",
        "relation": "cites",
        "tail": "Human-level control through deep reinforcement learning"
      },
      {
        "head": "CARLA: An Open Urban Driving Simulator",
        "relation": "cites",
        "tail": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "head": "CARLA: An Open Urban Driving Simulator",
        "relation": "cites",
        "tail": "RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation"
      },
      {
        "head": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving",
        "relation": "cites",
        "tail": "CARLA: An Open Urban Driving Simulator"
      },
      {
        "head": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving",
        "relation": "cites",
        "tail": "CARLA: An Open Urban Driving Simulator"
      },
      {
        "head": "Pseudo-Simulation for Autonomous Driving",
        "relation": "cites",
        "tail": "CARLA: An Open Urban Driving Simulator"
      },
      {
        "head": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving",
        "relation": "cites",
        "tail": "CARLA: An Open Urban Driving Simulator"
      },
      {
        "head": "A Survey on Vision-Language-Action Models for Autonomous Driving",
        "relation": "cites",
        "tail": "CARLA: An Open Urban Driving Simulator"
      },
      {
        "head": "Sora as a World Model? A Complete Survey on Text-to-Video Generation",
        "relation": "cites",
        "tail": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "Sora as a World Model? A Complete Survey on Text-to-Video Generation",
        "relation": "cites",
        "tail": "Long Short-Term Memory"
      },
      {
        "head": "Sora as a World Model? A Complete Survey on Text-to-Video Generation",
        "relation": "cites",
        "tail": "Characterization of Models for Identifying Physical and Cognitive Frailty in Older Adults With Diabetes: Systematic Review and Meta-Analysis"
      },
      {
        "head": "Sora as a World Model? A Complete Survey on Text-to-Video Generation",
        "relation": "cites",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "Sora as a World Model? A Complete Survey on Text-to-Video Generation",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond",
        "relation": "cites",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Understanding World or Predicting Future? A Comprehensive Survey of World Models",
        "relation": "cites",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "A Survey of Multimodal Learning: Methods, Applications, and Future",
        "relation": "cites",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey",
        "relation": "cites",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "Effects of Generative AI in Tourism Industry",
        "relation": "cites",
        "tail": "Sora as a World Model? A Complete Survey on Text-to-Video Generation"
      },
      {
        "head": "OmniNWM: Omniscient Driving Navigation World Models",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "OmniNWM: Omniscient Driving Navigation World Models",
        "relation": "cites",
        "tail": "Squeeze-and-Excitation Networks"
      },
      {
        "head": "OmniNWM: Omniscient Driving Navigation World Models",
        "relation": "cites",
        "tail": "Decoupled Weight Decay Regularization"
      },
      {
        "head": "OmniNWM: Omniscient Driving Navigation World Models",
        "relation": "cites",
        "tail": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"
      },
      {
        "head": "OmniNWM: Omniscient Driving Navigation World Models",
        "relation": "cites",
        "tail": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"
      },
      {
        "head": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control",
        "relation": "cites",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World",
        "relation": "cites",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "DVGT: Driving Visual Geometry Transformer",
        "relation": "cites",
        "tail": "OmniNWM: Omniscient Driving Navigation World Models"
      },
      {
        "head": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask",
        "relation": "cites",
        "tail": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"
      },
      {
        "head": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask",
        "relation": "cites",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask",
        "relation": "cites",
        "tail": "Adding Conditional Control to Text-to-Image Diffusion Models"
      },
      {
        "head": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask",
        "relation": "cites",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving",
        "relation": "cites",
        "tail": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "head": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving",
        "relation": "cites",
        "tail": "AUTO-ENCODING VARIATIONAL BAYES"
      },
      {
        "head": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving",
        "relation": "cites",
        "tail": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"
      },
      {
        "head": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving",
        "relation": "cites",
        "tail": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric"
      },
      {
        "head": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving",
        "relation": "cites",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
        "relation": "cites",
        "tail": "Decoupled Weight Decay Regularization"
      },
      {
        "head": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
        "relation": "cites",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
        "relation": "cites",
        "tail": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"
      },
      {
        "head": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
        "relation": "cites",
        "tail": "CARLA: An Open Urban Driving Simulator"
      },
      {
        "head": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
        "relation": "cites",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "What matters for Representation Alignment: Global Information or Spatial Structure?",
        "relation": "cites",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "What matters for Representation Alignment: Global Information or Spatial Structure?",
        "relation": "cites",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "What matters for Representation Alignment: Global Information or Spatial Structure?",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "What matters for Representation Alignment: Global Information or Spatial Structure?",
        "relation": "cites",
        "tail": "Histograms of oriented gradients for human detection"
      },
      {
        "head": "What matters for Representation Alignment: Global Information or Spatial Structure?",
        "relation": "cites",
        "tail": "Generative Adversarial Networks"
      },
      {
        "head": "Generalization of Diffusion Models Arises with a Balanced Representation Space",
        "relation": "cites",
        "tail": "What matters for Representation Alignment: Global Information or Spatial Structure?"
      },
      {
        "head": "Stable Velocity: A Variance Perspective on Flow Matching",
        "relation": "cites",
        "tail": "What matters for Representation Alignment: Global Information or Spatial Structure?"
      },
      {
        "head": "Laminating Representation Autoencoders for Efficient Diffusion",
        "relation": "cites",
        "tail": "What matters for Representation Alignment: Global Information or Spatial Structure?"
      },
      {
        "head": "Adaptive 1D Video Diffusion Autoencoder",
        "relation": "cites",
        "tail": "What matters for Representation Alignment: Global Information or Spatial Structure?"
      },
      {
        "head": "Test-Time Conditioning with Representation-Aligned Visual Features",
        "relation": "cites",
        "tail": "What matters for Representation Alignment: Global Information or Spatial Structure?"
      },
      {
        "head": "Bio-inspired fine-tuning for selective transfer learning in image classification",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Bio-inspired fine-tuning for selective transfer learning in image classification",
        "relation": "cites",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "Bio-inspired fine-tuning for selective transfer learning in image classification",
        "relation": "cites",
        "tail": "Gradient-based learning applied to document recognition"
      },
      {
        "head": "Bio-inspired fine-tuning for selective transfer learning in image classification",
        "relation": "cites",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "Bio-inspired fine-tuning for selective transfer learning in image classification",
        "relation": "cites",
        "tail": "Densely Connected Convolutional Networks"
      },
      {
        "head": "An extratropical cyclone center location method on satellite images based on transfer learning",
        "relation": "cites",
        "tail": "Bio-inspired fine-tuning for selective transfer learning in image classification"
      },
      {
        "head": "An Advanced Convolutional Neural Network Architecture Utilizing Transfer Learning for Melanoma Detection",
        "relation": "cites",
        "tail": "Bio-inspired fine-tuning for selective transfer learning in image classification"
      },
      {
        "head": "Adaptive Transfer Learning for Surgical Tool Presence Detection in Laparoscopic Videos Through Gradual Freezing Fine‐Tuning",
        "relation": "cites",
        "tail": "Bio-inspired fine-tuning for selective transfer learning in image classification"
      },
      {
        "head": "VGG Induced Deep Hand Sign Language Detection",
        "relation": "cites",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "VGG Induced Deep Hand Sign Language Detection",
        "relation": "cites",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "VGG Induced Deep Hand Sign Language Detection",
        "relation": "cites",
        "tail": "MediaPipe: A Framework for Building Perception Pipelines"
      },
      {
        "head": "VGG Induced Deep Hand Sign Language Detection",
        "relation": "cites",
        "tail": "Hand Pose Estimation and Hand Shape Classification Using Multi-layered Randomized Decision Forests"
      },
      {
        "head": "VGG Induced Deep Hand Sign Language Detection",
        "relation": "cites",
        "tail": "A robust static hand gesture recognition system using geometry based normalizations and Krawtchouk moments"
      },
      {
        "head": "Hand signal classification system for sign language communication in Virtual Reality",
        "relation": "cites",
        "tail": "VGG Induced Deep Hand Sign Language Detection"
      },
      {
        "head": "Real-Time Static Hand Sign Recognition System using MediaPipe and Fully Connected Neural Network",
        "relation": "cites",
        "tail": "VGG Induced Deep Hand Sign Language Detection"
      },
      {
        "head": "Real-Time Gesture Recognition to Aid Communication in Children with Motor Impairments",
        "relation": "cites",
        "tail": "VGG Induced Deep Hand Sign Language Detection"
      },
      {
        "head": "Microsoft COCO: Common Objects in Context",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "Microsoft COCO: Common Objects in Context",
        "relation": "cites",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "Microsoft COCO: Common Objects in Context",
        "relation": "cites",
        "tail": "Learning Multiple Layers of Features from Tiny Images"
      },
      {
        "head": "Microsoft COCO: Common Objects in Context",
        "relation": "cites",
        "tail": "Histograms of oriented gradients for human detection"
      },
      {
        "head": "Microsoft COCO: Common Objects in Context",
        "relation": "cites",
        "tail": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation"
      },
      {
        "head": "MuMu-LLaMA: Multi-modal music understanding and generation via large language models",
        "relation": "cites",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy",
        "relation": "cites",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism",
        "relation": "cites",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
        "relation": "cites",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness",
        "relation": "cites",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Representation Learning: A Review and New Perspectives",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "Representation Learning: A Review and New Perspectives",
        "relation": "cites",
        "tail": "Gradient-based learning applied to document recognition"
      },
      {
        "head": "Representation Learning: A Review and New Perspectives",
        "relation": "cites",
        "tail": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
      },
      {
        "head": "Representation Learning: A Review and New Perspectives",
        "relation": "cites",
        "tail": "Visualizing Data using t-SNE"
      },
      {
        "head": "Representation Learning: A Review and New Perspectives",
        "relation": "cites",
        "tail": "Learning Multiple Layers of Features from Tiny Images"
      },
      {
        "head": "LLM Social Simulations Are a Promising Research Method",
        "relation": "cites",
        "tail": "Representation Learning: A Review and New Perspectives"
      },
      {
        "head": "Federated Contrastive Learning With Feature-Based Distillation for Human Activity Recognition",
        "relation": "cites",
        "tail": "Representation Learning: A Review and New Perspectives"
      },
      {
        "head": "Diffuse and Disperse: Image Generation with Representation Regularization",
        "relation": "cites",
        "tail": "Representation Learning: A Review and New Perspectives"
      },
      {
        "head": "Deep Autoencoder Neural Networks: A Comprehensive Review and New Perspectives",
        "relation": "cites",
        "tail": "Representation Learning: A Review and New Perspectives"
      },
      {
        "head": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems",
        "relation": "cites",
        "tail": "Representation Learning: A Review and New Perspectives"
      },
      {
        "head": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation",
        "relation": "cites",
        "tail": "U-Net: Convolutional Networks for Biomedical Image Segmentation"
      },
      {
        "head": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation",
        "relation": "cites",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation",
        "relation": "cites",
        "tail": "Generative Adversarial Networks"
      },
      {
        "head": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation",
        "relation": "cites",
        "tail": "Decoupled Weight Decay Regularization"
      },
      {
        "head": "Improved Distribution Matching Distillation for Fast Image Synthesis",
        "relation": "cites",
        "tail": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation"
      },
      {
        "head": "Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation",
        "relation": "cites",
        "tail": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation"
      },
      {
        "head": "Evolutionary optimization of model merging recipes",
        "relation": "cites",
        "tail": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation"
      },
      {
        "head": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
        "relation": "cites",
        "tail": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation"
      },
      {
        "head": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment",
        "relation": "cites",
        "tail": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation"
      },
      {
        "head": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions",
        "relation": "cites",
        "tail": "U-Net: Convolutional Networks for Biomedical Image Segmentation"
      },
      {
        "head": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions",
        "relation": "cites",
        "tail": "Denoising Diffusion Probabilistic Models"
      },
      {
        "head": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions",
        "relation": "cites",
        "tail": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "head": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions",
        "relation": "cites",
        "tail": "Auto-Encoding Variational Bayes"
      },
      {
        "head": "HunyuanVideo: A Systematic Framework For Large Video Generative Models",
        "relation": "cites",
        "tail": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions"
      },
      {
        "head": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency",
        "relation": "cites",
        "tail": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions"
      },
      {
        "head": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models",
        "relation": "cites",
        "tail": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions"
      },
      {
        "head": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation",
        "relation": "cites",
        "tail": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions"
      },
      {
        "head": "EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation",
        "relation": "cites",
        "tail": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions"
      },
      {
        "head": "GenAD: Generative End-to-End Autonomous Driving",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "GenAD: Generative End-to-End Autonomous Driving",
        "relation": "cites",
        "tail": "Attention is All you Need"
      },
      {
        "head": "GenAD: Generative End-to-End Autonomous Driving",
        "relation": "cites",
        "tail": "Decoupled Weight Decay Regularization"
      },
      {
        "head": "GenAD: Generative End-to-End Autonomous Driving",
        "relation": "cites",
        "tail": "Feature Pyramid Networks for Object Detection"
      },
      {
        "head": "GenAD: Generative End-to-End Autonomous Driving",
        "relation": "cites",
        "tail": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"
      },
      {
        "head": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving",
        "relation": "cites",
        "tail": "GenAD: Generative End-to-End Autonomous Driving"
      },
      {
        "head": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability",
        "relation": "cites",
        "tail": "GenAD: Generative End-to-End Autonomous Driving"
      },
      {
        "head": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving",
        "relation": "cites",
        "tail": "GenAD: Generative End-to-End Autonomous Driving"
      },
      {
        "head": "How Far is Video Generation from World Model: A Physical Law Perspective",
        "relation": "cites",
        "tail": "GenAD: Generative End-to-End Autonomous Driving"
      },
      {
        "head": "EMMA: End-to-End Multimodal Model for Autonomous Driving",
        "relation": "cites",
        "tail": "GenAD: Generative End-to-End Autonomous Driving"
      },
      {
        "head": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model",
        "relation": "cites",
        "tail": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model",
        "relation": "cites",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model",
        "relation": "cites",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model",
        "relation": "cites",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling",
        "relation": "cites",
        "tail": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model"
      },
      {
        "head": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models",
        "relation": "cites",
        "tail": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model"
      },
      {
        "head": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models",
        "relation": "cites",
        "tail": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model"
      },
      {
        "head": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step",
        "relation": "cites",
        "tail": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model"
      },
      {
        "head": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency",
        "relation": "cites",
        "tail": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model"
      },
      {
        "head": "Improving Video Generation with Human Feedback",
        "relation": "cites",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "Improving Video Generation with Human Feedback",
        "relation": "cites",
        "tail": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
      },
      {
        "head": "Improving Video Generation with Human Feedback",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Improving Video Generation with Human Feedback",
        "relation": "cites",
        "tail": "Denoising Diffusion Probabilistic Models"
      },
      {
        "head": "Improving Video Generation with Human Feedback",
        "relation": "cites",
        "tail": "Proximal Policy Optimization Algorithms"
      },
      {
        "head": "Flow-GRPO: Training Flow Matching Models via Online RL",
        "relation": "cites",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "DanceGRPO: Unleashing GRPO on Visual Generation",
        "relation": "cites",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Seedance 1.0: Exploring the Boundaries of Video Generation Models",
        "relation": "cites",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "SkyReels-V2: Infinite-length Film Generative Model",
        "relation": "cites",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "Unified Reward Model for Multimodal Understanding and Generation",
        "relation": "cites",
        "tail": "Improving Video Generation with Human Feedback"
      },
      {
        "head": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling",
        "relation": "cites",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling",
        "relation": "cites",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling",
        "relation": "cites",
        "tail": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "head": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling",
        "relation": "cites",
        "tail": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling",
        "relation": "cites",
        "tail": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "head": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram",
        "relation": "cites",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram",
        "relation": "cites",
        "tail": "Crystal structure of the nucleosome core particle at 2.8 Å resolution"
      },
      {
        "head": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram",
        "relation": "cites",
        "tail": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"
      },
      {
        "head": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram",
        "relation": "cites",
        "tail": "A catalogue of splice junction sequences."
      },
      {
        "head": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram",
        "relation": "cites",
        "tail": "Origin of the Genetic Code"
      },
      {
        "head": "L$^3$: Large Lookup Layers",
        "relation": "cites",
        "tail": "Decoupled Weight Decay Regularization"
      },
      {
        "head": "L$^3$: Large Lookup Layers",
        "relation": "cites",
        "tail": "SGDR: Stochastic Gradient Descent with Warm Restarts"
      },
      {
        "head": "L$^3$: Large Lookup Layers",
        "relation": "cites",
        "tail": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"
      },
      {
        "head": "L$^3$: Large Lookup Layers",
        "relation": "cites",
        "tail": "Compression of individual sequences via variable-rate coding"
      },
      {
        "head": "L$^3$: Large Lookup Layers",
        "relation": "cites",
        "tail": "Pointer Sentinel Mixture Models"
      },
      {
        "head": "Scaling Embeddings Outperforms Scaling Experts in Language Models",
        "relation": "cites",
        "tail": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "head": "Scaling Embeddings Outperforms Scaling Experts in Language Models",
        "relation": "cites",
        "tail": "Measuring Massive Multitask Language Understanding"
      },
      {
        "head": "Scaling Embeddings Outperforms Scaling Experts in Language Models",
        "relation": "cites",
        "tail": "Let's Verify Step by Step"
      },
      {
        "head": "Scaling Embeddings Outperforms Scaling Experts in Language Models",
        "relation": "cites",
        "tail": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark"
      },
      {
        "head": "Scaling Embeddings Outperforms Scaling Experts in Language Models",
        "relation": "cites",
        "tail": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"
      },
      {
        "head": "LifeCLEF Plant Identification Task 2015",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "LifeCLEF Plant Identification Task 2015",
        "relation": "cites",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "LifeCLEF Plant Identification Task 2015",
        "relation": "cites",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "LifeCLEF Plant Identification Task 2015",
        "relation": "cites",
        "tail": "Pattern Recognition and Machine Learning"
      },
      {
        "head": "LifeCLEF Plant Identification Task 2015",
        "relation": "cites",
        "tail": "Bagging Predictors"
      },
      {
        "head": "Automatic Image-Based Plant Disease Severity Estimation Using Deep Learning",
        "relation": "cites",
        "tail": "LifeCLEF Plant Identification Task 2015"
      },
      {
        "head": "Plant identification using deep neural networks via optimization of transfer learning parameters",
        "relation": "cites",
        "tail": "LifeCLEF Plant Identification Task 2015"
      },
      {
        "head": "New perspectives on plant disease characterization based on deep learning",
        "relation": "cites",
        "tail": "LifeCLEF Plant Identification Task 2015"
      },
      {
        "head": "Deep Learning for Plant Identification in Natural Environment",
        "relation": "cites",
        "tail": "LifeCLEF Plant Identification Task 2015"
      },
      {
        "head": "Going deeper in the automated identification of Herbarium specimens",
        "relation": "cites",
        "tail": "LifeCLEF Plant Identification Task 2015"
      },
      {
        "head": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics",
        "relation": "cites",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics",
        "relation": "cites",
        "tail": "Densely Connected Convolutional Networks"
      },
      {
        "head": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics",
        "relation": "cites",
        "tail": "MobileNetV2: Inverted Residuals and Linear Bottlenecks"
      },
      {
        "head": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics",
        "relation": "cites",
        "tail": "Communication-Efficient Learning of Deep Networks from Decentralized Data"
      },
      {
        "head": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics",
        "relation": "cites",
        "tail": "A Comprehensive Survey on Transfer Learning"
      },
      {
        "head": "Privacy Preserved and Decentralized Smartphone Recommendation System",
        "relation": "cites",
        "tail": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics"
      },
      {
        "head": "A Survey on Heterogeneity Taxonomy, Security and Privacy Preservation in the Integration of IoT, Wireless Sensor Networks and Federated Learning",
        "relation": "cites",
        "tail": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics"
      },
      {
        "head": "A privacy-preserving federated learning with a secure collaborative for malware detection models using Internet of Things resources",
        "relation": "cites",
        "tail": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics"
      },
      {
        "head": "Graph-based deep learning techniques for remote sensing applications: Techniques, taxonomy, and applications - A comprehensive review",
        "relation": "cites",
        "tail": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics"
      },
      {
        "head": "Federated Learning Architectures: A Performance Evaluation With Crop Yield Prediction Application",
        "relation": "cites",
        "tail": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics"
      },
      {
        "head": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training",
        "relation": "cites",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training",
        "relation": "cites",
        "tail": "Segment Anything"
      },
      {
        "head": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training",
        "relation": "cites",
        "tail": "Visual Instruction Tuning"
      },
      {
        "head": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training",
        "relation": "cites",
        "tail": "Improved Baselines with Visual Instruction Tuning"
      },
      {
        "head": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset",
        "relation": "cites",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs",
        "relation": "cites",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views",
        "relation": "cites",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning",
        "relation": "cites",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe",
        "relation": "cites",
        "tail": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training"
      },
      {
        "head": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications",
        "relation": "cites",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications",
        "relation": "cites",
        "tail": "Long Short-Term Memory"
      },
      {
        "head": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications",
        "relation": "cites",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications",
        "relation": "cites",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "Vision-Language-Action (VLA) Models: Concepts, Progress, Applications and Challenges",
        "relation": "cites",
        "tail": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications"
      },
      {
        "head": "Leave No Observation Behind: Real-time Correction for VLA Action Chunks",
        "relation": "cites",
        "tail": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications"
      },
      {
        "head": "Latent Chain-of-Thought World Modeling for End-to-End Driving",
        "relation": "cites",
        "tail": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications"
      },
      {
        "head": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks",
        "relation": "cites",
        "tail": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications"
      },
      {
        "head": "Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models",
        "relation": "cites",
        "tail": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications"
      },
      {
        "head": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
        "relation": "cites",
        "tail": "Attention is All you Need"
      },
      {
        "head": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
        "relation": "cites",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
        "relation": "cites",
        "tail": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation",
        "relation": "cites",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "A comprehensive survey for real-world industrial surface defect detection: Challenges, approaches, and prospects",
        "relation": "cites",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "TransSIL: A Silhouette Cue-Aware Image Classification Framework for Bird Ecological Monitoring Systems",
        "relation": "cites",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "CBRFormer: rendering technology-based transformer for refinement segmentation of bridge crack images",
        "relation": "cites",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation",
        "relation": "cites",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "Learning Transferable Visual Models From Natural Language Supervision",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Learning Transferable Visual Models From Natural Language Supervision",
        "relation": "cites",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Learning Transferable Visual Models From Natural Language Supervision",
        "relation": "cites",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "Learning Transferable Visual Models From Natural Language Supervision",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "Learning Transferable Visual Models From Natural Language Supervision",
        "relation": "cites",
        "tail": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "Deep contrastive learning enables genome-wide virtual screening.",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "LLaVA-based semantic feature modulation diffusion model for underwater image enhancement",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "3DGS-Drag: Dragging Gaussians for Intuitive Point-Based 3D Editing",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Attention mechanisms in neural networks",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Improved Mean Flows: On the Challenges of Fastforward Generative Models",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Improved Mean Flows: On the Challenges of Fastforward Generative Models",
        "relation": "cites",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Improved Mean Flows: On the Challenges of Fastforward Generative Models",
        "relation": "cites",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "Improved Mean Flows: On the Challenges of Fastforward Generative Models",
        "relation": "cites",
        "tail": "Denoising Diffusion Probabilistic Models"
      },
      {
        "head": "Improved Mean Flows: On the Challenges of Fastforward Generative Models",
        "relation": "cites",
        "tail": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "head": "Bidirectional Normalizing Flow: From Data to Noise and Back",
        "relation": "cites",
        "tail": "Improved Mean Flows: On the Challenges of Fastforward Generative Models"
      },
      {
        "head": "One-step Latent-free Image Generation with Pixel Mean Flows",
        "relation": "cites",
        "tail": "Improved Mean Flows: On the Challenges of Fastforward Generative Models"
      },
      {
        "head": "Meta Flow Maps enable scalable reward alignment",
        "relation": "cites",
        "tail": "Improved Mean Flows: On the Challenges of Fastforward Generative Models"
      },
      {
        "head": "Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective",
        "relation": "cites",
        "tail": "Improved Mean Flows: On the Challenges of Fastforward Generative Models"
      },
      {
        "head": "Generative Modeling via Drifting",
        "relation": "cites",
        "tail": "Improved Mean Flows: On the Challenges of Fastforward Generative Models"
      },
      {
        "head": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training",
        "relation": "cites",
        "tail": "Attention is All you Need"
      },
      {
        "head": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training",
        "relation": "cites",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training",
        "relation": "cites",
        "tail": "Denoising Diffusion Probabilistic Models"
      },
      {
        "head": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training",
        "relation": "cites",
        "tail": "A Simple Framework for Contrastive Learning of Visual Representations"
      },
      {
        "head": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training",
        "relation": "cites",
        "tail": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "head": "PixelDiT: Pixel Diffusion Transformers for Image Generation",
        "relation": "cites",
        "tail": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training"
      },
      {
        "head": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning",
        "relation": "cites",
        "tail": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training"
      },
      {
        "head": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints",
        "relation": "cites",
        "tail": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training"
      },
      {
        "head": "One-step Latent-free Image Generation with Pixel Mean Flows",
        "relation": "cites",
        "tail": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training"
      },
      {
        "head": "Generative Modeling via Drifting",
        "relation": "cites",
        "tail": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training"
      },
      {
        "head": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
        "relation": "cites",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
        "relation": "cites",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
        "relation": "cites",
        "tail": "GENERATIVE ADVERSARIAL NETS"
      },
      {
        "head": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
        "relation": "cites",
        "tail": "Denoising Diffusion Probabilistic Models"
      },
      {
        "head": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
        "relation": "cites",
        "tail": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "head": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders",
        "relation": "cites",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "Simulating the Visual World with Artificial Intelligence: A Roadmap",
        "relation": "cites",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks",
        "relation": "cites",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "RecTok: Reconstruction Distillation along Rectified Flow",
        "relation": "cites",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "RePack then Refine: Efficient Diffusion Transformer with Vision Foundation Model",
        "relation": "cites",
        "tail": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"
      },
      {
        "head": "PixelDiT: Pixel Diffusion Transformers for Image Generation",
        "relation": "cites",
        "tail": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "head": "PixelDiT: Pixel Diffusion Transformers for Image Generation",
        "relation": "cites",
        "tail": "Decoupled Weight Decay Regularization"
      },
      {
        "head": "PixelDiT: Pixel Diffusion Transformers for Image Generation",
        "relation": "cites",
        "tail": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "head": "PixelDiT: Pixel Diffusion Transformers for Image Generation",
        "relation": "cites",
        "tail": "Diffusion Models Beat GANs on Image Synthesis"
      },
      {
        "head": "PixelDiT: Pixel Diffusion Transformers for Image Generation",
        "relation": "cites",
        "tail": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "head": "One-step Latent-free Image Generation with Pixel Mean Flows",
        "relation": "cites",
        "tail": "PixelDiT: Pixel Diffusion Transformers for Image Generation"
      },
      {
        "head": "Generative Modeling via Drifting",
        "relation": "cites",
        "tail": "PixelDiT: Pixel Diffusion Transformers for Image Generation"
      },
      {
        "head": "Laminating Representation Autoencoders for Efficient Diffusion",
        "relation": "cites",
        "tail": "PixelDiT: Pixel Diffusion Transformers for Image Generation"
      },
      {
        "head": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss",
        "relation": "cites",
        "tail": "PixelDiT: Pixel Diffusion Transformers for Image Generation"
      },
      {
        "head": "RePack then Refine: Efficient Diffusion Transformer with Vision Foundation Model",
        "relation": "cites",
        "tail": "PixelDiT: Pixel Diffusion Transformers for Image Generation"
      },
      {
        "head": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models",
        "relation": "cites",
        "tail": "U-Net: Convolutional Networks for Biomedical Image Segmentation"
      },
      {
        "head": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models",
        "relation": "cites",
        "tail": "Decoupled Weight Decay Regularization"
      },
      {
        "head": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models",
        "relation": "cites",
        "tail": "Denoising Diffusion Probabilistic Models"
      },
      {
        "head": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models",
        "relation": "cites",
        "tail": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "head": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
        "relation": "cites",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery",
        "relation": "cites",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation",
        "relation": "cites",
        "tail": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"
      },
      {
        "head": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "relation": "cites",
        "tail": "Attention is All you Need"
      },
      {
        "head": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "relation": "cites",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "relation": "cites",
        "tail": "Distributed Representations of Words and Phrases and their Compositionality"
      },
      {
        "head": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "relation": "cites",
        "tail": "GloVe: Global Vectors for Word Representation"
      },
      {
        "head": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "relation": "cites",
        "tail": "Deep Contextualized Word Representations"
      },
      {
        "head": "Protein Language Models: Is Scaling Necessary?",
        "relation": "cites",
        "tail": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis",
        "relation": "cites",
        "tail": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation",
        "relation": "cites",
        "tail": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "Generative Classifiers Avoid Shortcut Solutions",
        "relation": "cites",
        "tail": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "Edge Large AI Model Agent-Empowered Cognitive Multimodal Semantic Communication",
        "relation": "cites",
        "tail": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "Diffusion Language Models are Super Data Learners",
        "relation": "cites",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Diffusion Language Models are Super Data Learners",
        "relation": "cites",
        "tail": "Language Models are Unsupervised Multitask Learners"
      },
      {
        "head": "Diffusion Language Models are Super Data Learners",
        "relation": "cites",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "Diffusion Language Models are Super Data Learners",
        "relation": "cites",
        "tail": "The Llama 3 Herd of Models"
      },
      {
        "head": "Diffusion Language Models are Super Data Learners",
        "relation": "cites",
        "tail": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"
      },
      {
        "head": "A Survey on Diffusion Language Models",
        "relation": "cites",
        "tail": "Diffusion Language Models are Super Data Learners"
      },
      {
        "head": "Training Optimal Large Diffusion Language Models",
        "relation": "cites",
        "tail": "Diffusion Language Models are Super Data Learners"
      },
      {
        "head": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models",
        "relation": "cites",
        "tail": "Diffusion Language Models are Super Data Learners"
      },
      {
        "head": "Encoder-Decoder Diffusion Language Models for Efficient Training and Inference",
        "relation": "cites",
        "tail": "Diffusion Language Models are Super Data Learners"
      },
      {
        "head": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model",
        "relation": "cites",
        "tail": "Diffusion Language Models are Super Data Learners"
      },
      {
        "head": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas",
        "relation": "cites",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas",
        "relation": "cites",
        "tail": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
      },
      {
        "head": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas",
        "relation": "cites",
        "tail": "Paper"
      },
      {
        "head": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas",
        "relation": "cites",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas",
        "relation": "cites",
        "tail": "Code Llama: Open Foundation Models for Code"
      },
      {
        "head": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way",
        "relation": "cites",
        "tail": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas"
      },
      {
        "head": "Set Block Decoding is a Language Model Inference Accelerator",
        "relation": "cites",
        "tail": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas"
      },
      {
        "head": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows",
        "relation": "cites",
        "tail": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas"
      },
      {
        "head": "Constrained Decoding of Diffusion LLMs with Context-Free Grammars",
        "relation": "cites",
        "tail": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas"
      },
      {
        "head": "Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models",
        "relation": "cites",
        "tail": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas"
      },
      {
        "head": "FLEX: Continuous Agent Evolution via Forward Learning from Experience",
        "relation": "cites",
        "tail": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "FLEX: Continuous Agent Evolution via Forward Learning from Experience",
        "relation": "cites",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "FLEX: Continuous Agent Evolution via Forward Learning from Experience",
        "relation": "cites",
        "tail": "Learning representations by back-propagating errors"
      },
      {
        "head": "FLEX: Continuous Agent Evolution via Forward Learning from Experience",
        "relation": "cites",
        "tail": "Language Models are Unsupervised Multitask Learners"
      },
      {
        "head": "FLEX: Continuous Agent Evolution via Forward Learning from Experience",
        "relation": "cites",
        "tail": "Proximal Policy Optimization Algorithms"
      },
      {
        "head": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications",
        "relation": "cites",
        "tail": "FLEX: Continuous Agent Evolution via Forward Learning from Experience"
      },
      {
        "head": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search",
        "relation": "cites",
        "tail": "FLEX: Continuous Agent Evolution via Forward Learning from Experience"
      },
      {
        "head": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking",
        "relation": "cites",
        "tail": "FLEX: Continuous Agent Evolution via Forward Learning from Experience"
      },
      {
        "head": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents",
        "relation": "cites",
        "tail": "FLEX: Continuous Agent Evolution via Forward Learning from Experience"
      },
      {
        "head": "Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting",
        "relation": "cites",
        "tail": "FLEX: Continuous Agent Evolution via Forward Learning from Experience"
      },
      {
        "head": "Controllable Video Generation: A Survey",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Controllable Video Generation: A Survey",
        "relation": "cites",
        "tail": "GENERATIVE ADVERSARIAL NETS"
      },
      {
        "head": "Controllable Video Generation: A Survey",
        "relation": "cites",
        "tail": "Denoising Diffusion Probabilistic Models"
      },
      {
        "head": "Controllable Video Generation: A Survey",
        "relation": "cites",
        "tail": "AUTO-ENCODING VARIATIONAL BAYES"
      },
      {
        "head": "Controllable Video Generation: A Survey",
        "relation": "cites",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation",
        "relation": "cites",
        "tail": "Controllable Video Generation: A Survey"
      },
      {
        "head": "Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis",
        "relation": "cites",
        "tail": "Controllable Video Generation: A Survey"
      },
      {
        "head": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing",
        "relation": "cites",
        "tail": "Controllable Video Generation: A Survey"
      },
      {
        "head": "Paper2Video: Automatic Video Generation from Scientific Papers",
        "relation": "cites",
        "tail": "Controllable Video Generation: A Survey"
      },
      {
        "head": "ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment",
        "relation": "cites",
        "tail": "Controllable Video Generation: A Survey"
      },
      {
        "head": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes",
        "relation": "cites",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes",
        "relation": "cites",
        "tail": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
      },
      {
        "head": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes",
        "relation": "cites",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes",
        "relation": "cites",
        "tail": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "head": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes",
        "relation": "cites",
        "tail": "LLaVA-OneVision: Easy Visual Task Transfer"
      },
      {
        "head": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views",
        "relation": "cites",
        "tail": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes"
      },
      {
        "head": "Spatial Reasoning in Multimodal Large Language Models: A Survey of Tasks, Benchmarks and Methods",
        "relation": "cites",
        "tail": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes"
      },
      {
        "head": "SpatialTree: How Spatial Abilities Branch Out in MLLMs",
        "relation": "cites",
        "tail": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes"
      },
      {
        "head": "From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs",
        "relation": "cites",
        "tail": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes"
      },
      {
        "head": "SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery",
        "relation": "cites",
        "tail": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes"
      },
      {
        "head": "Detect Anything via Next Point Prediction",
        "relation": "cites",
        "tail": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "Detect Anything via Next Point Prediction",
        "relation": "cites",
        "tail": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
      },
      {
        "head": "Detect Anything via Next Point Prediction",
        "relation": "cites",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Detect Anything via Next Point Prediction",
        "relation": "cites",
        "tail": "You Only Look Once: Unified, Real-Time Object Detection"
      },
      {
        "head": "Detect Anything via Next Point Prediction",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes",
        "relation": "cites",
        "tail": "Detect Anything via Next Point Prediction"
      },
      {
        "head": "WeDetect: Fast Open-Vocabulary Object Detection as Retrieval",
        "relation": "cites",
        "tail": "Detect Anything via Next Point Prediction"
      },
      {
        "head": "v1: Learning to Point Visual Tokens for Multimodal Grounded Reasoning",
        "relation": "cites",
        "tail": "Detect Anything via Next Point Prediction"
      },
      {
        "head": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design",
        "relation": "cites",
        "tail": "Detect Anything via Next Point Prediction"
      },
      {
        "head": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving",
        "relation": "cites",
        "tail": "Detect Anything via Next Point Prediction"
      },
      {
        "head": "Asynchronous Methods for Deep Reinforcement Learning",
        "relation": "cites",
        "tail": "Adam: A Method for Stochastic Optimization"
      },
      {
        "head": "Asynchronous Methods for Deep Reinforcement Learning",
        "relation": "cites",
        "tail": "Long Short-Term Memory"
      },
      {
        "head": "Asynchronous Methods for Deep Reinforcement Learning",
        "relation": "cites",
        "tail": "Reinforcement Learning: An Introduction"
      },
      {
        "head": "Asynchronous Methods for Deep Reinforcement Learning",
        "relation": "cites",
        "tail": "Human-level control through deep reinforcement learning"
      },
      {
        "head": "Asynchronous Methods for Deep Reinforcement Learning",
        "relation": "cites",
        "tail": "Continuous control with deep reinforcement learning"
      },
      {
        "head": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning",
        "relation": "cites",
        "tail": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "head": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models",
        "relation": "cites",
        "tail": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "head": "Learning to Reason under Off-Policy Guidance",
        "relation": "cites",
        "tail": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "head": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models",
        "relation": "cites",
        "tail": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "head": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement learning for LLMs",
        "relation": "cites",
        "tail": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "head": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation",
        "relation": "cites",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation",
        "relation": "cites",
        "tail": "U-Net: Convolutional Networks for Biomedical Image Segmentation"
      },
      {
        "head": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation",
        "relation": "cites",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation",
        "relation": "cites",
        "tail": "Fully convolutional networks for semantic segmentation"
      },
      {
        "head": "Diffusion Models in Vision: A Survey",
        "relation": "cites",
        "tail": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation"
      },
      {
        "head": "SegNeXt: Rethinking Convolutional Attention Design for Semantic Segmentation",
        "relation": "cites",
        "tail": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation"
      },
      {
        "head": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation With Transformers",
        "relation": "cites",
        "tail": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation"
      },
      {
        "head": "Segment Anything in High Quality",
        "relation": "cites",
        "tail": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation"
      },
      {
        "head": "PIDNet: A Real-time Semantic Segmentation Network Inspired by PID Controllers",
        "relation": "cites",
        "tail": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation"
      },
      {
        "head": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving",
        "relation": "cites",
        "tail": "GPT-4 Technical Report"
      },
      {
        "head": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving",
        "relation": "cites",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving",
        "relation": "cites",
        "tail": "CARLA: An Open Urban Driving Simulator"
      },
      {
        "head": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving",
        "relation": "cites",
        "tail": "Qwen2 Technical Report"
      },
      {
        "head": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving",
        "relation": "cites",
        "tail": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning",
        "relation": "cites",
        "tail": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving",
        "relation": "cites",
        "tail": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models",
        "relation": "cites",
        "tail": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving",
        "relation": "cites",
        "tail": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving"
      },
      {
        "head": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving",
        "relation": "cites",
        "tail": "Denoising Diffusion Probabilistic Models"
      },
      {
        "head": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving",
        "relation": "cites",
        "tail": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "head": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving",
        "relation": "cites",
        "tail": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
      },
      {
        "head": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving",
        "relation": "cites",
        "tail": "Visual Instruction Tuning"
      },
      {
        "head": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving",
        "relation": "cites",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "A Survey on Vision-Language-Action Models for Autonomous Driving",
        "relation": "cites",
        "tail": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving"
      },
      {
        "head": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail",
        "relation": "cites",
        "tail": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving"
      },
      {
        "head": "Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning",
        "relation": "cites",
        "tail": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving"
      },
      {
        "head": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model",
        "relation": "cites",
        "tail": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving"
      },
      {
        "head": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving",
        "relation": "cites",
        "tail": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving"
      },
      {
        "head": "Pseudo-Simulation for Autonomous Driving",
        "relation": "cites",
        "tail": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric"
      },
      {
        "head": "Pseudo-Simulation for Autonomous Driving",
        "relation": "cites",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "Pseudo-Simulation for Autonomous Driving",
        "relation": "cites",
        "tail": "3D Gaussian Splatting for Real-Time Radiance Field Rendering"
      },
      {
        "head": "Pseudo-Simulation for Autonomous Driving",
        "relation": "cites",
        "tail": "Structure-from-Motion Revisited"
      },
      {
        "head": "Pseudo-Simulation for Autonomous Driving",
        "relation": "cites",
        "tail": "CARLA: An Open Urban Driving Simulator"
      },
      {
        "head": "Generalized Trajectory Scoring for End-to-end Multimodal Planning",
        "relation": "cites",
        "tail": "Pseudo-Simulation for Autonomous Driving"
      },
      {
        "head": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model",
        "relation": "cites",
        "tail": "Pseudo-Simulation for Autonomous Driving"
      },
      {
        "head": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving",
        "relation": "cites",
        "tail": "Pseudo-Simulation for Autonomous Driving"
      },
      {
        "head": "RAP: 3D Rasterization Augmented End-to-End Planning",
        "relation": "cites",
        "tail": "Pseudo-Simulation for Autonomous Driving"
      },
      {
        "head": "PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving",
        "relation": "cites",
        "tail": "Pseudo-Simulation for Autonomous Driving"
      },
      {
        "head": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving",
        "relation": "cites",
        "tail": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"
      },
      {
        "head": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving",
        "relation": "cites",
        "tail": "CARLA: An Open Urban Driving Simulator"
      },
      {
        "head": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving",
        "relation": "cites",
        "tail": "Qwen Technical Report"
      },
      {
        "head": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving",
        "relation": "cites",
        "tail": "Flow Matching for Generative Modeling"
      },
      {
        "head": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving",
        "relation": "cites",
        "tail": "Diffusion policy: Visuomotor policy learning via action diffusion"
      },
      {
        "head": "Large Language Models for Robotics: A Survey",
        "relation": "cites",
        "tail": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving"
      },
      {
        "head": "A Survey on Vision-Language-Action Models for Autonomous Driving",
        "relation": "cites",
        "tail": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving"
      },
      {
        "head": "Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)",
        "relation": "cites",
        "tail": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving"
      },
      {
        "head": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey",
        "relation": "cites",
        "tail": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving"
      },
      {
        "head": "ReSim: Reliable World Simulation for Autonomous Driving",
        "relation": "cites",
        "tail": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving"
      },
      {
        "head": "A Survey on Vision-Language-Action Models for Autonomous Driving",
        "relation": "cites",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "A Survey on Vision-Language-Action Models for Autonomous Driving",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "A Survey on Vision-Language-Action Models for Autonomous Driving",
        "relation": "cites",
        "tail": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "head": "A Survey on Vision-Language-Action Models for Autonomous Driving",
        "relation": "cites",
        "tail": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "head": "A Survey on Vision-Language-Action Models for Autonomous Driving",
        "relation": "cites",
        "tail": "Visual Instruction Tuning"
      },
      {
        "head": "Vision-Language-Action (VLA) Models: Concepts, Progress, Applications and Challenges",
        "relation": "cites",
        "tail": "A Survey on Vision-Language-Action Models for Autonomous Driving"
      },
      {
        "head": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving",
        "relation": "cites",
        "tail": "A Survey on Vision-Language-Action Models for Autonomous Driving"
      },
      {
        "head": "Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey",
        "relation": "cites",
        "tail": "A Survey on Vision-Language-Action Models for Autonomous Driving"
      },
      {
        "head": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving",
        "relation": "cites",
        "tail": "A Survey on Vision-Language-Action Models for Autonomous Driving"
      },
      {
        "head": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition",
        "relation": "cites",
        "tail": "A Survey on Vision-Language-Action Models for Autonomous Driving"
      },
      {
        "head": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond",
        "relation": "cites",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond",
        "relation": "cites",
        "tail": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond",
        "relation": "cites",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI",
        "relation": "cites",
        "tail": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond"
      },
      {
        "head": "A Survey on Vision-Language-Action Models for Embodied AI",
        "relation": "cites",
        "tail": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond"
      },
      {
        "head": "Understanding World or Predicting Future? A Comprehensive Survey of World Models",
        "relation": "cites",
        "tail": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond"
      },
      {
        "head": "DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation",
        "relation": "cites",
        "tail": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond"
      },
      {
        "head": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation",
        "relation": "cites",
        "tail": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond"
      },
      {
        "head": "Understanding World or Predicting Future? A Comprehensive Survey of World Models",
        "relation": "cites",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Understanding World or Predicting Future? A Comprehensive Survey of World Models",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "Understanding World or Predicting Future? A Comprehensive Survey of World Models",
        "relation": "cites",
        "tail": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
      },
      {
        "head": "Understanding World or Predicting Future? A Comprehensive Survey of World Models",
        "relation": "cites",
        "tail": "Gradient-based learning applied to document recognition"
      },
      {
        "head": "Understanding World or Predicting Future? A Comprehensive Survey of World Models",
        "relation": "cites",
        "tail": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "head": "Sora as a World Model? A Complete Survey on Text-to-Video Generation",
        "relation": "cites",
        "tail": "Understanding World or Predicting Future? A Comprehensive Survey of World Models"
      },
      {
        "head": "AgentSquare: Automatic LLM Agent Search in Modular Design Space",
        "relation": "cites",
        "tail": "Understanding World or Predicting Future? A Comprehensive Survey of World Models"
      },
      {
        "head": "Vision-Language-Action (VLA) Models: Concepts, Progress, Applications and Challenges",
        "relation": "cites",
        "tail": "Understanding World or Predicting Future? A Comprehensive Survey of World Models"
      },
      {
        "head": "CityGPT: Empowering Urban Spatial Cognition of Large Language Models",
        "relation": "cites",
        "tail": "Understanding World or Predicting Future? A Comprehensive Survey of World Models"
      },
      {
        "head": "Epona: Autoregressive Diffusion World Model for Autonomous Driving",
        "relation": "cites",
        "tail": "Understanding World or Predicting Future? A Comprehensive Survey of World Models"
      },
      {
        "head": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey",
        "relation": "cites",
        "tail": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
      },
      {
        "head": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey",
        "relation": "cites",
        "tail": "Denoising Diffusion Probabilistic Models"
      },
      {
        "head": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey",
        "relation": "cites",
        "tail": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "head": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey",
        "relation": "cites",
        "tail": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"
      },
      {
        "head": "Unlearning Concepts in Diffusion Model via Concept Domain Correction and Concept Preserving Gradient",
        "relation": "cites",
        "tail": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey"
      },
      {
        "head": "Safety at Scale: A Comprehensive Survey of Large Model Safety",
        "relation": "cites",
        "tail": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey"
      },
      {
        "head": "Large multimodal models evaluation: a survey",
        "relation": "cites",
        "tail": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey"
      },
      {
        "head": "The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense",
        "relation": "cites",
        "tail": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey"
      },
      {
        "head": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs",
        "relation": "cites",
        "tail": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey"
      },
      {
        "head": "Squeeze-and-Excitation Networks",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Squeeze-and-Excitation Networks",
        "relation": "cites",
        "tail": "Attention is All you Need"
      },
      {
        "head": "Squeeze-and-Excitation Networks",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "Squeeze-and-Excitation Networks",
        "relation": "cites",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "Squeeze-and-Excitation Networks",
        "relation": "cites",
        "tail": "Long Short-Term Memory"
      },
      {
        "head": "Gradient-Guided Learning Network for Infrared Small Target Detection",
        "relation": "cites",
        "tail": "Squeeze-and-Excitation Networks"
      },
      {
        "head": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism",
        "relation": "cites",
        "tail": "Squeeze-and-Excitation Networks"
      },
      {
        "head": "A Retrieval-Augmented Generation Approach to Extracting Algorithmic Logic from Neural Networks",
        "relation": "cites",
        "tail": "Squeeze-and-Excitation Networks"
      },
      {
        "head": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness",
        "relation": "cites",
        "tail": "Squeeze-and-Excitation Networks"
      },
      {
        "head": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation",
        "relation": "cites",
        "tail": "Squeeze-and-Excitation Networks"
      },
      {
        "head": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
        "relation": "cites",
        "tail": "Going deeper with convolutions"
      },
      {
        "head": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
        "relation": "cites",
        "tail": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
      },
      {
        "head": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
        "relation": "cites",
        "tail": "Dropout: a simple way to prevent neural networks from overfitting"
      },
      {
        "head": "Enhancing pine wilt disease detection with synthetic data and external attention-based transformers",
        "relation": "cites",
        "tail": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"
      },
      {
        "head": "CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation",
        "relation": "cites",
        "tail": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"
      },
      {
        "head": "IMobileTransformer: A fusion-based lightweight model for rice disease identification",
        "relation": "cites",
        "tail": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"
      },
      {
        "head": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images",
        "relation": "cites",
        "tail": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"
      },
      {
        "head": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers",
        "relation": "cites",
        "tail": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"
      },
      {
        "head": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control",
        "relation": "cites",
        "tail": "Image quality assessment: from error visibility to structural similarity"
      },
      {
        "head": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control",
        "relation": "cites",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control",
        "relation": "cites",
        "tail": "Generative Adversarial Networks"
      },
      {
        "head": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control",
        "relation": "cites",
        "tail": "Denoising Diffusion Probabilistic Models"
      },
      {
        "head": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction",
        "relation": "cites",
        "tail": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control"
      },
      {
        "head": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos",
        "relation": "cites",
        "tail": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control"
      },
      {
        "head": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World",
        "relation": "cites",
        "tail": "Attention is All you Need"
      },
      {
        "head": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World",
        "relation": "cites",
        "tail": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"
      },
      {
        "head": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World",
        "relation": "cites",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World",
        "relation": "cites",
        "tail": "Scalable Diffusion Models with Transformers"
      },
      {
        "head": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World",
        "relation": "cites",
        "tail": "RoFormer: Enhanced Transformer with Rotary Position Embedding"
      },
      {
        "head": "The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey",
        "relation": "cites",
        "tail": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World"
      },
      {
        "head": "DVGT: Driving Visual Geometry Transformer",
        "relation": "cites",
        "tail": "Decoupled Weight Decay Regularization"
      },
      {
        "head": "DVGT: Driving Visual Geometry Transformer",
        "relation": "cites",
        "tail": "Vision meets robotics: The KITTI dataset"
      },
      {
        "head": "DVGT: Driving Visual Geometry Transformer",
        "relation": "cites",
        "tail": "nuScenes: A Multimodal Dataset for Autonomous Driving"
      },
      {
        "head": "DVGT: Driving Visual Geometry Transformer",
        "relation": "cites",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "DVGT: Driving Visual Geometry Transformer",
        "relation": "cites",
        "tail": "Least-Squares Estimation of Transformation Parameters Between Two Point Patterns"
      },
      {
        "head": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations",
        "relation": "cites",
        "tail": "DVGT: Driving Visual Geometry Transformer"
      },
      {
        "head": "Adding Conditional Control to Text-to-Image Diffusion Models",
        "relation": "cites",
        "tail": "U-Net: Convolutional Networks for Biomedical Image Segmentation"
      },
      {
        "head": "Adding Conditional Control to Text-to-Image Diffusion Models",
        "relation": "cites",
        "tail": "Et al"
      },
      {
        "head": "Adding Conditional Control to Text-to-Image Diffusion Models",
        "relation": "cites",
        "tail": "Deep Learning"
      },
      {
        "head": "Adding Conditional Control to Text-to-Image Diffusion Models",
        "relation": "cites",
        "tail": "Gradient-based learning applied to document recognition"
      },
      {
        "head": "Adding Conditional Control to Text-to-Image Diffusion Models",
        "relation": "cites",
        "tail": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "head": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction",
        "relation": "cites",
        "tail": "Adding Conditional Control to Text-to-Image Diffusion Models"
      },
      {
        "head": "Seedream 4.0: Toward Next-generation Multimodal Image Generation",
        "relation": "cites",
        "tail": "Adding Conditional Control to Text-to-Image Diffusion Models"
      },
      {
        "head": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation",
        "relation": "cites",
        "tail": "Adding Conditional Control to Text-to-Image Diffusion Models"
      },
      {
        "head": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding",
        "relation": "cites",
        "tail": "Adding Conditional Control to Text-to-Image Diffusion Models"
      },
      {
        "head": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale",
        "relation": "cites",
        "tail": "Adding Conditional Control to Text-to-Image Diffusion Models"
      },
      {
        "head": "Scaling Instruction-Finetuned Language Models",
        "relation": "cites",
        "tail": "Language Models are Few-Shot Learners"
      },
      {
        "head": "Scaling Instruction-Finetuned Language Models",
        "relation": "cites",
        "tail": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
      },
      {
        "head": "Scaling Instruction-Finetuned Language Models",
        "relation": "cites",
        "tail": "Language Models are Unsupervised Multitask Learners"
      },
      {
        "head": "Scaling Instruction-Finetuned Language Models",
        "relation": "cites",
        "tail": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "head": "Scaling Instruction-Finetuned Language Models",
        "relation": "cites",
        "tail": "Training language models to follow instructions with human feedback"
      },
      {
        "head": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning",
        "relation": "cites",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling",
        "relation": "cites",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning",
        "relation": "cites",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "Toward expert-level medical question answering with large language models",
        "relation": "cites",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training",
        "relation": "cites",
        "tail": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "head": "High-Resolution Image Synthesis with Latent Diffusion Models",
        "relation": "cites",
        "tail": "Attention is All you Need"
      },
      {
        "head": "High-Resolution Image Synthesis with Latent Diffusion Models",
        "relation": "cites",
        "tail": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "head": "High-Resolution Image Synthesis with Latent Diffusion Models",
        "relation": "cites",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "High-Resolution Image Synthesis with Latent Diffusion Models",
        "relation": "cites",
        "tail": "U-Net: Convolutional Networks for Biomedical Image Segmentation"
      },
      {
        "head": "High-Resolution Image Synthesis with Latent Diffusion Models",
        "relation": "cites",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "Equivariant Diffusion for Crystal Structure Prediction",
        "relation": "cites",
        "tail": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "head": "Generative Classifiers Avoid Shortcut Solutions",
        "relation": "cites",
        "tail": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "head": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling",
        "relation": "cites",
        "tail": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "head": "Native and Compact Structured Latents for 3D Generation",
        "relation": "cites",
        "tail": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "head": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
        "relation": "cites",
        "tail": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "head": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
        "relation": "cites",
        "tail": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "head": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
        "relation": "cites",
        "tail": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "head": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
        "relation": "cites",
        "tail": "and as an in"
      },
      {
        "head": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
        "relation": "cites",
        "tail": "Et al"
      },
      {
        "head": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
        "relation": "cites",
        "tail": "Image quality assessment: from error visibility to structural similarity"
      },
      {
        "head": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?",
        "relation": "cites",
        "tail": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric"
      },
      {
        "head": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness",
        "relation": "cites",
        "tail": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric"
      },
      {
        "head": "Progressive Learned Image Transmission for Semantic Communication Using Hierarchical VAE",
        "relation": "cites",
        "tail": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric"
      },
      {
        "head": "3DGS-Drag: Dragging Gaussians for Intuitive Point-Based 3D Editing",
        "relation": "cites",
        "tail": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric"
      },
      {
        "head": "STORM: Search-Guided Generative World Models for Robotic Manipulation",
        "relation": "cites",
        "tail": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric"
      },
      {
        "head": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset",
        "relation": "cites",
        "tail": "Deep Residual Learning for Image Recognition"
      },
      {
        "head": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset",
        "relation": "cites",
        "tail": "ImageNet: A large-scale hierarchical image database"
      },
      {
        "head": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset",
        "relation": "cites",
        "tail": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
      },
      {
        "head": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset",
        "relation": "cites",
        "tail": "Microsoft COCO: Common Objects in Context"
      },
      {
        "head": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset",
        "relation": "cites",
        "tail": "A New Approach to Linear Filtering and Prediction Problems"
      },
      {
        "head": "Continuous 3D Perception Model with Persistent State",
        "relation": "cites",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Gen3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control",
        "relation": "cites",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "FLARE: Feed-Forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views",
        "relation": "cites",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "UniDepthV2: Universal Monocular Metric Depth Estimation Made Simpler",
        "relation": "cites",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving",
        "relation": "cites",
        "tail": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"
      },
      {
        "head": "Attention Is All You Need",
        "relation": "proposed_model",
        "tail": "Transformer"
      },
      {
        "head": "Transformer",
        "relation": "evaluated_on",
        "tail": "WMT 2014 English-to-German translation task"
      },
      {
        "head": "Transformer",
        "relation": "evaluated_on",
        "tail": "WMT 2014 English-to-French translation task"
      },
      {
        "head": "Transformer",
        "relation": "evaluated_on",
        "tail": "English constituency parsing"
      },
      {
        "head": "Transformer",
        "relation": "uses_metric",
        "tail": "BLEU"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "proposed_model",
        "tail": "residual learning framework"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "baseline_model",
        "tail": "VGG nets"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "evaluated_on",
        "tail": "CIFAR-10"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "evaluated_on",
        "tail": "COCO object detection dataset"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "uses_metric",
        "tail": "error"
      },
      {
        "head": "Deep Residual Learning for Image Recognition",
        "relation": "uses_metric",
        "tail": "relative improvement"
      },
      {
        "head": "Adam: A Method for Stochastic Optimization",
        "relation": "proposed_model",
        "tail": "Adam"
      },
      {
        "head": "Adam: A Method for Stochastic Optimization",
        "relation": "proposed_model",
        "tail": "AdaMax"
      },
      {
        "head": "Long Short-Term Memory",
        "relation": "proposed_model",
        "tail": "Long Short-Term Memory"
      },
      {
        "head": "Dropout: a simple way to prevent neural networks from overfitting",
        "relation": "proposed_model",
        "tail": "Dropout"
      },
      {
        "head": "Rethinking the Inception Architecture for Computer Vision",
        "relation": "proposed_model",
        "tail": "Inception Architecture"
      },
      {
        "head": "Rethinking the Inception Architecture for Computer Vision",
        "relation": "evaluated_on",
        "tail": "ILSVRC 2012 classification challenge validation set"
      },
      {
        "head": "Rethinking the Inception Architecture for Computer Vision",
        "relation": "uses_metric",
        "tail": "top-1 error"
      },
      {
        "head": "Rethinking the Inception Architecture for Computer Vision",
        "relation": "uses_metric",
        "tail": "top-5 error"
      },
      {
        "head": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
        "relation": "proposed_model",
        "tail": "InstaDrive"
      },
      {
        "head": "InstaDrive",
        "relation": "baseline_model",
        "tail": "world models"
      },
      {
        "head": "InstaDrive",
        "relation": "evaluated_on",
        "tail": "nuScenes"
      },
      {
        "head": "InstaDrive",
        "relation": "evaluated_on",
        "tail": "CARLA"
      },
      {
        "head": "InstaDrive",
        "relation": "uses_metric",
        "tail": "video generation quality"
      },
      {
        "head": "InstaDrive",
        "relation": "uses_metric",
        "tail": "safety evaluation"
      },
      {
        "head": "CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection",
        "relation": "proposed_model",
        "tail": "CNC-VLM"
      },
      {
        "head": "CNC-VLM",
        "relation": "evaluated_on",
        "tail": "CNC fault detection dataset"
      },
      {
        "head": "CNC-VLM",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "CNC-VLM",
        "relation": "uses_metric",
        "tail": "F1-score"
      },
      {
        "head": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation",
        "relation": "proposed_model",
        "tail": "mamba segmentation"
      },
      {
        "head": "UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation",
        "relation": "uses_metric",
        "tail": "four-point laser metric calibration"
      },
      {
        "head": "DiffusionEngine: Diffusion model is scalable data engine for object detection",
        "relation": "proposed_model",
        "tail": "DiffusionEngine"
      },
      {
        "head": "ImageNet classification with deep convolutional neural networks",
        "relation": "proposed_model",
        "tail": "deep convolutional neural network"
      },
      {
        "head": "ImageNet classification with deep convolutional neural networks",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "ImageNet classification with deep convolutional neural networks",
        "relation": "uses_metric",
        "tail": "top-1 error"
      },
      {
        "head": "ImageNet classification with deep convolutional neural networks",
        "relation": "uses_metric",
        "tail": "top-5 error"
      },
      {
        "head": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
        "relation": "proposed_model",
        "tail": "ConvNet models"
      },
      {
        "head": "ConvNet models",
        "relation": "evaluated_on",
        "tail": "ImageNet Challenge 2014"
      },
      {
        "head": "ConvNet models",
        "relation": "evaluated_on",
        "tail": "other datasets"
      },
      {
        "head": "ConvNet models",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
        "relation": "proposed_model",
        "tail": "Region Proposal Network (RPN)"
      },
      {
        "head": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
        "relation": "baseline_model",
        "tail": "Fast R-CNN"
      },
      {
        "head": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
        "relation": "baseline_model",
        "tail": "SPPnet"
      },
      {
        "head": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
        "relation": "evaluated_on",
        "tail": "PASCAL VOC 2007"
      },
      {
        "head": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
        "relation": "evaluated_on",
        "tail": "PASCAL VOC 2012"
      },
      {
        "head": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
        "relation": "evaluated_on",
        "tail": "MS COCO"
      },
      {
        "head": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
        "relation": "evaluated_on",
        "tail": "ILSVRC"
      },
      {
        "head": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
        "relation": "evaluated_on",
        "tail": "COCO 2015"
      },
      {
        "head": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
        "relation": "uses_metric",
        "tail": "frame rate"
      },
      {
        "head": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
        "relation": "uses_metric",
        "tail": "object detection accuracy"
      },
      {
        "head": "Optimal Transport Barycentric Aggregation for Byzantine-Resilient Federated Learning",
        "relation": "proposed_model",
        "tail": "Federated Learning Optimal Transport (FLOT)"
      },
      {
        "head": "Federated Learning Optimal Transport (FLOT)",
        "relation": "evaluated_on",
        "tail": "GTSRB"
      },
      {
        "head": "Federated Learning Optimal Transport (FLOT)",
        "relation": "evaluated_on",
        "tail": "KBTS"
      },
      {
        "head": "Federated Learning Optimal Transport (FLOT)",
        "relation": "evaluated_on",
        "tail": "CIFAR10"
      },
      {
        "head": "Federated Learning Optimal Transport (FLOT)",
        "relation": "evaluated_on",
        "tail": "EMNIST"
      },
      {
        "head": "Federated Learning Optimal Transport (FLOT)",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "Federated Learning Optimal Transport (FLOT)",
        "relation": "uses_metric",
        "tail": "scalability"
      },
      {
        "head": "WarmGait: Thermal Array-Based Gait Recognition for Privacy-Preserving Person Re-ID",
        "relation": "proposed_model",
        "tail": "WarmGait"
      },
      {
        "head": "WarmGait",
        "relation": "evaluated_on",
        "tail": "thermal array sensors"
      },
      {
        "head": "WarmGait",
        "relation": "uses_metric",
        "tail": "average recognition accuracy"
      },
      {
        "head": "WarmGait",
        "relation": "baseline_model",
        "tail": "Taylor Finite Difference (TFD)"
      },
      {
        "head": "Noise Perception Self-Supervised Learning for Unsupervised Person Re-Identification",
        "relation": "proposed_model",
        "tail": "NPSSL"
      },
      {
        "head": "Noise Perception Self-Supervised Learning for Unsupervised Person Re-Identification",
        "relation": "proposed_model",
        "tail": "Noise Perception Self-Paced Learning"
      },
      {
        "head": "NPSSL",
        "relation": "evaluated_on",
        "tail": "Duke dataset"
      },
      {
        "head": "Noise Perception Self-Supervised Learning for Unsupervised Person Re-Identification",
        "relation": "baseline_model",
        "tail": "Unsupervised Domain Adaptation"
      },
      {
        "head": "Auto-Encoding Variational Bayes",
        "relation": "proposed_model",
        "tail": "stochastic variational inference and learning algorithm"
      },
      {
        "head": "Auto-Encoding Variational Bayes",
        "relation": "proposed_model",
        "tail": "reparameterization of the variational lower bound"
      },
      {
        "head": "Auto-Encoding Variational Bayes",
        "relation": "proposed_model",
        "tail": "approximate inference model"
      },
      {
        "head": "stochastic variational inference and learning algorithm",
        "relation": "evaluated_on",
        "tail": "i.i.d. datasets"
      },
      {
        "head": "reparameterization of the variational lower bound",
        "relation": "uses_metric",
        "tail": "variational lower bound"
      },
      {
        "head": "approximate inference model",
        "relation": "evaluated_on",
        "tail": "i.i.d. datasets"
      },
      {
        "head": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization",
        "relation": "proposed_model",
        "tail": "AdaGrad"
      },
      {
        "head": "AdaGrad",
        "relation": "evaluated_on",
        "tail": "Online Learning"
      },
      {
        "head": "AdaGrad",
        "relation": "evaluated_on",
        "tail": "Stochastic Optimization"
      },
      {
        "head": "AdaGrad",
        "relation": "uses_metric",
        "tail": "Convergence Rate"
      },
      {
        "head": "Speech recognition with deep recurrent neural networks",
        "relation": "proposed_model",
        "tail": "deep recurrent neural networks"
      },
      {
        "head": "Speech recognition with deep recurrent neural networks",
        "relation": "proposed_model",
        "tail": "deep Long Short-term Memory RNNs"
      },
      {
        "head": "Speech recognition with deep recurrent neural networks",
        "relation": "baseline_model",
        "tail": "deep feedforward networks"
      },
      {
        "head": "deep Long Short-term Memory RNNs",
        "relation": "evaluated_on",
        "tail": "TIMIT phoneme recognition benchmark"
      },
      {
        "head": "deep Long Short-term Memory RNNs",
        "relation": "uses_metric",
        "tail": "test set error of 17.7%"
      },
      {
        "head": "Weakly Supervised Image Dehazing via Physics-Based Decomposition",
        "relation": "proposed_model",
        "tail": "PBD"
      },
      {
        "head": "Weakly Supervised Image Dehazing via Physics-Based Decomposition",
        "relation": "baseline_model",
        "tail": "GAN"
      },
      {
        "head": "PBD",
        "relation": "evaluated_on",
        "tail": "seven benchmarks"
      },
      {
        "head": "PBD",
        "relation": "uses_metric",
        "tail": "reconstruction loss"
      },
      {
        "head": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer",
        "relation": "proposed_model",
        "tail": "AdamW"
      },
      {
        "head": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer",
        "relation": "baseline_model",
        "tail": "Adam"
      },
      {
        "head": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer",
        "relation": "evaluated_on",
        "tail": "face mask detection model"
      },
      {
        "head": "A Machine Learning Approach for Face Mask Detection System with AdamW Optimizer",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
        "relation": "proposed_model",
        "tail": "Engram"
      },
      {
        "head": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
        "relation": "baseline_model",
        "tail": "Mixture-of-Experts (MoE)"
      },
      {
        "head": "Engram",
        "relation": "evaluated_on",
        "tail": "MMLU"
      },
      {
        "head": "Engram",
        "relation": "evaluated_on",
        "tail": "CMMLU"
      },
      {
        "head": "Engram",
        "relation": "evaluated_on",
        "tail": "BBH"
      },
      {
        "head": "Engram",
        "relation": "evaluated_on",
        "tail": "ARC-Challenge"
      },
      {
        "head": "Engram",
        "relation": "evaluated_on",
        "tail": "HumanEval"
      },
      {
        "head": "Engram",
        "relation": "evaluated_on",
        "tail": "MATH"
      },
      {
        "head": "Engram",
        "relation": "evaluated_on",
        "tail": "Multi-Query NIAH"
      },
      {
        "head": "Engram",
        "relation": "uses_metric",
        "tail": "MMLU"
      },
      {
        "head": "Engram",
        "relation": "uses_metric",
        "tail": "CMMLU"
      },
      {
        "head": "Engram",
        "relation": "uses_metric",
        "tail": "BBH"
      },
      {
        "head": "Engram",
        "relation": "uses_metric",
        "tail": "ARC-Challenge"
      },
      {
        "head": "Engram",
        "relation": "uses_metric",
        "tail": "HumanEval"
      },
      {
        "head": "Engram",
        "relation": "uses_metric",
        "tail": "MATH"
      },
      {
        "head": "Engram",
        "relation": "uses_metric",
        "tail": "Multi-Query NIAH"
      },
      {
        "head": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction",
        "relation": "proposed_model",
        "tail": "Neural Hierarchical Interpolation for Time Series Forecasting (N-HiTS)"
      },
      {
        "head": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction",
        "relation": "proposed_model",
        "tail": "Network-Based Expansion Analysis for Interpretable Time Series Forecasting (N-BEATS)"
      },
      {
        "head": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction",
        "relation": "baseline_model",
        "tail": "long short-term memory (LSTM)"
      },
      {
        "head": "Probabilistic hierarchical interpolation and interpretable neural network configurations for flood prediction",
        "relation": "evaluated_on",
        "tail": "two headwater streams in Georgia and North Carolina, USA"
      },
      {
        "head": "Neural Hierarchical Interpolation for Time Series Forecasting (N-HiTS)",
        "relation": "uses_metric",
        "tail": "Multi-Quantile Loss"
      },
      {
        "head": "Network-Based Expansion Analysis for Interpretable Time Series Forecasting (N-BEATS)",
        "relation": "uses_metric",
        "tail": "Multi-Quantile Loss"
      },
      {
        "head": "Neural Hierarchical Interpolation for Time Series Forecasting (N-HiTS)",
        "relation": "uses_metric",
        "tail": "95th percentile prediction uncertainty (95 PPU)"
      },
      {
        "head": "Network-Based Expansion Analysis for Interpretable Time Series Forecasting (N-BEATS)",
        "relation": "uses_metric",
        "tail": "95th percentile prediction uncertainty (95 PPU)"
      },
      {
        "head": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model",
        "relation": "proposed_model",
        "tail": "NASA-IBM geospatial foundation model"
      },
      {
        "head": "Generating an annual 30 m rice cover product for monsoon Asia (2018–2023) using harmonized Landsat and Sentinel-2 data and the NASA-IBM geospatial foundation model",
        "relation": "evaluated_on",
        "tail": "harmonized Landsat and Sentinel-2 data"
      },
      {
        "head": "Going deeper with convolutions",
        "relation": "proposed_model",
        "tail": "Inception"
      },
      {
        "head": "Going deeper with convolutions",
        "relation": "proposed_model",
        "tail": "GoogLeNet"
      },
      {
        "head": "Inception",
        "relation": "evaluated_on",
        "tail": "ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014)"
      },
      {
        "head": "GoogLeNet",
        "relation": "evaluated_on",
        "tail": "ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014)"
      },
      {
        "head": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
        "relation": "proposed_model",
        "tail": "Batch Normalization"
      },
      {
        "head": "Batch Normalization",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "Batch Normalization",
        "relation": "uses_metric",
        "tail": "top-5 validation error"
      },
      {
        "head": "Batch Normalization",
        "relation": "uses_metric",
        "tail": "test error"
      },
      {
        "head": "ImageNet Large Scale Visual Recognition Challenge",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "Diffusion Transformers with Representation Autoencoders",
        "relation": "proposed_model",
        "tail": "Representation Autoencoders (RAEs)"
      },
      {
        "head": "Diffusion Transformers with Representation Autoencoders",
        "relation": "baseline_model",
        "tail": "VAE"
      },
      {
        "head": "Diffusion Transformers with Representation Autoencoders",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "Diffusion Transformers with Representation Autoencoders",
        "relation": "uses_metric",
        "tail": "FID"
      },
      {
        "head": "Representation Autoencoders (RAEs)",
        "relation": "baseline_model",
        "tail": "VAE"
      },
      {
        "head": "Representation Autoencoders (RAEs)",
        "relation": "proposed_model",
        "tail": "DINO"
      },
      {
        "head": "Representation Autoencoders (RAEs)",
        "relation": "proposed_model",
        "tail": "SigLIP"
      },
      {
        "head": "Representation Autoencoders (RAEs)",
        "relation": "proposed_model",
        "tail": "MAE"
      },
      {
        "head": "Diffusion Transformers (DiT)",
        "relation": "proposed_model",
        "tail": "DDT head"
      },
      {
        "head": "Scaling Large Language Models for Next-Generation Single-Cell Analysis",
        "relation": "proposed_model",
        "tail": "C2S-Scale"
      },
      {
        "head": "C2S-Scale",
        "relation": "baseline_model",
        "tail": "Cell2Sentence (C2S) framework"
      },
      {
        "head": "C2S-Scale",
        "relation": "baseline_model",
        "tail": "single-cell foundation models (scFMs)"
      },
      {
        "head": "C2S-Scale",
        "relation": "baseline_model",
        "tail": "Large Language Models (LLMs)"
      },
      {
        "head": "C2S-Scale",
        "relation": "evaluated_on",
        "tail": "corpus comprising over one billion tokens of transcriptomic data, biological text, and metadata"
      },
      {
        "head": "C2S-Scale",
        "relation": "evaluated_on",
        "tail": "human cell models"
      },
      {
        "head": "C2S-Scale",
        "relation": "uses_metric",
        "tail": "predictive and generative capabilities"
      },
      {
        "head": "C2S-Scale",
        "relation": "uses_metric",
        "tail": "performance in perturbation response prediction, natural language interpretation, and complex biological reasoning"
      },
      {
        "head": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights",
        "relation": "proposed_model",
        "tail": "DI"
      },
      {
        "head": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights",
        "relation": "baseline_model",
        "tail": "DiffPure"
      },
      {
        "head": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights",
        "relation": "evaluated_on",
        "tail": "Google Cloud Vision"
      },
      {
        "head": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights",
        "relation": "uses_metric",
        "tail": "Lp constraint"
      },
      {
        "head": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights",
        "relation": "uses_metric",
        "tail": "imperceptibility metrics"
      },
      {
        "head": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights",
        "relation": "uses_metric",
        "tail": "finer-grained measures"
      },
      {
        "head": "Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights",
        "relation": "uses_metric",
        "tail": "user study"
      },
      {
        "head": "HybridVisionNet: An advanced hybrid deep learning framework for automated multi-class ocular disease diagnosis using fundus imaging",
        "relation": "proposed_model",
        "tail": "HybridVisionNet"
      },
      {
        "head": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot",
        "relation": "proposed_model",
        "tail": "YOLO-OG"
      },
      {
        "head": "Dataset Purification-Driven Lightweight Deep Learning Model Construction for Empty-Dish Recycling Robot",
        "relation": "proposed_model",
        "tail": "OGNet"
      },
      {
        "head": "YOLO-OG",
        "relation": "evaluated_on",
        "tail": "Dish-10"
      },
      {
        "head": "YOLO-OG",
        "relation": "evaluated_on",
        "tail": "Dish-20"
      },
      {
        "head": "YOLO-OG",
        "relation": "uses_metric",
        "tail": "mean Average Precision (mAP)"
      },
      {
        "head": "OGNet",
        "relation": "uses_metric",
        "tail": "mean Average Precision (mAP)"
      },
      {
        "head": "Attention is All you Need",
        "relation": "proposed_model",
        "tail": "Transformer"
      },
      {
        "head": "Transformer",
        "relation": "evaluated_on",
        "tail": "WMT 2014 English-to-German translation task"
      },
      {
        "head": "Transformer",
        "relation": "evaluated_on",
        "tail": "WMT 2014 English-to-French translation task"
      },
      {
        "head": "Transformer",
        "relation": "evaluated_on",
        "tail": "English constituency parsing"
      },
      {
        "head": "Transformer",
        "relation": "uses_metric",
        "tail": "BLEU"
      },
      {
        "head": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "relation": "proposed_model",
        "tail": "Unified Text-to-Text Transformer"
      },
      {
        "head": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "relation": "evaluated_on",
        "tail": "dozens of language understanding tasks"
      },
      {
        "head": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "relation": "evaluated_on",
        "tail": "Colossal Clean Crawled Corpus"
      },
      {
        "head": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "relation": "uses_metric",
        "tail": "summarization"
      },
      {
        "head": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "relation": "uses_metric",
        "tail": "question answering"
      },
      {
        "head": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "relation": "uses_metric",
        "tail": "text classification"
      },
      {
        "head": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
        "relation": "proposed_model",
        "tail": "Two Time-Scale Update Rule"
      },
      {
        "head": "Two Time-Scale Update Rule",
        "relation": "baseline_model",
        "tail": "GANs"
      },
      {
        "head": "nuScenes: A Multimodal Dataset for Autonomous Driving",
        "relation": "proposed_model",
        "tail": "nuScenes"
      },
      {
        "head": "nuScenes: A Multimodal Dataset for Autonomous Driving",
        "relation": "baseline_model",
        "tail": "lidar based detection and tracking"
      },
      {
        "head": "nuScenes: A Multimodal Dataset for Autonomous Driving",
        "relation": "baseline_model",
        "tail": "image based detection and tracking"
      },
      {
        "head": "nuScenes: A Multimodal Dataset for Autonomous Driving",
        "relation": "evaluated_on",
        "tail": "nuScenes"
      },
      {
        "head": "nuScenes: A Multimodal Dataset for Autonomous Driving",
        "relation": "evaluated_on",
        "tail": "KITTI"
      },
      {
        "head": "nuScenes: A Multimodal Dataset for Autonomous Driving",
        "relation": "uses_metric",
        "tail": "novel 3D detection and tracking metrics"
      },
      {
        "head": "CARLA: An Open Urban Driving Simulator",
        "relation": "proposed_model",
        "tail": "CARLA"
      },
      {
        "head": "CARLA: An Open Urban Driving Simulator",
        "relation": "baseline_model",
        "tail": "classic modular pipeline"
      },
      {
        "head": "CARLA: An Open Urban Driving Simulator",
        "relation": "baseline_model",
        "tail": "end-to-end model trained via imitation learning"
      },
      {
        "head": "CARLA: An Open Urban Driving Simulator",
        "relation": "baseline_model",
        "tail": "end-to-end model trained via reinforcement learning"
      },
      {
        "head": "CARLA: An Open Urban Driving Simulator",
        "relation": "evaluated_on",
        "tail": "controlled scenarios of increasing difficulty"
      },
      {
        "head": "CARLA: An Open Urban Driving Simulator",
        "relation": "uses_metric",
        "tail": "metrics provided by CARLA"
      },
      {
        "head": "Sora as a World Model? A Complete Survey on Text-to-Video Generation",
        "relation": "proposed_model",
        "tail": "text-to-video generation"
      },
      {
        "head": "Sora as a World Model? A Complete Survey on Text-to-Video Generation",
        "relation": "baseline_model",
        "tail": "Sora"
      },
      {
        "head": "Sora as a World Model? A Complete Survey on Text-to-Video Generation",
        "relation": "evaluated_on",
        "tail": "MNIST"
      },
      {
        "head": "Sora as a World Model? A Complete Survey on Text-to-Video Generation",
        "relation": "uses_metric",
        "tail": "world modeling"
      },
      {
        "head": "OmniNWM: Omniscient Driving Navigation World Models",
        "relation": "proposed_model",
        "tail": "OmniNWM"
      },
      {
        "head": "OmniNWM: Omniscient Driving Navigation World Models",
        "relation": "baseline_model",
        "tail": "existing models"
      },
      {
        "head": "OmniNWM",
        "relation": "evaluated_on",
        "tail": "video generation"
      },
      {
        "head": "OmniNWM",
        "relation": "evaluated_on",
        "tail": "control accuracy"
      },
      {
        "head": "OmniNWM",
        "relation": "evaluated_on",
        "tail": "long-horizon stability"
      },
      {
        "head": "OmniNWM",
        "relation": "uses_metric",
        "tail": "video generation"
      },
      {
        "head": "OmniNWM",
        "relation": "uses_metric",
        "tail": "control accuracy"
      },
      {
        "head": "OmniNWM",
        "relation": "uses_metric",
        "tail": "long-horizon stability"
      },
      {
        "head": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask",
        "relation": "proposed_model",
        "tail": "ConsisDrive"
      },
      {
        "head": "ConsisDrive",
        "relation": "evaluated_on",
        "tail": "nuScenes"
      },
      {
        "head": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving",
        "relation": "proposed_model",
        "tail": "UniDriveDreamer"
      },
      {
        "head": "UniDriveDreamer",
        "relation": "baseline_model",
        "tail": "previous state-of-the-art methods"
      },
      {
        "head": "UniDriveDreamer",
        "relation": "evaluated_on",
        "tail": "multi-camera video"
      },
      {
        "head": "UniDriveDreamer",
        "relation": "evaluated_on",
        "tail": "LiDAR sequence"
      },
      {
        "head": "UniDriveDreamer",
        "relation": "uses_metric",
        "tail": "video generation"
      },
      {
        "head": "UniDriveDreamer",
        "relation": "uses_metric",
        "tail": "LiDAR generation"
      },
      {
        "head": "UniDriveDreamer",
        "relation": "proposed_model",
        "tail": "LiDAR-specific variational autoencoder (VAE)"
      },
      {
        "head": "UniDriveDreamer",
        "relation": "proposed_model",
        "tail": "video VAE"
      },
      {
        "head": "UniDriveDreamer",
        "relation": "proposed_model",
        "tail": "Unified Latent Anchoring (ULA)"
      },
      {
        "head": "UniDriveDreamer",
        "relation": "proposed_model",
        "tail": "diffusion transformer"
      },
      {
        "head": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
        "relation": "proposed_model",
        "tail": "MAD-LTX"
      },
      {
        "head": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
        "relation": "baseline_model",
        "tail": "SVD"
      },
      {
        "head": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
        "relation": "baseline_model",
        "tail": "LTX"
      },
      {
        "head": "MAD-LTX",
        "relation": "evaluated_on",
        "tail": "autonomous driving"
      },
      {
        "head": "MAD-LTX",
        "relation": "evaluated_on",
        "tail": "driving domains"
      },
      {
        "head": "MAD-LTX",
        "relation": "uses_metric",
        "tail": "structured motion"
      },
      {
        "head": "MAD-LTX",
        "relation": "uses_metric",
        "tail": "physically consistent interactions"
      },
      {
        "head": "MAD-LTX",
        "relation": "uses_metric",
        "tail": "photorealistic, temporally coherent videos"
      },
      {
        "head": "MAD-LTX",
        "relation": "uses_metric",
        "tail": "text, ego, and object controls"
      },
      {
        "head": "What matters for Representation Alignment: Global Information or Spatial Structure?",
        "relation": "proposed_model",
        "tail": "iREPA"
      },
      {
        "head": "iREPA",
        "relation": "baseline_model",
        "tail": "REPA"
      },
      {
        "head": "iREPA",
        "relation": "baseline_model",
        "tail": "REPA-E"
      },
      {
        "head": "iREPA",
        "relation": "baseline_model",
        "tail": "Meanflow"
      },
      {
        "head": "iREPA",
        "relation": "baseline_model",
        "tail": "JiT"
      },
      {
        "head": "What matters for Representation Alignment: Global Information or Spatial Structure?",
        "relation": "evaluated_on",
        "tail": "ImageNet-1K"
      },
      {
        "head": "What matters for Representation Alignment: Global Information or Spatial Structure?",
        "relation": "uses_metric",
        "tail": "ImageNet-1K accuracy"
      },
      {
        "head": "SCB-DETR: Multiscale Deformable Transformers for Occlusion-Resilient Student Learning Behavior Detection in Smart Classroom",
        "relation": "proposed_model",
        "tail": "SCB-DETR"
      },
      {
        "head": "SCB-DETR: Multiscale Deformable Transformers for Occlusion-Resilient Student Learning Behavior Detection in Smart Classroom",
        "relation": "baseline_model",
        "tail": "baseline model"
      },
      {
        "head": "SCB-DETR",
        "relation": "evaluated_on",
        "tail": "SCBehavior"
      },
      {
        "head": "SCB-DETR",
        "relation": "uses_metric",
        "tail": "mean Average Precision (mAP)"
      },
      {
        "head": "SCB-DETR",
        "relation": "uses_metric",
        "tail": "AP50"
      },
      {
        "head": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination",
        "relation": "proposed_model",
        "tail": "ultrasound-cardiac-feature-net (UCF-Net)"
      },
      {
        "head": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination",
        "relation": "proposed_model",
        "tail": "filtered integral quasi-super-twisting algorithm (FIQSTA)"
      },
      {
        "head": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination",
        "relation": "baseline_model",
        "tail": "proportional (P) controller"
      },
      {
        "head": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination",
        "relation": "baseline_model",
        "tail": "sliding mode controller"
      },
      {
        "head": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination",
        "relation": "baseline_model",
        "tail": "super-twisting algorithm (STA)"
      },
      {
        "head": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination",
        "relation": "baseline_model",
        "tail": "integral quasi-STA"
      },
      {
        "head": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination",
        "relation": "evaluated_on",
        "tail": "cardiac phantom"
      },
      {
        "head": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination",
        "relation": "evaluated_on",
        "tail": "parasternal short axis"
      },
      {
        "head": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination",
        "relation": "evaluated_on",
        "tail": "parasternal long axis"
      },
      {
        "head": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination",
        "relation": "evaluated_on",
        "tail": "subcostal"
      },
      {
        "head": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination",
        "relation": "evaluated_on",
        "tail": "apical four chambers views"
      },
      {
        "head": "Robust Deep Feature Ultrasound Image-Based Visual Servoing: Focus on Cardiac Examination",
        "relation": "evaluated_on",
        "tail": "trajectory passing through the main views"
      },
      {
        "head": "Bio-Inspired Fine-Tuning for Selective Transfer Learning in Image Classification",
        "relation": "proposed_model",
        "tail": "BioTune"
      },
      {
        "head": "Bio-Inspired Fine-Tuning for Selective Transfer Learning in Image Classification",
        "relation": "baseline_model",
        "tail": "AutoRGN"
      },
      {
        "head": "Bio-Inspired Fine-Tuning for Selective Transfer Learning in Image Classification",
        "relation": "baseline_model",
        "tail": "LoRA"
      },
      {
        "head": "BioTune",
        "relation": "evaluated_on",
        "tail": "nine image classification datasets"
      },
      {
        "head": "BioTune",
        "relation": "evaluated_on",
        "tail": "medical imaging"
      },
      {
        "head": "BioTune",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "BioTune",
        "relation": "uses_metric",
        "tail": "efficiency"
      },
      {
        "head": "Hand Sign Language Detection Using Deep Learning",
        "relation": "proposed_model",
        "tail": "VGG-16 net"
      },
      {
        "head": "Hand Sign Language Detection Using Deep Learning",
        "relation": "evaluated_on",
        "tail": "NUS dataset"
      },
      {
        "head": "Hand Sign Language Detection Using Deep Learning",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "Microsoft COCO: Common Objects in Context",
        "relation": "baseline_model",
        "tail": "Deformable Parts Model"
      },
      {
        "head": "Microsoft COCO: Common Objects in Context",
        "relation": "evaluated_on",
        "tail": "PASCAL"
      },
      {
        "head": "Microsoft COCO: Common Objects in Context",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "Microsoft COCO: Common Objects in Context",
        "relation": "evaluated_on",
        "tail": "SUN"
      },
      {
        "head": "Deformable Parts Model",
        "relation": "uses_metric",
        "tail": "bounding box detection"
      },
      {
        "head": "Deformable Parts Model",
        "relation": "uses_metric",
        "tail": "segmentation detection"
      },
      {
        "head": "FANet: Frequency-Aware Attention-Based Tiny-Object Detection in Remote Sensing Images",
        "relation": "proposed_model",
        "tail": "FANet"
      },
      {
        "head": "FANet: Frequency-Aware Attention-Based Tiny-Object Detection in Remote Sensing Images",
        "relation": "proposed_model",
        "tail": "Multi-Scale Frequency Feature Enhancement Module (MSFFEM)"
      },
      {
        "head": "FANet: Frequency-Aware Attention-Based Tiny-Object Detection in Remote Sensing Images",
        "relation": "proposed_model",
        "tail": "Channel Attention-based RoI Enhancement Module (CAREM)"
      },
      {
        "head": "FANet",
        "relation": "evaluated_on",
        "tail": "AI-TOD"
      },
      {
        "head": "FANet",
        "relation": "evaluated_on",
        "tail": "VisDrone2019"
      },
      {
        "head": "FANet",
        "relation": "evaluated_on",
        "tail": "DOTA-v1.5"
      },
      {
        "head": "FANet",
        "relation": "uses_metric",
        "tail": "detection performance"
      },
      {
        "head": "Representation Learning: A Review and New Perspectives",
        "relation": "proposed_model",
        "tail": "probabilistic models"
      },
      {
        "head": "Representation Learning: A Review and New Perspectives",
        "relation": "proposed_model",
        "tail": "auto-encoders"
      },
      {
        "head": "Representation Learning: A Review and New Perspectives",
        "relation": "proposed_model",
        "tail": "manifold learning"
      },
      {
        "head": "Representation Learning: A Review and New Perspectives",
        "relation": "proposed_model",
        "tail": "deep networks"
      },
      {
        "head": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion",
        "relation": "proposed_model",
        "tail": "Stacked Denoising Autoencoders"
      },
      {
        "head": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation",
        "relation": "proposed_model",
        "tail": "SDXL-Lightning"
      },
      {
        "head": "SDXL-Lightning",
        "relation": "baseline_model",
        "tail": "SDXL"
      },
      {
        "head": "SDXL-Lightning",
        "relation": "uses_metric",
        "tail": "quality"
      },
      {
        "head": "SDXL-Lightning",
        "relation": "uses_metric",
        "tail": "mode coverage"
      },
      {
        "head": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions",
        "relation": "proposed_model",
        "tail": "EchoMimic"
      },
      {
        "head": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions",
        "relation": "evaluated_on",
        "tail": "various public datasets"
      },
      {
        "head": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions",
        "relation": "evaluated_on",
        "tail": "our collected dataset"
      },
      {
        "head": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions",
        "relation": "uses_metric",
        "tail": "quantitative evaluations"
      },
      {
        "head": "EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions",
        "relation": "uses_metric",
        "tail": "qualitative evaluations"
      },
      {
        "head": "GenAD: Generative End-to-End Autonomous Driving",
        "relation": "proposed_model",
        "tail": "GenAD"
      },
      {
        "head": "GenAD",
        "relation": "evaluated_on",
        "tail": "nuScenes"
      },
      {
        "head": "GenAD",
        "relation": "uses_metric",
        "tail": "state-of-the-art performance"
      },
      {
        "head": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model",
        "relation": "proposed_model",
        "tail": "Ovis"
      },
      {
        "head": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model",
        "relation": "baseline_model",
        "tail": "Multimodal Large Language Models (MLLMs)"
      },
      {
        "head": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model",
        "relation": "baseline_model",
        "tail": "Qwen-VL-Plus"
      },
      {
        "head": "Ovis",
        "relation": "evaluated_on",
        "tail": "various multimodal benchmarks"
      },
      {
        "head": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model",
        "relation": "uses_metric",
        "tail": "empirical evaluations"
      },
      {
        "head": "Improving Video Generation with Human Feedback",
        "relation": "proposed_model",
        "tail": "VideoReward"
      },
      {
        "head": "Improving Video Generation with Human Feedback",
        "relation": "proposed_model",
        "tail": "Flow-DPO"
      },
      {
        "head": "Improving Video Generation with Human Feedback",
        "relation": "proposed_model",
        "tail": "Flow-RWR"
      },
      {
        "head": "Improving Video Generation with Human Feedback",
        "relation": "proposed_model",
        "tail": "Flow-NRG"
      },
      {
        "head": "VideoReward",
        "relation": "evaluated_on",
        "tail": "large-scale human preference dataset"
      },
      {
        "head": "Flow-DPO",
        "relation": "baseline_model",
        "tail": "Flow-RWR"
      },
      {
        "head": "Flow-DPO",
        "relation": "baseline_model",
        "tail": "supervised fine-tuning methods"
      },
      {
        "head": "VideoReward",
        "relation": "uses_metric",
        "tail": "rewarding efficacy"
      },
      {
        "head": "Speech Recognition with Deep Recurrent Neural Networks",
        "relation": "proposed_model",
        "tail": "deep recurrent neural networks"
      },
      {
        "head": "Speech Recognition with Deep Recurrent Neural Networks",
        "relation": "proposed_model",
        "tail": "deep Long Short-term Memory RNNs"
      },
      {
        "head": "Speech Recognition with Deep Recurrent Neural Networks",
        "relation": "baseline_model",
        "tail": "deep feedforward networks"
      },
      {
        "head": "deep Long Short-term Memory RNNs",
        "relation": "evaluated_on",
        "tail": "TIMIT phoneme recognition benchmark"
      },
      {
        "head": "deep Long Short-term Memory RNNs",
        "relation": "uses_metric",
        "tail": "test set error"
      },
      {
        "head": "Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks",
        "relation": "proposed_model",
        "tail": "Connectionist Temporal Classification"
      },
      {
        "head": "Connectionist Temporal Classification",
        "relation": "baseline_model",
        "tail": "Recurrent Neural Networks"
      },
      {
        "head": "2005 Special Issue: Framewise phoneme classification with bidirectional LSTM and other neural network architectures",
        "relation": "proposed_model",
        "tail": "bidirectional LSTM"
      },
      {
        "head": "2005 Special Issue: Framewise phoneme classification with bidirectional LSTM and other neural network architectures",
        "relation": "proposed_model",
        "tail": "other neural network architectures"
      },
      {
        "head": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses",
        "relation": "proposed_model",
        "tail": "machine learning"
      },
      {
        "head": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses",
        "relation": "proposed_model",
        "tail": "artificial synapses"
      },
      {
        "head": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses",
        "relation": "evaluated_on",
        "tail": "flexible sensors"
      },
      {
        "head": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses",
        "relation": "evaluated_on",
        "tail": "human activities"
      },
      {
        "head": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses",
        "relation": "evaluated_on",
        "tail": "artificial sensory organs"
      },
      {
        "head": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses",
        "relation": "evaluated_on",
        "tail": "soft robotics"
      },
      {
        "head": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses",
        "relation": "uses_metric",
        "tail": "data analysis"
      },
      {
        "head": "Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses",
        "relation": "uses_metric",
        "tail": "intelligent decision-making"
      },
      {
        "head": "A high-performance speech neuroprosthesis",
        "relation": "proposed_model",
        "tail": "speech-to-text BCI"
      },
      {
        "head": "speech-to-text BCI",
        "relation": "evaluated_on",
        "tail": "50-word vocabulary"
      },
      {
        "head": "speech-to-text BCI",
        "relation": "evaluated_on",
        "tail": "125,000-word vocabulary"
      },
      {
        "head": "speech-to-text BCI",
        "relation": "uses_metric",
        "tail": "word error rate"
      },
      {
        "head": "speech-to-text BCI",
        "relation": "uses_metric",
        "tail": "words per minute"
      },
      {
        "head": "Loss of plasticity in deep continual learning",
        "relation": "proposed_model",
        "tail": "continual backpropagation algorithm"
      },
      {
        "head": "Loss of plasticity in deep continual learning",
        "relation": "baseline_model",
        "tail": "deep-learning methods"
      },
      {
        "head": "Loss of plasticity in deep continual learning",
        "relation": "baseline_model",
        "tail": "backpropagation algorithm"
      },
      {
        "head": "Loss of plasticity in deep continual learning",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "Loss of plasticity in deep continual learning",
        "relation": "uses_metric",
        "tail": "plasticity"
      },
      {
        "head": "An analog-AI chip for energy-efficient speech recognition and transcription",
        "relation": "proposed_model",
        "tail": "keyword-spotting network"
      },
      {
        "head": "An analog-AI chip for energy-efficient speech recognition and transcription",
        "relation": "proposed_model",
        "tail": "MLPerf recurrent neural-network transducer (RNNT)"
      },
      {
        "head": "keyword-spotting network",
        "relation": "evaluated_on",
        "tail": "speech-recognition tasks"
      },
      {
        "head": "MLPerf recurrent neural-network transducer (RNNT)",
        "relation": "evaluated_on",
        "tail": "speech-recognition tasks"
      },
      {
        "head": "keyword-spotting network",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "MLPerf recurrent neural-network transducer (RNNT)",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "An analog-AI chip for energy-efficient speech recognition and transcription",
        "relation": "uses_metric",
        "tail": "energy efficiency"
      },
      {
        "head": "An analog-AI chip for energy-efficient speech recognition and transcription",
        "relation": "uses_metric",
        "tail": "TOPS/W chip-sustained performance"
      },
      {
        "head": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers",
        "relation": "proposed_model",
        "tail": "LiteToken"
      },
      {
        "head": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers",
        "relation": "baseline_model",
        "tail": "BPE tokenizers"
      },
      {
        "head": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers",
        "relation": "evaluated_on",
        "tail": "commonly used tokenizers"
      },
      {
        "head": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers",
        "relation": "uses_metric",
        "tail": "token fragmentation"
      },
      {
        "head": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers",
        "relation": "uses_metric",
        "tail": "parameters"
      },
      {
        "head": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers",
        "relation": "uses_metric",
        "tail": "robustness to noisy or misspelled inputs"
      },
      {
        "head": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers",
        "relation": "uses_metric",
        "tail": "overall performance"
      },
      {
        "head": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling",
        "relation": "proposed_model",
        "tail": "MeKi"
      },
      {
        "head": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling",
        "relation": "baseline_model",
        "tail": "dense LLM baselines"
      },
      {
        "head": "MeKi",
        "relation": "evaluated_on",
        "tail": "edge devices"
      },
      {
        "head": "MeKi",
        "relation": "uses_metric",
        "tail": "inference speed"
      },
      {
        "head": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram",
        "relation": "proposed_model",
        "tail": "Gengram"
      },
      {
        "head": "Gengram",
        "relation": "baseline_model",
        "tail": "genomic foundation models (GFMs)"
      },
      {
        "head": "Gengram",
        "relation": "evaluated_on",
        "tail": "functional genomics tasks"
      },
      {
        "head": "L$^3$: Large Lookup Layers",
        "relation": "proposed_model",
        "tail": "L$^3$"
      },
      {
        "head": "L$^3$: Large Lookup Layers",
        "relation": "baseline_model",
        "tail": "Mixture-of-Experts (MoE)"
      },
      {
        "head": "L$^3$: Large Lookup Layers",
        "relation": "baseline_model",
        "tail": "dense models"
      },
      {
        "head": "L$^3$: Large Lookup Layers",
        "relation": "baseline_model",
        "tail": "iso-sparse MoEs"
      },
      {
        "head": "L$^3$",
        "relation": "evaluated_on",
        "tail": "language modeling"
      },
      {
        "head": "L$^3$",
        "relation": "evaluated_on",
        "tail": "downstream tasks"
      },
      {
        "head": "L$^3$",
        "relation": "uses_metric",
        "tail": "speed"
      },
      {
        "head": "L$^3$",
        "relation": "uses_metric",
        "tail": "quality"
      },
      {
        "head": "Scaling Embeddings Outperforms Scaling Experts in Language Models",
        "relation": "proposed_model",
        "tail": "LongCat-Flash-Lite"
      },
      {
        "head": "Scaling Embeddings Outperforms Scaling Experts in Language Models",
        "relation": "baseline_model",
        "tail": "Mixture-of-Experts (MoE)"
      },
      {
        "head": "LongCat-Flash-Lite",
        "relation": "evaluated_on",
        "tail": "agentic and coding domains"
      },
      {
        "head": "Going Deeper with Convolutions",
        "relation": "proposed_model",
        "tail": "Inception"
      },
      {
        "head": "Going Deeper with Convolutions",
        "relation": "proposed_model",
        "tail": "GoogLeNet"
      },
      {
        "head": "Inception",
        "relation": "evaluated_on",
        "tail": "ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014)"
      },
      {
        "head": "GoogLeNet",
        "relation": "evaluated_on",
        "tail": "ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014)"
      },
      {
        "head": "Gradient-based learning applied to document recognition",
        "relation": "proposed_model",
        "tail": "Convolutional Neural Network (CNN)"
      },
      {
        "head": "Gradient-based learning applied to document recognition",
        "relation": "evaluated_on",
        "tail": "MNIST"
      },
      {
        "head": "Regression Shrinkage and Selection via the Lasso",
        "relation": "proposed_model",
        "tail": "Lasso"
      },
      {
        "head": "LifeCLEF Plant Identification Task 2015",
        "relation": "evaluated_on",
        "tail": "LifeCLEF plant identification challenge"
      },
      {
        "head": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics",
        "relation": "proposed_model",
        "tail": "FedMicro-IDA"
      },
      {
        "head": "FedMicro-IDA",
        "relation": "evaluated_on",
        "tail": "MaleVis"
      },
      {
        "head": "FedMicro-IDA",
        "relation": "uses_metric",
        "tail": "detection and classification performance"
      },
      {
        "head": "Enhancing pine wilt disease detection with synthetic data and external attention-based transformers",
        "relation": "proposed_model",
        "tail": "external attention-based transformers"
      },
      {
        "head": "Harnessing large vision and language models in agriculture: a review",
        "relation": "proposed_model",
        "tail": "tailored models for agricultural question-answering"
      },
      {
        "head": "Harnessing large vision and language models in agriculture: a review",
        "relation": "proposed_model",
        "tail": "robotic automation"
      },
      {
        "head": "Harnessing large vision and language models in agriculture: a review",
        "relation": "proposed_model",
        "tail": "advanced image analysis from remote sensing and spectral data"
      },
      {
        "head": "Harnessing large vision and language models in agriculture: a review",
        "relation": "baseline_model",
        "tail": "traditional models"
      },
      {
        "head": "Harnessing large vision and language models in agriculture: a review",
        "relation": "evaluated_on",
        "tail": "Web of Science"
      },
      {
        "head": "Harnessing large vision and language models in agriculture: a review",
        "relation": "evaluated_on",
        "tail": "arXiv"
      },
      {
        "head": "Harnessing large vision and language models in agriculture: a review",
        "relation": "uses_metric",
        "tail": "bibliometric analysis"
      },
      {
        "head": "A systematic comparison of predictive models on the retina",
        "relation": "proposed_model",
        "tail": "linear-nonlinear (LN) models"
      },
      {
        "head": "A systematic comparison of predictive models on the retina",
        "relation": "proposed_model",
        "tail": "convolutional neural networks (CNNs)"
      },
      {
        "head": "linear-nonlinear (LN) models",
        "relation": "baseline_model",
        "tail": "convolutional neural networks (CNNs)"
      },
      {
        "head": "linear-nonlinear (LN) models",
        "relation": "evaluated_on",
        "tail": "marmoset and salamander retinas datasets"
      },
      {
        "head": "convolutional neural networks (CNNs)",
        "relation": "evaluated_on",
        "tail": "marmoset and salamander retinas datasets"
      },
      {
        "head": "linear-nonlinear (LN) models",
        "relation": "uses_metric",
        "tail": "predictive performance"
      },
      {
        "head": "convolutional neural networks (CNNs)",
        "relation": "uses_metric",
        "tail": "predictive performance"
      },
      {
        "head": "linear-nonlinear (LN) models",
        "relation": "uses_metric",
        "tail": "cross-stimulus generalization"
      },
      {
        "head": "convolutional neural networks (CNNs)",
        "relation": "uses_metric",
        "tail": "cross-stimulus generalization"
      },
      {
        "head": "Distinctive Image Features from Scale-Invariant Keypoints",
        "relation": "proposed_model",
        "tail": "SIFT"
      },
      {
        "head": "SIFT",
        "relation": "evaluated_on",
        "tail": "Image matching dataset"
      },
      {
        "head": "SIFT",
        "relation": "uses_metric",
        "tail": "Accuracy"
      },
      {
        "head": "LLaVA-OneVision-1.5",
        "relation": "proposed_model",
        "tail": "LLaVA-OneVision-1.5-8B"
      },
      {
        "head": "LLaVA-OneVision-1.5",
        "relation": "proposed_model",
        "tail": "LLaVA-OneVision-1.5-4B"
      },
      {
        "head": "LLaVA-OneVision-1.5",
        "relation": "baseline_model",
        "tail": "Qwen2.5-VL-7B"
      },
      {
        "head": "LLaVA-OneVision-1.5",
        "relation": "baseline_model",
        "tail": "Qwen2.5-VL-3B"
      },
      {
        "head": "LLaVA-OneVision-1.5-8B",
        "relation": "evaluated_on",
        "tail": "27 benchmarks"
      },
      {
        "head": "LLaVA-OneVision-1.5-4B",
        "relation": "evaluated_on",
        "tail": "27 benchmarks"
      },
      {
        "head": "LLaVA-OneVision-1.5",
        "relation": "evaluated_on",
        "tail": "27 benchmarks"
      },
      {
        "head": "LLaVA-OneVision-1.5",
        "relation": "uses_metric",
        "tail": "27 benchmarks"
      },
      {
        "head": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications",
        "relation": "proposed_model",
        "tail": "Vision-Language-Action (VLA) models"
      },
      {
        "head": "Vision-Language-Action (VLA) models",
        "relation": "baseline_model",
        "tail": "large language models (LLMs)"
      },
      {
        "head": "Vision-Language-Action (VLA) models",
        "relation": "baseline_model",
        "tail": "vision-language models (VLMs)"
      },
      {
        "head": "Vision-Language-Action (VLA) models",
        "relation": "evaluated_on",
        "tail": "publicly available datasets"
      },
      {
        "head": "Vision-Language-Action (VLA) models",
        "relation": "uses_metric",
        "tail": "evaluation benchmarks"
      },
      {
        "head": "Aligning machine and human visual representations across abstraction levels",
        "relation": "proposed_model",
        "tail": "teacher model"
      },
      {
        "head": "Aligning machine and human visual representations across abstraction levels",
        "relation": "proposed_model",
        "tail": "human-aligned models"
      },
      {
        "head": "Aligning machine and human visual representations across abstraction levels",
        "relation": "baseline_model",
        "tail": "pretrained state-of-the-art vision foundation models"
      },
      {
        "head": "human-aligned models",
        "relation": "evaluated_on",
        "tail": "dataset of human judgements spanning multiple levels of semantic abstractions"
      },
      {
        "head": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
        "relation": "proposed_model",
        "tail": "Vision Transformer (ViT)"
      },
      {
        "head": "Vision Transformer (ViT)",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "Vision Transformer (ViT)",
        "relation": "evaluated_on",
        "tail": "CIFAR-100"
      },
      {
        "head": "Vision Transformer (ViT)",
        "relation": "evaluated_on",
        "tail": "VTAB"
      },
      {
        "head": "Vision Transformer (ViT)",
        "relation": "baseline_model",
        "tail": "state-of-the-art convolutional networks"
      },
      {
        "head": "Learning Transferable Visual Models From Natural Language Supervision",
        "relation": "proposed_model",
        "tail": "CLIP"
      },
      {
        "head": "Learning Transferable Visual Models From Natural Language Supervision",
        "relation": "baseline_model",
        "tail": "ResNet-50"
      },
      {
        "head": "CLIP",
        "relation": "evaluated_on",
        "tail": "over 30 different existing computer vision datasets"
      },
      {
        "head": "CLIP",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "CLIP",
        "relation": "evaluated_on",
        "tail": "400 million (image, text) pairs"
      },
      {
        "head": "Learning Transferable Visual Models From Natural Language Supervision",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "GENERATIVE ADVERSARIAL NETS",
        "relation": "proposed_model",
        "tail": "Generative Adversarial Network (GAN)"
      },
      {
        "head": "Improved Mean Flows: On the Challenges of Fastforward Generative Models",
        "relation": "proposed_model",
        "tail": "improved MeanFlow (iMF)"
      },
      {
        "head": "Improved Mean Flows: On the Challenges of Fastforward Generative Models",
        "relation": "baseline_model",
        "tail": "MeanFlow (MF)"
      },
      {
        "head": "improved MeanFlow (iMF)",
        "relation": "evaluated_on",
        "tail": "ImageNet 256×256"
      },
      {
        "head": "improved MeanFlow (iMF)",
        "relation": "uses_metric",
        "tail": "FID"
      },
      {
        "head": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training",
        "relation": "proposed_model",
        "tail": "pixel-space diffusion and consistency models"
      },
      {
        "head": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training",
        "relation": "evaluated_on",
        "tail": "ImageNet-256"
      },
      {
        "head": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training",
        "relation": "evaluated_on",
        "tail": "ImageNet-512"
      },
      {
        "head": "There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training",
        "relation": "uses_metric",
        "tail": "FID"
      },
      {
        "head": "pixel-space diffusion and consistency models",
        "relation": "baseline_model",
        "tail": "DiT"
      },
      {
        "head": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
        "relation": "proposed_model",
        "tail": "SVG-T2I"
      },
      {
        "head": "SVG-T2I",
        "relation": "baseline_model",
        "tail": "SVG (Self-supervised representations for Visual Generation)"
      },
      {
        "head": "SVG-T2I",
        "relation": "evaluated_on",
        "tail": "GenEval"
      },
      {
        "head": "SVG-T2I",
        "relation": "evaluated_on",
        "tail": "DPG-Bench"
      },
      {
        "head": "SVG-T2I",
        "relation": "uses_metric",
        "tail": "0.75"
      },
      {
        "head": "SVG-T2I",
        "relation": "uses_metric",
        "tail": "85.78"
      },
      {
        "head": "PixelDiT: Pixel Diffusion Transformers for Image Generation",
        "relation": "proposed_model",
        "tail": "PixelDiT"
      },
      {
        "head": "PixelDiT",
        "relation": "baseline_model",
        "tail": "Diffusion Transformers (DiTs)"
      },
      {
        "head": "PixelDiT",
        "relation": "evaluated_on",
        "tail": "ImageNet 256x256"
      },
      {
        "head": "PixelDiT",
        "relation": "evaluated_on",
        "tail": "GenEval"
      },
      {
        "head": "PixelDiT",
        "relation": "evaluated_on",
        "tail": "DPG-bench"
      },
      {
        "head": "PixelDiT",
        "relation": "uses_metric",
        "tail": "FID"
      },
      {
        "head": "PixelDiT",
        "relation": "baseline_model",
        "tail": "pixel generative models"
      },
      {
        "head": "PixelDiT",
        "relation": "baseline_model",
        "tail": "latent diffusion models"
      },
      {
        "head": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models",
        "relation": "proposed_model",
        "tail": "TUNA"
      },
      {
        "head": "TUNA",
        "relation": "baseline_model",
        "tail": "prior UMMs with decoupled representations"
      },
      {
        "head": "TUNA",
        "relation": "evaluated_on",
        "tail": "multimodal understanding and generation benchmarks"
      },
      {
        "head": "TUNA",
        "relation": "uses_metric",
        "tail": "performance"
      },
      {
        "head": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "relation": "proposed_model",
        "tail": "BERT"
      },
      {
        "head": "BERT",
        "relation": "evaluated_on",
        "tail": "GLUE"
      },
      {
        "head": "BERT",
        "relation": "evaluated_on",
        "tail": "MultiNLI"
      },
      {
        "head": "BERT",
        "relation": "evaluated_on",
        "tail": "SQuAD v1.1"
      },
      {
        "head": "BERT",
        "relation": "evaluated_on",
        "tail": "SQuAD v2.0"
      },
      {
        "head": "BERT",
        "relation": "uses_metric",
        "tail": "GLUE score"
      },
      {
        "head": "BERT",
        "relation": "uses_metric",
        "tail": "MultiNLI accuracy"
      },
      {
        "head": "BERT",
        "relation": "uses_metric",
        "tail": "SQuAD v1.1 question answering Test F1"
      },
      {
        "head": "BERT",
        "relation": "uses_metric",
        "tail": "SQuAD v2.0 Test F1"
      },
      {
        "head": "Diffusion Language Models are Super Data Learners",
        "relation": "proposed_model",
        "tail": "Diffusion language models (DLMs)"
      },
      {
        "head": "Diffusion Language Models are Super Data Learners",
        "relation": "baseline_model",
        "tail": "autoregressive (AR) models"
      },
      {
        "head": "1.7B DLM",
        "relation": "evaluated_on",
        "tail": "10B unique Python tokens"
      },
      {
        "head": "1B-parameter DLM",
        "relation": "evaluated_on",
        "tail": "HellaSwag"
      },
      {
        "head": "1B-parameter DLM",
        "relation": "evaluated_on",
        "tail": "MMLU"
      },
      {
        "head": "1B-parameter DLM",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "Diffusion Language Models are Super Data Learners",
        "relation": "uses_metric",
        "tail": "validation cross-entropy"
      },
      {
        "head": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas",
        "relation": "proposed_model",
        "tail": "DreamOn"
      },
      {
        "head": "DreamOn",
        "relation": "baseline_model",
        "tail": "Dream-Coder-7B"
      },
      {
        "head": "DreamOn",
        "relation": "baseline_model",
        "tail": "DiffuCoder-7B"
      },
      {
        "head": "DreamOn",
        "relation": "evaluated_on",
        "tail": "HumanEval-Infilling"
      },
      {
        "head": "DreamOn",
        "relation": "evaluated_on",
        "tail": "SantaCoder-FIM"
      },
      {
        "head": "FLEX: Continuous Agent Evolution via Forward Learning from Experience",
        "relation": "proposed_model",
        "tail": "FLEX"
      },
      {
        "head": "FLEX",
        "relation": "evaluated_on",
        "tail": "AIME25"
      },
      {
        "head": "FLEX",
        "relation": "evaluated_on",
        "tail": "USPTO50k"
      },
      {
        "head": "FLEX",
        "relation": "evaluated_on",
        "tail": "ProteinGym"
      },
      {
        "head": "FLEX",
        "relation": "uses_metric",
        "tail": "mathematical reasoning"
      },
      {
        "head": "FLEX",
        "relation": "uses_metric",
        "tail": "chemical retrosynthesis"
      },
      {
        "head": "FLEX",
        "relation": "uses_metric",
        "tail": "protein fitness prediction"
      },
      {
        "head": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning",
        "relation": "proposed_model",
        "tail": "Agent-R1"
      },
      {
        "head": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning",
        "relation": "evaluated_on",
        "tail": "Multihop QA benchmark tasks"
      },
      {
        "head": "nuScenes: A multimodal dataset for autonomous driving",
        "relation": "proposed_model",
        "tail": "nuScenes"
      },
      {
        "head": "nuScenes: A multimodal dataset for autonomous driving",
        "relation": "baseline_model",
        "tail": "KITTI dataset"
      },
      {
        "head": "nuScenes: A multimodal dataset for autonomous driving",
        "relation": "evaluated_on",
        "tail": "nuScenes"
      },
      {
        "head": "nuScenes: A multimodal dataset for autonomous driving",
        "relation": "uses_metric",
        "tail": "novel 3D detection and tracking metrics"
      },
      {
        "head": "Controllable Video Generation: A Survey",
        "relation": "proposed_model",
        "tail": "single-condition generation"
      },
      {
        "head": "Controllable Video Generation: A Survey",
        "relation": "proposed_model",
        "tail": "multi-condition generation"
      },
      {
        "head": "Controllable Video Generation: A Survey",
        "relation": "proposed_model",
        "tail": "universal controllable generation"
      },
      {
        "head": "Controllable Video Generation: A Survey",
        "relation": "baseline_model",
        "tail": "video generation foundation models"
      },
      {
        "head": "Controllable Video Generation: A Survey",
        "relation": "baseline_model",
        "tail": "text-to-video generation"
      },
      {
        "head": "Controllable Video Generation: A Survey",
        "relation": "evaluated_on",
        "tail": "Awesome-Controllable-Video-Generation"
      },
      {
        "head": "Adversarial Attacks on Autonomous Driving Systems in the Physical World: A Survey",
        "relation": "proposed_model",
        "tail": "multi-label classification method"
      },
      {
        "head": "Adversarial Attacks on Autonomous Driving Systems in the Physical World: A Survey",
        "relation": "baseline_model",
        "tail": "Autonomous Driving Systems (ADS)"
      },
      {
        "head": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail",
        "relation": "proposed_model",
        "tail": "Alpamayo-R1 (AR1)"
      },
      {
        "head": "Alpamayo-R1 (AR1)",
        "relation": "baseline_model",
        "tail": "trajectory-only baseline"
      },
      {
        "head": "Alpamayo-R1 (AR1)",
        "relation": "evaluated_on",
        "tail": "Chain of Causation (CoC) dataset"
      },
      {
        "head": "Alpamayo-R1 (AR1)",
        "relation": "uses_metric",
        "tail": "planning accuracy"
      },
      {
        "head": "Alpamayo-R1 (AR1)",
        "relation": "uses_metric",
        "tail": "close encounter rate"
      },
      {
        "head": "Alpamayo-R1 (AR1)",
        "relation": "uses_metric",
        "tail": "reasoning quality"
      },
      {
        "head": "Alpamayo-R1 (AR1)",
        "relation": "uses_metric",
        "tail": "reasoning-action consistency"
      },
      {
        "head": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes",
        "relation": "proposed_model",
        "tail": "Ego3D-VLM"
      },
      {
        "head": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes",
        "relation": "baseline_model",
        "tail": "GPT-4o"
      },
      {
        "head": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes",
        "relation": "baseline_model",
        "tail": "Gemini1.5-Pro"
      },
      {
        "head": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes",
        "relation": "baseline_model",
        "tail": "InternVL3"
      },
      {
        "head": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes",
        "relation": "baseline_model",
        "tail": "Qwen2.5-VL"
      },
      {
        "head": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes",
        "relation": "evaluated_on",
        "tail": "Ego3D-Bench"
      },
      {
        "head": "Ego3D-VLM",
        "relation": "evaluated_on",
        "tail": "Ego3D-Bench"
      },
      {
        "head": "Ego3D-VLM",
        "relation": "uses_metric",
        "tail": "multi-choice QA"
      },
      {
        "head": "Ego3D-VLM",
        "relation": "uses_metric",
        "tail": "absolute distance estimation"
      },
      {
        "head": "Detect Anything via Next Point Prediction",
        "relation": "proposed_model",
        "tail": "Rex-Omni"
      },
      {
        "head": "Detect Anything via Next Point Prediction",
        "relation": "baseline_model",
        "tail": "YOLO"
      },
      {
        "head": "Detect Anything via Next Point Prediction",
        "relation": "baseline_model",
        "tail": "DETR"
      },
      {
        "head": "Detect Anything via Next Point Prediction",
        "relation": "baseline_model",
        "tail": "Grounding DINO"
      },
      {
        "head": "Rex-Omni",
        "relation": "evaluated_on",
        "tail": "COCO"
      },
      {
        "head": "Rex-Omni",
        "relation": "evaluated_on",
        "tail": "LVIS"
      },
      {
        "head": "Human-level control through deep reinforcement learning",
        "relation": "proposed_model",
        "tail": "Deep Q-Network (DQN)"
      },
      {
        "head": "Human-level control through deep reinforcement learning",
        "relation": "evaluated_on",
        "tail": "Atari 2600 games"
      },
      {
        "head": "Human-level control through deep reinforcement learning",
        "relation": "uses_metric",
        "tail": "human-level performance"
      },
      {
        "head": "Asynchronous Methods for Deep Reinforcement Learning",
        "relation": "proposed_model",
        "tail": "asynchronous gradient descent"
      },
      {
        "head": "Asynchronous Methods for Deep Reinforcement Learning",
        "relation": "proposed_model",
        "tail": "asynchronous actor-critic"
      },
      {
        "head": "asynchronous actor-critic",
        "relation": "evaluated_on",
        "tail": "Atari domain"
      },
      {
        "head": "asynchronous actor-critic",
        "relation": "evaluated_on",
        "tail": "continuous motor control problems"
      },
      {
        "head": "asynchronous actor-critic",
        "relation": "evaluated_on",
        "tail": "random 3D mazes"
      },
      {
        "head": "asynchronous actor-critic",
        "relation": "uses_metric",
        "tail": "training time"
      },
      {
        "head": "RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation",
        "relation": "proposed_model",
        "tail": "RefineNet"
      },
      {
        "head": "RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation",
        "relation": "evaluated_on",
        "tail": "PASCAL VOC 2012"
      },
      {
        "head": "RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation",
        "relation": "uses_metric",
        "tail": "intersection-over-union"
      },
      {
        "head": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving",
        "relation": "proposed_model",
        "tail": "LightEMMA"
      },
      {
        "head": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving",
        "relation": "baseline_model",
        "tail": "Vision-Language Models (VLMs)"
      },
      {
        "head": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving",
        "relation": "evaluated_on",
        "tail": "nuScenes"
      },
      {
        "head": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving",
        "relation": "uses_metric",
        "tail": "computational metrics"
      },
      {
        "head": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving",
        "relation": "proposed_model",
        "tail": "ReCogDrive"
      },
      {
        "head": "ReCogDrive",
        "relation": "baseline_model",
        "tail": "Vision-Language Models (VLMs)"
      },
      {
        "head": "ReCogDrive",
        "relation": "evaluated_on",
        "tail": "NAVSIM"
      },
      {
        "head": "ReCogDrive",
        "relation": "evaluated_on",
        "tail": "Bench2Drive"
      },
      {
        "head": "ReCogDrive",
        "relation": "uses_metric",
        "tail": "DriveBench"
      },
      {
        "head": "Pseudo-Simulation for Autonomous Driving",
        "relation": "proposed_model",
        "tail": "pseudo-simulation"
      },
      {
        "head": "pseudo-simulation",
        "relation": "baseline_model",
        "tail": "open-loop evaluation"
      },
      {
        "head": "pseudo-simulation",
        "relation": "evaluated_on",
        "tail": "real datasets"
      },
      {
        "head": "pseudo-simulation",
        "relation": "uses_metric",
        "tail": "closed-loop simulation"
      },
      {
        "head": "pseudo-simulation",
        "relation": "uses_metric",
        "tail": "R^2"
      },
      {
        "head": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving",
        "relation": "proposed_model",
        "tail": "DriveMoE"
      },
      {
        "head": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving",
        "relation": "baseline_model",
        "tail": "Drive-π₀"
      },
      {
        "head": "DriveMoE",
        "relation": "evaluated_on",
        "tail": "Bench2Drive"
      },
      {
        "head": "DriveMoE",
        "relation": "uses_metric",
        "tail": "state-of-the-art (SOTA) performance"
      },
      {
        "head": "A Survey on Vision-Language-Action Models for Autonomous Driving",
        "relation": "proposed_model",
        "tail": "VLA for Autonomous Driving (VLA4AD)"
      },
      {
        "head": "Vision-Language-Action (VLA) paradigms",
        "relation": "baseline_model",
        "tail": "multimodal large language models (MLLM)"
      },
      {
        "head": "A Survey on Vision-Language-Action Models for Autonomous Driving",
        "relation": "evaluated_on",
        "tail": "existing datasets and benchmarks"
      },
      {
        "head": "existing datasets and benchmarks",
        "relation": "uses_metric",
        "tail": "driving safety"
      },
      {
        "head": "existing datasets and benchmarks",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "existing datasets and benchmarks",
        "relation": "uses_metric",
        "tail": "explanation quality"
      },
      {
        "head": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond",
        "relation": "proposed_model",
        "tail": "General world models"
      },
      {
        "head": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond",
        "relation": "baseline_model",
        "tail": "Sora model"
      },
      {
        "head": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond",
        "relation": "baseline_model",
        "tail": "autonomous-driving world models"
      },
      {
        "head": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond",
        "relation": "baseline_model",
        "tail": "world models deployed within autonomous agents"
      },
      {
        "head": "Understanding World or Predicting Future? A Comprehensive Survey of World Models",
        "relation": "proposed_model",
        "tail": "GPT-4"
      },
      {
        "head": "Understanding World or Predicting Future? A Comprehensive Survey of World Models",
        "relation": "proposed_model",
        "tail": "Sora"
      },
      {
        "head": "A Survey of Multimodal Learning: Methods, Applications, and Future",
        "relation": "proposed_model",
        "tail": "multimodal machine learning"
      },
      {
        "head": "A Survey of Multimodal Learning: Methods, Applications, and Future",
        "relation": "baseline_model",
        "tail": "state-of-the-art methods"
      },
      {
        "head": "A Survey of Multimodal Learning: Methods, Applications, and Future",
        "relation": "evaluated_on",
        "tail": "datasets covered in multimodal learning research"
      },
      {
        "head": "multimodal machine learning",
        "relation": "uses_metric",
        "tail": "multimodal learning"
      },
      {
        "head": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey",
        "relation": "proposed_model",
        "tail": "multimodal generative models"
      },
      {
        "head": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey",
        "relation": "baseline_model",
        "tail": "multimodal foundation models"
      },
      {
        "head": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey",
        "relation": "evaluated_on",
        "tail": "Any-to-Text"
      },
      {
        "head": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey",
        "relation": "evaluated_on",
        "tail": "Any-to-Vision"
      },
      {
        "head": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey",
        "relation": "evaluated_on",
        "tail": "Any-to-Any"
      },
      {
        "head": "multimodal generative models",
        "relation": "uses_metric",
        "tail": "Any-to-Text"
      },
      {
        "head": "multimodal generative models",
        "relation": "uses_metric",
        "tail": "Any-to-Vision"
      },
      {
        "head": "multimodal generative models",
        "relation": "uses_metric",
        "tail": "Any-to-Any"
      },
      {
        "head": "Effects of Generative AI in Tourism Industry",
        "relation": "proposed_model",
        "tail": "new theoretical framework for decision making in the tourism industry"
      },
      {
        "head": "Effects of Generative AI in Tourism Industry",
        "relation": "baseline_model",
        "tail": "existing responsive AI instruments"
      },
      {
        "head": "Effects of Generative AI in Tourism Industry",
        "relation": "evaluated_on",
        "tail": "tourism and hospitality scenarios"
      },
      {
        "head": "Squeeze-and-Excitation Networks",
        "relation": "proposed_model",
        "tail": "Squeeze-and-Excitation (SE) block"
      },
      {
        "head": "Squeeze-and-Excitation Networks",
        "relation": "proposed_model",
        "tail": "SENet"
      },
      {
        "head": "Squeeze-and-Excitation Networks",
        "relation": "evaluated_on",
        "tail": "ILSVRC 2017"
      },
      {
        "head": "Squeeze-and-Excitation Networks",
        "relation": "uses_metric",
        "tail": "top-5 error"
      },
      {
        "head": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
        "relation": "proposed_model",
        "tail": "EfficientNets"
      },
      {
        "head": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
        "relation": "baseline_model",
        "tail": "MobileNets"
      },
      {
        "head": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
        "relation": "baseline_model",
        "tail": "ResNet"
      },
      {
        "head": "EfficientNets",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "EfficientNets",
        "relation": "evaluated_on",
        "tail": "CIFAR-100"
      },
      {
        "head": "EfficientNets",
        "relation": "evaluated_on",
        "tail": "Flowers"
      },
      {
        "head": "EfficientNet-B7",
        "relation": "uses_metric",
        "tail": "top-1 accuracy"
      },
      {
        "head": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control",
        "relation": "proposed_model",
        "tail": "Light-X"
      },
      {
        "head": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control",
        "relation": "proposed_model",
        "tail": "Light-Syn"
      },
      {
        "head": "Light-X",
        "relation": "evaluated_on",
        "tail": "Light-Syn dataset"
      },
      {
        "head": "Light-X",
        "relation": "baseline_model",
        "tail": "baseline methods"
      },
      {
        "head": "Light-X",
        "relation": "baseline_model",
        "tail": "prior video relighting methods"
      },
      {
        "head": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World",
        "relation": "proposed_model",
        "tail": "DriveLaW"
      },
      {
        "head": "DriveLaW",
        "relation": "evaluated_on",
        "tail": "NAVSIM"
      },
      {
        "head": "DriveLaW",
        "relation": "uses_metric",
        "tail": "FID"
      },
      {
        "head": "DriveLaW",
        "relation": "uses_metric",
        "tail": "FVD"
      },
      {
        "head": "DVGT: Driving Visual Geometry Transformer",
        "relation": "proposed_model",
        "tail": "DVGT"
      },
      {
        "head": "DVGT",
        "relation": "baseline_model",
        "tail": "DINO"
      },
      {
        "head": "DVGT",
        "relation": "evaluated_on",
        "tail": "nuScenes"
      },
      {
        "head": "DVGT",
        "relation": "evaluated_on",
        "tail": "OpenScene"
      },
      {
        "head": "DVGT",
        "relation": "evaluated_on",
        "tail": "Waymo"
      },
      {
        "head": "DVGT",
        "relation": "evaluated_on",
        "tail": "KITTI"
      },
      {
        "head": "DVGT",
        "relation": "evaluated_on",
        "tail": "DDAD"
      },
      {
        "head": "Adding Conditional Control to Text-to-Image Diffusion Models",
        "relation": "proposed_model",
        "tail": "ControlNet"
      },
      {
        "head": "ControlNet",
        "relation": "baseline_model",
        "tail": "Stable Diffusion"
      },
      {
        "head": "ControlNet",
        "relation": "evaluated_on",
        "tail": "small (<50k) and large (>1m) datasets"
      },
      {
        "head": "Scaling Instruction-Finetuned Language Models",
        "relation": "proposed_model",
        "tail": "Flan-PaLM 540B"
      },
      {
        "head": "Scaling Instruction-Finetuned Language Models",
        "relation": "proposed_model",
        "tail": "Flan-T5"
      },
      {
        "head": "Scaling Instruction-Finetuned Language Models",
        "relation": "baseline_model",
        "tail": "PaLM 540B"
      },
      {
        "head": "Scaling Instruction-Finetuned Language Models",
        "relation": "baseline_model",
        "tail": "PaLM 62B"
      },
      {
        "head": "Flan-PaLM 540B",
        "relation": "evaluated_on",
        "tail": "MMLU"
      },
      {
        "head": "Flan-PaLM 540B",
        "relation": "evaluated_on",
        "tail": "BBH"
      },
      {
        "head": "Flan-PaLM 540B",
        "relation": "evaluated_on",
        "tail": "TyDiQA"
      },
      {
        "head": "Flan-PaLM 540B",
        "relation": "evaluated_on",
        "tail": "MGSM"
      },
      {
        "head": "Flan-PaLM 540B",
        "relation": "uses_metric",
        "tail": "five-shot MMLU"
      },
      {
        "head": "High-Resolution Image Synthesis with Latent Diffusion Models",
        "relation": "proposed_model",
        "tail": "latent diffusion models (LDMs)"
      },
      {
        "head": "latent diffusion models (LDMs)",
        "relation": "baseline_model",
        "tail": "diffusion models (DMs)"
      },
      {
        "head": "latent diffusion models (LDMs)",
        "relation": "evaluated_on",
        "tail": "image inpainting"
      },
      {
        "head": "latent diffusion models (LDMs)",
        "relation": "evaluated_on",
        "tail": "unconditional image generation"
      },
      {
        "head": "latent diffusion models (LDMs)",
        "relation": "evaluated_on",
        "tail": "semantic scene synthesis"
      },
      {
        "head": "latent diffusion models (LDMs)",
        "relation": "evaluated_on",
        "tail": "super-resolution"
      },
      {
        "head": "latent diffusion models (LDMs)",
        "relation": "uses_metric",
        "tail": "visual fidelity"
      },
      {
        "head": "latent diffusion models (LDMs)",
        "relation": "uses_metric",
        "tail": "computational requirements"
      },
      {
        "head": "AUTO-ENCODING VARIATIONAL BAYES",
        "relation": "proposed_model",
        "tail": "Variational Autoencoder (VAE)"
      },
      {
        "head": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
        "relation": "proposed_model",
        "tail": "VGG network"
      },
      {
        "head": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
        "relation": "baseline_model",
        "tail": "PSNR"
      },
      {
        "head": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
        "relation": "baseline_model",
        "tail": "SSIM"
      },
      {
        "head": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
        "relation": "evaluated_on",
        "tail": "new dataset of human perceptual similarity judgments"
      },
      {
        "head": "VGG network",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
        "relation": "uses_metric",
        "tail": "PSNR"
      },
      {
        "head": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
        "relation": "uses_metric",
        "tail": "SSIM"
      },
      {
        "head": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset",
        "relation": "proposed_model",
        "tail": "Waymo Open Dataset"
      },
      {
        "head": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset",
        "relation": "evaluated_on",
        "tail": "Waymo Open Dataset"
      },
      {
        "head": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset",
        "relation": "uses_metric",
        "tail": "diversity metric"
      },
      {
        "head": "Generative Adversarial Networks",
        "relation": "proposed_model",
        "tail": "Generative Adversarial Networks"
      },
      {
        "head": "Generative Adversarial Networks",
        "relation": "evaluated_on",
        "tail": "Data augmentation"
      },
      {
        "head": "Generative Adversarial Networks",
        "relation": "evaluated_on",
        "tail": "face images generation"
      },
      {
        "head": "Generalization of Diffusion Models Arises with a Balanced Representation Space",
        "relation": "proposed_model",
        "tail": "representation-based method for detecting memorization"
      },
      {
        "head": "Generalization of Diffusion Models Arises with a Balanced Representation Space",
        "relation": "proposed_model",
        "tail": "training-free editing technique"
      },
      {
        "head": "Generalization of Diffusion Models Arises with a Balanced Representation Space",
        "relation": "evaluated_on",
        "tail": "unconditional and text-to-image diffusion models"
      },
      {
        "head": "two-layer ReLU denoising autoencoder (DAE)",
        "relation": "baseline_model",
        "tail": "unconditional and text-to-image diffusion models"
      },
      {
        "head": "Stable Velocity: A Variance Perspective on Flow Matching",
        "relation": "proposed_model",
        "tail": "Stable Velocity"
      },
      {
        "head": "Stable Velocity: A Variance Perspective on Flow Matching",
        "relation": "proposed_model",
        "tail": "Stable Velocity Matching (StableVM)"
      },
      {
        "head": "Stable Velocity: A Variance Perspective on Flow Matching",
        "relation": "proposed_model",
        "tail": "Variance-Aware Representation Alignment (VA-REPA)"
      },
      {
        "head": "Stable Velocity: A Variance Perspective on Flow Matching",
        "relation": "proposed_model",
        "tail": "Stable Velocity Sampling (StableVS)"
      },
      {
        "head": "Stable Velocity: A Variance Perspective on Flow Matching",
        "relation": "baseline_model",
        "tail": "SD3.5"
      },
      {
        "head": "Stable Velocity: A Variance Perspective on Flow Matching",
        "relation": "baseline_model",
        "tail": "Flux"
      },
      {
        "head": "Stable Velocity: A Variance Perspective on Flow Matching",
        "relation": "baseline_model",
        "tail": "Qwen-Image"
      },
      {
        "head": "Stable Velocity: A Variance Perspective on Flow Matching",
        "relation": "baseline_model",
        "tail": "Wan2.2"
      },
      {
        "head": "Stable Velocity: A Variance Perspective on Flow Matching",
        "relation": "evaluated_on",
        "tail": "ImageNet 256×256"
      },
      {
        "head": "Stable Velocity: A Variance Perspective on Flow Matching",
        "relation": "uses_metric",
        "tail": "training efficiency"
      },
      {
        "head": "Stable Velocity: A Variance Perspective on Flow Matching",
        "relation": "uses_metric",
        "tail": "sample quality"
      },
      {
        "head": "Laminating Representation Autoencoders for Efficient Diffusion",
        "relation": "proposed_model",
        "tail": "FlatDINO"
      },
      {
        "head": "Laminating Representation Autoencoders for Efficient Diffusion",
        "relation": "baseline_model",
        "tail": "DINOv2"
      },
      {
        "head": "FlatDINO",
        "relation": "evaluated_on",
        "tail": "ImageNet 256x256"
      },
      {
        "head": "DiT-XL",
        "relation": "evaluated_on",
        "tail": "ImageNet 256x256"
      },
      {
        "head": "Laminating Representation Autoencoders for Efficient Diffusion",
        "relation": "uses_metric",
        "tail": "gFID"
      },
      {
        "head": "Adaptive 1D Video Diffusion Autoencoder",
        "relation": "proposed_model",
        "tail": "One-Dimensional Diffusion Video Autoencoder (One-DVA)"
      },
      {
        "head": "Adaptive 1D Video Diffusion Autoencoder",
        "relation": "baseline_model",
        "tail": "3D-CNN VAEs"
      },
      {
        "head": "Test-Time Conditioning with Representation-Aligned Visual Features",
        "relation": "proposed_model",
        "tail": "REPA-G"
      },
      {
        "head": "REPA-G",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "REPA-G",
        "relation": "evaluated_on",
        "tail": "COCO"
      },
      {
        "head": "Bio-inspired fine-tuning for selective transfer learning in image classification",
        "relation": "proposed_model",
        "tail": "BioTune"
      },
      {
        "head": "Bio-inspired fine-tuning for selective transfer learning in image classification",
        "relation": "baseline_model",
        "tail": "AutoRGN"
      },
      {
        "head": "Bio-inspired fine-tuning for selective transfer learning in image classification",
        "relation": "baseline_model",
        "tail": "LoRA"
      },
      {
        "head": "BioTune",
        "relation": "evaluated_on",
        "tail": "nine image classification datasets"
      },
      {
        "head": "BioTune",
        "relation": "evaluated_on",
        "tail": "medical imaging"
      },
      {
        "head": "BioTune",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "BioTune",
        "relation": "uses_metric",
        "tail": "efficiency"
      },
      {
        "head": "Densely Connected Convolutional Networks",
        "relation": "proposed_model",
        "tail": "Dense Convolutional Network (DenseNet)"
      },
      {
        "head": "Dense Convolutional Network (DenseNet)",
        "relation": "evaluated_on",
        "tail": "CIFAR-10"
      },
      {
        "head": "Dense Convolutional Network (DenseNet)",
        "relation": "evaluated_on",
        "tail": "CIFAR-100"
      },
      {
        "head": "Dense Convolutional Network (DenseNet)",
        "relation": "evaluated_on",
        "tail": "SVHN"
      },
      {
        "head": "Dense Convolutional Network (DenseNet)",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "Dense Convolutional Network (DenseNet)",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "Dense Convolutional Network (DenseNet)",
        "relation": "uses_metric",
        "tail": "computation"
      },
      {
        "head": "An Advanced Convolutional Neural Network Architecture Utilizing Transfer Learning for Melanoma Detection",
        "relation": "proposed_model",
        "tail": "Convolutional Neural Network"
      },
      {
        "head": "An Advanced Convolutional Neural Network Architecture Utilizing Transfer Learning for Melanoma Detection",
        "relation": "proposed_model",
        "tail": "Transfer Learning"
      },
      {
        "head": "Adaptive Transfer Learning for Surgical Tool Presence Detection in Laparoscopic Videos Through Gradual Freezing Fine‐Tuning",
        "relation": "proposed_model",
        "tail": "staged adaptive fine-tuning approach"
      },
      {
        "head": "staged adaptive fine-tuning approach",
        "relation": "baseline_model",
        "tail": "ResNet-50"
      },
      {
        "head": "staged adaptive fine-tuning approach",
        "relation": "baseline_model",
        "tail": "DenseNet-121"
      },
      {
        "head": "Adaptive Transfer Learning for Surgical Tool Presence Detection in Laparoscopic Videos Through Gradual Freezing Fine‐Tuning",
        "relation": "evaluated_on",
        "tail": "Cholec80"
      },
      {
        "head": "Adaptive Transfer Learning for Surgical Tool Presence Detection in Laparoscopic Videos Through Gradual Freezing Fine‐Tuning",
        "relation": "evaluated_on",
        "tail": "CATARACTS"
      },
      {
        "head": "Adaptive Transfer Learning for Surgical Tool Presence Detection in Laparoscopic Videos Through Gradual Freezing Fine‐Tuning",
        "relation": "uses_metric",
        "tail": "mean average precision (mAP)"
      },
      {
        "head": "VGG Induced Deep Hand Sign Language Detection",
        "relation": "proposed_model",
        "tail": "VGG-16 net"
      },
      {
        "head": "VGG-16 net",
        "relation": "evaluated_on",
        "tail": "NUS dataset"
      },
      {
        "head": "VGG-16 net",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "MediaPipe: A Framework for Building Perception Pipelines",
        "relation": "proposed_model",
        "tail": "MediaPipe"
      },
      {
        "head": "Hand Pose Estimation and Hand Shape Classification Using Multi-layered Randomized Decision Forests",
        "relation": "proposed_model",
        "tail": "Multi-layered Randomized Decision Forests"
      },
      {
        "head": "A robust static hand gesture recognition system using geometry based normalizations and Krawtchouk moments",
        "relation": "proposed_model",
        "tail": "geometry based normalizations"
      },
      {
        "head": "A robust static hand gesture recognition system using geometry based normalizations and Krawtchouk moments",
        "relation": "proposed_model",
        "tail": "Krawtchouk moments"
      },
      {
        "head": "Hand signal classification system for sign language communication in Virtual Reality",
        "relation": "proposed_model",
        "tail": "machine learning model"
      },
      {
        "head": "machine learning model",
        "relation": "evaluated_on",
        "tail": "user's hand signals"
      },
      {
        "head": "Real-Time Static Hand Sign Recognition System using MediaPipe and Fully Connected Neural Network",
        "relation": "proposed_model",
        "tail": "MediaPipe and a fully connected neural network (FCNN)"
      },
      {
        "head": "MediaPipe and a fully connected neural network (FCNN)",
        "relation": "evaluated_on",
        "tail": "American Sign Language (ASL) dataset"
      },
      {
        "head": "Real-Time Static Hand Sign Recognition System using MediaPipe and Fully Connected Neural Network",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "Real-Time Static Hand Sign Recognition System using MediaPipe and Fully Connected Neural Network",
        "relation": "uses_metric",
        "tail": "fast recognition"
      },
      {
        "head": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation",
        "relation": "proposed_model",
        "tail": "R-CNN"
      },
      {
        "head": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation",
        "relation": "baseline_model",
        "tail": "OverFeat"
      },
      {
        "head": "R-CNN",
        "relation": "evaluated_on",
        "tail": "PASCAL VOC"
      },
      {
        "head": "R-CNN",
        "relation": "evaluated_on",
        "tail": "VOC 2012"
      },
      {
        "head": "R-CNN",
        "relation": "evaluated_on",
        "tail": "ILSVRC2013"
      },
      {
        "head": "R-CNN",
        "relation": "uses_metric",
        "tail": "mean average precision (mAP)"
      },
      {
        "head": "MuMu-LLaMA: Multi-modal music understanding and generation via large language models",
        "relation": "proposed_model",
        "tail": "MuMu-LLaMA"
      },
      {
        "head": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy",
        "relation": "proposed_model",
        "tail": "SuPLoRA"
      },
      {
        "head": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy",
        "relation": "proposed_model",
        "tail": "supertype-subtype concept hierarchy"
      },
      {
        "head": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy",
        "relation": "proposed_model",
        "tail": "group-wise suppression method"
      },
      {
        "head": "Mass Concept Erasure in Diffusion Models with Concept Hierarchy",
        "relation": "evaluated_on",
        "tail": "benchmark"
      },
      {
        "head": "SuPLoRA",
        "relation": "uses_metric",
        "tail": "standard diffusion regularization"
      },
      {
        "head": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism",
        "relation": "proposed_model",
        "tail": "Multi-modal Chain and Global Attention Network (MCGA-Net)"
      },
      {
        "head": "Intelligent Recognition of GPR Road Hidden Defect Images Based on Feature Fusion and Attention Mechanism",
        "relation": "proposed_model",
        "tail": "DCGAN-based data augmentation strategy"
      },
      {
        "head": "Multi-modal Chain and Global Attention Network (MCGA-Net)",
        "relation": "baseline_model",
        "tail": "other models"
      },
      {
        "head": "Multi-modal Chain and Global Attention Network (MCGA-Net)",
        "relation": "evaluated_on",
        "tail": "GPR images"
      },
      {
        "head": "Multi-modal Chain and Global Attention Network (MCGA-Net)",
        "relation": "uses_metric",
        "tail": "Precision"
      },
      {
        "head": "Multi-modal Chain and Global Attention Network (MCGA-Net)",
        "relation": "uses_metric",
        "tail": "Recall"
      },
      {
        "head": "Multi-modal Chain and Global Attention Network (MCGA-Net)",
        "relation": "uses_metric",
        "tail": "mAP@50"
      },
      {
        "head": "Multi-modal Chain and Global Attention Network (MCGA-Net)",
        "relation": "evaluated_on",
        "tail": "MS COCO"
      },
      {
        "head": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
        "relation": "proposed_model",
        "tail": "Qwen3-VL-Embedding"
      },
      {
        "head": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
        "relation": "proposed_model",
        "tail": "Qwen3-VL-Reranker"
      },
      {
        "head": "Qwen3-VL-Embedding",
        "relation": "baseline_model",
        "tail": "Qwen3-VL"
      },
      {
        "head": "Qwen3-VL-Reranker",
        "relation": "baseline_model",
        "tail": "Qwen3-VL"
      },
      {
        "head": "Qwen3-VL-Embedding",
        "relation": "evaluated_on",
        "tail": "MMEB-V2"
      },
      {
        "head": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness",
        "relation": "proposed_model",
        "tail": "FPSMark"
      },
      {
        "head": "Flexible Partial Screen-Shooting Watermarking With Provable Robustness",
        "relation": "proposed_model",
        "tail": "intrinsic signal localization network"
      },
      {
        "head": "FPSMark",
        "relation": "evaluated_on",
        "tail": "partial screen-shooting scenarios"
      },
      {
        "head": "FPSMark",
        "relation": "uses_metric",
        "tail": "extraction accuracy"
      },
      {
        "head": "existing methods",
        "relation": "baseline_model",
        "tail": "FPSMark"
      },
      {
        "head": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper",
        "relation": "proposed_model",
        "tail": "EM algorithm"
      },
      {
        "head": "Visualizing Data using t-SNE",
        "relation": "proposed_model",
        "tail": "t-SNE"
      },
      {
        "head": "LLM Social Simulations Are a Promising Research Method",
        "relation": "proposed_model",
        "tail": "LLM social simulations"
      },
      {
        "head": "LLM social simulations",
        "relation": "evaluated_on",
        "tail": "social science datasets"
      },
      {
        "head": "LLM social simulations",
        "relation": "uses_metric",
        "tail": "empirical comparisons"
      },
      {
        "head": "Federated Contrastive Learning With Feature-Based Distillation for Human Activity Recognition",
        "relation": "proposed_model",
        "tail": "FCLFD"
      },
      {
        "head": "FCLFD",
        "relation": "evaluated_on",
        "tail": "WISDM"
      },
      {
        "head": "FCLFD",
        "relation": "evaluated_on",
        "tail": "PAMAP2"
      },
      {
        "head": "FCLFD",
        "relation": "uses_metric",
        "tail": "F1"
      },
      {
        "head": "Diffuse and Disperse: Image Generation with Representation Regularization",
        "relation": "proposed_model",
        "tail": "Dispersive Loss"
      },
      {
        "head": "Diffuse and Disperse: Image Generation with Representation Regularization",
        "relation": "baseline_model",
        "tail": "REPA"
      },
      {
        "head": "Dispersive Loss",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems",
        "relation": "proposed_model",
        "tail": "Knowledge-Aware Bayesian Bandits (KABB)"
      },
      {
        "head": "Knowledge-Aware Bayesian Bandits (KABB)",
        "relation": "evaluated_on",
        "tail": "multi-agent systems"
      },
      {
        "head": "Knowledge-Aware Bayesian Bandits (KABB)",
        "relation": "uses_metric",
        "tail": "cost-performance balance"
      },
      {
        "head": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
        "relation": "proposed_model",
        "tail": "U-Net"
      },
      {
        "head": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
        "relation": "baseline_model",
        "tail": "sliding-window convolutional network"
      },
      {
        "head": "U-Net",
        "relation": "evaluated_on",
        "tail": "ISBI challenge for segmentation of neuronal structures in electron microscopic stacks"
      },
      {
        "head": "U-Net",
        "relation": "evaluated_on",
        "tail": "ISBI cell tracking challenge 2015"
      },
      {
        "head": "Improved Distribution Matching Distillation for Fast Image Synthesis",
        "relation": "proposed_model",
        "tail": "DMD2"
      },
      {
        "head": "Improved Distribution Matching Distillation for Fast Image Synthesis",
        "relation": "baseline_model",
        "tail": "Distribution Matching Distillation (DMD)"
      },
      {
        "head": "DMD2",
        "relation": "evaluated_on",
        "tail": "ImageNet-64x64"
      },
      {
        "head": "DMD2",
        "relation": "evaluated_on",
        "tail": "COCO 2014"
      },
      {
        "head": "DMD2",
        "relation": "uses_metric",
        "tail": "FID"
      },
      {
        "head": "Distribution Matching Distillation (DMD)",
        "relation": "uses_metric",
        "tail": "FID"
      },
      {
        "head": "Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation",
        "relation": "proposed_model",
        "tail": "Latent Adversarial Diffusion Distillation (LADD)"
      },
      {
        "head": "Latent Adversarial Diffusion Distillation (LADD)",
        "relation": "baseline_model",
        "tail": "adversarial diffusion distillation (ADD)"
      },
      {
        "head": "Latent Adversarial Diffusion Distillation (LADD)",
        "relation": "evaluated_on",
        "tail": "Stable Diffusion 3 (8B)"
      },
      {
        "head": "Latent Adversarial Diffusion Distillation (LADD)",
        "relation": "evaluated_on",
        "tail": "SD3-Turbo"
      },
      {
        "head": "adversarial diffusion distillation (ADD)",
        "relation": "uses_metric",
        "tail": "DINOv2"
      },
      {
        "head": "Latent Adversarial Diffusion Distillation (LADD)",
        "relation": "uses_metric",
        "tail": "latent diffusion models"
      },
      {
        "head": "SD3-Turbo",
        "relation": "baseline_model",
        "tail": "state-of-the-art text-to-image generators"
      },
      {
        "head": "Evolutionary optimization of model merging recipes",
        "relation": "proposed_model",
        "tail": "evolutionary approach"
      },
      {
        "head": "evolutionary approach",
        "relation": "proposed_model",
        "tail": "Japanese LLM with Math reasoning capabilities"
      },
      {
        "head": "evolutionary approach",
        "relation": "proposed_model",
        "tail": "culturally-aware Japanese VLM"
      },
      {
        "head": "Japanese LLM with Math reasoning capabilities",
        "relation": "evaluated_on",
        "tail": "Japanese LLM benchmarks"
      },
      {
        "head": "Japanese LLM with Math reasoning capabilities",
        "relation": "uses_metric",
        "tail": "state-of-the-art performance"
      },
      {
        "head": "culturally-aware Japanese VLM",
        "relation": "uses_metric",
        "tail": "state-of-the-art performance"
      },
      {
        "head": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
        "relation": "proposed_model",
        "tail": "autoregressive transformer"
      },
      {
        "head": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
        "relation": "baseline_model",
        "tail": "bidirectional diffusion transformer"
      },
      {
        "head": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
        "relation": "evaluated_on",
        "tail": "VBench-Long"
      },
      {
        "head": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
        "relation": "uses_metric",
        "tail": "total score"
      },
      {
        "head": "distribution matching distillation (DMD)",
        "relation": "proposed_model",
        "tail": "4-step generator"
      },
      {
        "head": "distribution matching distillation (DMD)",
        "relation": "baseline_model",
        "tail": "50-step diffusion model"
      },
      {
        "head": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment",
        "relation": "proposed_model",
        "tail": "PuLID"
      },
      {
        "head": "PuLID",
        "relation": "baseline_model",
        "tail": "Lightning T2I branch"
      },
      {
        "head": "PuLID",
        "relation": "baseline_model",
        "tail": "standard diffusion branch"
      },
      {
        "head": "Denoising Diffusion Probabilistic Models",
        "relation": "proposed_model",
        "tail": "diffusion probabilistic models"
      },
      {
        "head": "Denoising Diffusion Probabilistic Models",
        "relation": "evaluated_on",
        "tail": "CIFAR10"
      },
      {
        "head": "Denoising Diffusion Probabilistic Models",
        "relation": "evaluated_on",
        "tail": "LSUN"
      },
      {
        "head": "Denoising Diffusion Probabilistic Models",
        "relation": "uses_metric",
        "tail": "Inception score"
      },
      {
        "head": "Denoising Diffusion Probabilistic Models",
        "relation": "uses_metric",
        "tail": "FID score"
      },
      {
        "head": "Denoising Diffusion Probabilistic Models",
        "relation": "baseline_model",
        "tail": "ProgressiveGAN"
      },
      {
        "head": "HunyuanVideo: A Systematic Framework For Large Video Generative Models",
        "relation": "proposed_model",
        "tail": "HunyuanVideo"
      },
      {
        "head": "HunyuanVideo: A Systematic Framework For Large Video Generative Models",
        "relation": "baseline_model",
        "tail": "Runway Gen-3"
      },
      {
        "head": "HunyuanVideo: A Systematic Framework For Large Video Generative Models",
        "relation": "baseline_model",
        "tail": "Luma 1.6"
      },
      {
        "head": "HunyuanVideo: A Systematic Framework For Large Video Generative Models",
        "relation": "baseline_model",
        "tail": "three top-performing Chinese video generative models"
      },
      {
        "head": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency",
        "relation": "proposed_model",
        "tail": "Loopy"
      },
      {
        "head": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency",
        "relation": "baseline_model",
        "tail": "audio-driven portrait diffusion models"
      },
      {
        "head": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models",
        "relation": "proposed_model",
        "tail": "OmniHuman"
      },
      {
        "head": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models",
        "relation": "baseline_model",
        "tail": "existing end-to-end audio-driven methods"
      },
      {
        "head": "OmniHuman",
        "relation": "evaluated_on",
        "tail": "existing end-to-end audio-driven methods"
      },
      {
        "head": "OmniHuman",
        "relation": "uses_metric",
        "tail": "highly realistic human video generation"
      },
      {
        "head": "OmniHuman",
        "relation": "uses_metric",
        "tail": "more realistic videos"
      },
      {
        "head": "OmniHuman",
        "relation": "uses_metric",
        "tail": "greater flexibility in inputs"
      },
      {
        "head": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation",
        "relation": "proposed_model",
        "tail": "Hallo2"
      },
      {
        "head": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation",
        "relation": "baseline_model",
        "tail": "Hallo"
      },
      {
        "head": "Hallo2",
        "relation": "evaluated_on",
        "tail": "HDTF"
      },
      {
        "head": "Hallo2",
        "relation": "evaluated_on",
        "tail": "CelebV"
      },
      {
        "head": "Hallo2",
        "relation": "evaluated_on",
        "tail": "Wild"
      },
      {
        "head": "EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation",
        "relation": "proposed_model",
        "tail": "EchoMimicV2"
      },
      {
        "head": "EchoMimicV2",
        "relation": "baseline_model",
        "tail": "existing methods"
      },
      {
        "head": "EchoMimicV2",
        "relation": "evaluated_on",
        "tail": "novel benchmark for evaluating the effectiveness of half-body human animation"
      },
      {
        "head": "EchoMimicV2",
        "relation": "uses_metric",
        "tail": "quantitative evaluations"
      },
      {
        "head": "EchoMimicV2",
        "relation": "uses_metric",
        "tail": "qualitative evaluations"
      },
      {
        "head": "EchoMimicV2",
        "relation": "evaluated_on",
        "tail": "half-body data"
      },
      {
        "head": "EchoMimicV2",
        "relation": "evaluated_on",
        "tail": "headshot data"
      },
      {
        "head": "Feature Pyramid Networks for Object Detection",
        "relation": "proposed_model",
        "tail": "Feature Pyramid Network (FPN)"
      },
      {
        "head": "Feature Pyramid Networks for Object Detection",
        "relation": "baseline_model",
        "tail": "Faster R-CNN"
      },
      {
        "head": "Feature Pyramid Networks for Object Detection",
        "relation": "evaluated_on",
        "tail": "COCO detection benchmark"
      },
      {
        "head": "Feature Pyramid Networks for Object Detection",
        "relation": "uses_metric",
        "tail": "state-of-the-art single-model results"
      },
      {
        "head": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation",
        "relation": "proposed_model",
        "tail": "RNN Encoder-Decoder"
      },
      {
        "head": "RNN Encoder-Decoder",
        "relation": "baseline_model",
        "tail": "statistical machine translation system"
      },
      {
        "head": "RNN Encoder-Decoder",
        "relation": "baseline_model",
        "tail": "log-linear model"
      },
      {
        "head": "DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving",
        "relation": "proposed_model",
        "tail": "DriveMLM"
      },
      {
        "head": "DriveMLM",
        "relation": "baseline_model",
        "tail": "Autopilot"
      },
      {
        "head": "DriveMLM",
        "relation": "baseline_model",
        "tail": "Apollo"
      },
      {
        "head": "DriveMLM",
        "relation": "evaluated_on",
        "tail": "CARLA Town05 Long"
      },
      {
        "head": "DriveMLM",
        "relation": "uses_metric",
        "tail": "points"
      },
      {
        "head": "DriveMLM",
        "relation": "uses_metric",
        "tail": "improvements"
      },
      {
        "head": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability",
        "relation": "proposed_model",
        "tail": "Vista"
      },
      {
        "head": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability",
        "relation": "baseline_model",
        "tail": "most advanced general-purpose video generator"
      },
      {
        "head": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability",
        "relation": "baseline_model",
        "tail": "best-performing driving world model"
      },
      {
        "head": "Vista",
        "relation": "evaluated_on",
        "tail": "multiple datasets"
      },
      {
        "head": "Vista",
        "relation": "uses_metric",
        "tail": "FID"
      },
      {
        "head": "Vista",
        "relation": "uses_metric",
        "tail": "FVD"
      },
      {
        "head": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving",
        "relation": "proposed_model",
        "tail": "DiffusionDrive"
      },
      {
        "head": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving",
        "relation": "baseline_model",
        "tail": "vanilla diffusion policy"
      },
      {
        "head": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving",
        "relation": "evaluated_on",
        "tail": "NAVSIM"
      },
      {
        "head": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving",
        "relation": "uses_metric",
        "tail": "PDMS"
      },
      {
        "head": "DiffusionDrive",
        "relation": "evaluated_on",
        "tail": "NAVSIM"
      },
      {
        "head": "DiffusionDrive",
        "relation": "uses_metric",
        "tail": "PDMS"
      },
      {
        "head": "How Far is Video Generation from World Model: A Physical Law Perspective",
        "relation": "proposed_model",
        "tail": "diffusion-based video generation models"
      },
      {
        "head": "How Far is Video Generation from World Model: A Physical Law Perspective",
        "relation": "evaluated_on",
        "tail": "2D simulation testbed for object movement and collisions"
      },
      {
        "head": "How Far is Video Generation from World Model: A Physical Law Perspective",
        "relation": "uses_metric",
        "tail": "physical laws adherence"
      },
      {
        "head": "How Far is Video Generation from World Model: A Physical Law Perspective",
        "relation": "baseline_model",
        "tail": "Sora"
      },
      {
        "head": "EMMA: End-to-End Multimodal Model for Autonomous Driving",
        "relation": "proposed_model",
        "tail": "EMMA"
      },
      {
        "head": "EMMA",
        "relation": "baseline_model",
        "tail": "Gemini"
      },
      {
        "head": "EMMA",
        "relation": "evaluated_on",
        "tail": "nuScenes"
      },
      {
        "head": "EMMA",
        "relation": "evaluated_on",
        "tail": "Waymo Open Motion Dataset (WOMD)"
      },
      {
        "head": "EMMA",
        "relation": "evaluated_on",
        "tail": "Waymo Open Dataset (WOD)"
      },
      {
        "head": "EMMA",
        "relation": "uses_metric",
        "tail": "state-of-the-art performance in motion planning"
      },
      {
        "head": "EMMA",
        "relation": "uses_metric",
        "tail": "competitive results"
      },
      {
        "head": "Language Models are Few-Shot Learners",
        "relation": "proposed_model",
        "tail": "GPT-3"
      },
      {
        "head": "GPT-3",
        "relation": "evaluated_on",
        "tail": "translation, question-answering, and cloze tasks"
      },
      {
        "head": "GPT-3",
        "relation": "uses_metric",
        "tail": "few-shot performance"
      },
      {
        "head": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling",
        "relation": "proposed_model",
        "tail": "InternVL 2.5"
      },
      {
        "head": "InternVL 2.5",
        "relation": "baseline_model",
        "tail": "InternVL 2.0"
      },
      {
        "head": "InternVL 2.5",
        "relation": "evaluated_on",
        "tail": "MMMU benchmark"
      },
      {
        "head": "InternVL 2.5",
        "relation": "uses_metric",
        "tail": "Chain-of-Thought (CoT) reasoning"
      },
      {
        "head": "InternVL3",
        "relation": "proposed_model",
        "tail": "InternVL3-78B"
      },
      {
        "head": "InternVL3-78B",
        "relation": "evaluated_on",
        "tail": "MMMU"
      },
      {
        "head": "InternVL3-78B",
        "relation": "uses_metric",
        "tail": "score of 72.2"
      },
      {
        "head": "InternVL3-78B",
        "relation": "baseline_model",
        "tail": "ChatGPT-4o"
      },
      {
        "head": "InternVL3-78B",
        "relation": "baseline_model",
        "tail": "Claude 3.5 Sonnet"
      },
      {
        "head": "InternVL3-78B",
        "relation": "baseline_model",
        "tail": "Gemini 2.5 Pro"
      },
      {
        "head": "VLMEvalKit: An Open-Source ToolKit for Evaluating Large Multi-Modality Models",
        "relation": "proposed_model",
        "tail": "VLMEvalKit"
      },
      {
        "head": "VLMEvalKit",
        "relation": "uses_metric",
        "tail": "OpenVLM Leaderboard"
      },
      {
        "head": "LLaVA-CoT",
        "relation": "proposed_model",
        "tail": "LLaVA-CoT"
      },
      {
        "head": "LLaVA-CoT",
        "relation": "baseline_model",
        "tail": "Gemini-1.5-pro"
      },
      {
        "head": "LLaVA-CoT",
        "relation": "baseline_model",
        "tail": "GPT-4o-mini"
      },
      {
        "head": "LLaVA-CoT",
        "relation": "baseline_model",
        "tail": "Llama-3.2-90B-Vision-Instruct"
      },
      {
        "head": "LLaVA-CoT",
        "relation": "evaluated_on",
        "tail": "multimodal reasoning benchmarks"
      },
      {
        "head": "LLaVA-CoT",
        "relation": "uses_metric",
        "tail": "multimodal reasoning benchmarks"
      },
      {
        "head": "InternVL 3.5",
        "relation": "proposed_model",
        "tail": "Cascade Reinforcement Learning (Cascade RL) framework"
      },
      {
        "head": "InternVL 3.5",
        "relation": "proposed_model",
        "tail": "Visual Resolution Router (ViR)"
      },
      {
        "head": "InternVL 3.5",
        "relation": "proposed_model",
        "tail": "Decoupled Vision-Language Deployment (DvD) strategy"
      },
      {
        "head": "InternVL 3.5",
        "relation": "baseline_model",
        "tail": "InternVL3"
      },
      {
        "head": "InternVL 3.5",
        "relation": "evaluated_on",
        "tail": "MMMU"
      },
      {
        "head": "InternVL 3.5",
        "relation": "evaluated_on",
        "tail": "MathVista"
      },
      {
        "head": "InternVL 3.5",
        "relation": "uses_metric",
        "tail": "overall reasoning performance"
      },
      {
        "head": "InternVL 3.5",
        "relation": "uses_metric",
        "tail": "inference speedup"
      },
      {
        "head": "InternVL3.5-241B-A28B",
        "relation": "evaluated_on",
        "tail": "MMMU"
      },
      {
        "head": "InternVL3.5-241B-A28B",
        "relation": "evaluated_on",
        "tail": "MathVista"
      },
      {
        "head": "Proximal Policy Optimization Algorithms",
        "relation": "proposed_model",
        "tail": "Proximal Policy Optimization (PPO)"
      },
      {
        "head": "Proximal Policy Optimization (PPO)",
        "relation": "baseline_model",
        "tail": "Trust Region Policy Optimization (TRPO)"
      },
      {
        "head": "Proximal Policy Optimization (PPO)",
        "relation": "evaluated_on",
        "tail": "simulated robotic locomotion"
      },
      {
        "head": "Proximal Policy Optimization (PPO)",
        "relation": "evaluated_on",
        "tail": "Atari game playing"
      },
      {
        "head": "Proximal Policy Optimization (PPO)",
        "relation": "uses_metric",
        "tail": "sample complexity"
      },
      {
        "head": "Flow-GRPO",
        "relation": "proposed_model",
        "tail": "Flow-GRPO"
      },
      {
        "head": "Flow-GRPO",
        "relation": "baseline_model",
        "tail": "SD3.5-M"
      },
      {
        "head": "Flow-GRPO",
        "relation": "evaluated_on",
        "tail": "GenEval"
      },
      {
        "head": "Flow-GRPO",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "DanceGRPO",
        "relation": "proposed_model",
        "tail": "Group Relative Policy Optimization (GRPO)"
      },
      {
        "head": "DanceGRPO",
        "relation": "baseline_model",
        "tail": "DDPO"
      },
      {
        "head": "DanceGRPO",
        "relation": "baseline_model",
        "tail": "DPOK"
      },
      {
        "head": "DanceGRPO",
        "relation": "evaluated_on",
        "tail": "HPS-v2.1"
      },
      {
        "head": "DanceGRPO",
        "relation": "evaluated_on",
        "tail": "VideoAlign"
      },
      {
        "head": "DanceGRPO",
        "relation": "evaluated_on",
        "tail": "GenEval"
      },
      {
        "head": "DanceGRPO",
        "relation": "uses_metric",
        "tail": "CLIP Score"
      },
      {
        "head": "Seedance 1.0: Exploring the Boundaries of Video Generation Models",
        "relation": "proposed_model",
        "tail": "Seedance 1.0"
      },
      {
        "head": "Seedance 1.0: Exploring the Boundaries of Video Generation Models",
        "relation": "baseline_model",
        "tail": "state-of-the-art video generation models"
      },
      {
        "head": "Seedance 1.0",
        "relation": "evaluated_on",
        "tail": "multi-source data curation"
      },
      {
        "head": "Seedance 1.0",
        "relation": "uses_metric",
        "tail": "prompt following"
      },
      {
        "head": "Seedance 1.0",
        "relation": "uses_metric",
        "tail": "motion plausibility"
      },
      {
        "head": "Seedance 1.0",
        "relation": "uses_metric",
        "tail": "visual quality"
      },
      {
        "head": "Seedance 1.0",
        "relation": "uses_metric",
        "tail": "spatiotemporal fluidity"
      },
      {
        "head": "Seedance 1.0",
        "relation": "uses_metric",
        "tail": "structural stability"
      },
      {
        "head": "Seedance 1.0",
        "relation": "uses_metric",
        "tail": "instruction adherence"
      },
      {
        "head": "Seedance 1.0",
        "relation": "uses_metric",
        "tail": "narrative coherence"
      },
      {
        "head": "Seedance 1.0",
        "relation": "uses_metric",
        "tail": "subject representation"
      },
      {
        "head": "SkyReels-V2: Infinite-length Film Generative Model",
        "relation": "proposed_model",
        "tail": "SkyReels-V2"
      },
      {
        "head": "SkyReels-V2",
        "relation": "baseline_model",
        "tail": "Multi-modal Large Language Model (MLLM)"
      },
      {
        "head": "SkyReels-V2",
        "relation": "evaluated_on",
        "tail": "video data"
      },
      {
        "head": "SkyReels-V2",
        "relation": "evaluated_on",
        "tail": "human-annotated and synthetic distortion data"
      },
      {
        "head": "SkyReels-V2",
        "relation": "uses_metric",
        "tail": "prompt adherence"
      },
      {
        "head": "SkyReels-V2",
        "relation": "uses_metric",
        "tail": "visual quality"
      },
      {
        "head": "SkyReels-V2",
        "relation": "uses_metric",
        "tail": "motion dynamics"
      },
      {
        "head": "SkyReels-V2",
        "relation": "uses_metric",
        "tail": "duration"
      },
      {
        "head": "SkyReels-V2",
        "relation": "uses_metric",
        "tail": "temporal visual quality"
      },
      {
        "head": "SkyReels-V2",
        "relation": "uses_metric",
        "tail": "resolution"
      },
      {
        "head": "SkyReels-V2",
        "relation": "uses_metric",
        "tail": "shot-aware generation"
      },
      {
        "head": "SkyReels-V2",
        "relation": "uses_metric",
        "tail": "realistic long-form synthesis"
      },
      {
        "head": "SkyReels-V2",
        "relation": "uses_metric",
        "tail": "professional film-style generation"
      },
      {
        "head": "SkyReels-V2",
        "relation": "uses_metric",
        "tail": "dynamic artifacts"
      },
      {
        "head": "SkyReels-V2",
        "relation": "uses_metric",
        "tail": "visual fidelity"
      },
      {
        "head": "Unified Reward Model for Multimodal Understanding and Generation",
        "relation": "proposed_model",
        "tail": "UnifiedReward"
      },
      {
        "head": "UnifiedReward",
        "relation": "evaluated_on",
        "tail": "large-scale human preference dataset"
      },
      {
        "head": "UnifiedReward",
        "relation": "baseline_model",
        "tail": "Direct Preference Optimization (DPO)"
      },
      {
        "head": "UnifiedReward",
        "relation": "uses_metric",
        "tail": "performance"
      },
      {
        "head": "GPT-4 Technical Report",
        "relation": "proposed_model",
        "tail": "GPT-4"
      },
      {
        "head": "GPT-4",
        "relation": "baseline_model",
        "tail": "Transformer-based model"
      },
      {
        "head": "GPT-4",
        "relation": "evaluated_on",
        "tail": "simulated bar exam"
      },
      {
        "head": "GPT-4",
        "relation": "uses_metric",
        "tail": "factuality"
      },
      {
        "head": "GPT-4",
        "relation": "uses_metric",
        "tail": "adherence to desired behavior"
      },
      {
        "head": "LLaMA",
        "relation": "proposed_model",
        "tail": "LLaMA-13B"
      },
      {
        "head": "LLaMA",
        "relation": "proposed_model",
        "tail": "LLaMA-65B"
      },
      {
        "head": "LLaMA-13B",
        "relation": "baseline_model",
        "tail": "GPT-3 (175B)"
      },
      {
        "head": "LLaMA-65B",
        "relation": "baseline_model",
        "tail": "Chinchilla-70B"
      },
      {
        "head": "LLaMA-65B",
        "relation": "baseline_model",
        "tail": "PaLM-540B"
      },
      {
        "head": "LLaMA",
        "relation": "evaluated_on",
        "tail": "benchmarks"
      },
      {
        "head": "LLaMA",
        "relation": "evaluated_on",
        "tail": "publicly available datasets"
      },
      {
        "head": "LLaMA",
        "relation": "uses_metric",
        "tail": "benchmarks"
      },
      {
        "head": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
        "relation": "proposed_model",
        "tail": "chain of thought prompting"
      },
      {
        "head": "chain of thought prompting",
        "relation": "evaluated_on",
        "tail": "GSM8K"
      },
      {
        "head": "chain of thought prompting",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "chain of thought prompting",
        "relation": "baseline_model",
        "tail": "GPT-3"
      },
      {
        "head": "Training Verifiers to Solve Math Word Problems",
        "relation": "proposed_model",
        "tail": "verifiers"
      },
      {
        "head": "Training Verifiers to Solve Math Word Problems",
        "relation": "baseline_model",
        "tail": "finetuning baseline"
      },
      {
        "head": "Training Verifiers to Solve Math Word Problems",
        "relation": "evaluated_on",
        "tail": "GSM8K"
      },
      {
        "head": "Training Verifiers to Solve Math Word Problems",
        "relation": "uses_metric",
        "tail": "test performance"
      },
      {
        "head": "verifiers",
        "relation": "evaluated_on",
        "tail": "GSM8K"
      },
      {
        "head": "transformer models",
        "relation": "evaluated_on",
        "tail": "GSM8K"
      },
      {
        "head": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
        "relation": "proposed_model",
        "tail": "Mamba"
      },
      {
        "head": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
        "relation": "proposed_model",
        "tail": "Mamba-3B"
      },
      {
        "head": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
        "relation": "baseline_model",
        "tail": "Transformer"
      },
      {
        "head": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
        "relation": "baseline_model",
        "tail": "linear attention"
      },
      {
        "head": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
        "relation": "baseline_model",
        "tail": "gated convolution"
      },
      {
        "head": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
        "relation": "baseline_model",
        "tail": "recurrent models"
      },
      {
        "head": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
        "relation": "baseline_model",
        "tail": "structured state space models (SSMs)"
      },
      {
        "head": "Mamba",
        "relation": "evaluated_on",
        "tail": "language"
      },
      {
        "head": "Mamba",
        "relation": "evaluated_on",
        "tail": "audio"
      },
      {
        "head": "Mamba",
        "relation": "evaluated_on",
        "tail": "genomics"
      },
      {
        "head": "Mamba-3B",
        "relation": "evaluated_on",
        "tail": "language"
      },
      {
        "head": "Mamba",
        "relation": "uses_metric",
        "tail": "state-of-the-art performance"
      },
      {
        "head": "Mamba-3B",
        "relation": "uses_metric",
        "tail": "state-of-the-art performance"
      },
      {
        "head": "SGDR: Stochastic Gradient Descent with Warm Restarts",
        "relation": "proposed_model",
        "tail": "SGDR"
      },
      {
        "head": "SGDR",
        "relation": "evaluated_on",
        "tail": "CIFAR-10"
      },
      {
        "head": "SGDR",
        "relation": "evaluated_on",
        "tail": "CIFAR-100"
      },
      {
        "head": "SGDR",
        "relation": "evaluated_on",
        "tail": "EEG recordings dataset"
      },
      {
        "head": "SGDR",
        "relation": "evaluated_on",
        "tail": "downsampled ImageNet"
      },
      {
        "head": "SGDR",
        "relation": "uses_metric",
        "tail": "error rate"
      },
      {
        "head": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
        "relation": "proposed_model",
        "tail": "Sparsely-Gated Mixture-of-Experts layer (MoE)"
      },
      {
        "head": "Sparsely-Gated Mixture-of-Experts layer (MoE)",
        "relation": "evaluated_on",
        "tail": "language modeling"
      },
      {
        "head": "Sparsely-Gated Mixture-of-Experts layer (MoE)",
        "relation": "evaluated_on",
        "tail": "machine translation"
      },
      {
        "head": "Sparsely-Gated Mixture-of-Experts layer (MoE)",
        "relation": "evaluated_on",
        "tail": "large language modeling benchmarks"
      },
      {
        "head": "Sparsely-Gated Mixture-of-Experts layer (MoE)",
        "relation": "evaluated_on",
        "tail": "machine translation benchmarks"
      },
      {
        "head": "Sparsely-Gated Mixture-of-Experts layer (MoE)",
        "relation": "uses_metric",
        "tail": "computational efficiency"
      },
      {
        "head": "Pointer Sentinel Mixture Models",
        "relation": "proposed_model",
        "tail": "pointer sentinel mixture architecture"
      },
      {
        "head": "Pointer Sentinel Mixture Models",
        "relation": "proposed_model",
        "tail": "pointer sentinel-LSTM model"
      },
      {
        "head": "pointer sentinel-LSTM model",
        "relation": "baseline_model",
        "tail": "standard softmax LSTM"
      },
      {
        "head": "pointer sentinel-LSTM model",
        "relation": "evaluated_on",
        "tail": "Penn Treebank"
      },
      {
        "head": "Pointer Sentinel Mixture Models",
        "relation": "evaluated_on",
        "tail": "Penn Treebank"
      },
      {
        "head": "Pointer Sentinel Mixture Models",
        "relation": "proposed_model",
        "tail": "WikiText corpus"
      },
      {
        "head": "pointer sentinel-LSTM model",
        "relation": "uses_metric",
        "tail": "perplexity"
      },
      {
        "head": "Measuring Massive Multitask Language Understanding",
        "relation": "proposed_model",
        "tail": "massive multitask language understanding test"
      },
      {
        "head": "Measuring Massive Multitask Language Understanding",
        "relation": "evaluated_on",
        "tail": "massive multitask language understanding test"
      },
      {
        "head": "Measuring Massive Multitask Language Understanding",
        "relation": "uses_metric",
        "tail": "multitask accuracy"
      },
      {
        "head": "GPT-3",
        "relation": "baseline_model",
        "tail": "massive multitask language understanding test"
      },
      {
        "head": "Let's Verify Step by Step",
        "relation": "proposed_model",
        "tail": "process-supervised model"
      },
      {
        "head": "Let's Verify Step by Step",
        "relation": "evaluated_on",
        "tail": "MATH"
      },
      {
        "head": "process-supervised model",
        "relation": "evaluated_on",
        "tail": "MATH"
      },
      {
        "head": "Let's Verify Step by Step",
        "relation": "uses_metric",
        "tail": "78% of problems solved"
      },
      {
        "head": "process-supervised model",
        "relation": "uses_metric",
        "tail": "78% of problems solved"
      },
      {
        "head": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark",
        "relation": "proposed_model",
        "tail": "GPQA"
      },
      {
        "head": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark",
        "relation": "baseline_model",
        "tail": "GPT-4"
      },
      {
        "head": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark",
        "relation": "evaluated_on",
        "tail": "GPQA"
      },
      {
        "head": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "GPT-4",
        "relation": "evaluated_on",
        "tail": "GPQA"
      },
      {
        "head": "GPT-4",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
        "relation": "proposed_model",
        "tail": "GShard"
      },
      {
        "head": "GShard",
        "relation": "proposed_model",
        "tail": "multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts"
      },
      {
        "head": "GShard",
        "relation": "baseline_model",
        "tail": "prior art"
      },
      {
        "head": "multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts",
        "relation": "evaluated_on",
        "tail": "translation from 100 languages to English"
      },
      {
        "head": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
        "relation": "uses_metric",
        "tail": "model quality"
      },
      {
        "head": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
        "relation": "uses_metric",
        "tail": "computation cost"
      },
      {
        "head": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
        "relation": "uses_metric",
        "tail": "ease of programming"
      },
      {
        "head": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
        "relation": "uses_metric",
        "tail": "efficient implementation on parallel devices"
      },
      {
        "head": "Pattern Recognition and Machine Learning",
        "relation": "proposed_model",
        "tail": "Probability Distributions"
      },
      {
        "head": "Pattern Recognition and Machine Learning",
        "relation": "proposed_model",
        "tail": "Linear Models for Regression"
      },
      {
        "head": "Pattern Recognition and Machine Learning",
        "relation": "proposed_model",
        "tail": "Linear Models for Classification"
      },
      {
        "head": "Pattern Recognition and Machine Learning",
        "relation": "proposed_model",
        "tail": "Neural Networks"
      },
      {
        "head": "Pattern Recognition and Machine Learning",
        "relation": "proposed_model",
        "tail": "Kernel Methods"
      },
      {
        "head": "Pattern Recognition and Machine Learning",
        "relation": "proposed_model",
        "tail": "Sparse Kernel Machines"
      },
      {
        "head": "Pattern Recognition and Machine Learning",
        "relation": "proposed_model",
        "tail": "Graphical Models"
      },
      {
        "head": "Pattern Recognition and Machine Learning",
        "relation": "proposed_model",
        "tail": "Mixture Models and EM"
      },
      {
        "head": "Pattern Recognition and Machine Learning",
        "relation": "proposed_model",
        "tail": "Approximate Inference"
      },
      {
        "head": "Pattern Recognition and Machine Learning",
        "relation": "proposed_model",
        "tail": "Sampling Methods"
      },
      {
        "head": "Pattern Recognition and Machine Learning",
        "relation": "proposed_model",
        "tail": "Continuous Latent Variables"
      },
      {
        "head": "Pattern Recognition and Machine Learning",
        "relation": "proposed_model",
        "tail": "Sequential Data"
      },
      {
        "head": "Pattern Recognition and Machine Learning",
        "relation": "proposed_model",
        "tail": "Combining Models"
      },
      {
        "head": "Automatic Image-Based Plant Disease Severity Estimation Using Deep Learning",
        "relation": "proposed_model",
        "tail": "deep convolutional neural networks"
      },
      {
        "head": "Automatic Image-Based Plant Disease Severity Estimation Using Deep Learning",
        "relation": "baseline_model",
        "tail": "shallow networks"
      },
      {
        "head": "Automatic Image-Based Plant Disease Severity Estimation Using Deep Learning",
        "relation": "baseline_model",
        "tail": "deep models"
      },
      {
        "head": "deep convolutional neural networks",
        "relation": "evaluated_on",
        "tail": "PlantVillage"
      },
      {
        "head": "VGG16",
        "relation": "evaluated_on",
        "tail": "PlantVillage"
      },
      {
        "head": "VGG16",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "Plant identification using deep neural networks via optimization of transfer learning parameters",
        "relation": "proposed_model",
        "tail": "deep neural networks"
      },
      {
        "head": "Deep Learning for Plant Identification in Natural Environment",
        "relation": "proposed_model",
        "tail": "26-layer deep learning model consisting of 8 residual building blocks"
      },
      {
        "head": "Deep Learning for Plant Identification in Natural Environment",
        "relation": "evaluated_on",
        "tail": "BJFU100 dataset"
      },
      {
        "head": "26-layer deep learning model consisting of 8 residual building blocks",
        "relation": "uses_metric",
        "tail": "recognition rate"
      },
      {
        "head": "Going deeper in the automated identification of Herbarium specimens",
        "relation": "proposed_model",
        "tail": "Deep Learning"
      },
      {
        "head": "Going deeper in the automated identification of Herbarium specimens",
        "relation": "evaluated_on",
        "tail": "herbarium images"
      },
      {
        "head": "Going deeper in the automated identification of Herbarium specimens",
        "relation": "evaluated_on",
        "tail": "big dataset with thousands of species from herbaria"
      },
      {
        "head": "Going deeper in the automated identification of Herbarium specimens",
        "relation": "evaluated_on",
        "tail": "different datasets from different herbaria"
      },
      {
        "head": "Going deeper in the automated identification of Herbarium specimens",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "MobileNetV2: Inverted Residuals and Linear Bottlenecks",
        "relation": "proposed_model",
        "tail": "MobileNetV2"
      },
      {
        "head": "MobileNetV2: Inverted Residuals and Linear Bottlenecks",
        "relation": "proposed_model",
        "tail": "SSDLite"
      },
      {
        "head": "MobileNetV2: Inverted Residuals and Linear Bottlenecks",
        "relation": "proposed_model",
        "tail": "Mobile DeepLabv3"
      },
      {
        "head": "MobileNetV2",
        "relation": "evaluated_on",
        "tail": "Imagenet"
      },
      {
        "head": "MobileNetV2",
        "relation": "evaluated_on",
        "tail": "COCO"
      },
      {
        "head": "MobileNetV2",
        "relation": "evaluated_on",
        "tail": "VOC"
      },
      {
        "head": "MobileNetV2",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "MobileNetV2",
        "relation": "uses_metric",
        "tail": "multiply-adds (MAdd)"
      },
      {
        "head": "MobileNetV2",
        "relation": "uses_metric",
        "tail": "number of parameters"
      },
      {
        "head": "Communication-Efficient Learning of Deep Networks from Decentralized Data",
        "relation": "proposed_model",
        "tail": "Federated Learning"
      },
      {
        "head": "Federated Learning",
        "relation": "evaluated_on",
        "tail": "four datasets"
      },
      {
        "head": "Federated Learning",
        "relation": "baseline_model",
        "tail": "synchronized stochastic gradient descent"
      },
      {
        "head": "A Comprehensive Survey on Transfer Learning",
        "relation": "proposed_model",
        "tail": "Transfer Learning"
      },
      {
        "head": "A Comprehensive Survey on Transfer Learning",
        "relation": "evaluated_on",
        "tail": "Amazon Reviews"
      },
      {
        "head": "A Comprehensive Survey on Transfer Learning",
        "relation": "evaluated_on",
        "tail": "Reuters-21578"
      },
      {
        "head": "A Comprehensive Survey on Transfer Learning",
        "relation": "evaluated_on",
        "tail": "Office-31"
      },
      {
        "head": "Privacy Preserved and Decentralized Smartphone Recommendation System",
        "relation": "proposed_model",
        "tail": "federated deep neural network (FDNN)"
      },
      {
        "head": "Privacy Preserved and Decentralized Smartphone Recommendation System",
        "relation": "evaluated_on",
        "tail": "Flipkart dataset"
      },
      {
        "head": "Privacy Preserved and Decentralized Smartphone Recommendation System",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "A Survey on Heterogeneity Taxonomy, Security and Privacy Preservation in the Integration of IoT, Wireless Sensor Networks and Federated Learning",
        "relation": "proposed_model",
        "tail": "Federated learning (FL)"
      },
      {
        "head": "Federated learning (FL)",
        "relation": "evaluated_on",
        "tail": "Internet of Things (IoT)"
      },
      {
        "head": "Federated learning (FL)",
        "relation": "evaluated_on",
        "tail": "Wireless Sensor Networks (WSNs)"
      },
      {
        "head": "A privacy-preserving federated learning with a secure collaborative for malware detection models using Internet of Things resources",
        "relation": "proposed_model",
        "tail": "privacy-preserving federated learning"
      },
      {
        "head": "A privacy-preserving federated learning with a secure collaborative for malware detection models using Internet of Things resources",
        "relation": "proposed_model",
        "tail": "malware detection models"
      },
      {
        "head": "Federated Learning Architectures: A Performance Evaluation With Crop Yield Prediction Application",
        "relation": "proposed_model",
        "tail": "centralized federated learning framework"
      },
      {
        "head": "Federated Learning Architectures: A Performance Evaluation With Crop Yield Prediction Application",
        "relation": "proposed_model",
        "tail": "decentralized federated learning framework"
      },
      {
        "head": "centralized federated learning framework",
        "relation": "baseline_model",
        "tail": "Long Short-Term Memory Network"
      },
      {
        "head": "decentralized federated learning framework",
        "relation": "baseline_model",
        "tail": "Long Short-Term Memory Network"
      },
      {
        "head": "centralized federated learning framework",
        "relation": "evaluated_on",
        "tail": "crop yield prediction"
      },
      {
        "head": "decentralized federated learning framework",
        "relation": "evaluated_on",
        "tail": "crop yield prediction"
      },
      {
        "head": "centralized federated learning framework",
        "relation": "uses_metric",
        "tail": "prediction accuracy"
      },
      {
        "head": "centralized federated learning framework",
        "relation": "uses_metric",
        "tail": "precision"
      },
      {
        "head": "centralized federated learning framework",
        "relation": "uses_metric",
        "tail": "recall"
      },
      {
        "head": "centralized federated learning framework",
        "relation": "uses_metric",
        "tail": "F1-Score"
      },
      {
        "head": "centralized federated learning framework",
        "relation": "uses_metric",
        "tail": "training time"
      },
      {
        "head": "decentralized federated learning framework",
        "relation": "uses_metric",
        "tail": "prediction accuracy"
      },
      {
        "head": "decentralized federated learning framework",
        "relation": "uses_metric",
        "tail": "precision"
      },
      {
        "head": "decentralized federated learning framework",
        "relation": "uses_metric",
        "tail": "recall"
      },
      {
        "head": "decentralized federated learning framework",
        "relation": "uses_metric",
        "tail": "F1-Score"
      },
      {
        "head": "decentralized federated learning framework",
        "relation": "uses_metric",
        "tail": "training time"
      },
      {
        "head": "Segment Anything (SA) project",
        "relation": "proposed_model",
        "tail": "Segment Anything Model (SAM)"
      },
      {
        "head": "Segment Anything (SA) project",
        "relation": "evaluated_on",
        "tail": "SA-1B"
      },
      {
        "head": "Segment Anything (SA) project",
        "relation": "uses_metric",
        "tail": "zero-shot performance"
      },
      {
        "head": "Visual Instruction Tuning",
        "relation": "proposed_model",
        "tail": "LLaVA"
      },
      {
        "head": "Visual Instruction Tuning",
        "relation": "baseline_model",
        "tail": "GPT-4"
      },
      {
        "head": "LLaVA",
        "relation": "evaluated_on",
        "tail": "Science QA"
      },
      {
        "head": "LLaVA",
        "relation": "evaluated_on",
        "tail": "synthetic multimodal instruction-following dataset"
      },
      {
        "head": "LLaVA",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "LLaVA",
        "relation": "uses_metric",
        "tail": "relative score"
      },
      {
        "head": "Improved Baselines with Visual Instruction Tuning",
        "relation": "proposed_model",
        "tail": "LLaVA"
      },
      {
        "head": "Improved Baselines with Visual Instruction Tuning",
        "relation": "baseline_model",
        "tail": "CLIP-ViT-L-336px"
      },
      {
        "head": "Improved Baselines with Visual Instruction Tuning",
        "relation": "evaluated_on",
        "tail": "11 benchmarks"
      },
      {
        "head": "Improved Baselines with Visual Instruction Tuning",
        "relation": "uses_metric",
        "tail": "state-of-the-art"
      },
      {
        "head": "LLaVA",
        "relation": "baseline_model",
        "tail": "MLP projection"
      },
      {
        "head": "LLaVA",
        "relation": "evaluated_on",
        "tail": "academic-task-oriented VQA data"
      },
      {
        "head": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset",
        "relation": "proposed_model",
        "tail": "DepictQA-Wild"
      },
      {
        "head": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset",
        "relation": "baseline_model",
        "tail": "Vision Language Models (VLMs)"
      },
      {
        "head": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset",
        "relation": "baseline_model",
        "tail": "VLM-based Image Quality Assessment (IQA)"
      },
      {
        "head": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset",
        "relation": "baseline_model",
        "tail": "traditional score-based methods"
      },
      {
        "head": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset",
        "relation": "baseline_model",
        "tail": "GPT-4V"
      },
      {
        "head": "Enhancing Descriptive Image Quality Assessment With a Large-Scale Multi-Modal Dataset",
        "relation": "evaluated_on",
        "tail": "DQ-495K"
      },
      {
        "head": "DepictQA-Wild",
        "relation": "evaluated_on",
        "tail": "DQ-495K"
      },
      {
        "head": "DepictQA-Wild",
        "relation": "uses_metric",
        "tail": "distortion identification"
      },
      {
        "head": "DepictQA-Wild",
        "relation": "uses_metric",
        "tail": "instant rating"
      },
      {
        "head": "DepictQA-Wild",
        "relation": "uses_metric",
        "tail": "reasoning tasks"
      },
      {
        "head": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs",
        "relation": "proposed_model",
        "tail": "Chain-of-Focus (CoF)"
      },
      {
        "head": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs",
        "relation": "baseline_model",
        "tail": "Qwen2.5-VL"
      },
      {
        "head": "Chain-of-Focus (CoF)",
        "relation": "evaluated_on",
        "tail": "V* benchmark"
      },
      {
        "head": "Chain-of-Focus (CoF)",
        "relation": "uses_metric",
        "tail": "outcome accuracies"
      },
      {
        "head": "Chain-of-Focus (CoF)",
        "relation": "uses_metric",
        "tail": "formats"
      },
      {
        "head": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views",
        "relation": "proposed_model",
        "tail": "3DThinker"
      },
      {
        "head": "3DThinker",
        "relation": "evaluated_on",
        "tail": "multiple benchmarks"
      },
      {
        "head": "3DThinker",
        "relation": "uses_metric",
        "tail": "outcome signals"
      },
      {
        "head": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning",
        "relation": "proposed_model",
        "tail": "UniME-V2"
      },
      {
        "head": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning",
        "relation": "proposed_model",
        "tail": "MLLM-as-a-Judge mechanism"
      },
      {
        "head": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning",
        "relation": "proposed_model",
        "tail": "UniME-V2-Reranker"
      },
      {
        "head": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning",
        "relation": "evaluated_on",
        "tail": "MMEB benchmark"
      },
      {
        "head": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning",
        "relation": "evaluated_on",
        "tail": "retrieval tasks"
      },
      {
        "head": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning",
        "relation": "uses_metric",
        "tail": "state-of-the-art performance"
      },
      {
        "head": "OpenMMReasoner",
        "relation": "proposed_model",
        "tail": "OpenMMReasoner"
      },
      {
        "head": "OpenMMReasoner",
        "relation": "baseline_model",
        "tail": "Qwen2.5-VL-7B-Instruct"
      },
      {
        "head": "OpenMMReasoner",
        "relation": "evaluated_on",
        "tail": "nine multimodal reasoning benchmarks"
      },
      {
        "head": "OpenMMReasoner",
        "relation": "uses_metric",
        "tail": "nine multimodal reasoning benchmarks"
      },
      {
        "head": "Vision-Language-Action (VLA) Models: Concepts, Progress, Applications and Challenges",
        "relation": "proposed_model",
        "tail": "Vision-Language-Action (VLA) models"
      },
      {
        "head": "Vision-Language-Action (VLA) models",
        "relation": "baseline_model",
        "tail": "vision-language models (VLMs)"
      },
      {
        "head": "Vision-Language-Action (VLA) models",
        "relation": "evaluated_on",
        "tail": "autonomous vehicles"
      },
      {
        "head": "Vision-Language-Action (VLA) models",
        "relation": "evaluated_on",
        "tail": "medical and industrial robotics"
      },
      {
        "head": "Vision-Language-Action (VLA) models",
        "relation": "evaluated_on",
        "tail": "precision agriculture"
      },
      {
        "head": "Vision-Language-Action (VLA) models",
        "relation": "evaluated_on",
        "tail": "humanoid robotics"
      },
      {
        "head": "Vision-Language-Action (VLA) models",
        "relation": "evaluated_on",
        "tail": "augmented reality"
      },
      {
        "head": "Vision-Language-Action (VLA) models",
        "relation": "uses_metric",
        "tail": "agentic adaptation"
      },
      {
        "head": "Vision-Language-Action (VLA) models",
        "relation": "uses_metric",
        "tail": "cross-embodiment planning"
      },
      {
        "head": "Leave No Observation Behind: Real-time Correction for VLA Action Chunks",
        "relation": "proposed_model",
        "tail": "Asynchronous Action Chunk Correction (A2C2)"
      },
      {
        "head": "Asynchronous Action Chunk Correction (A2C2)",
        "relation": "baseline_model",
        "tail": "Real Time Chunking (RTC)"
      },
      {
        "head": "Asynchronous Action Chunk Correction (A2C2)",
        "relation": "evaluated_on",
        "tail": "dynamic Kinetix task suite (12 tasks)"
      },
      {
        "head": "Asynchronous Action Chunk Correction (A2C2)",
        "relation": "evaluated_on",
        "tail": "LIBERO Spatial"
      },
      {
        "head": "Asynchronous Action Chunk Correction (A2C2)",
        "relation": "uses_metric",
        "tail": "success rate"
      },
      {
        "head": "Latent Chain-of-Thought World Modeling for End-to-End Driving",
        "relation": "proposed_model",
        "tail": "Latent-CoT-Drive (LCDrive)"
      },
      {
        "head": "Latent-CoT-Drive (LCDrive)",
        "relation": "baseline_model",
        "tail": "Vision-Language-Action (VLA) models"
      },
      {
        "head": "Latent-CoT-Drive (LCDrive)",
        "relation": "baseline_model",
        "tail": "non-reasoning baselines"
      },
      {
        "head": "Latent-CoT-Drive (LCDrive)",
        "relation": "baseline_model",
        "tail": "text-reasoning baselines"
      },
      {
        "head": "Latent-CoT-Drive (LCDrive)",
        "relation": "evaluated_on",
        "tail": "large-scale end-to-end driving benchmark"
      },
      {
        "head": "Latent-CoT-Drive (LCDrive)",
        "relation": "uses_metric",
        "tail": "trajectory quality"
      },
      {
        "head": "HyperVLA",
        "relation": "proposed_model",
        "tail": "HyperVLA"
      },
      {
        "head": "HyperVLA",
        "relation": "baseline_model",
        "tail": "OpenVLA"
      },
      {
        "head": "HyperVLA",
        "relation": "baseline_model",
        "tail": "Vision-Language-Action (VLA) models"
      },
      {
        "head": "HyperVLA",
        "relation": "evaluated_on",
        "tail": "large-scale robotic data"
      },
      {
        "head": "HyperVLA",
        "relation": "uses_metric",
        "tail": "success rate"
      },
      {
        "head": "HyperVLA",
        "relation": "uses_metric",
        "tail": "inference costs"
      },
      {
        "head": "HyperVLA",
        "relation": "uses_metric",
        "tail": "number of activated parameters"
      },
      {
        "head": "HyperVLA",
        "relation": "uses_metric",
        "tail": "inference speed"
      },
      {
        "head": "Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models",
        "relation": "proposed_model",
        "tail": "TEAM-VLA"
      },
      {
        "head": "TEAM-VLA",
        "relation": "evaluated_on",
        "tail": "LIBERO"
      },
      {
        "head": "TEAM-VLA",
        "relation": "uses_metric",
        "tail": "inference speed"
      },
      {
        "head": "TEAM-VLA",
        "relation": "uses_metric",
        "tail": "task success rate"
      },
      {
        "head": "SegMamba-V2",
        "relation": "proposed_model",
        "tail": "SegMamba-V2"
      },
      {
        "head": "SegMamba-V2",
        "relation": "baseline_model",
        "tail": "Transformer"
      },
      {
        "head": "SegMamba-V2",
        "relation": "baseline_model",
        "tail": "Mamba"
      },
      {
        "head": "SegMamba-V2",
        "relation": "evaluated_on",
        "tail": "CRC-2000"
      },
      {
        "head": "SegMamba-V2",
        "relation": "evaluated_on",
        "tail": "three other large-scale 3D medical image segmentation datasets"
      },
      {
        "head": "TransSIL: A Silhouette Cue-Aware Image Classification Framework for Bird Ecological Monitoring Systems",
        "relation": "proposed_model",
        "tail": "TransSIL"
      },
      {
        "head": "TransSIL",
        "relation": "evaluated_on",
        "tail": "CUB200-2011"
      },
      {
        "head": "TransSIL",
        "relation": "evaluated_on",
        "tail": "NABirds"
      },
      {
        "head": "CBRFormer: rendering technology-based transformer for refinement segmentation of bridge crack images",
        "relation": "proposed_model",
        "tail": "CBRFormer"
      },
      {
        "head": "Deep contrastive learning enables genome-wide virtual screening",
        "relation": "proposed_model",
        "tail": "DrugCLIP"
      },
      {
        "head": "DrugCLIP",
        "relation": "baseline_model",
        "tail": "docking"
      },
      {
        "head": "DrugCLIP",
        "relation": "evaluated_on",
        "tail": "GenomeScreenDB"
      },
      {
        "head": "DrugCLIP",
        "relation": "uses_metric",
        "tail": "hit rate"
      },
      {
        "head": "DrugCLIP",
        "relation": "baseline_model",
        "tail": "AlphaFold2"
      },
      {
        "head": "LLaVA-based semantic feature modulation diffusion model for underwater image enhancement",
        "relation": "proposed_model",
        "tail": "LLaVA-based semantic feature modulation diffusion model"
      },
      {
        "head": "LLaVA-based semantic feature modulation diffusion model",
        "relation": "evaluated_on",
        "tail": "underwater image enhancement"
      },
      {
        "head": "3DGS-Drag: Dragging Gaussians for Intuitive Point-Based 3D Editing",
        "relation": "proposed_model",
        "tail": "3DGS-Drag"
      },
      {
        "head": "3DGS-Drag",
        "relation": "baseline_model",
        "tail": "3D Gaussian Splatting"
      },
      {
        "head": "3DGS-Drag",
        "relation": "baseline_model",
        "tail": "diffusion guidance"
      },
      {
        "head": "3DGS-Drag",
        "relation": "uses_metric",
        "tail": "state-of-the-art performance"
      },
      {
        "head": "Attention mechanisms in neural networks",
        "relation": "proposed_model",
        "tail": "autoregressive transformers"
      },
      {
        "head": "Attention mechanisms in neural networks",
        "relation": "proposed_model",
        "tail": "bidirectional encoders for representation learning"
      },
      {
        "head": "Attention mechanisms in neural networks",
        "relation": "proposed_model",
        "tail": "Vision Transformers"
      },
      {
        "head": "Attention mechanisms in neural networks",
        "relation": "proposed_model",
        "tail": "cross-modal attention"
      },
      {
        "head": "Attention mechanisms in neural networks",
        "relation": "evaluated_on",
        "tail": "standard datasets"
      },
      {
        "head": "Attention mechanisms in neural networks",
        "relation": "uses_metric",
        "tail": "performance benchmarks"
      },
      {
        "head": "Bidirectional Normalizing Flow: From Data to Noise and Back",
        "relation": "proposed_model",
        "tail": "BiFlow"
      },
      {
        "head": "Bidirectional Normalizing Flow: From Data to Noise and Back",
        "relation": "baseline_model",
        "tail": "TARFlow"
      },
      {
        "head": "BiFlow",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "BiFlow",
        "relation": "uses_metric",
        "tail": "generation quality"
      },
      {
        "head": "BiFlow",
        "relation": "uses_metric",
        "tail": "sampling speed"
      },
      {
        "head": "One-step Latent-free Image Generation with Pixel Mean Flows",
        "relation": "proposed_model",
        "tail": "pixel MeanFlow"
      },
      {
        "head": "pixel MeanFlow",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "pixel MeanFlow",
        "relation": "uses_metric",
        "tail": "FID"
      },
      {
        "head": "Meta Flow Maps enable scalable reward alignment",
        "relation": "proposed_model",
        "tail": "Meta Flow Maps (MFMs)"
      },
      {
        "head": "Meta Flow Maps (MFMs)",
        "relation": "baseline_model",
        "tail": "Best-of-1000"
      },
      {
        "head": "Meta Flow Maps (MFMs)",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "Best-of-1000",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective",
        "relation": "proposed_model",
        "tail": "Sequential Flow Matching"
      },
      {
        "head": "Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective",
        "relation": "baseline_model",
        "tail": "diffusion models"
      },
      {
        "head": "Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective",
        "relation": "baseline_model",
        "tail": "flow-matching models"
      },
      {
        "head": "Sequential Flow Matching",
        "relation": "evaluated_on",
        "tail": "forecasting tasks"
      },
      {
        "head": "Sequential Flow Matching",
        "relation": "evaluated_on",
        "tail": "decision-making tasks"
      },
      {
        "head": "Sequential Flow Matching",
        "relation": "evaluated_on",
        "tail": "state estimation tasks"
      },
      {
        "head": "Sequential Flow Matching",
        "relation": "uses_metric",
        "tail": "performance"
      },
      {
        "head": "Sequential Flow Matching",
        "relation": "uses_metric",
        "tail": "sampling speed"
      },
      {
        "head": "Generative Modeling via Drifting",
        "relation": "proposed_model",
        "tail": "Drifting Models"
      },
      {
        "head": "Drifting Models",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "Drifting Models",
        "relation": "uses_metric",
        "tail": "FID"
      },
      {
        "head": "A Simple Framework for Contrastive Learning of Visual Representations",
        "relation": "proposed_model",
        "tail": "SimCLR"
      },
      {
        "head": "SimCLR",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "SimCLR",
        "relation": "uses_metric",
        "tail": "top-1 accuracy"
      },
      {
        "head": "SimCLR",
        "relation": "uses_metric",
        "tail": "top-5 accuracy"
      },
      {
        "head": "SimCLR",
        "relation": "baseline_model",
        "tail": "ResNet-50"
      },
      {
        "head": "SimCLR",
        "relation": "baseline_model",
        "tail": "AlexNet"
      },
      {
        "head": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning",
        "relation": "proposed_model",
        "tail": "Directional Decoupling Alignment (D²-Align)"
      },
      {
        "head": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning",
        "relation": "proposed_model",
        "tail": "DivGenBench"
      },
      {
        "head": "Directional Decoupling Alignment (D²-Align)",
        "relation": "evaluated_on",
        "tail": "DivGenBench"
      },
      {
        "head": "Directional Decoupling Alignment (D²-Align)",
        "relation": "uses_metric",
        "tail": "quantitative metrics for both quality and diversity"
      },
      {
        "head": "existing methods",
        "relation": "uses_metric",
        "tail": "automated reward metrics"
      },
      {
        "head": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints",
        "relation": "proposed_model",
        "tail": "ImagerySearch"
      },
      {
        "head": "ImagerySearch",
        "relation": "evaluated_on",
        "tail": "LDT-Bench"
      },
      {
        "head": "ImagerySearch",
        "relation": "evaluated_on",
        "tail": "VBench"
      },
      {
        "head": "ImagerySearch",
        "relation": "uses_metric",
        "tail": "creative generation capabilities"
      },
      {
        "head": "LDT-Bench",
        "relation": "uses_metric",
        "tail": "creative generation capabilities"
      },
      {
        "head": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders",
        "relation": "proposed_model",
        "tail": "Representation Autoencoders (RAEs)"
      },
      {
        "head": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders",
        "relation": "baseline_model",
        "tail": "FLUX VAE"
      },
      {
        "head": "Representation Autoencoders (RAEs)",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "Representation Autoencoders (RAEs)",
        "relation": "evaluated_on",
        "tail": "web, synthetic, and text-rendering data"
      },
      {
        "head": "Representation Autoencoders (RAEs)",
        "relation": "evaluated_on",
        "tail": "high-quality datasets"
      },
      {
        "head": "FLUX VAE",
        "relation": "evaluated_on",
        "tail": "high-quality datasets"
      },
      {
        "head": "Representation Autoencoders (RAEs)",
        "relation": "uses_metric",
        "tail": "general fidelity"
      },
      {
        "head": "Representation Autoencoders (RAEs)",
        "relation": "uses_metric",
        "tail": "generation quality"
      },
      {
        "head": "Representation Autoencoders (RAEs)",
        "relation": "uses_metric",
        "tail": "performance"
      },
      {
        "head": "FLUX VAE",
        "relation": "uses_metric",
        "tail": "performance"
      },
      {
        "head": "Simulating the Visual World with Artificial Intelligence: A Roadmap",
        "relation": "proposed_model",
        "tail": "video foundation models"
      },
      {
        "head": "video foundation models",
        "relation": "baseline_model",
        "tail": "implicit world model"
      },
      {
        "head": "video foundation models",
        "relation": "baseline_model",
        "tail": "video renderer"
      },
      {
        "head": "world model",
        "relation": "baseline_model",
        "tail": "video generation model"
      },
      {
        "head": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks",
        "relation": "proposed_model",
        "tail": "World models"
      },
      {
        "head": "World models",
        "relation": "evaluated_on",
        "tail": "visual prediction"
      },
      {
        "head": "World models",
        "relation": "evaluated_on",
        "tail": "3D estimation"
      },
      {
        "head": "World models",
        "relation": "evaluated_on",
        "tail": "symbol grounding"
      },
      {
        "head": "RecTok: Reconstruction Distillation along Rectified Flow",
        "relation": "proposed_model",
        "tail": "RecTok"
      },
      {
        "head": "RecTok",
        "relation": "baseline_model",
        "tail": "visual tokenizers"
      },
      {
        "head": "RecTok",
        "relation": "evaluated_on",
        "tail": "gFID-50K"
      },
      {
        "head": "RecTok",
        "relation": "uses_metric",
        "tail": "gFID-50K"
      },
      {
        "head": "RePack then Refine",
        "relation": "proposed_model",
        "tail": "Repack then Refine"
      },
      {
        "head": "Repack then Refine",
        "relation": "baseline_model",
        "tail": "Latent Diffusion Models"
      },
      {
        "head": "Repack then Refine",
        "relation": "evaluated_on",
        "tail": "ImageNet-1K"
      },
      {
        "head": "Repack then Refine",
        "relation": "uses_metric",
        "tail": "FID"
      },
      {
        "head": "RePack-DiT-XL/1",
        "relation": "evaluated_on",
        "tail": "ImageNet-1K"
      },
      {
        "head": "RePack-DiT-XL/1",
        "relation": "uses_metric",
        "tail": "FID"
      },
      {
        "head": "Diffusion Models Beat GANs on Image Synthesis",
        "relation": "proposed_model",
        "tail": "diffusion models"
      },
      {
        "head": "Diffusion Models Beat GANs on Image Synthesis",
        "relation": "baseline_model",
        "tail": "BigGAN-deep"
      },
      {
        "head": "diffusion models",
        "relation": "evaluated_on",
        "tail": "ImageNet 128×128"
      },
      {
        "head": "diffusion models",
        "relation": "evaluated_on",
        "tail": "ImageNet 256×256"
      },
      {
        "head": "diffusion models",
        "relation": "evaluated_on",
        "tail": "ImageNet 512×512"
      },
      {
        "head": "diffusion models",
        "relation": "uses_metric",
        "tail": "FID"
      },
      {
        "head": "DINOv2: Learning Robust Visual Features without Supervision",
        "relation": "proposed_model",
        "tail": "ViT model (Dosovitskiy et al., 2020)"
      },
      {
        "head": "DINOv2: Learning Robust Visual Features without Supervision",
        "relation": "baseline_model",
        "tail": "OpenCLIP (Ilharco et al., 2021)"
      },
      {
        "head": "DINOv2: Learning Robust Visual Features without Supervision",
        "relation": "evaluated_on",
        "tail": "automatic pipeline to build a dedicated, diverse, and curated image dataset"
      },
      {
        "head": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss",
        "relation": "proposed_model",
        "tail": "PixelGen"
      },
      {
        "head": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss",
        "relation": "baseline_model",
        "tail": "latent diffusion models"
      },
      {
        "head": "PixelGen",
        "relation": "evaluated_on",
        "tail": "ImageNet-256"
      },
      {
        "head": "PixelGen",
        "relation": "uses_metric",
        "tail": "FID"
      },
      {
        "head": "PixelGen",
        "relation": "uses_metric",
        "tail": "GenEval"
      },
      {
        "head": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
        "relation": "proposed_model",
        "tail": "GPT-4o"
      },
      {
        "head": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
        "relation": "baseline_model",
        "tail": "diffusion-based models"
      },
      {
        "head": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
        "relation": "baseline_model",
        "tail": "autoregressive-based architectures"
      },
      {
        "head": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
        "relation": "proposed_model",
        "tail": "hybrid approaches"
      },
      {
        "head": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
        "relation": "evaluated_on",
        "tail": "datasets and benchmarks"
      },
      {
        "head": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery",
        "relation": "proposed_model",
        "tail": "MentisOculi"
      },
      {
        "head": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery",
        "relation": "evaluated_on",
        "tail": "MentisOculi"
      },
      {
        "head": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery",
        "relation": "baseline_model",
        "tail": "multimodal large language models (MLLMs)"
      },
      {
        "head": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery",
        "relation": "baseline_model",
        "tail": "unified multimodal models (UMMs)"
      },
      {
        "head": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery",
        "relation": "baseline_model",
        "tail": "visual strategies"
      },
      {
        "head": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery",
        "relation": "uses_metric",
        "tail": "performance"
      },
      {
        "head": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation",
        "relation": "proposed_model",
        "tail": "OpenVision 3"
      },
      {
        "head": "OpenVision 3",
        "relation": "baseline_model",
        "tail": "CLIP vision encoder"
      },
      {
        "head": "OpenVision 3",
        "relation": "evaluated_on",
        "tail": "SeedBench"
      },
      {
        "head": "OpenVision 3",
        "relation": "evaluated_on",
        "tail": "POPE"
      },
      {
        "head": "OpenVision 3",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "OpenVision 3",
        "relation": "uses_metric",
        "tail": "gFID"
      },
      {
        "head": "Distributed Representations of Words and Phrases and their Compositionality",
        "relation": "proposed_model",
        "tail": "continuous Skip-gram model"
      },
      {
        "head": "Distributed Representations of Words and Phrases and their Compositionality",
        "relation": "proposed_model",
        "tail": "negative sampling"
      },
      {
        "head": "Distributed Representations of Words and Phrases and their Compositionality",
        "relation": "baseline_model",
        "tail": "hierarchical softmax"
      },
      {
        "head": "GloVe: Global Vectors for Word Representation",
        "relation": "proposed_model",
        "tail": "global logbilinear regression model"
      },
      {
        "head": "global logbilinear regression model",
        "relation": "baseline_model",
        "tail": "global matrix factorization"
      },
      {
        "head": "global logbilinear regression model",
        "relation": "baseline_model",
        "tail": "local context window methods"
      },
      {
        "head": "global logbilinear regression model",
        "relation": "evaluated_on",
        "tail": "word analogy task"
      },
      {
        "head": "global logbilinear regression model",
        "relation": "evaluated_on",
        "tail": "similarity tasks"
      },
      {
        "head": "global logbilinear regression model",
        "relation": "evaluated_on",
        "tail": "named entity recognition"
      },
      {
        "head": "global logbilinear regression model",
        "relation": "uses_metric",
        "tail": "75%"
      },
      {
        "head": "Deep Contextualized Word Representations",
        "relation": "proposed_model",
        "tail": "deep bidirectional language model (biLM)"
      },
      {
        "head": "deep bidirectional language model (biLM)",
        "relation": "evaluated_on",
        "tail": "large text corpus"
      },
      {
        "head": "Deep Contextualized Word Representations",
        "relation": "uses_metric",
        "tail": "question answering"
      },
      {
        "head": "Deep Contextualized Word Representations",
        "relation": "uses_metric",
        "tail": "textual entailment"
      },
      {
        "head": "Deep Contextualized Word Representations",
        "relation": "uses_metric",
        "tail": "sentiment analysis"
      },
      {
        "head": "Protein Language Models: Is Scaling Necessary?",
        "relation": "proposed_model",
        "tail": "Protein Language Models"
      },
      {
        "head": "Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis",
        "relation": "proposed_model",
        "tail": "Modern models for common NLP tasks"
      },
      {
        "head": "Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis",
        "relation": "evaluated_on",
        "tail": "2018 Twitter data spanning 51 U.S. regions and 99 countries"
      },
      {
        "head": "Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis",
        "relation": "uses_metric",
        "tail": "gender bias in word embeddings"
      },
      {
        "head": "Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis",
        "relation": "uses_metric",
        "tail": "statistical gender gaps in education, politics, economics, and health"
      },
      {
        "head": "Generative Classifiers Avoid Shortcut Solutions",
        "relation": "proposed_model",
        "tail": "generative classifiers"
      },
      {
        "head": "Generative Classifiers Avoid Shortcut Solutions",
        "relation": "evaluated_on",
        "tail": "five standard image and text distribution shift benchmarks"
      },
      {
        "head": "Generative Classifiers Avoid Shortcut Solutions",
        "relation": "evaluated_on",
        "tail": "medical datasets"
      },
      {
        "head": "Generative Classifiers Avoid Shortcut Solutions",
        "relation": "evaluated_on",
        "tail": "satellite datasets"
      },
      {
        "head": "Generative Classifiers Avoid Shortcut Solutions",
        "relation": "evaluated_on",
        "tail": "Gaussian toy setting"
      },
      {
        "head": "Generative Classifiers Avoid Shortcut Solutions",
        "relation": "uses_metric",
        "tail": "state-of-the-art performance"
      },
      {
        "head": "Generative Classifiers Avoid Shortcut Solutions",
        "relation": "uses_metric",
        "tail": "impact of spurious correlations"
      },
      {
        "head": "generative classifiers",
        "relation": "baseline_model",
        "tail": "diffusion-based generative classifiers"
      },
      {
        "head": "generative classifiers",
        "relation": "baseline_model",
        "tail": "autoregressive generative classifiers"
      },
      {
        "head": "Language Models are Unsupervised Multitask Learners",
        "relation": "proposed_model",
        "tail": "GPT-2"
      },
      {
        "head": "GPT-2",
        "relation": "evaluated_on",
        "tail": "WebText"
      },
      {
        "head": "The Llama 3 Herd of Models",
        "relation": "proposed_model",
        "tail": "Llama 3"
      },
      {
        "head": "The Llama 3 Herd of Models",
        "relation": "baseline_model",
        "tail": "GPT-4"
      },
      {
        "head": "A Survey on Diffusion Language Models",
        "relation": "proposed_model",
        "tail": "Diffusion Language Models (DLMs)"
      },
      {
        "head": "Diffusion Language Models (DLMs)",
        "relation": "baseline_model",
        "tail": "autoregressive (AR) paradigm"
      },
      {
        "head": "Diffusion Language Models (DLMs)",
        "relation": "baseline_model",
        "tail": "masked language models"
      },
      {
        "head": "Training Optimal Large Diffusion Language Models",
        "relation": "proposed_model",
        "tail": "Quokka"
      },
      {
        "head": "Quokka",
        "relation": "baseline_model",
        "tail": "Chinchilla"
      },
      {
        "head": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models",
        "relation": "proposed_model",
        "tail": "IGPO"
      },
      {
        "head": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models",
        "relation": "baseline_model",
        "tail": "GRPO"
      },
      {
        "head": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models",
        "relation": "evaluated_on",
        "tail": "GSM8K"
      },
      {
        "head": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models",
        "relation": "evaluated_on",
        "tail": "Math500"
      },
      {
        "head": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models",
        "relation": "evaluated_on",
        "tail": "AMC"
      },
      {
        "head": "IGPO",
        "relation": "uses_metric",
        "tail": "GSM8K"
      },
      {
        "head": "IGPO",
        "relation": "uses_metric",
        "tail": "Math500"
      },
      {
        "head": "IGPO",
        "relation": "uses_metric",
        "tail": "AMC"
      },
      {
        "head": "Encoder-Decoder Diffusion Language Models for Efficient Training and Inference",
        "relation": "proposed_model",
        "tail": "E2D2"
      },
      {
        "head": "E2D2",
        "relation": "evaluated_on",
        "tail": "summarization"
      },
      {
        "head": "E2D2",
        "relation": "evaluated_on",
        "tail": "translation"
      },
      {
        "head": "E2D2",
        "relation": "evaluated_on",
        "tail": "mathematical reasoning"
      },
      {
        "head": "E2D2",
        "relation": "uses_metric",
        "tail": "generation quality"
      },
      {
        "head": "E2D2",
        "relation": "uses_metric",
        "tail": "inference throughput"
      },
      {
        "head": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model",
        "relation": "proposed_model",
        "tail": "Stable-DiffCoder"
      },
      {
        "head": "Stable-DiffCoder",
        "relation": "baseline_model",
        "tail": "autoregressive (AR) models"
      },
      {
        "head": "Stable-DiffCoder",
        "relation": "baseline_model",
        "tail": "Diffusion-based language models (DLLMs)"
      },
      {
        "head": "Stable-DiffCoder",
        "relation": "evaluated_on",
        "tail": "code benchmarks"
      },
      {
        "head": "Stable-DiffCoder",
        "relation": "evaluated_on",
        "tail": "low-resource coding languages"
      },
      {
        "head": "Stable-DiffCoder",
        "relation": "uses_metric",
        "tail": "performance"
      },
      {
        "head": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
        "relation": "proposed_model",
        "tail": "XLNet"
      },
      {
        "head": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
        "relation": "baseline_model",
        "tail": "BERT"
      },
      {
        "head": "XLNet",
        "relation": "evaluated_on",
        "tail": "question answering"
      },
      {
        "head": "XLNet",
        "relation": "evaluated_on",
        "tail": "natural language inference"
      },
      {
        "head": "XLNet",
        "relation": "evaluated_on",
        "tail": "sentiment analysis"
      },
      {
        "head": "XLNet",
        "relation": "evaluated_on",
        "tail": "document ranking"
      },
      {
        "head": "Qwen Technical Report",
        "relation": "proposed_model",
        "tail": "Qwen"
      },
      {
        "head": "Qwen Technical Report",
        "relation": "proposed_model",
        "tail": "Qwen-Chat"
      },
      {
        "head": "Qwen Technical Report",
        "relation": "proposed_model",
        "tail": "Code-Qwen"
      },
      {
        "head": "Qwen Technical Report",
        "relation": "proposed_model",
        "tail": "Code-Qwen-Chat"
      },
      {
        "head": "Qwen Technical Report",
        "relation": "proposed_model",
        "tail": "Math-Qwen-Chat"
      },
      {
        "head": "Qwen-Chat",
        "relation": "baseline_model",
        "tail": "Reinforcement Learning from Human Feedback (RLHF)"
      },
      {
        "head": "Qwen",
        "relation": "evaluated_on",
        "tail": "downstream tasks"
      },
      {
        "head": "Qwen-Chat",
        "relation": "evaluated_on",
        "tail": "code interpreter"
      },
      {
        "head": "Qwen",
        "relation": "uses_metric",
        "tail": "performance"
      },
      {
        "head": "Qwen-Chat",
        "relation": "uses_metric",
        "tail": "performance"
      },
      {
        "head": "Code-Qwen",
        "relation": "uses_metric",
        "tail": "performance"
      },
      {
        "head": "Code-Qwen-Chat",
        "relation": "uses_metric",
        "tail": "performance"
      },
      {
        "head": "Math-Qwen-Chat",
        "relation": "uses_metric",
        "tail": "performance"
      },
      {
        "head": "Code Llama",
        "relation": "proposed_model",
        "tail": "Code Llama"
      },
      {
        "head": "Code Llama",
        "relation": "proposed_model",
        "tail": "Code Llama - Python"
      },
      {
        "head": "Code Llama",
        "relation": "proposed_model",
        "tail": "Code Llama - Instruct"
      },
      {
        "head": "Code Llama",
        "relation": "baseline_model",
        "tail": "Llama 2"
      },
      {
        "head": "Code Llama",
        "relation": "evaluated_on",
        "tail": "HumanEval"
      },
      {
        "head": "Code Llama",
        "relation": "evaluated_on",
        "tail": "MBPP"
      },
      {
        "head": "Code Llama",
        "relation": "evaluated_on",
        "tail": "MultiPL-E"
      },
      {
        "head": "Code Llama - Python",
        "relation": "evaluated_on",
        "tail": "HumanEval"
      },
      {
        "head": "Code Llama - Python",
        "relation": "evaluated_on",
        "tail": "MBPP"
      },
      {
        "head": "Code Llama",
        "relation": "uses_metric",
        "tail": "HumanEval"
      },
      {
        "head": "Code Llama",
        "relation": "uses_metric",
        "tail": "MBPP"
      },
      {
        "head": "Code Llama",
        "relation": "uses_metric",
        "tail": "MultiPL-E"
      },
      {
        "head": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way",
        "relation": "proposed_model",
        "tail": "dLLM-Var"
      },
      {
        "head": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way",
        "relation": "baseline_model",
        "tail": "dLLMs"
      },
      {
        "head": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way",
        "relation": "baseline_model",
        "tail": "autoregressive models"
      },
      {
        "head": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way",
        "relation": "baseline_model",
        "tail": "Qwen"
      },
      {
        "head": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way",
        "relation": "baseline_model",
        "tail": "Llama"
      },
      {
        "head": "dLLM-Var",
        "relation": "evaluated_on",
        "tail": "standard benchmarks"
      },
      {
        "head": "dLLM-Var",
        "relation": "uses_metric",
        "tail": "speedup"
      },
      {
        "head": "dLLM-Var",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "Set Block Decoding is a Language Model Inference Accelerator",
        "relation": "proposed_model",
        "tail": "Set Block Decoding (SBD)"
      },
      {
        "head": "Set Block Decoding (SBD)",
        "relation": "baseline_model",
        "tail": "next token prediction (NTP)"
      },
      {
        "head": "Set Block Decoding (SBD)",
        "relation": "evaluated_on",
        "tail": "discrete diffusion literature"
      },
      {
        "head": "Set Block Decoding (SBD)",
        "relation": "uses_metric",
        "tail": "performance"
      },
      {
        "head": "Set Block Decoding (SBD)",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "Set Block Decoding (SBD)",
        "relation": "uses_metric",
        "tail": "speedups"
      },
      {
        "head": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows",
        "relation": "proposed_model",
        "tail": "OneFlow"
      },
      {
        "head": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows",
        "relation": "baseline_model",
        "tail": "autoregressive baselines"
      },
      {
        "head": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows",
        "relation": "baseline_model",
        "tail": "autoregressive models"
      },
      {
        "head": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows",
        "relation": "baseline_model",
        "tail": "diffusion-based approaches"
      },
      {
        "head": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows",
        "relation": "evaluated_on",
        "tail": "generation and understanding tasks"
      },
      {
        "head": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows",
        "relation": "uses_metric",
        "tail": "training FLOPs"
      },
      {
        "head": "Constrained Decoding of Diffusion LLMs with Context-Free Grammars",
        "relation": "proposed_model",
        "tail": "constrained decoding method for diffusion models"
      },
      {
        "head": "Constrained Decoding of Diffusion LLMs with Context-Free Grammars",
        "relation": "baseline_model",
        "tail": "Large language models (LLMs)"
      },
      {
        "head": "Constrained Decoding of Diffusion LLMs with Context-Free Grammars",
        "relation": "evaluated_on",
        "tail": "C++ code infilling"
      },
      {
        "head": "Constrained Decoding of Diffusion LLMs with Context-Free Grammars",
        "relation": "evaluated_on",
        "tail": "structured data extraction in JSON"
      },
      {
        "head": "Constrained Decoding of Diffusion LLMs with Context-Free Grammars",
        "relation": "uses_metric",
        "tail": "syntactic correctness"
      },
      {
        "head": "Constrained Decoding of Diffusion LLMs with Context-Free Grammars",
        "relation": "uses_metric",
        "tail": "functional correctness"
      },
      {
        "head": "Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models",
        "relation": "proposed_model",
        "tail": "mask-agnostic loss function"
      },
      {
        "head": "Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models",
        "relation": "baseline_model",
        "tail": "Masked Diffusion Language Models (MDLMs)"
      },
      {
        "head": "Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models",
        "relation": "baseline_model",
        "tail": "Autoregressive Language Models (ARLMs)"
      },
      {
        "head": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications",
        "relation": "proposed_model",
        "tail": "Large Language Model (LLM)-based agents"
      },
      {
        "head": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications",
        "relation": "evaluated_on",
        "tail": "open-sourced libraries and benchmarks"
      },
      {
        "head": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications",
        "relation": "uses_metric",
        "tail": "self-evolving agent memory"
      },
      {
        "head": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search",
        "relation": "proposed_model",
        "tail": "Empirical-MCTS"
      },
      {
        "head": "Empirical-MCTS",
        "relation": "baseline_model",
        "tail": "Monte Carlo Tree Search (MCTS)"
      },
      {
        "head": "Empirical-MCTS",
        "relation": "evaluated_on",
        "tail": "AIME25"
      },
      {
        "head": "Empirical-MCTS",
        "relation": "evaluated_on",
        "tail": "ARC-AGI-2"
      },
      {
        "head": "Empirical-MCTS",
        "relation": "evaluated_on",
        "tail": "MathArena Apex"
      },
      {
        "head": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking",
        "relation": "proposed_model",
        "tail": "TAME"
      },
      {
        "head": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking",
        "relation": "evaluated_on",
        "tail": "Trust-Memevo"
      },
      {
        "head": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking",
        "relation": "uses_metric",
        "tail": "trustworthiness"
      },
      {
        "head": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking",
        "relation": "uses_metric",
        "tail": "task performance"
      },
      {
        "head": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents",
        "relation": "proposed_model",
        "tail": "ProcMEM"
      },
      {
        "head": "ProcMEM",
        "relation": "baseline_model",
        "tail": "Non-Parametric PPO"
      },
      {
        "head": "ProcMEM",
        "relation": "evaluated_on",
        "tail": "in-domain, cross-task, and cross-agent scenarios"
      },
      {
        "head": "ProcMEM",
        "relation": "uses_metric",
        "tail": "reuse rates"
      },
      {
        "head": "ProcMEM",
        "relation": "uses_metric",
        "tail": "performance gains"
      },
      {
        "head": "ProcMEM",
        "relation": "uses_metric",
        "tail": "memory compression"
      },
      {
        "head": "Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting",
        "relation": "proposed_model",
        "tail": "agentic time series forecasting (ATSF)"
      },
      {
        "head": "agentic time series forecasting (ATSF)",
        "relation": "baseline_model",
        "tail": "workflow-based design"
      },
      {
        "head": "agentic time series forecasting (ATSF)",
        "relation": "baseline_model",
        "tail": "agentic reinforcement learning"
      },
      {
        "head": "agentic time series forecasting (ATSF)",
        "relation": "baseline_model",
        "tail": "hybrid agentic workflow paradigm"
      },
      {
        "head": "Follow-Your-Emoji-Faster",
        "relation": "proposed_model",
        "tail": "Stable Diffusion"
      },
      {
        "head": "Follow-Your-Emoji-Faster",
        "relation": "evaluated_on",
        "tail": "EmojiBench++"
      },
      {
        "head": "Follow-Your-Emoji-Faster",
        "relation": "uses_metric",
        "tail": "animation quality"
      },
      {
        "head": "Follow-Your-Emoji-Faster",
        "relation": "uses_metric",
        "tail": "controllability"
      },
      {
        "head": "Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis",
        "relation": "proposed_model",
        "tail": "Follow-Your-Instruction"
      },
      {
        "head": "Follow-Your-Instruction",
        "relation": "baseline_model",
        "tail": "existing baseline models"
      },
      {
        "head": "Follow-Your-Instruction",
        "relation": "evaluated_on",
        "tail": "2D, 3D, and 4D generative tasks"
      },
      {
        "head": "Follow-Your-Instruction",
        "relation": "uses_metric",
        "tail": "performance"
      },
      {
        "head": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing",
        "relation": "proposed_model",
        "tail": "HunyuanVideoT2V"
      },
      {
        "head": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing",
        "relation": "evaluated_on",
        "tail": "approximately 1M real video clips"
      },
      {
        "head": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing",
        "relation": "evaluated_on",
        "tail": "fewer than 150k curated editing pairs"
      },
      {
        "head": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing",
        "relation": "uses_metric",
        "tail": "editing instruction following"
      },
      {
        "head": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing",
        "relation": "uses_metric",
        "tail": "editing quality"
      },
      {
        "head": "Paper2Video: Automatic Video Generation from Scientific Papers",
        "relation": "proposed_model",
        "tail": "PaperTalker"
      },
      {
        "head": "Paper2Video: Automatic Video Generation from Scientific Papers",
        "relation": "evaluated_on",
        "tail": "Paper2Video"
      },
      {
        "head": "Paper2Video: Automatic Video Generation from Scientific Papers",
        "relation": "uses_metric",
        "tail": "Meta Similarity"
      },
      {
        "head": "Paper2Video: Automatic Video Generation from Scientific Papers",
        "relation": "uses_metric",
        "tail": "PresentArena"
      },
      {
        "head": "Paper2Video: Automatic Video Generation from Scientific Papers",
        "relation": "uses_metric",
        "tail": "PresentQuiz"
      },
      {
        "head": "Paper2Video: Automatic Video Generation from Scientific Papers",
        "relation": "uses_metric",
        "tail": "IP Memory"
      },
      {
        "head": "ContextFlow",
        "relation": "proposed_model",
        "tail": "ContextFlow"
      },
      {
        "head": "ContextFlow",
        "relation": "baseline_model",
        "tail": "U-Net"
      },
      {
        "head": "ContextFlow",
        "relation": "baseline_model",
        "tail": "Diffusion Transformers (DiTs)"
      },
      {
        "head": "ContextFlow",
        "relation": "uses_metric",
        "tail": "Guidance Responsiveness Metric"
      },
      {
        "head": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning",
        "relation": "proposed_model",
        "tail": "DeepSeek-R1"
      },
      {
        "head": "DeepSeek-R1",
        "relation": "baseline_model",
        "tail": "conventional supervised learning on human demonstrations"
      },
      {
        "head": "DeepSeek-R1",
        "relation": "evaluated_on",
        "tail": "mathematics"
      },
      {
        "head": "DeepSeek-R1",
        "relation": "evaluated_on",
        "tail": "coding competitions"
      },
      {
        "head": "DeepSeek-R1",
        "relation": "evaluated_on",
        "tail": "STEM fields"
      },
      {
        "head": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone",
        "relation": "proposed_model",
        "tail": "phi-3-mini"
      },
      {
        "head": "phi-3-mini",
        "relation": "baseline_model",
        "tail": "Mixtral 8x7B"
      },
      {
        "head": "phi-3-mini",
        "relation": "baseline_model",
        "tail": "GPT-3.5"
      },
      {
        "head": "phi-3-mini",
        "relation": "evaluated_on",
        "tail": "MMLU"
      },
      {
        "head": "phi-3-mini",
        "relation": "evaluated_on",
        "tail": "MT-bench"
      },
      {
        "head": "phi-3-mini",
        "relation": "uses_metric",
        "tail": "MMLU"
      },
      {
        "head": "phi-3-mini",
        "relation": "uses_metric",
        "tail": "MT-bench"
      },
      {
        "head": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone",
        "relation": "proposed_model",
        "tail": "phi-3-small"
      },
      {
        "head": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone",
        "relation": "proposed_model",
        "tail": "phi-3-medium"
      },
      {
        "head": "phi-3-small",
        "relation": "evaluated_on",
        "tail": "MMLU"
      },
      {
        "head": "phi-3-small",
        "relation": "evaluated_on",
        "tail": "MT-bench"
      },
      {
        "head": "phi-3-small",
        "relation": "uses_metric",
        "tail": "MMLU"
      },
      {
        "head": "phi-3-small",
        "relation": "uses_metric",
        "tail": "MT-bench"
      },
      {
        "head": "phi-3-medium",
        "relation": "evaluated_on",
        "tail": "MMLU"
      },
      {
        "head": "phi-3-medium",
        "relation": "evaluated_on",
        "tail": "MT-bench"
      },
      {
        "head": "phi-3-medium",
        "relation": "uses_metric",
        "tail": "MMLU"
      },
      {
        "head": "phi-3-medium",
        "relation": "uses_metric",
        "tail": "MT-bench"
      },
      {
        "head": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone",
        "relation": "proposed_model",
        "tail": "phi-3.5-mini"
      },
      {
        "head": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone",
        "relation": "proposed_model",
        "tail": "phi-3.5-MoE"
      },
      {
        "head": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone",
        "relation": "proposed_model",
        "tail": "phi-3.5-Vision"
      },
      {
        "head": "phi-3.5-MoE",
        "relation": "baseline_model",
        "tail": "Llama 3.1"
      },
      {
        "head": "phi-3.5-MoE",
        "relation": "baseline_model",
        "tail": "Mixtral series"
      },
      {
        "head": "phi-3.5-MoE",
        "relation": "baseline_model",
        "tail": "Gemini-1.5-Flash"
      },
      {
        "head": "phi-3.5-MoE",
        "relation": "baseline_model",
        "tail": "GPT-4o-mini"
      },
      {
        "head": "LLaVA-OneVision",
        "relation": "proposed_model",
        "tail": "LLaVA-NeXT blog series"
      },
      {
        "head": "LLaVA-OneVision",
        "relation": "evaluated_on",
        "tail": "single-image scenario"
      },
      {
        "head": "LLaVA-OneVision",
        "relation": "evaluated_on",
        "tail": "multi-image scenario"
      },
      {
        "head": "LLaVA-OneVision",
        "relation": "evaluated_on",
        "tail": "video scenario"
      },
      {
        "head": "Spatial Reasoning in Multimodal Large Language Models: A Survey of Tasks, Benchmarks and Methods",
        "relation": "proposed_model",
        "tail": "Multimodal large language models (MLLMs)"
      },
      {
        "head": "Spatial Reasoning in Multimodal Large Language Models: A Survey of Tasks, Benchmarks and Methods",
        "relation": "evaluated_on",
        "tail": "existing benchmarks across text only, vision language, and embodied settings"
      },
      {
        "head": "Spatial Reasoning in Multimodal Large Language Models: A Survey of Tasks, Benchmarks and Methods",
        "relation": "uses_metric",
        "tail": "evaluation metrics and methodologies for assessing spatial reasoning ability"
      },
      {
        "head": "SpatialTree: How Spatial Abilities Branch Out in MLLMs",
        "relation": "proposed_model",
        "tail": "SpatialTree"
      },
      {
        "head": "SpatialTree: How Spatial Abilities Branch Out in MLLMs",
        "relation": "baseline_model",
        "tail": "mainstream MLLMs"
      },
      {
        "head": "SpatialTree: How Spatial Abilities Branch Out in MLLMs",
        "relation": "evaluated_on",
        "tail": "capability-centric hierarchical benchmark"
      },
      {
        "head": "SpatialTree: How Spatial Abilities Branch Out in MLLMs",
        "relation": "uses_metric",
        "tail": "27 sub-abilities"
      },
      {
        "head": "From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs",
        "relation": "proposed_model",
        "tail": "Multimodal Large Language Models (MLLMs)"
      },
      {
        "head": "From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs",
        "relation": "evaluated_on",
        "tail": "large-scale benchmark built from pedestrian-perspective videos captured with synchronized stereo cameras, LiDAR, and IMU/GPS sensors"
      },
      {
        "head": "From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs",
        "relation": "uses_metric",
        "tail": "spatial reasoning questions"
      },
      {
        "head": "SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery",
        "relation": "proposed_model",
        "tail": "SpatialDreamer"
      },
      {
        "head": "SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery",
        "relation": "proposed_model",
        "tail": "Geometric Policy Optimization (GeoPO)"
      },
      {
        "head": "SpatialDreamer",
        "relation": "baseline_model",
        "tail": "Multi-modal Large Language Models (MLLMs)"
      },
      {
        "head": "SpatialDreamer",
        "relation": "evaluated_on",
        "tail": "multiple challenging benchmarks"
      },
      {
        "head": "SpatialDreamer",
        "relation": "uses_metric",
        "tail": "highly competitive results"
      },
      {
        "head": "You Only Look Once: Unified, Real-Time Object Detection",
        "relation": "proposed_model",
        "tail": "YOLO"
      },
      {
        "head": "You Only Look Once: Unified, Real-Time Object Detection",
        "relation": "proposed_model",
        "tail": "Fast YOLO"
      },
      {
        "head": "YOLO",
        "relation": "baseline_model",
        "tail": "DPM"
      },
      {
        "head": "YOLO",
        "relation": "baseline_model",
        "tail": "R-CNN"
      },
      {
        "head": "YOLO",
        "relation": "evaluated_on",
        "tail": "Picasso Dataset"
      },
      {
        "head": "YOLO",
        "relation": "evaluated_on",
        "tail": "People-Art Dataset"
      },
      {
        "head": "YOLO",
        "relation": "uses_metric",
        "tail": "mAP"
      },
      {
        "head": "Fast YOLO",
        "relation": "uses_metric",
        "tail": "mAP"
      },
      {
        "head": "RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes",
        "relation": "proposed_model",
        "tail": "Number GroundingDINO (NGDINO)"
      },
      {
        "head": "RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes",
        "relation": "proposed_model",
        "tail": "RDAgent (referring drone annotation framework with multi-agent system)"
      },
      {
        "head": "RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes",
        "relation": "evaluated_on",
        "tail": "RefDrone"
      },
      {
        "head": "RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes",
        "relation": "evaluated_on",
        "tail": "gRefCOCO"
      },
      {
        "head": "Number GroundingDINO (NGDINO)",
        "relation": "baseline_model",
        "tail": "state-of-the-art REC methods"
      },
      {
        "head": "Number GroundingDINO (NGDINO)",
        "relation": "uses_metric",
        "tail": "superior performance"
      },
      {
        "head": "WeDetect: Fast Open-Vocabulary Object Detection as Retrieval",
        "relation": "proposed_model",
        "tail": "WeDetect"
      },
      {
        "head": "WeDetect: Fast Open-Vocabulary Object Detection as Retrieval",
        "relation": "proposed_model",
        "tail": "WeDetect-Uni"
      },
      {
        "head": "WeDetect: Fast Open-Vocabulary Object Detection as Retrieval",
        "relation": "proposed_model",
        "tail": "WeDetect-Ref"
      },
      {
        "head": "WeDetect",
        "relation": "evaluated_on",
        "tail": "15 benchmarks"
      },
      {
        "head": "WeDetect-Uni",
        "relation": "evaluated_on",
        "tail": "15 benchmarks"
      },
      {
        "head": "WeDetect-Ref",
        "relation": "evaluated_on",
        "tail": "15 benchmarks"
      },
      {
        "head": "v1: Learning to Point Visual Tokens for Multimodal Grounded Reasoning",
        "relation": "proposed_model",
        "tail": "v1"
      },
      {
        "head": "v1",
        "relation": "evaluated_on",
        "tail": "multimodal mathematical reasoning benchmarks"
      },
      {
        "head": "v1",
        "relation": "uses_metric",
        "tail": "multimodal mathematical reasoning benchmarks"
      },
      {
        "head": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design",
        "relation": "proposed_model",
        "tail": "PosterCopilot"
      },
      {
        "head": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design",
        "relation": "baseline_model",
        "tail": "existing methods"
      },
      {
        "head": "PosterCopilot",
        "relation": "uses_metric",
        "tail": "geometric understanding"
      },
      {
        "head": "PosterCopilot",
        "relation": "uses_metric",
        "tail": "aesthetic reasoning"
      },
      {
        "head": "PosterCopilot",
        "relation": "uses_metric",
        "tail": "geometrically accurate layouts"
      },
      {
        "head": "PosterCopilot",
        "relation": "uses_metric",
        "tail": "aesthetically superior layouts"
      },
      {
        "head": "PosterCopilot",
        "relation": "uses_metric",
        "tail": "controllability"
      },
      {
        "head": "PosterCopilot",
        "relation": "evaluated_on",
        "tail": "professional graphic design"
      },
      {
        "head": "PosterCopilot",
        "relation": "evaluated_on",
        "tail": "layout design"
      },
      {
        "head": "PosterCopilot",
        "relation": "evaluated_on",
        "tail": "iterative editing"
      },
      {
        "head": "PosterCopilot",
        "relation": "evaluated_on",
        "tail": "layer-controllable editing"
      },
      {
        "head": "PosterCopilot",
        "relation": "evaluated_on",
        "tail": "global visual consistency"
      },
      {
        "head": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving",
        "relation": "proposed_model",
        "tail": "Percept-WAM"
      },
      {
        "head": "Percept-WAM",
        "relation": "evaluated_on",
        "tail": "COCO"
      },
      {
        "head": "Percept-WAM",
        "relation": "evaluated_on",
        "tail": "nuScenes"
      },
      {
        "head": "Percept-WAM",
        "relation": "evaluated_on",
        "tail": "NAVSIM"
      },
      {
        "head": "Percept-WAM",
        "relation": "uses_metric",
        "tail": "mAP"
      },
      {
        "head": "Percept-WAM",
        "relation": "uses_metric",
        "tail": "PMDS"
      },
      {
        "head": "Percept-WAM",
        "relation": "baseline_model",
        "tail": "classical detectors and segmenters"
      },
      {
        "head": "Percept-WAM",
        "relation": "baseline_model",
        "tail": "DiffusionDrive"
      },
      {
        "head": "Continuous control with deep reinforcement learning",
        "relation": "proposed_model",
        "tail": "actor-critic, model-free algorithm based on the deterministic policy gradient"
      },
      {
        "head": "Continuous control with deep reinforcement learning",
        "relation": "baseline_model",
        "tail": "Deep Q-Learning"
      },
      {
        "head": "Continuous control with deep reinforcement learning",
        "relation": "evaluated_on",
        "tail": "simulated physics tasks"
      },
      {
        "head": "Continuous control with deep reinforcement learning",
        "relation": "evaluated_on",
        "tail": "cartpole swing-up"
      },
      {
        "head": "Continuous control with deep reinforcement learning",
        "relation": "evaluated_on",
        "tail": "dexterous manipulation"
      },
      {
        "head": "Continuous control with deep reinforcement learning",
        "relation": "evaluated_on",
        "tail": "legged locomotion"
      },
      {
        "head": "Continuous control with deep reinforcement learning",
        "relation": "evaluated_on",
        "tail": "car driving"
      },
      {
        "head": "actor-critic, model-free algorithm based on the deterministic policy gradient",
        "relation": "uses_metric",
        "tail": "performance"
      },
      {
        "head": "planning algorithm with full access to the dynamics of the domain and its derivatives",
        "relation": "uses_metric",
        "tail": "performance"
      },
      {
        "head": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning",
        "relation": "proposed_model",
        "tail": "Reinforcement Learning with Verifiable Rewards (RLVR)"
      },
      {
        "head": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning",
        "relation": "baseline_model",
        "tail": "Qwen3-8B"
      },
      {
        "head": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning",
        "relation": "baseline_model",
        "tail": "Qwen3-32B"
      },
      {
        "head": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning",
        "relation": "baseline_model",
        "tail": "Qwen3-14B"
      },
      {
        "head": "Reinforcement Learning with Verifiable Rewards (RLVR)",
        "relation": "evaluated_on",
        "tail": "AIME'25"
      },
      {
        "head": "Reinforcement Learning with Verifiable Rewards (RLVR)",
        "relation": "evaluated_on",
        "tail": "AIME'24"
      },
      {
        "head": "Reinforcement Learning with Verifiable Rewards (RLVR)",
        "relation": "uses_metric",
        "tail": "performance"
      },
      {
        "head": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models",
        "relation": "proposed_model",
        "tail": "Clip-Cov"
      },
      {
        "head": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models",
        "relation": "proposed_model",
        "tail": "KL-Cov"
      },
      {
        "head": "Clip-Cov",
        "relation": "uses_metric",
        "tail": "covariance"
      },
      {
        "head": "KL-Cov",
        "relation": "uses_metric",
        "tail": "covariance"
      },
      {
        "head": "Policy Gradient-like algorithms",
        "relation": "uses_metric",
        "tail": "entropy"
      },
      {
        "head": "Learning to Reason under Off-Policy Guidance",
        "relation": "proposed_model",
        "tail": "LUFFY"
      },
      {
        "head": "LUFFY",
        "relation": "baseline_model",
        "tail": "RLVR"
      },
      {
        "head": "LUFFY",
        "relation": "evaluated_on",
        "tail": "six math benchmarks"
      },
      {
        "head": "LUFFY",
        "relation": "evaluated_on",
        "tail": "out-of-distribution tasks"
      },
      {
        "head": "LUFFY",
        "relation": "uses_metric",
        "tail": "average gain"
      },
      {
        "head": "LUFFY",
        "relation": "uses_metric",
        "tail": "advantage"
      },
      {
        "head": "LUFFY",
        "relation": "baseline_model",
        "tail": "Mixed-Policy GRPO"
      },
      {
        "head": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models",
        "relation": "proposed_model",
        "tail": "Large Language Models (LLMs)"
      },
      {
        "head": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models",
        "relation": "baseline_model",
        "tail": "Fine-tuning"
      },
      {
        "head": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models",
        "relation": "baseline_model",
        "tail": "Reinforcement learning"
      },
      {
        "head": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models",
        "relation": "baseline_model",
        "tail": "Test-time scaling"
      },
      {
        "head": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models",
        "relation": "evaluated_on",
        "tail": "Pretraining"
      },
      {
        "head": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models",
        "relation": "evaluated_on",
        "tail": "Post-training"
      },
      {
        "head": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models",
        "relation": "uses_metric",
        "tail": "Catastrophic forgetting"
      },
      {
        "head": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models",
        "relation": "uses_metric",
        "tail": "Reward hacking"
      },
      {
        "head": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models",
        "relation": "uses_metric",
        "tail": "Inference-time trade-offs"
      },
      {
        "head": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement learning for LLMs",
        "relation": "proposed_model",
        "tail": "Tapered Off-Policy REINFORCE (TOPR)"
      },
      {
        "head": "Tapered Off-Policy REINFORCE (TOPR)",
        "relation": "evaluated_on",
        "tail": "GSM8K"
      },
      {
        "head": "Tapered Off-Policy REINFORCE (TOPR)",
        "relation": "evaluated_on",
        "tail": "MATH"
      },
      {
        "head": "Tapered Off-Policy REINFORCE (TOPR)",
        "relation": "uses_metric",
        "tail": "test-time accuracy"
      },
      {
        "head": "Tapered Off-Policy REINFORCE (TOPR)",
        "relation": "uses_metric",
        "tail": "training data efficiency"
      },
      {
        "head": "Tapered Off-Policy REINFORCE (TOPR)",
        "relation": "baseline_model",
        "tail": "REINFORCE"
      },
      {
        "head": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation",
        "relation": "proposed_model",
        "tail": "RefineNet"
      },
      {
        "head": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation",
        "relation": "evaluated_on",
        "tail": "PASCAL VOC 2012"
      },
      {
        "head": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation",
        "relation": "uses_metric",
        "tail": "intersection-over-union"
      },
      {
        "head": "Fully convolutional networks for semantic segmentation",
        "relation": "proposed_model",
        "tail": "Fully convolutional network"
      },
      {
        "head": "Fully convolutional network",
        "relation": "baseline_model",
        "tail": "AlexNet"
      },
      {
        "head": "Fully convolutional network",
        "relation": "baseline_model",
        "tail": "VGG net"
      },
      {
        "head": "Fully convolutional network",
        "relation": "baseline_model",
        "tail": "GoogLeNet"
      },
      {
        "head": "Fully convolutional network",
        "relation": "evaluated_on",
        "tail": "PASCAL VOC"
      },
      {
        "head": "Fully convolutional network",
        "relation": "evaluated_on",
        "tail": "NYUDv2"
      },
      {
        "head": "Fully convolutional network",
        "relation": "evaluated_on",
        "tail": "SIFT Flow"
      },
      {
        "head": "Fully convolutional network",
        "relation": "uses_metric",
        "tail": "mean IU"
      },
      {
        "head": "Diffusion Models in Vision: A Survey",
        "relation": "proposed_model",
        "tail": "denoising diffusion probabilistic models"
      },
      {
        "head": "Diffusion Models in Vision: A Survey",
        "relation": "proposed_model",
        "tail": "noise conditioned score networks"
      },
      {
        "head": "Diffusion Models in Vision: A Survey",
        "relation": "proposed_model",
        "tail": "stochastic differential equations"
      },
      {
        "head": "Diffusion Models in Vision: A Survey",
        "relation": "baseline_model",
        "tail": "variational auto-encoders"
      },
      {
        "head": "Diffusion Models in Vision: A Survey",
        "relation": "baseline_model",
        "tail": "generative adversarial networks"
      },
      {
        "head": "Diffusion Models in Vision: A Survey",
        "relation": "baseline_model",
        "tail": "energy-based models"
      },
      {
        "head": "Diffusion Models in Vision: A Survey",
        "relation": "baseline_model",
        "tail": "autoregressive models"
      },
      {
        "head": "Diffusion Models in Vision: A Survey",
        "relation": "baseline_model",
        "tail": "normalizing flows"
      },
      {
        "head": "SegNeXt",
        "relation": "proposed_model",
        "tail": "SegNeXt"
      },
      {
        "head": "SegNeXt",
        "relation": "baseline_model",
        "tail": "EfficientNet-L2 w/ NAS-FPN"
      },
      {
        "head": "SegNeXt",
        "relation": "evaluated_on",
        "tail": "ADE20K"
      },
      {
        "head": "SegNeXt",
        "relation": "evaluated_on",
        "tail": "Cityscapes"
      },
      {
        "head": "SegNeXt",
        "relation": "evaluated_on",
        "tail": "COCO-Stuff"
      },
      {
        "head": "SegNeXt",
        "relation": "evaluated_on",
        "tail": "Pascal VOC"
      },
      {
        "head": "SegNeXt",
        "relation": "evaluated_on",
        "tail": "Pascal Context"
      },
      {
        "head": "SegNeXt",
        "relation": "evaluated_on",
        "tail": "iSAID"
      },
      {
        "head": "SegNeXt",
        "relation": "uses_metric",
        "tail": "mIoU"
      },
      {
        "head": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation With Transformers",
        "relation": "proposed_model",
        "tail": "CMX"
      },
      {
        "head": "CMX",
        "relation": "proposed_model",
        "tail": "Cross-Modal Feature Rectification Module (CM-FRM)"
      },
      {
        "head": "CMX",
        "relation": "proposed_model",
        "tail": "Feature Fusion Module (FFM)"
      },
      {
        "head": "CMX",
        "relation": "evaluated_on",
        "tail": "depth"
      },
      {
        "head": "CMX",
        "relation": "evaluated_on",
        "tail": "thermal"
      },
      {
        "head": "CMX",
        "relation": "evaluated_on",
        "tail": "polarization"
      },
      {
        "head": "CMX",
        "relation": "evaluated_on",
        "tail": "event"
      },
      {
        "head": "CMX",
        "relation": "evaluated_on",
        "tail": "LiDAR"
      },
      {
        "head": "CMX",
        "relation": "evaluated_on",
        "tail": "RGB-Depth benchmarks"
      },
      {
        "head": "CMX",
        "relation": "evaluated_on",
        "tail": "RGB-Thermal"
      },
      {
        "head": "CMX",
        "relation": "evaluated_on",
        "tail": "RGB-Polarization"
      },
      {
        "head": "CMX",
        "relation": "evaluated_on",
        "tail": "RGB-LiDAR"
      },
      {
        "head": "CMX",
        "relation": "evaluated_on",
        "tail": "EventScape dataset"
      },
      {
        "head": "CMX",
        "relation": "evaluated_on",
        "tail": "RGB-Event semantic segmentation benchmark"
      },
      {
        "head": "CMX",
        "relation": "uses_metric",
        "tail": "state-of-the-art performances"
      },
      {
        "head": "Segment Anything in High Quality",
        "relation": "proposed_model",
        "tail": "HQ-SAM"
      },
      {
        "head": "HQ-SAM",
        "relation": "baseline_model",
        "tail": "Segment Anything Model (SAM)"
      },
      {
        "head": "HQ-SAM",
        "relation": "evaluated_on",
        "tail": "suite of 10 diverse segmentation datasets"
      },
      {
        "head": "HQ-SAM",
        "relation": "evaluated_on",
        "tail": "dataset of 44K fine-grained masks"
      },
      {
        "head": "PIDNet: A Real-time Semantic Segmentation Network Inspired by PID Controllers",
        "relation": "proposed_model",
        "tail": "PIDNet"
      },
      {
        "head": "PIDNet: A Real-time Semantic Segmentation Network Inspired by PID Controllers",
        "relation": "proposed_model",
        "tail": "PIDNet-S"
      },
      {
        "head": "PIDNet",
        "relation": "evaluated_on",
        "tail": "Cityscapes"
      },
      {
        "head": "PIDNet",
        "relation": "evaluated_on",
        "tail": "CamVid"
      },
      {
        "head": "PIDNet-S",
        "relation": "evaluated_on",
        "tail": "Cityscapes"
      },
      {
        "head": "PIDNet-S",
        "relation": "evaluated_on",
        "tail": "CamVid"
      },
      {
        "head": "PIDNet-S",
        "relation": "uses_metric",
        "tail": "mIOU"
      },
      {
        "head": "Qwen2 Technical Report",
        "relation": "proposed_model",
        "tail": "Qwen2 series"
      },
      {
        "head": "Qwen2 series",
        "relation": "baseline_model",
        "tail": "Qwen1.5"
      },
      {
        "head": "Qwen2-72B",
        "relation": "evaluated_on",
        "tail": "MMLU"
      },
      {
        "head": "Qwen2-72B",
        "relation": "evaluated_on",
        "tail": "GPQA"
      },
      {
        "head": "Qwen2-72B",
        "relation": "evaluated_on",
        "tail": "HumanEval"
      },
      {
        "head": "Qwen2-72B",
        "relation": "evaluated_on",
        "tail": "GSM8K"
      },
      {
        "head": "Qwen2-72B",
        "relation": "evaluated_on",
        "tail": "BBH"
      },
      {
        "head": "Qwen2-72B-Instruct",
        "relation": "evaluated_on",
        "tail": "MT-Bench"
      },
      {
        "head": "Qwen2-72B-Instruct",
        "relation": "evaluated_on",
        "tail": "Arena-Hard"
      },
      {
        "head": "Qwen2-72B-Instruct",
        "relation": "evaluated_on",
        "tail": "LiveCodeBench"
      },
      {
        "head": "Qwen2-72B",
        "relation": "uses_metric",
        "tail": "MMLU"
      },
      {
        "head": "Qwen2-72B",
        "relation": "uses_metric",
        "tail": "GPQA"
      },
      {
        "head": "Qwen2-72B",
        "relation": "uses_metric",
        "tail": "HumanEval"
      },
      {
        "head": "Qwen2-72B",
        "relation": "uses_metric",
        "tail": "GSM8K"
      },
      {
        "head": "Qwen2-72B",
        "relation": "uses_metric",
        "tail": "BBH"
      },
      {
        "head": "Qwen2-72B-Instruct",
        "relation": "uses_metric",
        "tail": "MT-Bench"
      },
      {
        "head": "Qwen2-72B-Instruct",
        "relation": "uses_metric",
        "tail": "Arena-Hard"
      },
      {
        "head": "Qwen2-72B-Instruct",
        "relation": "uses_metric",
        "tail": "LiveCodeBench"
      },
      {
        "head": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving",
        "relation": "proposed_model",
        "tail": "FSDrive"
      },
      {
        "head": "FSDrive",
        "relation": "evaluated_on",
        "tail": "nuScenes"
      },
      {
        "head": "FSDrive",
        "relation": "evaluated_on",
        "tail": "NAVSIM"
      },
      {
        "head": "FSDrive",
        "relation": "evaluated_on",
        "tail": "DriveLM"
      },
      {
        "head": "FSDrive",
        "relation": "uses_metric",
        "tail": "FID"
      },
      {
        "head": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning",
        "relation": "proposed_model",
        "tail": "AutoVLA"
      },
      {
        "head": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning",
        "relation": "baseline_model",
        "tail": "Vision-Language-Action (VLA) models"
      },
      {
        "head": "AutoVLA",
        "relation": "evaluated_on",
        "tail": "nuPlan"
      },
      {
        "head": "AutoVLA",
        "relation": "evaluated_on",
        "tail": "nuScenes"
      },
      {
        "head": "AutoVLA",
        "relation": "evaluated_on",
        "tail": "Waymo"
      },
      {
        "head": "AutoVLA",
        "relation": "evaluated_on",
        "tail": "CARLA"
      },
      {
        "head": "AutoVLA",
        "relation": "uses_metric",
        "tail": "Group Relative Policy Optimization (GRPO)"
      },
      {
        "head": "Impromptu VLA",
        "relation": "proposed_model",
        "tail": "Impromptu VLA Dataset"
      },
      {
        "head": "Impromptu VLA",
        "relation": "evaluated_on",
        "tail": "NeuroNCAP"
      },
      {
        "head": "Impromptu VLA",
        "relation": "evaluated_on",
        "tail": "collision rates"
      },
      {
        "head": "Impromptu VLA",
        "relation": "evaluated_on",
        "tail": "nuScenes"
      },
      {
        "head": "Impromptu VLA",
        "relation": "uses_metric",
        "tail": "L2 accuracy"
      },
      {
        "head": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving",
        "relation": "proposed_model",
        "tail": "AgentThink"
      },
      {
        "head": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving",
        "relation": "baseline_model",
        "tail": "Vision-Language Models (VLMs)"
      },
      {
        "head": "AgentThink",
        "relation": "evaluated_on",
        "tail": "DriveLMM-o1"
      },
      {
        "head": "AgentThink",
        "relation": "uses_metric",
        "tail": "overall reasoning scores"
      },
      {
        "head": "AgentThink",
        "relation": "uses_metric",
        "tail": "answer accuracy"
      },
      {
        "head": "Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning",
        "relation": "proposed_model",
        "tail": "Drive-R1"
      },
      {
        "head": "Drive-R1",
        "relation": "evaluated_on",
        "tail": "nuScenes"
      },
      {
        "head": "Drive-R1",
        "relation": "evaluated_on",
        "tail": "DriveLM-nuScenes"
      },
      {
        "head": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model",
        "relation": "proposed_model",
        "tail": "IRL-VLA"
      },
      {
        "head": "IRL-VLA",
        "relation": "proposed_model",
        "tail": "VLA architecture"
      },
      {
        "head": "IRL-VLA",
        "relation": "proposed_model",
        "tail": "VLA policy"
      },
      {
        "head": "IRL-VLA",
        "relation": "proposed_model",
        "tail": "reward world model"
      },
      {
        "head": "IRL-VLA",
        "relation": "baseline_model",
        "tail": "PPO (Proximal Policy Optimization)"
      },
      {
        "head": "IRL-VLA",
        "relation": "evaluated_on",
        "tail": "NAVSIM v2 end-to-end driving benchmark"
      },
      {
        "head": "IRL-VLA",
        "relation": "evaluated_on",
        "tail": "CVPR2025 Autonomous Grand Challenge"
      },
      {
        "head": "IRL-VLA",
        "relation": "uses_metric",
        "tail": "state-of-the-art performance"
      },
      {
        "head": "IRL-VLA",
        "relation": "uses_metric",
        "tail": "1st runner up"
      },
      {
        "head": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving",
        "relation": "proposed_model",
        "tail": "DriveVLA-W0"
      },
      {
        "head": "DriveVLA-W0",
        "relation": "baseline_model",
        "tail": "BEV"
      },
      {
        "head": "DriveVLA-W0",
        "relation": "baseline_model",
        "tail": "VLA"
      },
      {
        "head": "DriveVLA-W0",
        "relation": "evaluated_on",
        "tail": "NAVSIM v1/v2 benchmark"
      },
      {
        "head": "DriveVLA-W0",
        "relation": "evaluated_on",
        "tail": "in-house dataset"
      },
      {
        "head": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
        "relation": "proposed_model",
        "tail": "3D Gaussian Splatting"
      },
      {
        "head": "3D Gaussian Splatting",
        "relation": "evaluated_on",
        "tail": "several established datasets"
      },
      {
        "head": "Generalized Trajectory Scoring for End-to-end Multimodal Planning",
        "relation": "proposed_model",
        "tail": "GTRS (Generalized Trajectory Scoring)"
      },
      {
        "head": "GTRS (Generalized Trajectory Scoring)",
        "relation": "evaluated_on",
        "tail": "Navsim v2 Challenge"
      },
      {
        "head": "RAP: 3D Rasterization Augmented End-to-End Planning",
        "relation": "proposed_model",
        "tail": "3D Rasterization"
      },
      {
        "head": "RAP: 3D Rasterization Augmented End-to-End Planning",
        "relation": "proposed_model",
        "tail": "Rasterization Augmented Planning (RAP)"
      },
      {
        "head": "RAP: 3D Rasterization Augmented End-to-End Planning",
        "relation": "proposed_model",
        "tail": "Raster-to-Real feature-space alignment"
      },
      {
        "head": "Rasterization Augmented Planning (RAP)",
        "relation": "evaluated_on",
        "tail": "NAVSIM v1"
      },
      {
        "head": "Rasterization Augmented Planning (RAP)",
        "relation": "evaluated_on",
        "tail": "NAVSIM v2"
      },
      {
        "head": "Rasterization Augmented Planning (RAP)",
        "relation": "evaluated_on",
        "tail": "Waymo Open Dataset Vision-based E2E Driving"
      },
      {
        "head": "Rasterization Augmented Planning (RAP)",
        "relation": "evaluated_on",
        "tail": "Bench2Drive"
      },
      {
        "head": "Rasterization Augmented Planning (RAP)",
        "relation": "uses_metric",
        "tail": "closed-loop robustness"
      },
      {
        "head": "Rasterization Augmented Planning (RAP)",
        "relation": "uses_metric",
        "tail": "long-tail generalization"
      },
      {
        "head": "PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving",
        "relation": "proposed_model",
        "tail": "PRIX"
      },
      {
        "head": "PRIX",
        "relation": "baseline_model",
        "tail": "multimodal diffusion planners"
      },
      {
        "head": "PRIX",
        "relation": "evaluated_on",
        "tail": "NavSim"
      },
      {
        "head": "PRIX",
        "relation": "evaluated_on",
        "tail": "nuScenes"
      },
      {
        "head": "PRIX",
        "relation": "uses_metric",
        "tail": "inference speed"
      },
      {
        "head": "PRIX",
        "relation": "uses_metric",
        "tail": "model size"
      },
      {
        "head": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
        "relation": "proposed_model",
        "tail": "BLIP-2"
      },
      {
        "head": "BLIP-2",
        "relation": "baseline_model",
        "tail": "Flamingo80B"
      },
      {
        "head": "BLIP-2",
        "relation": "evaluated_on",
        "tail": "VQAv2"
      },
      {
        "head": "BLIP-2",
        "relation": "uses_metric",
        "tail": "zero-shot VQAv2"
      },
      {
        "head": "Flow Matching for Generative Modeling",
        "relation": "proposed_model",
        "tail": "Flow Matching (FM)"
      },
      {
        "head": "Flow Matching (FM)",
        "relation": "baseline_model",
        "tail": "diffusion models"
      },
      {
        "head": "Flow Matching (FM)",
        "relation": "evaluated_on",
        "tail": "ImageNet"
      },
      {
        "head": "Flow Matching (FM)",
        "relation": "uses_metric",
        "tail": "likelihood"
      },
      {
        "head": "Flow Matching (FM)",
        "relation": "uses_metric",
        "tail": "sample quality"
      },
      {
        "head": "Diffusion policy: Visuomotor policy learning via action diffusion",
        "relation": "proposed_model",
        "tail": "Diffusion Policy"
      },
      {
        "head": "Diffusion policy: Visuomotor policy learning via action diffusion",
        "relation": "baseline_model",
        "tail": "existing state-of-the-art robot learning methods"
      },
      {
        "head": "Diffusion Policy",
        "relation": "evaluated_on",
        "tail": "4 different robot manipulation benchmarks"
      },
      {
        "head": "Diffusion Policy",
        "relation": "uses_metric",
        "tail": "average improvement of 46.9%"
      },
      {
        "head": "Large Language Models for Robotics: A Survey",
        "relation": "proposed_model",
        "tail": "LLM-based robotic models"
      },
      {
        "head": "Large Language Models for Robotics: A Survey",
        "relation": "evaluated_on",
        "tail": "robot control"
      },
      {
        "head": "Large Language Models for Robotics: A Survey",
        "relation": "evaluated_on",
        "tail": "perception"
      },
      {
        "head": "Large Language Models for Robotics: A Survey",
        "relation": "evaluated_on",
        "tail": "decision-making"
      },
      {
        "head": "Large Language Models for Robotics: A Survey",
        "relation": "evaluated_on",
        "tail": "planning"
      },
      {
        "head": "Large Language Models for Robotics: A Survey",
        "relation": "uses_metric",
        "tail": "dexterity intelligence"
      },
      {
        "head": "Large Language Models for Robotics: A Survey",
        "relation": "uses_metric",
        "tail": "human-robot interaction"
      },
      {
        "head": "Large Language Models for Robotics: A Survey",
        "relation": "uses_metric",
        "tail": "autonomy"
      },
      {
        "head": "LLM-based robotic models",
        "relation": "baseline_model",
        "tail": "Large Language Models (LLMs)"
      },
      {
        "head": "Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)",
        "relation": "proposed_model",
        "tail": "Raw2Drive"
      },
      {
        "head": "Raw2Drive",
        "relation": "evaluated_on",
        "tail": "CARLA Leaderboard 2.0"
      },
      {
        "head": "Raw2Drive",
        "relation": "evaluated_on",
        "tail": "Bench2Drive"
      },
      {
        "head": "Raw2Drive",
        "relation": "uses_metric",
        "tail": "state-of-the-art performance"
      },
      {
        "head": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey",
        "relation": "proposed_model",
        "tail": "Vision Language Action (VLA) models"
      },
      {
        "head": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey",
        "relation": "baseline_model",
        "tail": "Vision Language Models (VLMs)"
      },
      {
        "head": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey",
        "relation": "evaluated_on",
        "tail": "foundational datasets"
      },
      {
        "head": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey",
        "relation": "evaluated_on",
        "tail": "simulation platforms"
      },
      {
        "head": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey",
        "relation": "uses_metric",
        "tail": "benchmarks"
      },
      {
        "head": "ReSim: Reliable World Simulation for Autonomous Driving",
        "relation": "proposed_model",
        "tail": "ReSim"
      },
      {
        "head": "ReSim: Reliable World Simulation for Autonomous Driving",
        "relation": "proposed_model",
        "tail": "Video2Reward"
      },
      {
        "head": "ReSim",
        "relation": "baseline_model",
        "tail": "diffusion transformer architecture"
      },
      {
        "head": "ReSim",
        "relation": "evaluated_on",
        "tail": "NAVSIM"
      },
      {
        "head": "ReSim",
        "relation": "uses_metric",
        "tail": "visual fidelity"
      },
      {
        "head": "ReSim",
        "relation": "uses_metric",
        "tail": "controllability"
      },
      {
        "head": "ReSim",
        "relation": "uses_metric",
        "tail": "planning performance"
      },
      {
        "head": "ReSim",
        "relation": "uses_metric",
        "tail": "policy selection performance"
      },
      {
        "head": "ReSim",
        "relation": "evaluated_on",
        "tail": "CARLA"
      },
      {
        "head": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
        "relation": "proposed_model",
        "tail": "Llama 2"
      },
      {
        "head": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
        "relation": "proposed_model",
        "tail": "Llama 2-Chat"
      },
      {
        "head": "Llama 2-Chat",
        "relation": "baseline_model",
        "tail": "open-source chat models"
      },
      {
        "head": "Llama 2-Chat",
        "relation": "baseline_model",
        "tail": "closed-source models"
      },
      {
        "head": "Llama 2-Chat",
        "relation": "evaluated_on",
        "tail": "benchmarks"
      },
      {
        "head": "Llama 2-Chat",
        "relation": "uses_metric",
        "tail": "human evaluations for helpfulness and safety"
      },
      {
        "head": "Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey",
        "relation": "proposed_model",
        "tail": "Vision-Language-Action (VLA) models"
      },
      {
        "head": "Vision-Language-Action (VLA) models",
        "relation": "baseline_model",
        "tail": "vision-language models"
      },
      {
        "head": "AdaThinkDrive",
        "relation": "proposed_model",
        "tail": "AdaThinkDrive"
      },
      {
        "head": "AdaThinkDrive",
        "relation": "baseline_model",
        "tail": "never Think baseline"
      },
      {
        "head": "AdaThinkDrive",
        "relation": "baseline_model",
        "tail": "always Think baseline"
      },
      {
        "head": "AdaThinkDrive",
        "relation": "baseline_model",
        "tail": "vision only baseline"
      },
      {
        "head": "AdaThinkDrive",
        "relation": "evaluated_on",
        "tail": "Navsim"
      },
      {
        "head": "AdaThinkDrive",
        "relation": "uses_metric",
        "tail": "PDMS"
      },
      {
        "head": "AdaThinkDrive",
        "relation": "proposed_model",
        "tail": "Group Relative Policy Optimization (GRPO)"
      },
      {
        "head": "AdaThinkDrive",
        "relation": "baseline_model",
        "tail": "Chain of Thought (CoT)"
      },
      {
        "head": "AdaThinkDrive",
        "relation": "baseline_model",
        "tail": "Vision Language Action (VLA) models"
      },
      {
        "head": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition",
        "relation": "proposed_model",
        "tail": "UniV2X framework"
      },
      {
        "head": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition",
        "relation": "evaluated_on",
        "tail": "V2X-Seq-SPD dataset"
      },
      {
        "head": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition",
        "relation": "evaluated_on",
        "tail": "End-to-End Autonomous Driving through V2X Cooperation Challenge"
      },
      {
        "head": "End-to-End Autonomous Driving through V2X Cooperation Challenge",
        "relation": "uses_metric",
        "tail": "cooperative temporal perception"
      },
      {
        "head": "End-to-End Autonomous Driving through V2X Cooperation Challenge",
        "relation": "uses_metric",
        "tail": "cooperative end-to-end planning"
      },
      {
        "head": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI",
        "relation": "proposed_model",
        "tail": "Multi-modal Large Models (MLMs)"
      },
      {
        "head": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI",
        "relation": "proposed_model",
        "tail": "World Models (WMs)"
      },
      {
        "head": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI",
        "relation": "baseline_model",
        "tail": "embodied robots"
      },
      {
        "head": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI",
        "relation": "baseline_model",
        "tail": "simulators"
      },
      {
        "head": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI",
        "relation": "evaluated_on",
        "tail": "embodied perception"
      },
      {
        "head": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI",
        "relation": "evaluated_on",
        "tail": "embodied interaction"
      },
      {
        "head": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI",
        "relation": "evaluated_on",
        "tail": "embodied agent"
      },
      {
        "head": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI",
        "relation": "evaluated_on",
        "tail": "sim-to-real adaptation"
      },
      {
        "head": "Aligning Cyber Space With Physical World: A Comprehensive Survey on Embodied AI",
        "relation": "uses_metric",
        "tail": "comprehensive datasets"
      },
      {
        "head": "A Survey on Vision-Language-Action Models for Embodied AI",
        "relation": "proposed_model",
        "tail": "vision-language-action models (VLAs)"
      },
      {
        "head": "vision-language-action models (VLAs)",
        "relation": "baseline_model",
        "tail": "large language models"
      },
      {
        "head": "vision-language-action models (VLAs)",
        "relation": "baseline_model",
        "tail": "vision-language models"
      },
      {
        "head": "vision-language-action models (VLAs)",
        "relation": "evaluated_on",
        "tail": "datasets"
      },
      {
        "head": "vision-language-action models (VLAs)",
        "relation": "evaluated_on",
        "tail": "simulators"
      },
      {
        "head": "vision-language-action models (VLAs)",
        "relation": "uses_metric",
        "tail": "benchmarks"
      },
      {
        "head": "VLA-based control policies",
        "relation": "proposed_model",
        "tail": "vision-language-action models (VLAs)"
      },
      {
        "head": "high-level task planners",
        "relation": "proposed_model",
        "tail": "vision-language-action models (VLAs)"
      },
      {
        "head": "DriveDreamer4D",
        "relation": "proposed_model",
        "tail": "DriveDreamer4D"
      },
      {
        "head": "DriveDreamer4D",
        "relation": "baseline_model",
        "tail": "PVG"
      },
      {
        "head": "DriveDreamer4D",
        "relation": "baseline_model",
        "tail": "S3Gaussian"
      },
      {
        "head": "DriveDreamer4D",
        "relation": "baseline_model",
        "tail": "Deformable-GS"
      },
      {
        "head": "DriveDreamer4D",
        "relation": "evaluated_on",
        "tail": "DriveDreamer4D"
      },
      {
        "head": "DriveDreamer4D",
        "relation": "uses_metric",
        "tail": "FID"
      },
      {
        "head": "DriveDreamer4D",
        "relation": "uses_metric",
        "tail": "NTA-IoU"
      },
      {
        "head": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation",
        "relation": "proposed_model",
        "tail": "PhyGenBench"
      },
      {
        "head": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation",
        "relation": "proposed_model",
        "tail": "PhyGenEval"
      },
      {
        "head": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation",
        "relation": "baseline_model",
        "tail": "Sora"
      },
      {
        "head": "PhyGenBench",
        "relation": "evaluated_on",
        "tail": "Sora"
      },
      {
        "head": "PhyGenEval",
        "relation": "uses_metric",
        "tail": "PhyGenBench"
      },
      {
        "head": "AgentSquare: Automatic LLM Agent Search in Modular Design Space",
        "relation": "proposed_model",
        "tail": "AgentSquare"
      },
      {
        "head": "AgentSquare: Automatic LLM Agent Search in Modular Design Space",
        "relation": "proposed_model",
        "tail": "Modularized LLM Agent Search (MoLAS)"
      },
      {
        "head": "AgentSquare",
        "relation": "evaluated_on",
        "tail": "six benchmarks"
      },
      {
        "head": "AgentSquare",
        "relation": "uses_metric",
        "tail": "performance gain"
      },
      {
        "head": "CityGPT",
        "relation": "proposed_model",
        "tail": "SWFT"
      },
      {
        "head": "CityGPT",
        "relation": "evaluated_on",
        "tail": "CityEval"
      },
      {
        "head": "SWFT",
        "relation": "baseline_model",
        "tail": "ChatGLM3-6B"
      },
      {
        "head": "SWFT",
        "relation": "baseline_model",
        "tail": "Llama3-8B"
      },
      {
        "head": "SWFT",
        "relation": "baseline_model",
        "tail": "Qwen2.5-7B"
      },
      {
        "head": "SWFT",
        "relation": "uses_metric",
        "tail": "CityEval"
      },
      {
        "head": "SWFT",
        "relation": "evaluated_on",
        "tail": "CityInstruction"
      },
      {
        "head": "Epona: Autoregressive Diffusion World Model for Autonomous Driving",
        "relation": "proposed_model",
        "tail": "Epona"
      },
      {
        "head": "Epona",
        "relation": "evaluated_on",
        "tail": "NAVSIM"
      },
      {
        "head": "Epona",
        "relation": "uses_metric",
        "tail": "FVD"
      },
      {
        "head": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
        "relation": "proposed_model",
        "tail": "RoBERTa"
      },
      {
        "head": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
        "relation": "baseline_model",
        "tail": "BERT"
      },
      {
        "head": "RoBERTa",
        "relation": "evaluated_on",
        "tail": "GLUE"
      },
      {
        "head": "RoBERTa",
        "relation": "evaluated_on",
        "tail": "RACE"
      },
      {
        "head": "RoBERTa",
        "relation": "evaluated_on",
        "tail": "SQuAD"
      },
      {
        "head": "Unlearning Concepts in Diffusion Model via Concept Domain Correction and Concept Preserving Gradient",
        "relation": "proposed_model",
        "tail": "DoCo"
      },
      {
        "head": "Unlearning Concepts in Diffusion Model via Concept Domain Correction and Concept Preserving Gradient",
        "relation": "baseline_model",
        "tail": "Machine Unlearning (MU)"
      },
      {
        "head": "DoCo",
        "relation": "evaluated_on",
        "tail": "various instances, styles, and offensive concepts"
      },
      {
        "head": "DoCo",
        "relation": "uses_metric",
        "tail": "generalization"
      },
      {
        "head": "DoCo",
        "relation": "uses_metric",
        "tail": "utility"
      },
      {
        "head": "Machine Unlearning (MU)",
        "relation": "uses_metric",
        "tail": "generalization"
      },
      {
        "head": "Machine Unlearning (MU)",
        "relation": "uses_metric",
        "tail": "utility"
      },
      {
        "head": "Safety at Scale: A Comprehensive Survey of Large Model Safety",
        "relation": "proposed_model",
        "tail": "Vision Foundation Models (VFMs)"
      },
      {
        "head": "Safety at Scale: A Comprehensive Survey of Large Model Safety",
        "relation": "proposed_model",
        "tail": "Large Language Models (LLMs)"
      },
      {
        "head": "Safety at Scale: A Comprehensive Survey of Large Model Safety",
        "relation": "proposed_model",
        "tail": "Vision-Language Pre-training (VLP) models"
      },
      {
        "head": "Safety at Scale: A Comprehensive Survey of Large Model Safety",
        "relation": "proposed_model",
        "tail": "Vision-Language Models (VLMs)"
      },
      {
        "head": "Safety at Scale: A Comprehensive Survey of Large Model Safety",
        "relation": "proposed_model",
        "tail": "Diffusion Models (DMs)"
      },
      {
        "head": "Safety at Scale: A Comprehensive Survey of Large Model Safety",
        "relation": "proposed_model",
        "tail": "large-model-based Agents"
      },
      {
        "head": "Safety at Scale: A Comprehensive Survey of Large Model Safety",
        "relation": "evaluated_on",
        "tail": "commonly used datasets and benchmarks for safety research"
      },
      {
        "head": "The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense",
        "relation": "proposed_model",
        "tail": "LLM-Pipeline"
      },
      {
        "head": "The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense",
        "relation": "evaluated_on",
        "tail": "benchmark datasets"
      },
      {
        "head": "The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense",
        "relation": "uses_metric",
        "tail": "benchmark evaluations"
      },
      {
        "head": "The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense",
        "relation": "uses_metric",
        "tail": "evaluation methods for jailbreak"
      },
      {
        "head": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs",
        "relation": "proposed_model",
        "tail": "Video-SafetyBench"
      },
      {
        "head": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs",
        "relation": "proposed_model",
        "tail": "RJScore"
      },
      {
        "head": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs",
        "relation": "evaluated_on",
        "tail": "Video-SafetyBench"
      },
      {
        "head": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs",
        "relation": "uses_metric",
        "tail": "RJScore"
      },
      {
        "head": "Video-SafetyBench",
        "relation": "baseline_model",
        "tail": "Large Vision-Language Models (LVLMs)"
      },
      {
        "head": "Gradient-Guided Learning Network for Infrared Small Target Detection",
        "relation": "proposed_model",
        "tail": "GGL-Net"
      },
      {
        "head": "GGL-Net",
        "relation": "evaluated_on",
        "tail": "NUAA-SIRST dataset"
      },
      {
        "head": "GGL-Net",
        "relation": "evaluated_on",
        "tail": "NUDT-SIRST dataset"
      },
      {
        "head": "GGL-Net",
        "relation": "uses_metric",
        "tail": "state-of-the-art results"
      },
      {
        "head": "A Retrieval-Augmented Generation Approach to Extracting Algorithmic Logic from Neural Networks",
        "relation": "proposed_model",
        "tail": "NN-RAG"
      },
      {
        "head": "NN-RAG",
        "relation": "evaluated_on",
        "tail": "LEMUR"
      },
      {
        "head": "NN-RAG",
        "relation": "uses_metric",
        "tail": "structural uniqueness"
      },
      {
        "head": "NN-RAG",
        "relation": "uses_metric",
        "tail": "novel network structures"
      },
      {
        "head": "CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation",
        "relation": "proposed_model",
        "tail": "CenterMamba-SAM"
      },
      {
        "head": "CenterMamba-SAM",
        "relation": "proposed_model",
        "tail": "CenterMamba encoder"
      },
      {
        "head": "CenterMamba-SAM",
        "relation": "proposed_model",
        "tail": "memory-driven structural prompt generator"
      },
      {
        "head": "CenterMamba-SAM",
        "relation": "proposed_model",
        "tail": "memory-augmented multi-scale decoder"
      },
      {
        "head": "CenterMamba-SAM",
        "relation": "evaluated_on",
        "tail": "public benchmarks"
      },
      {
        "head": "IMobileTransformer: A fusion-based lightweight model for rice disease identification",
        "relation": "proposed_model",
        "tail": "IMobileTransformer"
      },
      {
        "head": "Non-invasive diagnosis of nutrient deficiencies in winter wheat and winter rye using UAV-based RGB images",
        "relation": "evaluated_on",
        "tail": "UAV-based RGB images"
      },
      {
        "head": "RF-DETR",
        "relation": "proposed_model",
        "tail": "RF-DETR (nano)"
      },
      {
        "head": "RF-DETR",
        "relation": "proposed_model",
        "tail": "RF-DETR (2x-large)"
      },
      {
        "head": "RF-DETR",
        "relation": "baseline_model",
        "tail": "D-FINE (nano)"
      },
      {
        "head": "RF-DETR",
        "relation": "baseline_model",
        "tail": "GroundingDINO (tiny)"
      },
      {
        "head": "RF-DETR (nano)",
        "relation": "evaluated_on",
        "tail": "COCO"
      },
      {
        "head": "RF-DETR (2x-large)",
        "relation": "evaluated_on",
        "tail": "COCO"
      },
      {
        "head": "RF-DETR (2x-large)",
        "relation": "evaluated_on",
        "tail": "Roboflow100-VL"
      },
      {
        "head": "RF-DETR (nano)",
        "relation": "uses_metric",
        "tail": "AP"
      },
      {
        "head": "RF-DETR (2x-large)",
        "relation": "uses_metric",
        "tail": "AP"
      },
      {
        "head": "FreeOrbit4D",
        "relation": "proposed_model",
        "tail": "FreeOrbit4D"
      },
      {
        "head": "FreeOrbit4D",
        "relation": "baseline_model",
        "tail": "diffusion-based methods"
      },
      {
        "head": "FreeOrbit4D",
        "relation": "evaluated_on",
        "tail": "monocular video"
      },
      {
        "head": "FreeOrbit4D",
        "relation": "evaluated_on",
        "tail": "redirected videos"
      },
      {
        "head": "FreeOrbit4D",
        "relation": "uses_metric",
        "tail": "faithful redirected videos"
      },
      {
        "head": "FreeOrbit4D",
        "relation": "proposed_model",
        "tail": "object-centric multi-view diffusion model"
      },
      {
        "head": "FreeOrbit4D",
        "relation": "proposed_model",
        "tail": "conditional video diffusion model"
      },
      {
        "head": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos",
        "relation": "proposed_model",
        "tail": "NeoVerse"
      },
      {
        "head": "NeoVerse",
        "relation": "evaluated_on",
        "tail": "standard reconstruction and generation benchmarks"
      },
      {
        "head": "Scalable Diffusion Models with Transformers",
        "relation": "proposed_model",
        "tail": "Diffusion Transformers (DiTs)"
      },
      {
        "head": "Scalable Diffusion Models with Transformers",
        "relation": "baseline_model",
        "tail": "U-Net"
      },
      {
        "head": "Diffusion Transformers (DiTs)",
        "relation": "evaluated_on",
        "tail": "ImageNet 512x512"
      },
      {
        "head": "Diffusion Transformers (DiTs)",
        "relation": "evaluated_on",
        "tail": "ImageNet 256x256"
      },
      {
        "head": "DiT-XL/2",
        "relation": "evaluated_on",
        "tail": "ImageNet 512x512"
      },
      {
        "head": "DiT-XL/2",
        "relation": "evaluated_on",
        "tail": "ImageNet 256x256"
      },
      {
        "head": "Diffusion Transformers (DiTs)",
        "relation": "uses_metric",
        "tail": "FID"
      },
      {
        "head": "Diffusion Transformers (DiTs)",
        "relation": "uses_metric",
        "tail": "Gflops"
      },
      {
        "head": "DiT-XL/2",
        "relation": "uses_metric",
        "tail": "FID"
      },
      {
        "head": "RoFormer: Enhanced Transformer with Rotary Position Embedding",
        "relation": "proposed_model",
        "tail": "RoPE"
      },
      {
        "head": "RoFormer: Enhanced Transformer with Rotary Position Embedding",
        "relation": "proposed_model",
        "tail": "RoFormer"
      },
      {
        "head": "RoFormer: Enhanced Transformer with Rotary Position Embedding",
        "relation": "evaluated_on",
        "tail": "long text classification benchmark datasets"
      },
      {
        "head": "The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey",
        "relation": "proposed_model",
        "tail": "Driving World Model (DWM)"
      },
      {
        "head": "Driving World Model (DWM)",
        "relation": "evaluated_on",
        "tail": "mainstream simulators"
      },
      {
        "head": "Driving World Model (DWM)",
        "relation": "evaluated_on",
        "tail": "high-impact datasets"
      },
      {
        "head": "Driving World Model (DWM)",
        "relation": "uses_metric",
        "tail": "various metrics"
      },
      {
        "head": "Vision meets robotics: The KITTI dataset",
        "relation": "evaluated_on",
        "tail": "KITTI"
      },
      {
        "head": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations",
        "relation": "proposed_model",
        "tail": "FlexMap"
      },
      {
        "head": "FlexMap: Generalized HD Map Construction from Flexible Camera Configurations",
        "relation": "baseline_model",
        "tail": "existing methods"
      },
      {
        "head": "FlexMap",
        "relation": "evaluated_on",
        "tail": "multiple configurations"
      },
      {
        "head": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction",
        "relation": "proposed_model",
        "tail": "MapAnything"
      },
      {
        "head": "MapAnything",
        "relation": "evaluated_on",
        "tail": "diverse datasets"
      },
      {
        "head": "MapAnything",
        "relation": "baseline_model",
        "tail": "specialist feed-forward models"
      },
      {
        "head": "Seedream 4.0",
        "relation": "proposed_model",
        "tail": "Seedream 4.0"
      },
      {
        "head": "Seedream 4.0",
        "relation": "proposed_model",
        "tail": "diffusion transformer"
      },
      {
        "head": "Seedream 4.0",
        "relation": "proposed_model",
        "tail": "VLM model"
      },
      {
        "head": "Seedream 4.0",
        "relation": "evaluated_on",
        "tail": "billions of text-image pairs"
      },
      {
        "head": "Seedream 4.0",
        "relation": "uses_metric",
        "tail": "T2I"
      },
      {
        "head": "Seedream 4.0",
        "relation": "uses_metric",
        "tail": "multimodal image editing"
      },
      {
        "head": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation",
        "relation": "proposed_model",
        "tail": "Echo-4o"
      },
      {
        "head": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation",
        "relation": "proposed_model",
        "tail": "Echo-4o-Image"
      },
      {
        "head": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation",
        "relation": "proposed_model",
        "tail": "GenEval++"
      },
      {
        "head": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation",
        "relation": "proposed_model",
        "tail": "Imagine-Bench"
      },
      {
        "head": "Echo-4o",
        "relation": "baseline_model",
        "tail": "Bagel"
      },
      {
        "head": "Echo-4o",
        "relation": "evaluated_on",
        "tail": "GenEval++"
      },
      {
        "head": "Echo-4o",
        "relation": "evaluated_on",
        "tail": "Imagine-Bench"
      },
      {
        "head": "Echo-4o",
        "relation": "uses_metric",
        "tail": "GenEval++"
      },
      {
        "head": "Echo-4o",
        "relation": "uses_metric",
        "tail": "Imagine-Bench"
      },
      {
        "head": "Echo-4o-Image",
        "relation": "evaluated_on",
        "tail": "OmniGen2"
      },
      {
        "head": "Echo-4o-Image",
        "relation": "evaluated_on",
        "tail": "BLIP3-o"
      },
      {
        "head": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding",
        "relation": "proposed_model",
        "tail": "Lumina-DiMOO"
      },
      {
        "head": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding",
        "relation": "baseline_model",
        "tail": "autoregressive (AR) or hybrid AR-Diffusion paradigms"
      },
      {
        "head": "Lumina-DiMOO",
        "relation": "evaluated_on",
        "tail": "multiple benchmarks"
      },
      {
        "head": "Lumina-DiMOO",
        "relation": "uses_metric",
        "tail": "state-of-the-art performance"
      },
      {
        "head": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale",
        "relation": "proposed_model",
        "tail": "NextStep-1"
      },
      {
        "head": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale",
        "relation": "baseline_model",
        "tail": "autoregressive models"
      },
      {
        "head": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale",
        "relation": "baseline_model",
        "tail": "diffusion models"
      },
      {
        "head": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale",
        "relation": "baseline_model",
        "tail": "vector quantization (VQ)"
      },
      {
        "head": "NextStep-1",
        "relation": "evaluated_on",
        "tail": "text-to-image generation tasks"
      },
      {
        "head": "NextStep-1",
        "relation": "uses_metric",
        "tail": "high-fidelity image synthesis"
      },
      {
        "head": "NextStep-1",
        "relation": "uses_metric",
        "tail": "image editing"
      },
      {
        "head": "Training language models to follow instructions with human feedback",
        "relation": "proposed_model",
        "tail": "InstructGPT"
      },
      {
        "head": "Training language models to follow instructions with human feedback",
        "relation": "baseline_model",
        "tail": "GPT-3"
      },
      {
        "head": "InstructGPT",
        "relation": "evaluated_on",
        "tail": "prompt distribution"
      },
      {
        "head": "InstructGPT",
        "relation": "evaluated_on",
        "tail": "public NLP datasets"
      },
      {
        "head": "InstructGPT",
        "relation": "uses_metric",
        "tail": "human evaluations"
      },
      {
        "head": "InstructGPT",
        "relation": "uses_metric",
        "tail": "truthfulness"
      },
      {
        "head": "InstructGPT",
        "relation": "uses_metric",
        "tail": "toxic output generation"
      },
      {
        "head": "Search-R1",
        "relation": "proposed_model",
        "tail": "Search-R1"
      },
      {
        "head": "Search-R1",
        "relation": "baseline_model",
        "tail": "various RAG baselines"
      },
      {
        "head": "Search-R1",
        "relation": "evaluated_on",
        "tail": "seven question-answering datasets"
      },
      {
        "head": "Search-R1",
        "relation": "uses_metric",
        "tail": "performance"
      },
      {
        "head": "Toward expert-level medical question answering with large language models",
        "relation": "proposed_model",
        "tail": "Med-PaLM 2"
      },
      {
        "head": "Toward expert-level medical question answering with large language models",
        "relation": "baseline_model",
        "tail": "Med-PaLM"
      },
      {
        "head": "Med-PaLM 2",
        "relation": "evaluated_on",
        "tail": "MedQA"
      },
      {
        "head": "Med-PaLM 2",
        "relation": "evaluated_on",
        "tail": "MedMCQA"
      },
      {
        "head": "Med-PaLM 2",
        "relation": "evaluated_on",
        "tail": "PubMedQA"
      },
      {
        "head": "Med-PaLM 2",
        "relation": "evaluated_on",
        "tail": "MMLU clinical topics"
      },
      {
        "head": "Med-PaLM 2",
        "relation": "uses_metric",
        "tail": "score"
      },
      {
        "head": "Med-PaLM 2",
        "relation": "uses_metric",
        "tail": "performance"
      },
      {
        "head": "Med-PaLM 2",
        "relation": "uses_metric",
        "tail": "evaluation metrics"
      },
      {
        "head": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training",
        "relation": "proposed_model",
        "tail": "Supervised fine-tuning (SFT)"
      },
      {
        "head": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training",
        "relation": "proposed_model",
        "tail": "reinforcement learning (RL)"
      },
      {
        "head": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training",
        "relation": "evaluated_on",
        "tail": "GeneralPoints"
      },
      {
        "head": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training",
        "relation": "evaluated_on",
        "tail": "V-IRL"
      },
      {
        "head": "reinforcement learning (RL)",
        "relation": "baseline_model",
        "tail": "Supervised fine-tuning (SFT)"
      },
      {
        "head": "reinforcement learning (RL)",
        "relation": "uses_metric",
        "tail": "outcome-based reward"
      },
      {
        "head": "Equivariant Diffusion for Crystal Structure Prediction",
        "relation": "proposed_model",
        "tail": "EquiCSP"
      },
      {
        "head": "Equivariant Diffusion for Crystal Structure Prediction",
        "relation": "baseline_model",
        "tail": "existing models"
      },
      {
        "head": "EquiCSP",
        "relation": "uses_metric",
        "tail": "accurate structures"
      },
      {
        "head": "EquiCSP",
        "relation": "uses_metric",
        "tail": "faster convergence"
      },
      {
        "head": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling",
        "relation": "proposed_model",
        "tail": "WorldPlay"
      },
      {
        "head": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling",
        "relation": "baseline_model",
        "tail": "existing techniques"
      },
      {
        "head": "WorldPlay",
        "relation": "evaluated_on",
        "tail": "diverse scenes"
      },
      {
        "head": "WorldPlay",
        "relation": "uses_metric",
        "tail": "long-term geometric consistency"
      },
      {
        "head": "WorldPlay",
        "relation": "uses_metric",
        "tail": "real-time speeds"
      },
      {
        "head": "WorldPlay",
        "relation": "uses_metric",
        "tail": "24 FPS"
      },
      {
        "head": "WorldPlay",
        "relation": "uses_metric",
        "tail": "720p video"
      },
      {
        "head": "Native and Compact Structured Latents for 3D Generation",
        "relation": "proposed_model",
        "tail": "O-Voxel"
      },
      {
        "head": "Native and Compact Structured Latents for 3D Generation",
        "relation": "proposed_model",
        "tail": "Sparse Compression VAE"
      },
      {
        "head": "Native and Compact Structured Latents for 3D Generation",
        "relation": "proposed_model",
        "tail": "flow-matching models"
      },
      {
        "head": "Native and Compact Structured Latents for 3D Generation",
        "relation": "baseline_model",
        "tail": "existing models"
      },
      {
        "head": "Native and Compact Structured Latents for 3D Generation",
        "relation": "evaluated_on",
        "tail": "diverse public 3D asset datasets"
      },
      {
        "head": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?",
        "relation": "proposed_model",
        "tail": "JEPA-WMs"
      },
      {
        "head": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?",
        "relation": "baseline_model",
        "tail": "DINO-WM"
      },
      {
        "head": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?",
        "relation": "baseline_model",
        "tail": "V-JEPA-2-AC"
      },
      {
        "head": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?",
        "relation": "evaluated_on",
        "tail": "simulated environments"
      },
      {
        "head": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?",
        "relation": "evaluated_on",
        "tail": "real-world robotic data"
      },
      {
        "head": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?",
        "relation": "uses_metric",
        "tail": "planning success"
      },
      {
        "head": "Progressive Learned Image Transmission for Semantic Communication Using Hierarchical VAE",
        "relation": "proposed_model",
        "tail": "PLIT"
      },
      {
        "head": "PLIT",
        "relation": "uses_metric",
        "tail": "rate-distortion performance"
      },
      {
        "head": "STORM: Search-Guided Generative World Models for Robotic Manipulation",
        "relation": "proposed_model",
        "tail": "STORM"
      },
      {
        "head": "STORM",
        "relation": "baseline_model",
        "tail": "Vision-Language-Action (VLA) models"
      },
      {
        "head": "STORM",
        "relation": "baseline_model",
        "tail": "CogACT"
      },
      {
        "head": "STORM",
        "relation": "evaluated_on",
        "tail": "SimplerEnv manipulation benchmark"
      },
      {
        "head": "STORM",
        "relation": "uses_metric",
        "tail": "average success rate"
      },
      {
        "head": "STORM",
        "relation": "uses_metric",
        "tail": "Frechet Video Distance"
      },
      {
        "head": "Continuous 3D Perception Model with Persistent State",
        "relation": "proposed_model",
        "tail": "CUT3R (Continuous Updating Transformer for 3D Reconstruction)"
      },
      {
        "head": "CUT3R (Continuous Updating Transformer for 3D Reconstruction)",
        "relation": "evaluated_on",
        "tail": "various 3D/4D tasks"
      },
      {
        "head": "Gen3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control",
        "relation": "proposed_model",
        "tail": "GEN3C"
      },
      {
        "head": "GEN3C",
        "relation": "evaluated_on",
        "tail": "sparse-view novel view synthesis"
      },
      {
        "head": "GEN3C",
        "relation": "evaluated_on",
        "tail": "driving scenes"
      },
      {
        "head": "GEN3C",
        "relation": "evaluated_on",
        "tail": "monocular dynamic video"
      },
      {
        "head": "GEN3C",
        "relation": "uses_metric",
        "tail": "camera control"
      },
      {
        "head": "GEN3C",
        "relation": "uses_metric",
        "tail": "temporal 3D consistency"
      },
      {
        "head": "FLARE: Feed-Forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views",
        "relation": "proposed_model",
        "tail": "FLARE"
      },
      {
        "head": "FLARE",
        "relation": "evaluated_on",
        "tail": "large-scale public datasets"
      },
      {
        "head": "FLARE",
        "relation": "uses_metric",
        "tail": "pose estimation"
      },
      {
        "head": "FLARE",
        "relation": "uses_metric",
        "tail": "geometry reconstruction"
      },
      {
        "head": "FLARE",
        "relation": "uses_metric",
        "tail": "novel view synthesis"
      },
      {
        "head": "UniDepthV2",
        "relation": "proposed_model",
        "tail": "UniDepth"
      },
      {
        "head": "UniDepthV2",
        "relation": "evaluated_on",
        "tail": "ten depth datasets"
      },
      {
        "head": "OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving",
        "relation": "proposed_model",
        "tail": "OpenEMMA"
      },
      {
        "head": "OpenEMMA",
        "relation": "baseline_model",
        "tail": "Multimodal Large Language Models (MLLMs)"
      },
      {
        "head": "OpenEMMA",
        "relation": "uses_metric",
        "tail": "Chain-of-Thought reasoning process"
      },
      {
        "head": "Adaptive transfer learning for surgical tool presence detection in laparoscopic videos through gradual freezing fine-tuning",
        "relation": "proposed_model",
        "tail": "staged adaptive fine-tuning approach"
      },
      {
        "head": "staged adaptive fine-tuning approach",
        "relation": "baseline_model",
        "tail": "ResNet-50"
      },
      {
        "head": "staged adaptive fine-tuning approach",
        "relation": "baseline_model",
        "tail": "DenseNet-121"
      },
      {
        "head": "staged adaptive fine-tuning approach",
        "relation": "evaluated_on",
        "tail": "Cholec80"
      },
      {
        "head": "staged adaptive fine-tuning approach",
        "relation": "evaluated_on",
        "tail": "CATARACTS"
      },
      {
        "head": "staged adaptive fine-tuning approach",
        "relation": "uses_metric",
        "tail": "mean average precision (mAP)"
      },
      {
        "head": "Rich feature hierarchies for accurate object detection and semantic segmentation",
        "relation": "proposed_model",
        "tail": "R-CNN"
      },
      {
        "head": "Rich feature hierarchies for accurate object detection and semantic segmentation",
        "relation": "baseline_model",
        "tail": "OverFeat"
      },
      {
        "head": "R-CNN",
        "relation": "evaluated_on",
        "tail": "PASCAL VOC dataset"
      },
      {
        "head": "R-CNN",
        "relation": "evaluated_on",
        "tail": "VOC 2012"
      },
      {
        "head": "R-CNN",
        "relation": "evaluated_on",
        "tail": "ILSVRC2013 detection dataset"
      },
      {
        "head": "R-CNN",
        "relation": "uses_metric",
        "tail": "mean average precision (mAP)"
      },
      {
        "head": "Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism",
        "relation": "proposed_model",
        "tail": "Multi-modal Chain and Global Attention Network (MCGA-Net)"
      },
      {
        "head": "Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism",
        "relation": "proposed_model",
        "tail": "DCGAN-based data augmentation strategy"
      },
      {
        "head": "Multi-modal Chain and Global Attention Network (MCGA-Net)",
        "relation": "evaluated_on",
        "tail": "MS COCO"
      },
      {
        "head": "Multi-modal Chain and Global Attention Network (MCGA-Net)",
        "relation": "uses_metric",
        "tail": "Precision"
      },
      {
        "head": "Multi-modal Chain and Global Attention Network (MCGA-Net)",
        "relation": "uses_metric",
        "tail": "Recall"
      },
      {
        "head": "Multi-modal Chain and Global Attention Network (MCGA-Net)",
        "relation": "uses_metric",
        "tail": "mAP@50"
      },
      {
        "head": "Evolutionary Optimization of Model Merging Recipes",
        "relation": "proposed_model",
        "tail": "evolutionary approach"
      },
      {
        "head": "evolutionary approach",
        "relation": "proposed_model",
        "tail": "Japanese LLM with Math reasoning capabilities"
      },
      {
        "head": "evolutionary approach",
        "relation": "proposed_model",
        "tail": "culturally-aware Japanese VLM"
      },
      {
        "head": "Japanese LLM with Math reasoning capabilities",
        "relation": "evaluated_on",
        "tail": "Japanese LLM benchmarks"
      },
      {
        "head": "culturally-aware Japanese VLM",
        "relation": "evaluated_on",
        "tail": "Japanese LLM benchmarks"
      },
      {
        "head": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
        "relation": "proposed_model",
        "tail": "RNN Encoder-Decoder"
      },
      {
        "head": "RNN Encoder-Decoder",
        "relation": "baseline_model",
        "tail": "statistical machine translation system"
      },
      {
        "head": "RNN Encoder-Decoder",
        "relation": "baseline_model",
        "tail": "log-linear model"
      },
      {
        "head": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving",
        "relation": "proposed_model",
        "tail": "DriveMLM"
      },
      {
        "head": "DriveMLM",
        "relation": "baseline_model",
        "tail": "Autopilot"
      },
      {
        "head": "DriveMLM",
        "relation": "baseline_model",
        "tail": "Apollo"
      },
      {
        "head": "DriveMLM",
        "relation": "evaluated_on",
        "tail": "CARLA Town05 Long"
      },
      {
        "head": "DriveMLM",
        "relation": "uses_metric",
        "tail": "improvements of 3.2 and 4.7 points"
      },
      {
        "head": "DriveMLM",
        "relation": "evaluated_on",
        "tail": "dataset that includes decision state and corresponding explanation annotation"
      },
      {
        "head": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models",
        "relation": "proposed_model",
        "tail": "VLMEvalKit"
      },
      {
        "head": "VLMEvalKit",
        "relation": "uses_metric",
        "tail": "OpenVLM Leaderboard"
      },
      {
        "head": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
        "relation": "proposed_model",
        "tail": "chain of thought prompting"
      },
      {
        "head": "chain of thought prompting",
        "relation": "baseline_model",
        "tail": "GPT-3"
      },
      {
        "head": "chain of thought prompting",
        "relation": "evaluated_on",
        "tail": "GSM8K"
      },
      {
        "head": "chain of thought prompting",
        "relation": "uses_metric",
        "tail": "accuracy"
      },
      {
        "head": "Federated Learning Architectures: A Performance Evaluation with Crop Yield Prediction Application",
        "relation": "proposed_model",
        "tail": "centralized federated learning framework"
      },
      {
        "head": "Federated Learning Architectures: A Performance Evaluation with Crop Yield Prediction Application",
        "relation": "proposed_model",
        "tail": "decentralized federated learning framework"
      },
      {
        "head": "centralized federated learning framework",
        "relation": "baseline_model",
        "tail": "cloud-only framework"
      },
      {
        "head": "centralized federated learning framework",
        "relation": "evaluated_on",
        "tail": "crop yield prediction dataset"
      },
      {
        "head": "decentralized federated learning framework",
        "relation": "evaluated_on",
        "tail": "crop yield prediction dataset"
      },
      {
        "head": "centralized federated learning framework",
        "relation": "uses_metric",
        "tail": "prediction accuracy"
      },
      {
        "head": "centralized federated learning framework",
        "relation": "uses_metric",
        "tail": "precision"
      },
      {
        "head": "centralized federated learning framework",
        "relation": "uses_metric",
        "tail": "recall"
      },
      {
        "head": "centralized federated learning framework",
        "relation": "uses_metric",
        "tail": "F1-Score"
      },
      {
        "head": "centralized federated learning framework",
        "relation": "uses_metric",
        "tail": "training time"
      },
      {
        "head": "centralized federated learning framework",
        "relation": "uses_metric",
        "tail": "response time"
      },
      {
        "head": "decentralized federated learning framework",
        "relation": "uses_metric",
        "tail": "prediction accuracy"
      },
      {
        "head": "decentralized federated learning framework",
        "relation": "uses_metric",
        "tail": "precision"
      },
      {
        "head": "decentralized federated learning framework",
        "relation": "uses_metric",
        "tail": "recall"
      },
      {
        "head": "decentralized federated learning framework",
        "relation": "uses_metric",
        "tail": "F1-Score"
      },
      {
        "head": "decentralized federated learning framework",
        "relation": "uses_metric",
        "tail": "training time"
      },
      {
        "head": "Enhancing Descriptive Image Quality Assessment with A Large-scale Multi-modal Dataset",
        "relation": "proposed_model",
        "tail": "DepictQA-Wild"
      },
      {
        "head": "DepictQA-Wild",
        "relation": "baseline_model",
        "tail": "traditional score-based methods"
      },
      {
        "head": "DepictQA-Wild",
        "relation": "baseline_model",
        "tail": "prior VLM-based IQA models"
      },
      {
        "head": "DepictQA-Wild",
        "relation": "baseline_model",
        "tail": "GPT-4V"
      },
      {
        "head": "DepictQA-Wild",
        "relation": "evaluated_on",
        "tail": "DQ-495K"
      },
      {
        "head": "DepictQA-Wild",
        "relation": "uses_metric",
        "tail": "distortion identification"
      },
      {
        "head": "DepictQA-Wild",
        "relation": "uses_metric",
        "tail": "instant rating"
      },
      {
        "head": "DepictQA-Wild",
        "relation": "uses_metric",
        "tail": "reasoning tasks"
      },
      {
        "head": "Deep contextualized word representations",
        "relation": "proposed_model",
        "tail": "deep bidirectional language model (biLM)"
      },
      {
        "head": "deep bidirectional language model (biLM)",
        "relation": "evaluated_on",
        "tail": "large text corpus"
      },
      {
        "head": "Deep contextualized word representations",
        "relation": "uses_metric",
        "tail": "question answering"
      },
      {
        "head": "Deep contextualized word representations",
        "relation": "uses_metric",
        "tail": "textual entailment"
      },
      {
        "head": "Deep contextualized word representations",
        "relation": "uses_metric",
        "tail": "sentiment analysis"
      },
      {
        "head": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
        "relation": "proposed_model",
        "tail": "DeepSeek-R1"
      },
      {
        "head": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
        "relation": "baseline_model",
        "tail": "conventional supervised learning on human demonstrations"
      },
      {
        "head": "DeepSeek-R1",
        "relation": "evaluated_on",
        "tail": "mathematics"
      },
      {
        "head": "DeepSeek-R1",
        "relation": "evaluated_on",
        "tail": "coding competitions"
      },
      {
        "head": "DeepSeek-R1",
        "relation": "evaluated_on",
        "tail": "STEM fields"
      },
      {
        "head": "Fully Convolutional Networks for Semantic Segmentation",
        "relation": "proposed_model",
        "tail": "Fully Convolutional Networks"
      },
      {
        "head": "Fully Convolutional Networks",
        "relation": "baseline_model",
        "tail": "AlexNet"
      },
      {
        "head": "Fully Convolutional Networks",
        "relation": "baseline_model",
        "tail": "VGG net"
      },
      {
        "head": "Fully Convolutional Networks",
        "relation": "baseline_model",
        "tail": "GoogLeNet"
      },
      {
        "head": "Fully Convolutional Networks",
        "relation": "evaluated_on",
        "tail": "PASCAL VOC"
      },
      {
        "head": "Fully Convolutional Networks",
        "relation": "evaluated_on",
        "tail": "NYUDv2"
      },
      {
        "head": "Fully Convolutional Networks",
        "relation": "evaluated_on",
        "tail": "SIFT Flow"
      },
      {
        "head": "Fully Convolutional Networks",
        "relation": "uses_metric",
        "tail": "mean IU"
      },
      {
        "head": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with Transformers",
        "relation": "proposed_model",
        "tail": "CMX"
      },
      {
        "head": "CMX",
        "relation": "baseline_model",
        "tail": "Cross-Modal Feature Rectification Module (CM-FRM)"
      },
      {
        "head": "CMX",
        "relation": "baseline_model",
        "tail": "Feature Fusion Module (FFM)"
      },
      {
        "head": "CMX",
        "relation": "evaluated_on",
        "tail": "RGB-Depth benchmarks"
      },
      {
        "head": "CMX",
        "relation": "evaluated_on",
        "tail": "RGB-Thermal datasets"
      },
      {
        "head": "CMX",
        "relation": "evaluated_on",
        "tail": "RGB-Polarization datasets"
      },
      {
        "head": "CMX",
        "relation": "evaluated_on",
        "tail": "RGB-LiDAR datasets"
      },
      {
        "head": "CMX",
        "relation": "evaluated_on",
        "tail": "EventScape dataset"
      },
      {
        "head": "CMX",
        "relation": "uses_metric",
        "tail": "state-of-the-art performances"
      },
      {
        "head": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion",
        "relation": "proposed_model",
        "tail": "Diffusion Policy"
      },
      {
        "head": "Diffusion Policy",
        "relation": "evaluated_on",
        "tail": "robot manipulation benchmarks"
      },
      {
        "head": "Diffusion Policy",
        "relation": "uses_metric",
        "tail": "average improvement of 46.9%"
      },
      {
        "head": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI",
        "relation": "proposed_model",
        "tail": "Multi-modal Large Models (MLMs)"
      },
      {
        "head": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI",
        "relation": "proposed_model",
        "tail": "World Models (WMs)"
      },
      {
        "head": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI",
        "relation": "baseline_model",
        "tail": "embodied robots"
      },
      {
        "head": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI",
        "relation": "baseline_model",
        "tail": "simulators"
      },
      {
        "head": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI",
        "relation": "evaluated_on",
        "tail": "embodied perception"
      },
      {
        "head": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI",
        "relation": "evaluated_on",
        "tail": "embodied interaction"
      },
      {
        "head": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI",
        "relation": "evaluated_on",
        "tail": "embodied agent"
      },
      {
        "head": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI",
        "relation": "evaluated_on",
        "tail": "sim-to-real adaptation"
      },
      {
        "head": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control",
        "relation": "proposed_model",
        "tail": "GEN3C"
      },
      {
        "head": "GEN3C",
        "relation": "evaluated_on",
        "tail": "sparse-view novel view synthesis"
      },
      {
        "head": "GEN3C",
        "relation": "evaluated_on",
        "tail": "driving scenes"
      },
      {
        "head": "GEN3C",
        "relation": "evaluated_on",
        "tail": "monocular dynamic video"
      },
      {
        "head": "GEN3C",
        "relation": "uses_metric",
        "tail": "camera control"
      },
      {
        "head": "GEN3C",
        "relation": "uses_metric",
        "tail": "temporal 3D consistency"
      },
      {
        "head": "FLARE: Feed-forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views",
        "relation": "proposed_model",
        "tail": "FLARE"
      },
      {
        "head": "FLARE",
        "relation": "evaluated_on",
        "tail": "large-scale public datasets"
      },
      {
        "head": "FLARE",
        "relation": "uses_metric",
        "tail": "pose estimation"
      },
      {
        "head": "FLARE",
        "relation": "uses_metric",
        "tail": "geometry reconstruction"
      },
      {
        "head": "FLARE",
        "relation": "uses_metric",
        "tail": "novel view synthesis"
      }
    ]
  }
}