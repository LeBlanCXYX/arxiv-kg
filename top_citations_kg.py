"""
åŸºäº Top å¼•ç”¨çš„çŸ¥è¯†å›¾è°±ç”Ÿæˆå™¨ (Top Citations KG)
åŠŸèƒ½ï¼šè¾“å…¥ä¸€ç¯‡è®ºæ–‡çš„ arxiv id å’Œæ•°å­— Nï¼Œè‡ªåŠ¨æŸ¥æ‰¾ï¼š
  - è¯¥è®ºæ–‡å¼•ç”¨çš„è®ºæ–‡ä¸­å¼•ç”¨é‡æœ€é å‰çš„ N ç¯‡ï¼ˆreferencesï¼‰
  - å¼•ç”¨è¯¥è®ºæ–‡çš„è®ºæ–‡ä¸­å¼•ç”¨é‡æœ€é å‰çš„ N ç¯‡ï¼ˆcitationsï¼‰
ä¸é€’å½’æŸ¥æ‰¾è¿™äº›ç›¸å…³è®ºæ–‡çš„å¼•ç”¨/è¢«å¼•ç”¨ã€‚ç›¸å…³è®ºæ–‡å¿…é¡»æœ‰æ‘˜è¦ã€ä½œè€…ç­‰å…ƒæ•°æ®ã€‚
è¾“å‡ºï¼šJSON çŸ¥è¯†å›¾è°± + å¯è§†åŒ– HTMLï¼ˆç±»ä¼¼ visualizeï¼‰

è¿è¡Œç¯å¢ƒï¼šéœ€å…ˆ conda æ¿€æ´»åä¸º kg çš„ç¯å¢ƒ
  conda activate kg
  python top_citations_kg.py <arxiv_id> -n <N>
ç¤ºä¾‹ï¼š
  python top_citations_kg.py 1706.03762 -n 5
  python top_citations_kg.py 1706.03762 -n 3 --llm
è‹¥åœ¨ Windows ä¸‹ç”¨ conda run å‡ºç°ç¼–ç é”™è¯¯ï¼Œè¯·ç›´æ¥åœ¨å·²æ¿€æ´» kg çš„ç»ˆç«¯ä¸­è¿è¡Œä¸Šè¿°å‘½ä»¤ã€‚
"""

import arxiv
import json
import os
import requests
import time
import sys
import argparse
from openai import OpenAI

from config import API_KEY, BASE_URL, MODEL_NAME, is_api_configured
from class_schema import (
    get_all_type_names,
    normalize_entity_type,
    get_types_for_llm_prompt,
    get_categories_for_entities,
)
ALLOWED_TYPES = get_all_type_names()


def fetch_arxiv_paper(paper_id):
    """
    è·å– ArXiv è®ºæ–‡çš„å…ƒæ•°æ®ï¼ˆæ‘˜è¦ã€ä½œè€…ã€æ—¥æœŸç­‰ï¼‰
    """
    print(f"[*] [ArXiv] è·å–è®ºæ–‡å…ƒæ•°æ®: {paper_id} ...")
    client = arxiv.Client()
    search = arxiv.Search(id_list=[paper_id])
    try:
        paper = next(client.results(search))
        return {
            "id": paper_id,
            "title": paper.title,
            "abstract": paper.summary,
            "published_date": paper.published.strftime("%Y-%m-%d"),
            "pdf_url": paper.pdf_url,
            "authors": [a.name for a in paper.authors],
        }
    except (StopIteration, Exception) as e:
        print(f"âŒ ArXiv è·å–å¤±è´¥ {paper_id}: {e}")
        return None


def _request_s2_with_retry(url, max_retries=4, base_delay=5):
    """è¯·æ±‚ S2 APIï¼Œé‡ 429 æ—¶æŒ‡æ•°é€€é¿é‡è¯•ã€‚æ—  Key æ—¶ S2 çº¦ 100 æ¬¡/5 åˆ†é’Ÿï¼Œæ•…è¯·æ±‚å‰ç•™é—´éš”ã€‚"""
    for attempt in range(max_retries + 1):
        time.sleep(3 if attempt == 0 else 0)  # æ¯æ¬¡è°ƒç”¨å‰é—´éš”ï¼Œé™ä½è§¦å‘ 429 æ¦‚ç‡
        try:
            r = requests.get(url, timeout=20)
            if r.status_code == 200:
                return r.json()
            if r.status_code == 429:
                delay = base_delay * (2 ** attempt)
                print(f"   âš ï¸ é€Ÿç‡é™åˆ¶ (429)ï¼Œ{delay} ç§’åé‡è¯• ({attempt + 1}/{max_retries + 1})...")
                time.sleep(delay)
                continue
            if r.status_code == 404:
                return None
            if attempt < max_retries and r.status_code in (503, 502):
                delay = base_delay * (2 ** attempt)
                print(f"   âš ï¸ æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ ({r.status_code})ï¼Œ{delay} ç§’åé‡è¯•...")
                time.sleep(delay)
                continue
            return None
        except Exception as e:
            if attempt < max_retries:
                delay = base_delay * (2 ** attempt)
                print(f"   âš ï¸ è¯·æ±‚å¼‚å¸¸: {e}ï¼Œ{delay} ç§’åé‡è¯•...")
                time.sleep(delay)
            else:
                raise
    return None


def fetch_paper_from_semantic_scholar(paper_id_s2):
    """
    é€šè¿‡ Semantic Scholar paperId è·å–è®ºæ–‡çš„ title, abstract, authorsï¼ˆç”¨äºæ—  ArXiv ID çš„è®ºæ–‡ï¼‰
    """
    url = f"https://api.semanticscholar.org/graph/v1/paper/{paper_id_s2}?fields=title,abstract,authors,year"
    try:
        data = _request_s2_with_retry(url)
        if not data:
            return None
        authors = [a.get("name") or "" for a in data.get("authors", [])]
        return {
            "title": data.get("title") or "",
            "abstract": data.get("abstract") or "",
            "authors": authors,
            "published_date": str(data.get("year") or ""),
            "pdf_url": "",
        }
    except Exception as e:
        print(f"   âš ï¸ S2 è·å–å¤±è´¥ {paper_id_s2}: {e}")
        return None


def fetch_related_papers_via_semantic_scholar(arxiv_id, top_n=5):
    """
    è·å–è¯¥è®ºæ–‡çš„ references å’Œ citationsï¼Œå¹¶æŒ‰å¼•ç”¨é‡æ’åºå„å–å‰ top_n ç¯‡ã€‚
    é‡ 429 æ—¶æŒ‡æ•°é€€é¿é‡è¯•ï¼Œé¿å…å› é€Ÿç‡é™åˆ¶å¯¼è‡´æ¼çˆ¬ã€‚
    """
    print(f"[*] [S2] è·å–å¼•ç”¨å…³ç³» (top {top_n}): {arxiv_id} ...")
    url = (
        "https://api.semanticscholar.org/graph/v1/paper/ARXIV:"
        + arxiv_id
        + "?fields=title,year,citationCount,"
        "references.title,references.externalIds,references.citationCount,references.year,references.paperId,"
        "citations.title,citations.externalIds,citations.citationCount,citations.year,citations.paperId"
    )
    try:
        data = _request_s2_with_retry(url, max_retries=4, base_delay=5)
        if not data:
            print("   âš ï¸ æœªè·å–åˆ°æ•°æ®ï¼ˆæœªæ”¶å½•æˆ–å·²è¾¾é‡è¯•ä¸Šé™ï¼‰")
            return {"references": [], "citations": []}
        references = []
        if data.get("references"):
            for item in data["references"]:
                if not item.get("title"):
                    continue
                arxiv_id_ref = None
                if item.get("externalIds") and item["externalIds"].get("ArXiv"):
                    arxiv_id_ref = item["externalIds"]["ArXiv"]
                references.append({
                    "title": item["title"],
                    "arxiv_id": arxiv_id_ref,
                    "citation_count": item.get("citationCount") or 0,
                    "year": item.get("year") or 0,
                    "paper_id_s2": item.get("paperId"),
                })
        references = sorted(
            references, key=lambda x: (x["citation_count"] or 0), reverse=True
        )[:top_n]

        citations = []
        if data.get("citations"):
            for item in data["citations"]:
                if not item.get("title"):
                    continue
                arxiv_id_cite = None
                if item.get("externalIds") and item["externalIds"].get("ArXiv"):
                    arxiv_id_cite = item["externalIds"]["ArXiv"]
                citations.append({
                    "title": item["title"],
                    "arxiv_id": arxiv_id_cite,
                    "citation_count": item.get("citationCount") or 0,
                    "year": item.get("year") or 0,
                    "paper_id_s2": item.get("paperId"),
                })
        citations = sorted(
            citations, key=lambda x: (x["citation_count"] or 0), reverse=True
        )[:top_n]

        print(f"   --> å‚è€ƒæ–‡çŒ® top{top_n}: {len(references)} ç¯‡, è¢«å¼•æ–‡çŒ® top{top_n}: {len(citations)} ç¯‡")
        return {"references": references, "citations": citations}
    except Exception as e:
        print(f"âŒ ç½‘ç»œé”™è¯¯: {e}")
        return {"references": [], "citations": []}


def ensure_paper_metadata(paper_item):
    """
    ç¡®ä¿è®ºæ–‡æœ‰æ‘˜è¦ã€ä½œè€…ï¼šæœ‰ arxiv_id åˆ™ä» ArXiv æ‹‰å–ï¼Œå¦åˆ™ä» S2 ç”¨ paperId æ‹‰å–ã€‚
    ä¸é€’å½’æŸ¥æ‰¾è¯¥è®ºæ–‡çš„å¼•ç”¨/è¢«å¼•ç”¨ã€‚
    """
    if paper_item.get("abstract") and paper_item.get("authors"):
        return paper_item
    if paper_item.get("arxiv_id"):
        meta = fetch_arxiv_paper(paper_item["arxiv_id"])
        if meta:
            paper_item["abstract"] = meta.get("abstract", "")
            paper_item["authors"] = meta.get("authors", [])
            paper_item["published_date"] = meta.get("published_date", "")
            paper_item["pdf_url"] = meta.get("pdf_url", "")
            return paper_item
    if paper_item.get("paper_id_s2"):
        meta = fetch_paper_from_semantic_scholar(paper_item["paper_id_s2"])
        if meta:
            paper_item["abstract"] = meta.get("abstract", "")
            paper_item["authors"] = meta.get("authors", [])
            paper_item["published_date"] = meta.get("published_date", "") or str(
                paper_item.get("year", "")
            )
            paper_item["pdf_url"] = meta.get("pdf_url", "")
            return paper_item
    paper_item.setdefault("abstract", "")
    paper_item.setdefault("authors", [])
    return paper_item


def batch_ensure_metadata(paper_list):
    """æ‰¹é‡è¡¥å…¨æ‘˜è¦ã€ä½œè€…ç­‰ï¼Œä¸é€’å½’æŸ¥å¼•ç”¨ã€‚"""
    print(f"\n[*] è¡¥å…¨ {len(paper_list)} ç¯‡è®ºæ–‡çš„æ‘˜è¦ä¸ä½œè€…...")
    for idx, paper in enumerate(paper_list):
        ensure_paper_metadata(paper)
        time.sleep(0.5)
    return paper_list


def extract_knowledge_with_llm(paper_info):
    """å¯é€‰ï¼šLLM æ·±åº¦æŠ½å–ã€‚æœªé…ç½® API Key åˆ™è·³è¿‡ã€‚å®ä½“ç±»å‹å¿…é¡»ä¸º classes.json ä¸­çš„ç±»å‹ã€‚"""
    if not is_api_configured():
        return {"entities": [], "triples": []}
    print(f"[*] [LLM] æ·±åº¦æŠ½å–: {paper_info['title'][:30]}...")
    client = OpenAI(api_key=API_KEY, base_url=BASE_URL)
    allowed_types = get_types_for_llm_prompt()
    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†å›¾è°±ä¸“å®¶ã€‚ä»è®ºæ–‡æ‘˜è¦ä¸­æå–å®ä½“å’Œå…³ç³»ã€‚
å®ä½“ç±»å‹å¿…é¡»ä¸”ä»…èƒ½ä»ä»¥ä¸‹ç±»å‹ä¸­é€‰æ‹©ï¼ˆæ¥è‡ª classes.json è§„èŒƒï¼‰: {allowed_types}
å…³ç³»ç±»å‹: proposed_model, baseline_model, evaluated_on, uses_metric, cites, author_of
è¦æ±‚ï¼štriples å¿…é¡»ä½¿ç”¨ "head" å’Œ "tail" å­—æ®µï¼ˆä¸è¦ç”¨ subject/objectï¼‰ï¼›head å’Œ tail çš„å€¼å¿…é¡»æ˜¯å®ä½“åç§°ï¼ˆå¦‚è®ºæ–‡æ ‡é¢˜ã€æ¨¡å‹åã€æ•°æ®é›†åï¼‰ï¼Œä¸è¦ç”¨ E1ã€E2 ç­‰ IDã€‚
ä¸¥æ ¼è¾“å‡º JSON: {{"entities": [{{"name": "...", "type": "..."}}], "triples": [{{"head": "...", "relation": "...", "tail": "..."}}]}}"""
    try:
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Title: {paper_info['title']}\nAbstract: {paper_info.get('abstract', '')}"},
            ],
            temperature=0.1,
            response_format={"type": "json_object"},
        )
        return json.loads(response.choices[0].message.content)
    except Exception as e:
        print(f"âŒ LLM é”™è¯¯: {e}")
        return {"entities": [], "triples": []}


def build_top_citations_kg(arxiv_id, top_n=5, run_llm=False):
    """
    ä¸»æµç¨‹ï¼šæ ¹æ® arxiv_id å’Œ top_n æ„å»ºçŸ¥è¯†å›¾è°±ï¼ˆä¸é€’å½’ï¼‰ï¼Œè¾“å‡º JSON ä¸ HTMLã€‚
    """
    print("\n" + "=" * 60)
    print("ğŸš€ Top å¼•ç”¨çŸ¥è¯†å›¾è°± (Top Citations KG)")
    print("=" * 60)
    print(f"   ArXiv ID: {arxiv_id}, Top N: {top_n}")

    seed = fetch_arxiv_paper(arxiv_id)
    if not seed:
        return False

    relation = fetch_related_papers_via_semantic_scholar(arxiv_id, top_n=top_n)
    refs = relation["references"]
    cites = relation["citations"]
    related = refs + cites
    batch_ensure_metadata(related)

    all_papers = [seed] + related
    seen_names = set()
    entities = []
    triples = []

    for p in all_papers:
        name = p.get("title") or ""
        if name and name not in seen_names:
            arxiv_id_val = p.get("arxiv_id") or p.get("id", "")
            entities.append({
                "name": name,
                "type": normalize_entity_type("AIPaper", allowed=ALLOWED_TYPES),
                "arxiv_id": arxiv_id_val,
            })
            seen_names.add(name)
    for p in all_papers:
        for a in p.get("authors", []):
            a = (a or "").strip()
            if a and a not in seen_names:
                entities.append({"name": a, "type": normalize_entity_type("Researcher", allowed=ALLOWED_TYPES)})
                seen_names.add(a)

    for p in all_papers:
        for a in p.get("authors", []):
            a = (a or "").strip()
            if a and p.get("title"):
                triples.append({"head": a, "relation": "author_of", "tail": p["title"]})
    for r in refs:
        if r.get("title") and seed.get("title"):
            triples.append({"head": seed["title"], "relation": "cites", "tail": r["title"]})
    for c in cites:
        if c.get("title") and seed.get("title"):
            triples.append({"head": c["title"], "relation": "cites", "tail": seed["title"]})

    if run_llm:
        for p in all_papers:
            llm_data = extract_knowledge_with_llm(p)
            # å®ä½“ ID -> nameï¼Œç”¨äºæŠŠä¸‰å…ƒç»„é‡Œçš„ E1/E2 è§£ææˆè®ºæ–‡åã€æ¨¡å‹åç­‰
            id_to_name = {}
            for e in llm_data.get("entities", []):
                n = e.get("name")
                if n:
                    if n not in seen_names:
                        raw_type = e.get("type", "Thesis")
                        entities.append({"name": n, "type": normalize_entity_type(raw_type, allowed=ALLOWED_TYPES)})
                        seen_names.add(n)
                    eid = e.get("id")
                    if eid:
                        id_to_name[eid] = n
                    id_to_name[n] = n  # åå­—ä¹Ÿæ˜ å°„åˆ°è‡ªå·±ï¼Œæ–¹ä¾¿ triples é‡Œå·²ç”¨ name çš„æƒ…å†µ
            # ç»Ÿä¸€ä¸‰å…ƒç»„æ ¼å¼å¹¶è§£æ IDï¼šå…¼å®¹ head/tail ä¸ subject/objectï¼ŒID è½¬ä¸º name
            for t in llm_data.get("triples", []):
                head = t.get("head") or t.get("subject")
                tail = t.get("tail") or t.get("object")
                if not head or not tail:
                    continue
                head = id_to_name.get(head, head)
                tail = id_to_name.get(tail, tail)
                triples.append({"head": head, "relation": t.get("relation", ""), "tail": tail})

    # ç›¸å…³è®ºæ–‡ä¿ç•™å®Œæ•´å…ƒæ•°æ®ï¼ˆæ‘˜è¦ã€ä½œè€…ç­‰ï¼‰ï¼Œä¸é€’å½’æŸ¥å…¶å¼•ç”¨/è¢«å¼•
    def _paper_meta(p):
        return {
            "title": p.get("title", ""),
            "arxiv_id": p.get("arxiv_id", ""),
            "abstract": p.get("abstract", ""),
            "authors": p.get("authors", []),
            "published_date": p.get("published_date", ""),
            "pdf_url": p.get("pdf_url", ""),
            "citation_count": p.get("citation_count"),
            "year": p.get("year"),
        }
    related_with_meta = [_paper_meta(p) for p in related]
    output_data = {
        "paper_metadata": seed,
        "related_papers_count": {"references": len(refs), "citations": len(cites)},
        "related_papers": related_with_meta,
        "top_n": top_n,
        "knowledge_graph": {"entities": entities, "triples": triples},
    }

    base_name = f"top_citations_kg_{arxiv_id}"
    json_path = f"{base_name}.json"
    html_path = f"{base_name}.html"
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    print(f"\nâœ… JSON å·²ä¿å­˜: {json_path}")

    generate_html(json_path, html_path, output_data)
    return True


def generate_html(json_file, output_html_file, data=None):
    """
    æ ¹æ® JSON ç”Ÿæˆç±»ä¼¼ visualize çš„ ECharts åŠ›å¯¼å‘å›¾ HTMLã€‚
    """
    if data is None:
        if not os.path.exists(json_file):
            print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {json_file}")
            return
        with open(json_file, "r", encoding="utf-8") as f:
            data = json.load(f)
    paper_meta = data.get("paper_metadata", {})
    kg = data.get("knowledge_graph", {})
    entities = kg.get("entities", [])
    triples = kg.get("triples", [])

    # åŸºäº classes.json æ‰©å±•ï¼šä»…ä½¿ç”¨ schema ä¸­å­˜åœ¨çš„ç±»å‹ä½œä¸º categories
    raw_types = [e.get("type", "Thesis") for e in entities]
    type_list = get_categories_for_entities(raw_types)
    category_map = {t: i for i, t in enumerate(type_list)}
    categories = [{"name": t} for t in type_list]

    # æŒ‰ name å»é‡ï¼šåŒä¸€äººï¼ˆåŒåï¼‰åªä¿ç•™ä¸€ä¸ªèŠ‚ç‚¹ï¼Œé¿å…å¤šç¯‡è®ºæ–‡ä½œè€…å‡ºç°é‡å¤èŠ‚ç‚¹
    name_to_entity = {}
    for e in entities:
        n = (e.get("name") or "").strip()
        if n and n not in name_to_entity:
            name_to_entity[n] = e
    echarts_nodes = []
    for n, e in name_to_entity.items():
        norm_type = normalize_entity_type(e.get("type", "Thesis"), allowed=ALLOWED_TYPES)
        sz = 50 if norm_type in ("Thesis", "Article", "CreativeWork") else 25
        echarts_nodes.append({
            "name": n,
            "category": category_map.get(norm_type, 0),
            "symbolSize": sz,
            "draggable": True,
            "value": norm_type,
        })
    seen = set(name_to_entity.keys())

    # å…¼å®¹ head/tail ä¸ subject/objectï¼›åªä¿ç•™ä¸¤ç«¯éƒ½åœ¨èŠ‚ç‚¹é›†åˆä¸­çš„è¾¹ï¼›head/tail åš strip ä¸èŠ‚ç‚¹åä¸€è‡´
    node_names = seen
    echarts_links = []
    for t in triples:
        head = (t.get("head") or t.get("subject") or "").strip()
        tail = (t.get("tail") or t.get("object") or "").strip()
        if head and tail and head in node_names and tail in node_names:
            echarts_links.append({
                "source": head,
                "target": tail,
                "value": t.get("relation", ""),
            })

    # ä¼˜å…ˆä½¿ç”¨åŒç›®å½•ä¸‹çš„ echarts.min.jsï¼ˆé¿å… CDN è¶…æ—¶/è¢«æ‹¦æˆªï¼‰ï¼Œå¦åˆ™ç”¨ unpkg
    out_dir = os.path.dirname(os.path.abspath(output_html_file))
    echarts_local = os.path.join(out_dir, "echarts.min.js")
    script_src = "echarts.min.js" if os.path.exists(echarts_local) else "https://unpkg.com/echarts@5.4.3/dist/echarts.min.js"

    html_content = f"""<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Top Citations KG - {paper_meta.get('title', '')[:50]}</title>
    <script src="{script_src}"></script>
    <style>
        * {{ margin: 0; padding: 0; box-sizing: border-box; }}
        html, body {{ height: 100%; }}
        body {{ background: #f5f5f5; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; }}
        #main {{ width: 100%; height: 100%; min-height: 400px; }}
        .panel {{
            position: absolute; background: white; border-radius: 8px;
            box-shadow: 0 2px 12px rgba(0,0,0,0.1); padding: 20px; font-size: 14px; z-index: 999; max-width: 420px;
        }}
        .header {{ top: 20px; left: 20px; }}
        .stats {{ top: 20px; right: 20px; }}
        .header h2 {{ margin-bottom: 10px; color: #333; }}
        .header p {{ color: #666; margin: 5px 0; line-height: 1.5; }}
        .stat-item {{ margin: 8px 0; }}
        .stat-label {{ font-weight: bold; color: #333; }}
        .stat-value {{ color: #0066cc; }}
    </style>
</head>
<body>
    <div id="main"></div>
    <div class="panel header">
        <h2>ğŸ“„ {paper_meta.get('title', 'Paper')}</h2>
        <p><strong>ä½œè€…:</strong> {', '.join(paper_meta.get('authors', [])[:4])}</p>
        <p><strong>å‘è¡¨æ—¥æœŸ:</strong> {paper_meta.get('published_date', '')}</p>
        <p><strong>ArXiv ID:</strong> <code>{paper_meta.get('id', '')}</code></p>
    </div>
    <div class="panel stats">
        <div class="stat-item"><span class="stat-label">è®ºæ–‡èŠ‚ç‚¹:</span> <span class="stat-value">{sum(1 for e in name_to_entity.values() if e.get('type') == 'AIPaper')}</span></div>
        <div class="stat-item"><span class="stat-label">ç ”ç©¶è€…èŠ‚ç‚¹:</span> <span class="stat-value">{sum(1 for e in name_to_entity.values() if e.get('type') == 'Researcher')}</span></div>
        <div class="stat-item"><span class="stat-label">å…³ç³»æ•°:</span> <span class="stat-value">{len(triples)}</span></div>
        <div class="stat-item"><span class="stat-label">å¼•ç”¨çš„è®ºæ–‡ (top N):</span> <span class="stat-value">{data.get('related_papers_count', {}).get('references', 0)}</span></div>
        <div class="stat-item"><span class="stat-label">è¢«å¼•ç”¨çš„è®ºæ–‡ (top N):</span> <span class="stat-value">{data.get('related_papers_count', {}).get('citations', 0)}</span></div>
    </div>
    <script type="text/javascript">
        function initChart() {{
            if (typeof echarts === 'undefined') {{
                document.getElementById('main').innerHTML = '<p style="padding:20px">æ— æ³•åŠ è½½ EChartsï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ– CDNã€‚</p>';
                return;
            }}
            var chartDom = document.getElementById('main');
            var myChart = echarts.init(chartDom);
            var option = {{
                tooltip: {{ formatter: function(params) {{
                    if (params.dataType === 'node') return params.name + ' (' + (params.value || '') + ')';
                    return (params.source && params.source.name) + ' ' + (params.value || '') + ' ' + (params.target && params.target.name);
                }}}},
                legend: {{ data: {json.dumps([c['name'] for c in categories])} }},
                series: [{{
                    type: 'graph', layout: 'force',
                    data: {json.dumps(echarts_nodes)},
                    links: {json.dumps(echarts_links)},
                    categories: {json.dumps(categories)},
                    roam: true,
                    label: {{ show: true, position: 'right', formatter: '{{b}}' }},
                    edgeLabel: {{ fontSize: 11, formatter: '{{c}}' }},
                    edgeSymbol: ['none', 'arrow'], edgeSymbolSize: 10,
                    lineStyle: {{ color: 'source', curveness: 0.3 }},
                    force: {{ repulsion: 1500, edgeLength: 250 }},
                    emphasis: {{ focus: 'adjacency', lineStyle: {{ width: 4 }} }}
                }}]
            }};
            myChart.setOption(option);
            setTimeout(function() {{ myChart.resize(); }}, 100);
            window.addEventListener('resize', function() {{ myChart.resize(); }});
        }}
        if (document.readyState === 'loading') {{
            document.addEventListener('DOMContentLoaded', initChart);
        }} else {{
            initChart();
        }}
    </script>
</body>
</html>"""
    with open(output_html_file, "w", encoding="utf-8") as f:
        f.write(html_content)
    print(f"âœ… HTML å·²ç”Ÿæˆ: {os.path.abspath(output_html_file)}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="è¾“å…¥è®ºæ–‡ ArXiv ID å’Œæ•°å­— Nï¼Œç”Ÿæˆè¯¥è®ºæ–‡å¼•ç”¨/è¢«å¼•ä¸­ top N çš„çŸ¥è¯†å›¾è°± JSON ä¸ HTML"
    )
    parser.add_argument("arxiv_id", nargs="?", default="1706.03762", help="è®ºæ–‡ ArXiv IDï¼Œä¾‹å¦‚ 1706.03762")
    parser.add_argument("-n", "--top", type=int, default=5, help="å¼•ç”¨/è¢«å¼•å„å–å‰ N ç¯‡ (é»˜è®¤ 5)")
    parser.add_argument("--llm", action="store_true", help="æ˜¯å¦è¿›è¡Œ LLM æ·±åº¦æŠ½å–ï¼ˆéœ€é…ç½® API_KEYï¼‰")
    args = parser.parse_args()

    build_top_citations_kg(args.arxiv_id, top_n=args.top, run_llm=args.llm)
